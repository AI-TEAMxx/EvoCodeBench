{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n\n            # Connect to the database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create the table if it doesn't exist\n            c.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name text, args text, kwargs text, result text)\"\n            )\n\n            # Hash the arguments and keyword arguments\n            arg_hash = hashlib.sha256(\n                json.dumps({\"args\": args, \"kwargs\": kwargs}).encode(\"utf-8\")\n            ).hexdigest()\n\n            # Check if the function has already been called with the same arguments\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=?\",\n                (func_name, arg_hash),\n            )\n            result = c.fetchone()\n\n            # If the function has already been called with the same arguments, retrieve the result from the database\n            if result:\n                conn.close()\n                return json.loads(result[0])\n\n            # Otherwise, compute the result and store it in the database\n            result = func(*args, **kwargs)\n            c.execute(\n                \"INSERT INTO cache VALUES (?, ?, ?, ?)\",\n                (func_name, arg_hash, json.dumps(kwargs), json.dumps(result)),\n            )\n            conn.commit()\n            conn.close()\n\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    # Check if the bounding box is valid\n    if values[\"x_min\"] > values[\"x_max\"] or values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: The bounding box values are invalid. The minimum x and y values must be less than the maximum x and y values, respectively.\")\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the norms of each column in each matrix.\n  norm0 = np.linalg.norm(mat0, axis=0)\n  norm1 = np.linalg.norm(mat1, axis=0)\n\n  # Compute the dot product between each column in each matrix.\n  dot = np.dot(mat0.T, mat1)\n\n  # Compute the squared Euclidean distance between each column in each matrix.\n  sq_dist = norm0 ** 2 - 2 * dot + norm1 ** 2\n\n  # Set negative distances to zero.\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return True\n    if path.startswith(\"gs://\") or path.startswith(\"s3://\") or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if dim not in (1, 2):\n        raise ValueError(f\"Expected dim to be 1 or 2, got {dim}\")\n\n    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.array(list(items.keys()))\n        else:\n            assets_names = np.array(assets_names)\n\n        if assets_names.shape[0] != len(items):\n            raise ValueError(\n                f\"Expected {len(assets_names)} assets names, got {len(items)}\"\n            )\n\n        if fill_value is None:\n            raise ValueError(\n                f\"Expected fill_value to be provided when items is a dictionary, got None\"\n            )\n\n        if dim == 1:\n            if assets_names.shape[0] != n_assets:\n                raise ValueError(\n                    f\"Expected {n_assets} assets names, got {len(items)}\"\n                )\n\n            # Check if all assets are present\n            if not np.all(np.isin(assets_names, list(items.keys()))):\n                raise ValueError(\n                    f\"Expected {n_assets} assets names, got {len(items)}\"\n                )\n\n            # Check if all assets have a value\n            if not np.all(np.array(list(items.values()))):\n                raise ValueError(\n                    f\"Expected {n_assets} non-null values for {name}, got {len(items)}\"\n                )\n\n            # Fill missing values\n            items = np.array(\n                [items[asset_name] if asset_name in items else fill_value for asset_name in assets_names]\n            )\n\n        elif dim == 2:\n            # Check if all assets are present\n            if not np.all(np.isin(assets_names, list(items.keys()))):\n                raise ValueError(\n                    f\"Expected {n_assets} assets names, got {len(items)}\"\n                )\n\n            # Check if all assets have a value\n            if"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data[\"purpose_embedding\"]\n        if isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 12.92 * srgb\n  linear1 = 1.055 * xnp.power(srgb, 1 / 2.4) - 0.055\n  return xnp.where(srgb <= 0.0031308, linear0, linear1)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust spline degree to be at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Interpolate the signal using a spline of the specified degree and smoothness.\n  tck, _ = scipy.interpolate.splprep(x, k=spline_degree, s=smoothness)\n\n  # Query the interpolated signal at the specified output times.\n  return scipy.interpolate.splev(t_output, tck)\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word.capitalize()\n    if word[0].isupper() and word[1].islower():\n        return word\n    if word[0].islower() and word[1].isupper():\n        return word.lower()\n    return word\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.all(np.isin(v, [True, False])):\n        raise ValueError(f\"{cls.__name__}: {field.name} must contain only boolean values.\")\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Compute the norm of the input.\n  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n\n  # Compute the scaled version of the input.\n  x_scaled = x / x_norm\n\n  return x_scaled\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix.\n  det = jnp.linalg.det(cov)\n\n  # Compute the eigenvalues and eigenvectors of the covariance matrix.\n  eigval, eigvec = jnp.linalg.eigh(cov)\n\n  # Compute the isotropic covariance matrix.\n  if mode == 'fast':\n    iso_cov = eigvec * det[Ellipsis, None] * eigvec.T\n  elif mode == 'accurate':\n    iso_cov = eigvec * jnp.exp(jnp.log(det) / 2)[Ellipsis, None] * eigvec.T\n  else:\n    raise ValueError('Invalid mode.')\n\n  return iso_cov\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.getenv(\"CONFIG_"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}.\")\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = string.ascii_lowercase + string.digits + \"_\"\n    # Define the number of characters in the character set\n    n_chars = len(char_set)\n    # Define the string to be returned\n    s = \"\"\n    # Loop over the number of digits in the integer\n    while n > 0:\n        # Define the index of the character to be added to the string\n        index = n % n_chars\n        # Add the character to the string\n        s = char_set[index] + s\n        # Update the integer\n        n = n // n_chars\n    # Return the string\n    return s\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    for worker_idx in range(len(workers_intervals)):\n        for i, interval in enumerate(workers_intervals[worker_idx]):\n            if indexes[worker_idx] >= interval[1]:\n                indexes[worker_idx] = 0\n                continue\n            if indexes[worker_idx] < interval[0]:\n                indexes[worker_idx] = interval[0]\n                continue\n            if indexes[worker_idx] < interval[1]:\n                indexes[worker_idx] = interval[1]\n                continue\n\n    return indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError('Invalid datastructure.')\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check that the tessellation factor is valid.\n  if v < 1:\n    raise ValueError(\"The tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the integer weights for each vertex of the triangle.\n  weights = np.arange(v**2 + 1)\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights)\n\n  return weights\n\n"}
{"namespace": "linspline.query", "completion": "  tq = jnp.asarray(tq)\n  t = jnp.asarray(t)\n  v = jnp.asarray(v)\n\n  check_zero_endpoints(v)\n\n  # Interpolate the spline at the query points.\n  vq = math.interpolate_linear(tq, t, v)\n\n  # Extrapolate the spline at the query points.\n  vq = jnp.where(tq < t[0], jnp.zeros_like(vq), vq)\n  vq = jnp.where(tq > t[-1], jnp.zeros_like(vq), vq)\n\n  return vq\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if any(x <= 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert ray origins to NDC.\n  origins_ndc = xnp.dot(origins, pixtocam[0:3, 0:3].T)\n  origins_ndc = origins_ndc + pixtocam[0:3, 3]\n  origins_ndc = origins_ndc / origins_ndc[:, 2:]\n  origins_ndc = origins_ndc[:, 0:2]\n\n  # Convert ray directions to NDC.\n  directions_ndc = xnp.dot(directions, pixtocam[0:3, 0:3].T)\n  directions_ndc = directions_ndc / directions_ndc[:, 2:]\n  directions_ndc = directions_ndc[:, 0:2]\n\n  # Adjust ray origins to the near plane.\n  origins_ndc = origins_ndc * near\n\n  return origins_ndc, directions_ndc\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Make sure direction vectors are unit.\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the direction vectors.\n  dot_product = jnp.sum(dir1 * dir2, axis=-1)\n\n  # If the dot product is close to 1, the lines are parallel.\n  return jnp.isclose(dot_product, 1.0)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = jieba.lcut(continuation)\n    reference_tokens = jieba.lcut(reference)\n\n    # Calculate the BLEU-4 score\n    bleu4_score = evaluate.bleu4(continuation_tokens, reference_tokens)\n\n    # Optionally include the brevity penalty in the final score calculation\n    if with_penalty:\n        bleu4_score = evaluate.brevity_penalty(continuation_tokens, reference_tokens, bleu4_score)\n\n    return bleu4_score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.where(x <= eps, value_at_zero, x))\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / np.diff(t)\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - torch.floor(val / period + offset) * period\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent, MicroAgent):\n            if isinstance(agent.purpose_embedding, np.ndarray):\n                purpose_embedding = agent.purpose_embedding.tolist()\n            else:\n                purpose_embedding = agent.purpose_embedding\n\n            return {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": purpose_embedding,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        else:\n            raise TypeError(\"Input agent must be of type MicroAgent.\")\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # TODO: Write your own implementation of the greedy packing algorithm.\n\n    # Raise an error if the number of bins is less than or equal to zero.\n    if num_bins <= 0:\n        raise ValueError('The number of bins must be a positive integer.')\n\n    # Raise an error if the number of items is less than or equal to zero.\n    if len(items) <= 0:\n        raise ValueError('The list of items cannot be empty.')\n\n    # Raise an error if the number of items is not equal to the number of weights.\n    if len(items) != len(weights):\n        raise ValueError('The number of items and weights must be the same.')\n\n    # Raise an error if any of the weights is negative.\n    if any(weight < 0 for weight in weights):\n        raise ValueError('All weights must be non-negative.')\n\n    # Raise an error if the total weight of the items is zero.\n    if sum(weights) == 0:\n        raise ValueError('The total weight of the items cannot be zero.')\n\n    # Create a dictionary that maps each item to its weight.\n    item_to_weight: Dict[Any, int] = dict(zip(items, weights))\n\n    # Create a dictionary that maps each bin index to the total weight of the items in that bin.\n    bin_to_weight: Dict[int, int] = defaultdict(int)\n\n    # Create a dictionary that maps each bin index to a list of items that have been placed in that bin.\n    bin_to_items: Dict[int, List[Any]] = defaultdict(list)\n\n    # Sort the items by weight in descending order.\n    sorted_items: List[Any] = sorted(items, key=lambda item: item_to_weight[item], reverse=True)\n\n    # Iterate over the items.\n    for item in sorted_items:\n\n        # Find the bin index with the lowest total weight.\n        min_bin_index: int = min(bin_to_weight, key=bin_to_weight.get)"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = {\n            \"func_name\": func_name,\n            \"args\": args,\n            \"kwargs\": kwargs\n        }\n\n        data_json = json.dumps(data, sort_keys=True)\n        data_hash = hashlib.sha256(data_json.encode(\"utf-8\")).hexdigest()\n\n        return data_hash\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the total length of the polygon by summing the distances between consecutive points, excluding distances that exceed a specified maximum, to avoid counting large gaps between disjoint arcs as part of the polygon's length.\n    # Input-Output Arguments\n\n    # Initialize the total length of the polygon to zero.\n    total_length = 0\n\n    # Iterate over the points in the polygon.\n    for i in range(len(polygon)):\n\n        # If this is the last point in the polygon, then compute the distance between this point and the first point in the polygon.\n        if i == len(polygon) - 1:\n            distance = np.linalg.norm(polygon[i] - polygon[0])\n\n        # Otherwise, compute the distance between this point and the next point in the polygon.\n        else:\n            distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n\n        # If the distance between this point and the next point is below the specified maximum distance, then add the distance to the total length of the polygon.\n        if distance < max_point_distance:\n            total_length += distance\n\n    # Return the total length of the polygon.\n    return total_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Get the area of the largest polygon\n    max_area = max([area(polygon) for polygon in polygons])\n\n    # Filter out polygons based on their area\n    filtered_polygons = []\n    for polygon in polygons:\n        if area(polygon) > max_area * rel_tr or area(polygon) > abs_tr:\n            filtered_polygons.append(polygon)\n\n    return filtered_polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed.\n    samples_per_worker = num_samples_yielded // num_workers\n    samples_remaining = num_samples_yielded % num_workers\n\n    # Calculate the number of batches each worker has processed.\n    batches_per_worker = samples_per_worker // batch_size\n    batches_remaining = samples_per_worker % batch_size\n\n    # Calculate the number of samples each worker has processed.\n    samples_per_worker = batches_per_worker * batch_size\n    samples_remaining = batches_remaining * batch_size\n\n    # Calculate the number of batches each worker has processed.\n    batches_per_worker = samples_per_worker // batch_size\n    batches_remaining = samples_per_worker % batch_size\n\n    # Calculate the number of samples each worker has processed.\n    samples_per_worker = batches_per_worker * batch_size\n    samples_remaining = batches_remaining * batch_size\n\n    # Calculate the number of batches each worker has processed.\n    batches_per_worker = samples_per_worker // batch_size\n    batches_remaining = samples_per_worker % batch_size\n\n    # Calculate the number of samples each worker has processed.\n    samples_per_worker = batches_per_worker * batch_size\n    samples_remaining = batches_remaining * batch_size\n\n    # Calculate the number of batches each worker has processed.\n    batches_per_worker = samples_per_worker // batch_size\n    batches_remaining = samples_per_worker % batch_size\n\n    # Calculate the number of samples each worker has processed.\n    samples_per_worker = batches_per_worker * batch_size\n    samples_remaining = batches_remaining * batch_size\n\n    # Calculate the number of batches each worker has processed.\n    batches_per_worker = samples_per_worker // batch_size\n    batches_remaining = samples_per_"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    if len(results) != len(value) or len(results) != len(metadatas):\n        raise ValueError(\"The length of the results, value and metadata lists must be the same.\")\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have the shape (_, 2), where _ can be any number of points.\")\n\n    return 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # a and v should be 1D tensors\n    assert a.ndim == 1\n    assert v.ndim == 1\n\n    # a should be sorted\n    assert torch.all(a[1:] > a[:-1])\n\n    # a and v should have the same size in the last dimension\n    assert a.shape[-1] == v.shape[-1]\n\n    # a and v should have the same size in all but the last dimension\n    assert a.shape[:-1] == v.shape[:-1]\n\n    # a and v should have the same dtype\n    assert a.dtype == v.dtype\n\n    # a and v should have the same device\n    assert a.device == v.device\n\n    # a and v should have the same number of elements\n    assert a.numel() == v.numel()\n\n    # a and v should have the same last dimension\n    assert a.shape[-1] == v.shape[-1]\n\n    # a and v should have the same dtype\n    assert a.dtype == v.dtype\n\n    # a and v should have the same device\n    assert a.device == v.device\n\n    # a and v should have the same number of elements\n    assert a.numel() == v.numel()\n\n    # a and v should have the same last dimension\n    assert a.shape[-1] == v.shape[-1]\n\n    # a and v should have the same dtype\n    assert a.dtype == v.dtype\n\n    # a and v should have the same device\n    assert a.device == v.device\n\n    # a and v should have the same number of elements\n    assert a.numel() == v.numel()\n\n    # a and v should have the same last dimension\n    assert a.shape[-1] == v.shape[-1]\n\n    # a and v should have the same dtype\n    assert a.dtype == v.dtype\n\n    # a and v should have the same device\n    assert a.device == v.device\n\n    # a and v should have the same number of elements\n    assert"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n"}
{"namespace": "coord.contract", "completion": "  return x / jnp.sqrt(jnp.sum(x ** 2, axis=-1, keepdims=True))\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"b\", \"kb\", \"mb\", \"gb\", \"tb\", \"pb\"]:\n        if abs(num_bytes) < 1024.0:\n            return f\"{num_bytes:3.1f}{unit}\"\n        num_bytes /= 1024.0\n    raise ValueError(\"Number is too large\")\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"\n        This function checks if a given array has a specific number of dimensions. It is designed to be used within Pydantic models to validate the dimensions of array fields.\n\n        Input-Output Arguments\n        :param cls: Type. The class of the model being validated.\n        :param v: np.ndarray. The array being validated.\n        :param field: fields.ModelField. The field of the model that is being validated.\n        :return: np.ndarray. The validated array if it meets the specified number of dimensions.\n        \"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Received {v.shape}\"\n            )\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    score = results['rougeL']\n    return score\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Fallback to using standard method to locate the object.\n    try:\n        return pydoc.locate(name)\n    except pydoc.ErrorDuringImport as e:\n        # Fallback to using importlib to locate the object.\n        try:\n            module_name, qualname = name.split(\":\")\n            from importlib import import_module\n\n            m = import_module(module_name)\n            t = m\n            for attr_name in qualname.split(\".\"):\n                t = getattr(t, attr_name)\n            return t\n        except Exception as e:\n            raise e"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the length of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\n            \"The length of the ids and scores tuples must match. The length of the ids tuple is {} and the length of the scores tuple is {}.\".format(\n                len(ids), len(scores)))\n\n    # Check if the length of the ids, scores, and weights tuples match\n    if len(ids) != len(weights):\n        raise ValueError(\n            \"The length of the ids, scores, and weights tuples must match. The length of the ids tuple is {} and the length of the weights tuple is {}.\".format(\n                len(ids), len(weights)))\n\n    # Check if the sum of the weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\n            \"The sum of the weights must equal 1. The sum of the weights is {}.\".format(\n                sum(weights)))\n\n    # Check if the length of the weights tuple matches the length of the ids and scores tuples\n    if len(weights) != len(ids) or len(weights) != len(scores):\n        raise ValueError(\n            \"The length of the weights tuple must match the length of the ids and scores tuples. The length of the weights tuple is {} and the length of the ids tuple is {}.\".format(\n                len(weights), len(ids)))\n\n    # Normalize the scores\n    normalized_scores = []\n    for score_list in scores:\n        normalized_scores.append(pd.Series(score_list).divide(sum(score_list)))\n\n    # Combine the scores\n    combined_scores = []\n    for i in range(len(ids[0])):\n        combined_scores.append(\n            sum([normalized_scores[j][i] * weights[j] for j in range(len(ids))]))\n\n    # Select the top_k results\n    top_combined_scores = sorted(combined_scores, reverse=True)[:top_k]"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x == 0:\n        return f\"{x:.0f}%\" if percent else f\"{x:.0f}\"\n\n    if x < 1:\n        return f\"{x:.2f}%\" if percent else f\"{x:.2f}\"\n\n    if x < 10:\n        return f\"{x:.1f}%\" if percent else f\"{x:.1f}\"\n\n    return f\"{x:.0f}%\" if percent else f\"{x:.0f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        if disk_usage.free < threshold_in_gb * 1024 * 1024 * 1024:\n            sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all spaces from the input text.\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens.\n    line_text = line_text.split()\n\n    # Return the segmented text.\n    return line_text"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the total number of weights.\")\n\n    if zeros == 0:\n        return np.random.dirichlet(np.ones(n))\n\n    else:\n        weights = np.random.dirichlet(np.ones(n - zeros))\n        zeros_indices = np.random.choice(n, zeros, replace=False)\n        weights[zeros_indices] = 0\n        return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    center_y, center_x = (bbox[1] + bbox[3]) / 2, (bbox[0] + bbox[2]) / 2\n\n    # Find the largest rotation-free crop size that fits within the image boundaries.\n    base_size = max(crop_size)\n    max_size = min(image_size)\n    crop_dims = [min(s, max_size) for s in (base_size, base_size)]\n\n    # Find a cropping region centered at the instance's center that fits within the image boundaries and satisfies the desired crop size.\n    cropping_region = [\n        max(0, center_x - crop_dims[1] / 2),\n        max(0, center_y - crop_dims[0] / 2),\n        min(image_size[1], center_x + crop_dims[1] / 2),\n        min(image_size[0], center_y + crop_dims[0] / 2),\n    ]\n\n    return T.CropTransform(*cropping_region)\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of the input vector(s).\n  x_sq = jnp.sum(x * x, axis=-1, keepdims=True)\n\n  # Compute the norm of the input vector(s).\n  x_norm = jnp.sqrt(jnp.maximum(x_sq, grad_eps))\n\n  # Compute the normalization constant.\n  x_norm_const = 1.0 / x_norm\n\n  # Normalize the input vector(s).\n  x_normalized = x * x_norm_const\n\n  return x_normalized\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        input_text = response.split(\"Use Agent[\")[1].split(\"]\")[1].split(\":\")[1]\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"bitmask\":\n            masks = convert_bitmasks_to_masks(segms, *image_size)\n        else:\n            masks = [polygons_to_bitmask(p, *image_size) for p in segms]\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n        target.gt_masks = BitMasks(masks)\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"The input must be a 2D array\")\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Get the original training mode of the model\n    original_training_mode = model.training\n\n    # Get the original training mode of every submodule\n    original_submodule_training_modes = {}\n    for name, submodule in model.named_modules():\n        original_submodule_training_modes[name] = submodule.training\n\n    # Set the training mode of every submodule to a constant value\n    for name, submodule in model.named_modules():\n        submodule.training = torch.jit.Final[bool](True)\n\n    # Set the training mode of the model to a constant value\n    model.training = torch.jit.Final[bool](True)\n\n    # Yield control to the context manager\n    yield\n\n    # Revert the training mode of the model to its original value\n    model.training = original_training_mode\n\n    # Revert the training mode of every submodule to its original value\n    for name, submodule in model.named_modules():\n        submodule.training = original_submodule_training_modes[name]\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[Any]]) -> Dict[str, List[Any]]:\n        \"\"\"Check if shape(field1) equals shape(field2).\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize variables\n    metric_names = []\n    metric_params = []\n\n    # Iterate over metrics\n    for metric in metrics:\n\n        # If metric is a string, add it to the list of metric names\n        if isinstance(metric, str):\n            metric_names.append(metric)\n\n        # If metric is a dictionary, add it to the list of metric parameters\n        elif isinstance(metric, dict):\n            metric_params.append(metric)\n\n        # Raise error if metric is neither a string nor a dictionary\n        else:\n            raise TypeError(f\"Metric {metric} is neither a string nor a dictionary.\")\n\n    # Return list of metric names and list of metric parameters\n    return metric_names, metric_params\n\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == contract3_isoscale:\n      fn_inv = inv_contract3_isoscale\n    else:\n      raise ValueError(\n          'No inverse function provided for non-contract3_isoscale function.')\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n    \"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    return fn(t)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances to metric distances.\n    \"\"\"\n    return fn_inv(s)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.cos(theta) * jnp.sin(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(phi)\n\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.sum(w[1:] + w[:-1]) * (t[1] - t[0]) / 2\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check that the length of the input tuples are the same\n    assert len(ids) == len(scores) == len(weights), \"The length of the input tuples must be the same.\"\n\n    # Check that the length of the input tuples are greater than 1\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n\n    # Check that the length of the input tuples are greater than the number of top_k\n    assert len(ids) >= top_k, \"The length of the input tuples must be greater than the number of top_k.\"\n\n    # Check that the sum of the input tuples is 1\n    assert sum(weights) == 1, \"The sum of the input tuples must be 1.\"\n\n    # Check that the number of top_k is greater than 0\n    assert top_k > 0, \"top_k must be greater than 0.\"\n\n    # Initialize a list to store the weighted sum of scores for each ID\n    weighted_scores = []\n\n    # Iterate through each category or group\n    for i in range(len(ids)):\n\n        # Iterate through each ID in the category or group\n        for j in range(len(ids[i])):\n\n            # Calculate the weighted sum of scores for each ID\n            weighted_scores.append(scores[i][j] * weights[i])\n\n    # Create a dataframe from the list of IDs and the list of weighted scores\n    df = pd.DataFrame({'id': ids[0], 'score': weighted_scores})\n\n    # Sort the dataframe by the weighted scores in descending order\n    df = df.sort_values(by=['score'], ascending=False)\n\n    # Return the top K IDs and their corresponding weighted scores\n    return df['id'][:top_k].tolist(), df['score'][:top_k].tolist()"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jacobian = jax.jacfwd(fn)(mean)\n\n  # Compute the transformed mean\n  fn_mean = fn(mean)\n\n  # Compute the transformed covariances\n  fn_cov = jnp.matmul(jnp.matmul(jacobian, cov), jacobian.T)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield [x[i][: len(x[i]) // 2], x[i][len(x[i]) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Generate the scales\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Scale the input\n  scaled_x = x * scales\n\n  # Apply the sine function\n  encoded_x = jnp.sin(scaled_x)\n\n  # Optionally append the original input\n  if append_identity:\n    encoded_x = jnp.concatenate([encoded_x, x], axis=-1)\n\n  return encoded_x\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                             f\"resp. {len(values[field1])} and {len(values[field2])}\")\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                                 f\"resp. {values[field1][i].shape} and {values[field2][i].shape}\")\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the mesh\n        self.render(eglctx, camera)\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new configuration object for a Nomic model.\n    nomic_config = NomicBertConfig()\n\n    # Copy the settings from the BERT configuration object to the new Nomic configuration object.\n    for key, value in bert_config.__dict__.items():\n        if key in nomic_config.__dict__:\n            nomic_config.__dict__[key] = value\n\n    # Set the new configuration object's attributes.\n    nomic_config.hidden_act = \"gelu\"\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.vocab_size = bert_config.vocab_size\n\n    # Return the new configuration object.\n    return nomic_config\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        # Check if the mesh is visible\n        if not self.visible:\n            return\n\n        # Set up the shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            use_gl_program(self.point_program)\n        else:\n            use_gl_program(self.mesh_program)\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the vertex array object\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, self.n_verts_bytes)\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, self.n_faces_bytes, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, self.n_faces_bytes, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, self.n_faces_bytes, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, self.n_faces_bytes, gl.GL_UNSIGNED_INT, None)\n        else:\n            raise NotImplementedError(f'Render type {self.render_type} not supported')\n\n        # Unbind the vertex array object\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        if not isinstance(ptr, np.ndarray):\n            raise TypeError(f'Expected a numpy array or a torch tensor, but got {type(ptr)}')\n\n        if ptr.ndim == 3:\n            if ptr.shape[-1] == 3:\n                ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n            elif ptr.shape[-1] == 4:\n                pass\n            else:\n                raise ValueError(f'Expected a 3-channel or 4-channel image, but got {ptr.shape[-1]} channels')\n\n        if not hasattr(self, 'tex'):\n            self.init_texture()\n\n        w = w or self.W\n        h = h or self.H\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate the input arguments\n    assert R.shape[-2:] == (3, 3), \"R must be a batch of 3x3 matrices.\"\n    assert tvec.shape[-2:] == (3, 1), \"tvec must be a batch of 3x1 vectors.\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"camera_matrix must be a batch of 3x3 matrices.\"\n    assert image_size.shape[-2:] == (2, 1), \"image_size must be a batch of 2x1 vectors.\"\n\n    # Ensure that all inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.expand(batch_dim + (3, 3))\n    tvec = tvec.expand(batch_dim + (3, 1))\n    camera_matrix = camera_matrix.expand(batch_dim + (3, 3))\n    image_size = image_size.expand(batch_dim + (2, 1))\n\n    # Compute the camera position\n    camera_position = -torch.bmm(R.transpose(-2, -1), tvec)\n\n    # Compute the camera rotation\n    camera_rotation = R\n\n    # Compute the focal length\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    if fx.mean() / fy.mean() > 1.01 or fx.mean() / fy.mean() < 0.99:\n        warn_once_about_pulsar_fxfy()\n    fx = fy = (fx + fy) / 2\n\n    # Compute the principal point\n    px = camera_matrix[..., 0, 2]\n    py = camera_matrix[..., 1, 2]\n    if px.mean() / py.mean() > 1.01 or px.mean() / py.mean() < 0.99:\n        warn_once_about_pulsar_fxfy"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.visible: return\n\n        if self.use_quad_draw:\n            # Set up viewport and scissor box\n            old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n            old_scissor_box = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n            gl.glViewport(x, y, w or self.W, h or self.H)\n            gl.glScissor(x, y, w or self.W, h or self.H)\n\n            # Activate the quad program\n            gl.glUseProgram(self.quad_program)\n            gl.glBindVertexArray(self.vao)\n\n            # Bind the texture\n            gl.glActiveTexture(gl.GL_TEXTURE0)\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n            # Draw the quadrilateral\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n            # Restore the viewport and scissor box\n            gl.glViewport(*old_viewport)\n            gl.glScissor(*old_scissor_box)\n\n        else:\n            # Blit the texture to the screen\n            old_fbo = gl.glGetIntegerv(gl.GL_FRAMEBUFFER_BINDING)\n            gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self.fbo)\n            gl.glBlitFramebuffer(x, y, w or self.W, h or self.H,\n                                 x, y, w or self.W, h or self.H,\n                                 gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n            gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, old_fbo)\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance has been initialized\n        if not hasattr(self, 'fbo'):\n            self.init_texture()\n\n        # Check if the Quad instance"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # t0 = t0.unsqueeze(1)\n    # t1 = t1.unsqueeze(1)\n    # y1 = y1.unsqueeze(1)\n\n    # t0 = t0.unsqueeze(1)\n    # t1 = t1.unsqueeze(1)\n    # y1 = y1.unsqueeze(1)\n\n    # t0 = t0.unsqueeze(1)\n    # t1 = t1.unsqueeze(1)\n    # y1 = y1.unsqueeze(1)\n\n    # t0 = t0.unsqueeze(1)\n    # t1 = t1.unsqueeze(1)\n    # y1 = y1.unsqueeze(1)\n\n    # t0 = t0.unsqueeze(1)\n    # t1 = t1.unsqueeze(1)\n    # y1 = y1.unsqueeze(1)\n\n    # t0 = t0.unsqueeze(1)\n    # t1 = t1.unsqueeze(1)\n    # y1 = y1.unsqueeze(1)\n\n    # t0 = t0.unsqueeze(1)\n    # t1 = t1.unsqueeze(1)\n    # y1 = y1.unsqueeze(1)\n\n    # t0 = t0.unsqueeze(1)\n    # t1 = t1.unsqueeze(1)\n    # y1 = y1.unsqueeze(1)\n\n    # t0 = t0.unsqueeze(1)\n    # t1 = t1.unsqueeze(1)\n    # y1 = y1.unsqueeze(1)\n\n    # t0 = t0.unsqueeze(1)\n    # t1 = t1.unsqueeze(1)\n    # y1 = y1.unsqueeze(1)\n\n    # t0 = t0."}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the inner and outer measures\n    y0_inner, y0_outer = inner_outer(t, t_env, w_env)\n\n    # calculate the loss\n    loss = (y0_outer - y0_inner) ** 2\n\n    # scale the loss\n    loss = loss * (w / (w + eps))\n\n    # sum the loss\n    loss = loss.sum()\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n\n    # Calculate the inter-interval loss\n    inter_loss = torch.sum(torch.abs(w[..., 1:] - w[..., :-1]) * (t[..., 1:] - t[..., :-1]))\n\n    # Calculate the intra-interval loss\n    intra_loss = torch.sum(torch.abs(w[..., 1:] - w[..., :-1]) * (t[..., 1:] - t[..., :-1]))\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_loss + intra_loss\n\n    return distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps).to(t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that t and w are the same shape.\n    t, w = matchup_channels(t, w)\n\n    # Compute the CDF of the PDF.\n    cw = integrate_weights(w)\n\n    # Generate uniform samples from the unit interval.\n    u = torch.rand(num_samples, t.shape[-1], device=t.device)\n\n    # Invert the CDF to get the samples.\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb the samples if requested.\n    if perturb:\n        if single_jitter:\n            # Apply the same jitter to every sample along each dimension.\n            jitter = torch.rand_like(t_new)\n        else:\n            # Apply independent jitter to each sample along each dimension.\n            jitter = torch.rand(t_new.shape, device=t_new.device)\n        t_new += jitter\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # preparing for size change\n    t = t.reshape(-1, t.shape[-1])\n    w = w.reshape(-1, w.shape[-1])\n\n    # assuming sampling in s space\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)\n\n    # dilation\n    t_dilated = t.unsqueeze(1) + torch.arange(dilation, device=t.device, dtype=t.dtype).reshape(1, -1, 1)\n    t_dilated = t_dilated.reshape(-1, t_dilated.shape[-1])\n\n    # clipping\n    t_dilated = torch.cat([torch.tensor([domain[0]], device=t.device, dtype=t.dtype), t_dilated, torch.tensor([domain[1]], device=t.device, dtype=t.dtype)], dim=0)\n    w_dilated = torch.cat([torch.zeros_like(w[..., :1]), w], dim=-1)\n\n    # preparing for size change\n    t_dilated = t_dilated.reshape(t.shape[:-1] + (t_dilated.shape[-1],))\n    w_dilated = w_dilated.reshape(w.shape[:-1] + (w_dilated.shape[-1],))\n\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # If the query times are the same as the step function times, return the step function values.\n    if torch.equal(tq, t):\n        return y\n\n    # If the query times are outside the range of the step function times, return the outside value.\n    if tq.min() < t.min() or tq.max() > t.max():\n        return torch.full(tq.shape, outside_value, device=tq.device, dtype=tq.dtype)\n\n    # If the query times are outside the range of the step function times, return the outside value.\n    if tq.min() < t.min() or tq.max() > t.max():\n        return torch.full(tq.shape, outside_value, device=tq.device, dtype=tq.dtype)\n\n    # If the query times are outside the range of the step function times, return the outside value.\n    if tq.min() < t.min() or tq.max() > t.max():\n        return torch.full(tq.shape, outside_value, device=tq.device, dtype=tq.dtype)\n\n    # If the query times are outside the range of the step function times, return the outside value.\n    if tq.min() < t.min() or tq.max() > t.max():\n        return torch.full(tq.shape, outside_value, device=tq.device, dtype=tq.dtype)\n\n    # If the query times are outside the range of the step function times, return the outside value.\n    if tq.min() < t.min() or tq.max() > t.max():\n        return torch.full(tq.shape, outside_value, device=tq.device, dtype=tq.dtype)\n\n    # If the query times are outside the range of the step function times, return the outside value.\n    if tq.min() < t.min() or tq.max() > t.max():\n        return torch.full(tq.shape, outside_value, device"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # preparing for size change\n    t = t.reshape(-1, t.shape[-1])\n    w = w.reshape(-1, w.shape[-1])\n\n    # assuming sampling in s space\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)\n\n    # Calculate the bias function\n    bias = torch.exp(-anneal_slope * train_frac)\n\n    # Calculate the adjusted weights\n    w_adj = w * bias\n\n    # Ensure stability in the computation by handling cases where adjacent intervals have zero distance, setting their weight to zero, and preventing NaN values by using a softmax operation on the adjusted weights.\n    w_adj[..., 1:] = torch.where(\n        (t[..., 1:] - t[..., :-1]) == 0,\n        torch.zeros_like(w_adj[..., 1:]),\n        w_adj[..., 1:]\n    )\n    w_adj = torch.softmax(w_adj, dim=-1)\n\n    # preparing for size change\n    w_adj = w_adj.reshape(w.shape)\n    return w_adj\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return batch\n        else:\n            return {k: to_cuda(v, device=device, ignore_list=ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, list):\n        if ignore_list:\n            return batch\n        else:\n            return [to_cuda(v, device=device, ignore_list=ignore_list) for v in batch]\n    elif isinstance(batch, tuple):\n        if ignore_list:\n            return batch\n        else:\n            return tuple([to_cuda(v, device=device, ignore_list=ignore_list) for v in batch])\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # expand the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim < v.ndim:\n        f = f[(slice(None),) * (v.ndim - f.ndim) + (0,) * (f.ndim - v.ndim + 1)]\n\n    # compute the normals of the faces\n    v1 = multi_gather(v, f[..., 0], dim=dim)\n    v2 = multi_gather(v, f[..., 1], dim=dim)\n    v3 = multi_gather(v, f[..., 2], dim=dim)\n    n = torch.cross(v2 - v1, v3 - v1, dim=-1)\n\n    # reshape the normals to match the original faces tensor structure\n    return n.reshape(*f.shape[:-1], *n.shape[1:])\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n        batch.movement_speed = self.movement_speed\n        batch.movement_force = self.movement_force\n        batch.drag_coeff_mult = self.drag_coeff_mult\n        batch.constant_drag = self.constant_drag\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_torque = self.movement_torque\n        batch.angular_friction = self.angular_friction\n        batch.constant_torque = self.constant_torque\n        batch.min_interval = self.min_interval\n        batch.pause_physics = self.pause_physics\n\n        # Meta\n        meta = dotdict()\n        meta.H, meta.W, meta.K, meta.R, meta.T, meta.n, meta.f, meta.t, meta.v, meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        meta.origin = self.origin\n        meta.world_up = self.world_up\n        meta.movement_speed = self.movement_speed\n        meta.movement_force = self.movement_force\n        meta.drag_coeff_mult = self.drag_coeff_mult\n        meta.constant_drag = self.constant_drag\n        meta.mass = self.mass\n        meta.moment_of_inertia = self.moment_of_inertia"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working() and not agent.is_prime():\n            agent_dict = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(agent_dict)\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([e1], [purpose_embedding])[0][0] for e1 in self.agents]\n            if similarities:\n                max_similarity = max(similarities)\n                if max_similarity > 0.999:\n                    return None, max_similarity\n                else:\n                    return self.agents[np.argmax(similarities)], max_similarity\n            else:\n                return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            name=PRIME_NAME,\n            prompt=PRIME_PROMPT,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            prime_only=True,\n            openai_api_wrapper=self.openai_wrapper,\n            agent_persistence_manager=self.agent_persistence\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n        else:\n            return None"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            agents.append(agent)\n\n        return agents\n\n    def load_all_agents_for_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents_for_purpose(purpose)\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            agents.append(agent)\n\n        return agents\n\n    def load_all_agents_for_purpose_and_state(self, purpose, state, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents_for_purpose_and_state(purpose, state)\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            agents.append(agent)\n\n        return agents\n\n    def load_all_agents_for_purpose_and_state_and_prime(self, purpose, state, prime, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents_for_purpose_and_state_and_prime(purpose, state, prime)\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            agents.append(agent)\n\n        return agents\n\n    def load_all_agents_for_purpose_and_state"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent {agent.id} with error {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT.format(goal=goal, sample_input=sample_input)\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_TEMPLATE\n        prompt = prompt + \"\\n\" + EXAMPLES\n        prompt = prompt + \"\\n\" + PROMPT_ENGINEERING_"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n                ON CONFLICT(id) DO UPDATE SET data = ?\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict), json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result is None:\n                return None\n            return json.loads(result[0])"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result))\n        )\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(vars(args))\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG['record_dir'], 'output.txt'), 'w') as f:\n            with redirect_stdout(f):\n                CommandLine(CONFIG).execute()\n    else:\n        CommandLine(CONFIG).execute()\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.Completion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method is not supported in a DataLoader worker process.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"item_loader\": self.item_loader,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        if self.item_loader is not None:\n            state[\"item_loader_state\"] = self.item_loader.state_dict()\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        self.input_dir = Dir(state_dict[\"input_dir_path\"], state_dict[\"input_dir_url\"])\n        self.item_loader = BaseItemLoader.load_state_dict(state_dict[\"item_loader\"])\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.distributed_env = _DistributedEnv(state_dict[\"world_size\"])\n        self.shuffle = state_dict[\"shuffle\"]\n\n        self.cache = None\n        self.worker_env = None\n        self.worker_chunks = []\n        self.worker_intervals = []\n        self.current_indexes = []\n        self.chunk_index = 0\n        self.num_chunks = None\n        self.global_index = state_dict[\"num_samples_yielded\"]\n        self.index = 0\n        self.has_triggered_download = False\n        self.min_items_per_replica: Optional[int] = None\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.random_state = None\n        self.shuffler: Optional[Shuffle] = None\n        self.serializers = None\n        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state dictionary is None.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path in the state dictionary ({state['input_dir_path']}) does not match the current input directory path ({self.input_dir.path}).\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL in the state dictionary ({state['input_dir_url']}) does not match the current input directory URL ({self.input_dir.url}).\"\n            )\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle flag in the state dictionary ({state['shuffle']}) does not match the current shuffle flag ({self.shuffle}).\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last flag in the state dictionary ({state['drop_last']}) does not match the current drop_last flag ({self.drop_last}).\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed in the state dictionary ({state['seed']}) does not match the current seed ({self.seed}).\"\n            )\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The world size in the state dictionary ({state['world_size']}) does not match the current world size ({self.distributed_env.world_size}).\"\n            )\n\n        if state[\"item_loader\"] is not None:\n            if self.item_loader is None:\n                raise ValueError(\n                    \"The item loader in the state dictionary is not None, but the current item loader is None.\"\n                )\n            self.item_loader.load_state_dict(state[\"item_loader"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Hash the input directory\n    input_dir_hash = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Create the cache directory\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, input_dir_hash)\n\n    # Create the cache directory if it does not exist\n    if not os.path.exists(cache_dir):\n        try:\n            os.makedirs(cache_dir)\n        except Exception as e:\n            logger.error(f\"Failed to create cache directory {cache_dir}.\")\n            return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(\n                \"The remote file path must be an S3 URL (i.e. start with 's3://').\"\n            )\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Acquire a file lock\n        lock_path = local_filepath + \".lock\"\n        lock = FileLock(lock_path)\n        try:\n            lock.acquire(timeout=10)\n        except Timeout:\n            raise Timeout(\n                \"The file lock could not be acquired. Make sure no other process is accessing the local file.\"\n            )\n\n        # Download the file\n        if self._s5cmd_available:\n            # Use s5cmd to download the file\n            s5cmd_args = [\n                \"s5cmd\",\n                \"cp\",\n                \"--no-progress\",\n                \"--no-checksum\",\n                \"--no-encryption\",\n                parsed_url.netloc + parsed_url.path,\n                local_filepath,\n            ]\n            subprocess.run(s5cmd_args, check=True)\n        else:\n            # Use boto3 to download the file\n            self._client.download_file(parsed_url.netloc, parsed_url.path, local_filepath)\n\n        # Release the file lock\n        lock.release()\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # The number of chunks per worker\n    chunks_per_worker = len(chunks_replica) // num_workers\n\n    # The number of chunks that are left over after distributing the chunks among the workers\n    chunks_left_over = len(chunks_replica) % num_workers\n\n    # The number of workers that are assigned the extra chunks\n    workers_with_extra_chunks = min(chunks_left_over, num_workers)\n\n    # The number of chunks that are distributed among the workers\n    chunks_per_worker_with_extra_chunks = chunks_per_worker + 1\n\n    # The number of chunks that are distributed among the workers\n    chunks_per_worker_without_extra_chunks = chunks_per_worker\n\n    # The number of chunks that are distributed among the workers\n    chunks_per_worker_without_extra_chunks = chunks_per_worker\n\n    # The number of chunks that are distributed among the workers\n    chunks_per_worker_without_extra_chunks = chunks_per_worker\n\n    # The number of chunks that are distributed among the workers\n    chunks_per_worker_without_extra_chunks = chunks_per_worker\n\n    # The number of chunks that are distributed among the workers\n    chunks_per_worker_without_extra_chunks = chunks_per_worker\n\n    # The number of chunks that are distributed among the workers\n    chunks_per_worker_without_extra_chunks = chunks_per_worker\n\n    # The number of chunks that are distributed among the workers\n    chunks_per_worker_without_extra_chunks = chunks_per_worker\n\n    # The number of chunks that are distributed among the workers\n    chunks_per_worker_without_extra_chunks = chunks_per_worker\n\n    # The number of chunks that are distributed among the workers\n    chunks_per_worker_without_extra_chunks = chunks_per_worker\n\n    # The number of chunks that"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not _PIL_AVAILABLE:\n            raise ImportError(\"PIL is not available.\")\n\n        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL Image, got {type(item)}.\")\n\n        buffer = io.BytesIO()\n        item.save(buffer, format=\"png\")\n        buffer.seek(0)\n        return buffer.read(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n        else:\n            with io.BytesIO() as buffer:\n                item.save(buffer, format=\"JPEG\")\n                return buffer.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        data = io.BytesIO(data)\n        ints = np.frombuffer(data.read(12), np.uint32)\n        width, height, mode_len = ints\n        mode = data.read(mode_len).decode(\"utf-8\")\n        raw = data.read()\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        idx = 0\n        dtype_indice = int.from_bytes(data[idx : idx + 4], byteorder=\"big\")\n        idx += 4\n        num_dims = int.from_bytes(data[idx : idx + 4], byteorder=\"big\")\n        idx += 4\n        shape = []\n        for _ in range(num_dims):\n            shape.append(int.from_bytes(data[idx : idx + 4], byteorder=\"big\"))\n            idx += 4\n        dtype = list(_TORCH_DTYPES_MAPPING.keys())[dtype_indice]\n        return torch.from_numpy(np.frombuffer(data[idx:], dtype=dtype).reshape(*shape))\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Serialize the tensor's data type\n        dtype_index = self._dtype_to_indices[item.dtype]\n        dtype_bytes = dtype_index.to_bytes(1, byteorder=\"big\")\n\n        # Serialize the tensor's shape\n        shape_bytes = b\"\"\n        for shape_dim in item.shape:\n            shape_dim_bytes = shape_dim.to_bytes(4, byteorder=\"big\")\n            shape_bytes += shape_dim_bytes\n\n        # Serialize the tensor's raw data\n        data_bytes = item.numpy().tobytes()\n\n        # Concatenate the dtype, shape, and data bytes to form the serialized tensor\n        serialized_tensor = dtype_bytes + shape_bytes + data_bytes\n\n        return serialized_tensor, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            # try to decode the data as a JPEG image\n            image = decode_jpeg(torch.ByteTensor(data))\n            # convert the image to a PyTorch tensor\n            image = pil_to_tensor(image)\n            return image\n        except RuntimeError:\n            # if the decoding fails, fall back to using PIL\n            return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"NoHeaderTensorSerializer is not setup\")\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{self._dtype_to_indices[self._dtype]}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type must be set before calling the deserialize function.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        return np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {}\n        state_dict[\"dataset\"] = self.dataset.state_dict()\n        state_dict[\"current_epoch\"] = self.current_epoch\n        state_dict[\"num_samples_yielded_streaming\"] = self._num_samples_yielded_streaming\n        state_dict[\"num_samples_yielded_combined\"] = self._num_samples_yielded_combined\n        state_dict[\"latest_worker_idx\"] = self._latest_worker_idx\n        return state_dict\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise RuntimeError(\"TorchVision is not available on this platform\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, audio, info = torch.ops.torchvision.read_video(\n                f.name,\n                # pyre-ignore[6]\n                # need to import from torchvision to use this function\n                # but torchvision imports from torch so we have a problem\n                # hence we need to disable this mypy check\n                # see https://github.com/python/mypy/issues/6910\n                # and https://github.com/pytorch/pytorch/issues/50014\n                # for more details\n                # pyre-ignore[28]\n                # need to import from torchvision to use this function\n                # but torchvision imports from torch so we have a problem\n                # hence we need to disable this mypy check\n                # see https://github.com/python/mypy/issues/6910\n                # and https://github.com/pytorch/pytorch/issues/50014\n                # for more details\n                # pyre-ignore[28]\n                # need to import from torchvision to use this function\n                # but torchvision imports from torch so we have a problem\n                # hence we need to disable this mypy check\n                # see https://github.com/python/mypy/issues/6910\n                # and https://github.com/pytorch/pytorch/issues/50014\n                # for more details\n                # pyre-ignore[28]\n                # need to import from torchvision to use this function\n                # but torchvision imports from torch so we have a problem\n                # hence we need to disable this mypy check\n                # see https://github.com/python/mypy/issues/6910\n                # and https://github.com/pytorch/pytor"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        while self._should_write():\n            self.write_chunk()\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, f) for f in os.listdir(self._cache_dir) if f.endswith(\".bin\")]\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return self._iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n\n        return self._iterator.state_dict(num_workers, batch_size)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            for dataset in self._datasets:\n                dataset.load_state_dict(state_dict[__SAMPLES_KEY__][dataset.name])\n            return\n\n        self._iterator.load_state_dict(state_dict)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(url=dir_path)\n        else:\n            return Dir(path=dir_path)\n    else:\n        return dir_path\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The `output_dir` must be a `Dir` object, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"The `output_dir` must be an S3 directory, got: {output_dir}\")\n\n    if not _is_empty_s3_dir(output_dir.url):\n        raise ValueError(f\"The `output_dir` must be empty, got: {output_dir}\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # If the directory is empty, we can return\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # If the directory is not empty, we need to check if it contains an index file\n    for obj in objects[\"Contents\"]:\n        if obj[\"Key\"] == obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\":\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file. Datasets are meant to be immutable.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    # If the directory is not empty and does not contain an index file, we need to delete all objects within the specified prefix in the bucket\n    for obj in objects[\"Contents\"]:\n        s3.delete_object(Bucket=obj.netloc, Key=obj[\"Key\"])\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # If the number of workers is not specified, assume a non-distributed environment\n        if num_workers == 1:\n            return\n\n        # If the node rank is not specified, assume a non-distributed environment\n        if node_rank is None:\n            return\n\n        # If the current node is not the master node, wait until the merged index file is available\n        if node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\")):\n                sleep(1)\n            return\n\n        # Wait until all index parts are available\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(1)\n\n        # Merge the index files\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        index_files.sort()\n        index_files = [os.path.join(self._cache_dir, f) for f in index_files]\n\n        # Merge the index files\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        index_files.sort()\n        index_files = [os.path.join(self._cache_dir, f) for f in index_files]\n\n        chunks_info = []\n        for index_file in index_files:\n            with open(index_file, \"r\") as in_file:\n                chunks_info.extend(json.load(in_file)[\"chunks\"])\n\n        # Write the merged index file\n        with open(os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\"), \"w\") as out:\n            json.dump({\"chunks\": chunks_info, \"config\": self.get_config()}, out, sort_keys=True)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning-sdk` is not available. Please install the `lightning-sdk` to use this function.\"\n        )\n\n    if not machine:\n        machine = Machine()\n\n    if not command:\n        command = f\"cd {os.getcwd()}; export PYTHONPATH=$(pwd); python -m {name}\"\n\n    studio = Studio()\n    job = studio.create_job(name, num_nodes, machine, command)\n\n    print(f\"Job URL: {job.url}\")\n\n    while True:\n        sleep(10)\n        job = studio.get_job(job.id)\n        if job.status == \"running\":\n            break\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"The job failed with the following error: {job.error}\")\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        try:\n            self._config = ChunksConfig(\n                cache_dir=self._cache_dir,\n                remote_input_dir=self._remote_input_dir,\n                serializers=self._serializers,\n                compression=self._compression,\n                item_loader=self._item_loader,\n            )\n        except Exception as e:\n            logger.warning(f\"Couldn't load the config: {e}\")\n            return None\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not set.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index is not an instance of ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        if self._prepare_thread is None:\n            raise Exception(\"The prepare thread isn't defined.\")\n\n        # Get the chunk index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # If the chunk is not available, download it\n        if chunk_index != self._last_chunk_index:\n            self._prepare_thread.download([chunk_index])\n            self._last_chunk_index = chunk_index\n\n        # Get the chunk filepath\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n\n        # Load the chunk\n        return self._item_loader.load(chunk_index, chunk_filepath)\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if _is_distributed():\n        return _get_distributed_map().set_and_get(key, obj)\n    else:\n        return obj\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of chunks per rank\n    num_chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Get the number of chunks per node\n    num_chunks_per_node = np.array(num_chunks_per_rank)\n    num_chunks_per_node = num_chunks_per_node.reshape(distributed_env.world_size, -1)\n    num_chunks_per_node = num_chunks_per_node.sum(axis=1)\n\n    # Get the number of chunks per node\n    num_chunks_per_node = np.array(num_chunks_per_rank)\n    num_chunks_per_node = num_chunks_per_node.reshape(distributed_env.world_size, -1)\n    num_chunks_per_node = num_chunks_per_node.sum(axis=1)\n\n    # Get the number of chunks per node\n    num_chunks_per_node = np.array(num_chunks_per_rank)\n    num_chunks_per_node = num_chunks_per_node.reshape(distributed_env.world_size, -1)\n    num_chunks_per_node = num_chunks_per_node.sum(axis=1)\n\n    # Get the number of chunks per node\n    num_chunks_per_node = np.array(num_chunks_per_rank)\n    num_chunks_per_node = num_chunks_per_node.reshape(distributed_env.world_size, -1)\n    num_chunks_per_node = num_chunks_per_node.sum(axis=1)\n\n    # Get the number of chunks per node\n    num_chunks_per_node = np.array(num_chunks_per_rank)\n    num_chunks_per_node = num_chunks_per_node.reshape(distributed_env.world_size, -1)"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Get indexed paths from the first two elements of the input sequence\n    indexed_paths = _get_indexed_paths(inputs[0])\n    indexed_paths.update(_get_indexed_paths(inputs[1]))\n\n    # Check if the indexed paths are valid\n    if len(indexed_paths) == 0:\n        return None\n\n    # Get the first valid path\n    first_path = indexed_paths[0]\n\n    # Get the project root\n    project_root = Path(__file__).parent.parent.parent\n\n    # Get the depth of the input directory\n    depth = len(first_path.split(\"/\"))\n\n    # Get the input directory\n    input_dir = os.path.join(project_root, *first_path.split(\"/\")[:depth])\n\n    # Check if the input directory is valid\n    if not os.path.isdir(input_dir):\n        return None\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        os.environ[\"LIGHTNING_CLOUD_DNS_ENABLE\"] = \"1\"\n\n    try:\n        yield\n    finally:\n        if enable:\n            os.environ[\"LIGHTNING_CLOUD_DNS_ENABLE\"] = \"0\"\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items per rank\n    num_items_per_rank = [\n        len(indexes) // distributed_env.world_size\n        if i < len(indexes) % distributed_env.world_size\n        else len(indexes) // distributed_env.world_size + 1\n        for i in range(distributed_env.world_size)\n    ]\n\n    # calculate the number of items to drop\n    num_items_to_drop = 0\n    if drop_last:\n        num_items_to_drop = sum(num_items_per_rank) - len(indexes)\n\n    # calculate the number of items to add\n    num_items_to_add = 0\n    if not drop_last:\n        num_items_to_add = sum(num_items_per_rank) - len(indexes)\n\n    # calculate the number of items to add to each rank\n    num_items_to_add_per_rank = [\n        num_items_to_add // distributed_env.world_size\n        if i < num_items_to_add % distributed_env.world_size\n        else num_items_to_add // distributed_env.world_size + 1\n        for i in range(distributed_env.world_size)\n    ]\n\n    # calculate the number of items to drop from each rank\n    num_items_to_drop_per_rank = [\n        num_items_to_drop // distributed_env.world_size\n        if i < num_items_to_drop % distributed_env.world_size\n        else num_items_to_drop // distributed_env.world_size + 1\n        for i in range(distributed_env.world_size)\n    ]\n\n    # calculate the number of items to add to the first rank\n    num_items_to_add_per_first_rank = num_items_to_add_per_rank[0]\n\n    # calculate the number of items to drop from the first rank\n    num_items_to_drop_per_first_rank = num_items_to_"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.netloc, obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine. Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if isinstance(batch_size, int) and batch_size > 1:\n            inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n\n        data_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not isinstance(inputs, Sequence):\n        raise ValueError(f\"The provided inputs {inputs} isn't a sequence.\")\n\n    if not isinstance(fn, (FunctionType, partial)):\n        raise ValueError(f\"The provided fn {fn} isn't a function.\")\n\n    if not isinstance(output_dir, (str, Dir)):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't a string or a Dir object.\")\n\n    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if not isinstance(weights, (list, tuple)):\n        weights = None\n\n    if not isinstance(num_workers, int):\n        num_workers = _get_default_num_workers()\n\n    if not isinstance(fast_dev_run, (bool, int)):\n        raise ValueError(f\"The provided fast_dev_run {fast_dev_run} isn't a boolean or an integer.\")\n\n    if not isinstance(num_nodes, int):\n        num_nodes = None\n\n    if not isinstance(machine, str):\n        machine = None\n\n    if not isinstance(num_downloaders, int):\n        num_downloaders = None\n\n    if not isinstance(num_uploaders, int):\n        num_uploaders = None\n\n    if not isinstance(reorder_files, bool):\n        raise ValueError(f\"The provided reorder_files {reorder_files} isn't a boolean.\")\n\n    if not isinstance(error_when_not_empty, bool):\n        raise ValueError(f\"The provided error_when_not_empty {error_when_not_empty} isn't a boolean.\")\n\n    if not isinstance(reader, (BaseReader, type(None))):\n        raise ValueError(f\"The provided reader {reader} isn't a BaseReader.\")\n\n    if not isinstance(batch_size, int):\n        batch_size = None\n\n    if _IS_IN_STUDIO:\n        # TODO: Add"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir.netloc)\n\n    while True:\n        try:\n            task = queue_in.get(block=True, timeout=1)\n        except Empty:\n            continue\n\n        if task is None:\n            break\n\n        index, files = task\n\n        for file in files:\n            obj = parse.urlparse(file)\n            obj = parse.ParseResult(\n                scheme=obj.scheme,\n                netloc=obj.netloc,\n                path=obj.path,\n                params=obj.params,\n                query=obj.query,\n                fragment=obj.fragment,\n            )\n\n            file_path = os.path.join(cache_dir, obj.path.lstrip(\"/\"))\n            file_dir = os.path.dirname(file_path)\n\n            if not os.path.exists(file_dir):\n                os.makedirs(file_dir)\n\n            if not os.path.exists(file_path):\n                _wait_for_file_to_exist(s3, obj)\n                s3.download_file(obj.path, file_path)\n\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(r, tuple):\n            tmp_dir, path = r\n        else:\n            path = r\n\n        # 4. Check whether the file exists\n        if not os.path.exists(path):\n            continue\n\n        # 5. Check whether the file is already uploaded\n        if output_dir.url is not None and output_dir.path is not None:\n            if output_dir.url:\n                # 6. Wait for the removers to catch up when we are uploading data.\n                _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n            # 7. Upload all the required paths to unblock the current index\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n            elif os.path.isfile(path):\n                if not path.startswith(\"/teamspace/studios/this_studio\"):\n                    s3.client.upload_file(path, output_dir.netloc, output_dir.path.lstrip(\"/\"))\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 8. Inform the worker the current files are available\n        remove_queue.put([path])\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Associate the items to the workers based on number of nodes and node rank.\n    weights = [1] * len(user_items) if weights is None else weights\n\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for workers on the current node.\n    print(f\"Node {node_rank} has {len(worker_ids_this_node)} workers.\")\n    print(f\"The distribution of items to workers on this node is as follows:\")\n    for i in worker_ids_this_node:\n        print(f\"Worker {i} has {len(worker_items[i])} items.\")\n        if file_size:\n            print(f\"The total size of these items is {sum(worker_weights[i]) / 1024 / 1024:.2f} MB.\")\n        else:\n            print(f\"The total weight of these items is {sum(worker_weights[i]):.2f}.\")\n\n    # Shuffle the items for each worker.\n    for i in worker_ids_this_node:\n        random.shuffle(worker_items[i])\n\n    # Return the items for each worker.\n    return [worker_items[i] for i in worker_ids_this_node]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # 1. Calculate the total number of workers across all nodes\n    num_workers_total = num_workers * _get_num_nodes()\n\n    # 2. Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // num_workers_total\n\n    # 3. Calculate the remainder\n    remainder = len(user_items) % num_workers_total\n\n    # 4. Calculate the cumulative sum\n    cumulative_sum = np.cumsum([num_items_per_worker + 1] * remainder + [num_items_per_worker] * (num_workers_total - remainder))\n\n    # 5. Calculate the start and end indices for each worker\n    start_indices = [0] + cumulative_sum[:-1].tolist()\n    end_indices = cumulative_sum.tolist()\n\n    # 6. Calculate the items assigned to each worker\n    items_per_worker = [user_items[start_indices[i]:end_indices[i]] for i in range(num_workers_total)]\n\n    # 7. Ensure the output list has the correct length\n    if len(items_per_worker) != num_workers:\n        raise RuntimeError(f\"The number of workers ({num_workers}) is not equal to the number of workers per node ({num_workers_total}).\")\n\n    return items_per_worker\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        try:\n            item_sizes.append(future.result())\n        except Exception as e:\n            print(e)\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if _IS_IN_STUDIO and input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n        element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"The number of layers must be greater than 0.\"\n        assert n_neurons > 0, \"The number of neurons must be greater than 0.\"\n\n        if self.tcnn:\n            from tinycudann import Network\n\n            if n_layers == 1:\n                return Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                    use_bias=True,\n                    use_bn=False,\n                    use_ln=False,\n                    architecture=\"CNN\",\n                    n_hidden_layers=0,\n                    use_dropout=False,\n                    dropout_probability=0.0,\n                    use_kaiming_normal=True,\n                    use_xavier_normal=False,\n                    seed=self._get_seed(),\n                )\n            else:\n                return Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                    use_bias=True,\n                    use_bn=False,\n                    use_ln=False,\n                    architecture=\"CNN\",\n                    n_hidden_layers=n_layers - 1,\n                    use_dropout=False,\n                    dropout_probability=0.0,\n                    use_kaiming_normal=True,\n                    use_xavier_normal=False,\n                    seed=self._get_seed(),\n                )\n        else:\n            if n_layers == 1:\n                return nn.Linear(n_input_dims, n_output_dims)\n            else:\n                layers = []\n                layers.append("}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Pad the signal with 2 * kernel_offset elements on each side.\n        signal_padded = np.pad(signal, 2 * kernel_offset, mode=\"edge\")\n\n        # Shift the signal by a range of kernel_offset elements in both directions.\n        signal_shifted = np.array(\n            [signal_padded[i : signal_padded.size - 2 * kernel_offset + i] for i in range(2 * kernel_offset)]\n        )\n\n        # Calculate the median of the shifted signal.\n        signal_median = np.median(signal_shifted, axis=0)\n\n        # Trim the signal median to remove edge effects.\n        signal_median = signal_median[kernel_offset : signal_median.size - kernel_offset]\n\n        return signal_median"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the probe and gallery templates have the same size.\n    if template_probe.size != template_gallery.size:\n        raise MatcherError(\"The probe and gallery templates must have the same size.\")\n\n    # Check if the probe and gallery templates have the same number of channels.\n    if template_probe.channels != template_gallery.channels:\n        raise MatcherError(\"The probe and gallery templates must have the same number of channels.\")\n\n    # Check if the probe and gallery templates have the same number of rows.\n    if template_probe.rows != template_gallery.rows:\n        raise MatcherError(\"The probe and gallery templates must have the same number of rows.\")\n\n    # Check if the probe and gallery templates have the same number of columns.\n    if template_probe.columns != template_gallery.columns:\n        raise MatcherError(\"The probe and gallery templates must have the same number of columns.\")\n\n    # Check if the probe and gallery templates have the same number of masks.\n    if template_probe.masks != template_gallery.masks:\n        raise MatcherError(\"The probe and gallery templates must have the same number of masks.\")\n\n    # Check if the probe and gallery templates have the same number of maskbits.\n    if template_probe.maskbits != template_gallery.maskbits:\n        raise MatcherError(\"The probe and gallery templates must have the same number of maskbits.\")\n\n    # Check if the probe and gallery templates have the same number of irisbits.\n    if template_probe.irisbits != template_gallery.irisbits:\n        raise MatcherError(\"The probe and gallery templates must have the same number of irisbits.\")\n\n    # Check if the probe and gallery templates have the same number of iriscodes.\n    if template_probe.iriscodes != template_gallery.iriscodes:\n        raise MatcherError(\"The probe and gallery templates must have the same number of iriscodes.\")\n\n    # Check if the probe and gallery templates have the same number of half widths."}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the arrays that will contain the starting and ending points of the perpendicular bisectors.\n        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n\n        # Initialize the counter that will keep track of the number of iterations.\n        iteration = 0\n\n        # Initialize the counter that will keep track of the number of point pairs that meet the distance criterion.\n        num_valid_pairs = 0\n\n        # Initialize the counter that will keep track of the number of point pairs that do not meet the distance criterion.\n        num_invalid_pairs = 0\n\n        # Initialize the counter that will keep track of the number of point pairs that have been chosen.\n        num_chosen_pairs = 0\n\n        # Initialize the counter that will keep track of the number of point pairs that have been chosen and meet the distance criterion.\n        num_valid_chosen_pairs = 0\n\n        # Initialize the counter that will keep track of the number of point pairs that have been chosen and do not meet the distance criterion.\n        num_invalid_chosen_pairs = 0\n\n        # Initialize the counter that will keep track of the number of point pairs that have been chosen and do not meet the distance criterion.\n        num_invalid_chosen_pairs = 0\n\n        # Initialize the counter that will keep track of the number of point pairs that have been chosen and do not meet the distance criterion.\n        num_invalid_chosen_pairs = 0\n\n        # Initialize the counter that will keep track of the number of point pairs that have been chosen and do not meet the distance criterion.\n        num_invalid_chosen_pairs = 0\n\n        # Initialize the counter that will keep track of the number of point pairs that have been chosen and do not meet the distance criterion.\n        num_invalid_chosen_pairs = 0\n\n        # Initialize the counter that will keep track of the number of point pairs that have been chosen and do not meet the distance criterion.\n        num_"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.on_before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.on_after_execute(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_dict = json.loads(output)\n            return self.check_type(output_dict, type_definition)\n        except Exception as e:\n            return False\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get function name\n        func_name = func_object.__name__\n\n        # Get function signature\n        signature = inspect.signature(func_object)\n\n        # Get function docstring\n        docstring = inspect.getdoc(func_object)\n\n        # Get function type hints\n        type_hints = get_type_hints(func_object)\n\n        # Get function input and output type hints\n        input_type_hints = {\n            name: type_hints[name]\n            for name in signature.parameters.keys()\n            if name in type_hints\n        }\n        output_type_hints = {\n            name: type_hints[name]\n            for name in signature.parameters.keys()\n            if name in type_hints\n        }\n\n        # Get function input and output class definitions\n        input_class_definitions = {\n            name: get_class_definition(type_hint)\n            for name, type_hint in input_type_hints.items()\n        }\n        output_class_definitions = {\n            name: get_class_definition(type_hint)\n            for name, type_hint in output_type_hints.items()\n        }\n\n        # Get function type\n        output_type_hint = output_type_hints[\"return\"]\n        if get_origin(output_type_hint) == Union:\n            output_type_hint = get_args(output_type_hint)[0]\n        if issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        # Create function description\n        function_description = FunctionDescription(\n            func_name,\n            docstring,\n            input_type_hints,\n            output_type_hints,\n            input_class_definitions,\n            output_class_definitions,\n            function_type,\n        )\n\n        return function_description\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        self.bit_array = self.persistence.load()\n        if len(self.bit_array) != self.size:\n            logging.warning(\"Loaded bit array length does not match expected length. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        indices = self.indices\n        indices[0] = hash1 % self.size\n        indices[1] = hash2 % self.size\n        indices[2] = (hash1 + hash2) % self.size\n        indices[3] = (hash1 + hash2 + 1) % self.size\n        indices[4] = (hash1 + hash2 + 2) % self.size\n        indices[5] = (hash1 + hash2 + 3) % self.size\n        indices[6] = (hash1 + hash2 + 4) % self.size\n        indices[7] = (hash1 + hash2 + 5) % self.size\n        indices[8] = (hash1 + hash2 + 6) % self.size\n        indices[9] = (hash1 + hash2 + 7) % self.size\n        indices[10] = (hash1 + hash2 + 8) % self.size\n        indices[11] = (hash1 + hash2 + 9) % self.size\n        indices[12] = (hash1 + hash2 + 10) % self.size\n        indices[13] = (hash1 + hash2 + 11) % self.size\n        indices[14] = (hash1 + hash2 + 12) % self.size\n        indices[15] = (hash1 + hash2 + 13) % self.size\n        indices[16] = (hash1 + hash2 + 14) % self.size\n        indices[17] = (hash1 + hash2 + 15) % self.size\n        indices[18] = (hash1 + hash2 + 16) % self.size\n        indices[19] = (hash1 + hash2 + 17) % self.size\n        indices[20] = (hash1 + hash2 + 18) % self.size\n        indices[21] = (hash1 + hash2 + 19) % self.size\n        indices[22] = (hash"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if \"distilled_model\" in json_dict:\n            self.distilled_model = config_factory.create(json_dict[\"distilled_model\"])\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.create(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is valid\n        self.check_api_key()\n\n        # Check if the model is valid\n        self.check_model(model)\n\n        # Check if the system message is valid\n        self.check_system_message(system_message)\n\n        # Check if the prompt is valid\n        self.check_prompt(prompt)\n\n        # Check if the additional parameters are valid\n        self.check_parameters(**kwargs)\n\n        # Retry generation up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Generate the response\n                response = self.client.engines.generate_text(\n                    engine=model.model_name,\n                    prompt=system_message + prompt,\n                    **kwargs\n                )\n\n                # Process the response\n                response = self.process_response(response)\n\n                # Return the response\n                return response\n\n            except Exception as e:\n                # Handle the exception\n                self.handle_exception(e, i)\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the matrix must be close to zero\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        model = self.get_model(function_description, llm_parameters)\n\n        # get the prompt\n        prompt = self.get_prompt(args, kwargs, function_description)\n\n        # get the examples\n        examples = self.get_examples(args, kwargs, function_description, model, func_hash)\n\n        # get the examples\n        save_to_finetune = self.get_save_to_finetune(args, kwargs, function_description, model, examples)\n\n        # get the examples\n        is_distilled_model = self.get_is_distilled_model(args, kwargs, function_description, model)\n\n        return prompt, model, save_to_finetune, is_distilled_model\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Check if the input matrix is positive definite\n    if not is_positive_definite(cov):\n        # If not, raise an error\n        raise ValueError(\"The input matrix is not positive definite\")\n\n    # Check if the input matrix is symmetric\n    if not is_symmetric(cov):\n        # If not, raise an error\n        raise ValueError(\"The input matrix is not symmetric\")\n\n    # Check if the input matrix is square\n    if cov.shape[0] != cov.shape[1]:\n        # If not, raise an error\n        raise ValueError(\"The input matrix is not square\")\n\n    # Check if the input matrix is a distance matrix\n    if not is_distance(cov):\n        # If not, raise an error\n        raise ValueError(\"The input matrix is not a distance matrix\")\n\n    # Check if the input matrix is a correlation matrix\n    if not is_correlation(cov):\n        # If not, raise an error\n        raise ValueError(\"The input matrix is not a correlation matrix\")\n\n    # Check if the input matrix is a Cholesky matrix\n    if not is_cholesky_dec(cov):\n        # If not, raise an error\n        raise ValueError(\"The input matrix is not a Cholesky matrix\")\n\n    # Check if the input matrix is a correlation matrix\n    if not is_correlation(cov):\n        # If not, raise an error\n        raise ValueError(\"The input matrix is not a correlation matrix\")\n\n    # Check if the input matrix is a Cholesky matrix\n    if not is_cholesky_dec(cov):\n        # If not, raise an error\n        raise ValueError(\"The input matrix is not a Cholesky matrix\")\n\n    # Check if the input matrix is a correlation matrix\n    if not is_correlation(cov):\n        # If not, raise an error\n        raise ValueError(\"The input matrix is not a correlation matrix\")\n\n    # Check if the input matrix is a Cholesky matrix\n    if not is_cholesky_dec(cov):\n        # If not, raise an error\n        raise ValueError(\"The input matrix is not a Cholesky matrix\")"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, (Boxes, RotatedBoxes, BitMasks)):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    raise ValueError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} must be a 2D array of shape (n_groups, n_assets).\"\n        )\n\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"{names[1]} must be a 1D array of shape (n_equations,).\"\n        )\n\n    if sum_to_one:\n        if not np.allclose(np.sum(groups, axis=1), 1):\n            raise ValueError(\n                f\"{names[0]} must sum to one along the second axis.\"\n            )\n\n    if raise_if_group_missing:\n        if not np.all(np.isin(equations, groups)):\n            raise GroupNotFoundError(\n                f\"{names[1]} contains groups that are not in {names[0]}.\"\n            )\n\n    if not np.all(np.isin(equations, groups)):\n        warnings.warn(\n            f\"{names[1]} contains groups that are not in {names[0]}.\",\n            UserWarning,\n        )\n\n    if np.all(np.isin(equations, groups)):\n        left = np.zeros((equations.shape[0], groups.shape[1]))\n        right = np.zeros((equations.shape[0],))\n\n        for i, equation in enumerate(equations):\n            left[i, np.where(groups == equation)[0]] = 1\n            right[i] = 1\n\n            if sum_to_one:\n                left[i, np.where(groups == equation)[0]] = 1 / len(\n                    np.where(groups == equation)[0]\n                )\n\n        return left, right\n\n    else:\n        raise EquationToMatrixError(\n            f\"None of the groups in {names[1]} are in {names[0]}.\"\n        )\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    # Create new class\n    cls_name = \"InstancesScriptable\"\n    cls_def = f\"\"\""}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        # work around this bug: https://github.com/python-pillow/Pillow/issues/3973\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Convert bounding boxes to XYXY_ABS format\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Note that bbox is a single list of [x0, y0, x1, y1]\n    # and not a list of list as it is in the case of segmentation polygons\n    annotation[\"bbox\"] = transforms.apply_box([bbox])[0]\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Apply transformations to segmentation polygons\n    if \"segmentation\" in annotation:\n        # Transform polygon segmentation to binary mask\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygons\n            polygons = [np.asarray(p).reshape(-1, 2) for p in annotation[\"segmentation\"]]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            mask = mask_util.decode(annotation[\"segmentation\"])\n            mask = transforms.apply_segmentation(mask)\n            assert tuple(mask.shape[:2]) == image_size\n            annotation[\"segmentation\"] = mask\n        else:\n            raise ValueError(\n                \"Cannot transform segmentation of type '{}'!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict.\".format(type(annotation[\"segmentation\"]))\n            )\n\n    # Apply transformations to keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.asarray(coords, dtype=np.float32)\n        # This assumes that the center of the image is the\n        # coordinate origin\n        center_shift = np.array([self.center[0], self.center[1]])\n        coords = coords - center_shift\n\n        # Apply the rotation matrix to the coordinates\n        coords = np.dot(self.rm_coords, coords.T).T\n\n        # Shift the coordinates back so that the origin is the\n        # coordinate origin\n        coords = coords + center_shift\n\n        return coords\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Run the model once to get the trace\n    trace = FlopCountAnalysis(model, inputs).traced_model\n\n    # Get the total number of flops\n    total_flops = flop_count(trace, inputs)\n\n    # Get the total number of activations\n    total_activations = activation_count(trace, inputs)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count(model)\n\n    # Get the total number of parameters\n    total_params = parameter_count"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if interp is None:\n            interp = self.interp\n        if not interp in (cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_LANCZOS4):\n            raise ValueError(\"Invalid interpolation method {} specified. Valid methods are cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_LANCZOS4\".format(interp))\n        if img.size < 1:\n            return img\n        if self.angle % 360 == 0:\n            return img\n\n        img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n            ]\n            alpha = 0.8\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.img = self._create_grayscale_image(\n                (predictions.pred_masks.any(dim=0) > 0).numpy()\n            )\n            alpha = 0.3\n\n        self.overlay_instances(\n            masks=masks,\n            boxes=boxes,\n            labels=labels,\n            keypoints=keypoints,\n            assigned_colors=colors,\n            alpha=alpha,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the canvas as a numpy array\n        canvas_out = self.canvas.get_renderer().tostring_rgb()\n        canvas_out = np.fromstring(canvas_out, dtype=\"uint8\").reshape(\n            self.height, self.width, 3\n        )\n\n        # Convert from RGBA to RGB\n        canvas_out = canvas_out[:, :, :3]\n\n        return canvas_out\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", [])\n\n        # draw segmentation masks if in the dict\n        if \"segmentation\" in annos[0]:\n            masks = [x[\"segmentation\"] for x in annos]\n            # TODO: currently this only supports the RLE format\n            # TODO: currently this only supports binary masks\n            if isinstance(masks[0], list):\n                masks = [x[0] for x in masks]\n            self.draw_binary_masks(masks, alpha=0.5)\n\n        # draw semantic segmentation masks if in the dict\n        if \"sem_seg\" in annos[0]:\n            sem_segs = [x[\"sem_seg\"] for x in annos]\n            self.draw_sem_seg(sem_segs, area_threshold=0)\n\n        # draw panoptic segmentation masks if in the dict\n        if \"panoptic_seg\" in annos[0]:\n            pan_segs = [x[\"panoptic_seg\"] for x in annos]\n            pan_segs_fmt = [\n                panoptic_seg.to_png() if isinstance(panoptic_seg, PanopticSegmentation) else panoptic_seg\n                for panoptic_seg in pan_segs\n            ]\n            segments_info = [x[\"segments_info\"] for x in annos]\n            self.draw_panoptic_seg(pan_segs_fmt, segments_info, area_threshold=0)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # If the mask is in uint8 format, convert it to bool.\n        if binary_mask.dtype == np.uint8:\n            binary_mask = binary_mask > 0\n\n        # If the mask is in bool format, convert it to uint8.\n        if binary_mask.dtype == np.bool:\n            binary_mask = binary_mask.astype(np.uint8)\n\n        # If the mask is in uint8 format, convert it to uint8.\n        if binary_mask.dtype == np.uint8:\n            binary_mask = binary_mask.astype(np.uint8)\n\n        # If the mask is in uint8 format, convert it to uint8.\n        if binary_mask.dtype == np.uint8:\n            binary_mask = binary_mask.astype(np.uint8)\n\n        # If the mask is in uint8 format, convert it to uint8.\n        if binary_mask.dtype == np.uint8:\n            binary_mask = binary_mask.astype(np.uint8)\n\n        # If the mask is in uint8 format, convert it to uint8.\n        if binary_mask.dtype == np.uint8:\n            binary_mask = binary_mask.astype(np.uint8)\n\n        # If the mask is in uint8 format, convert it to uint8.\n        if binary_mask.dtype == np.uint8:\n            binary_mask = binary_mask.astype(np.uint8)\n\n        # If the mask is in uint8 format, convert it to uint8.\n        if binary_mask.dtype == np.uint8:\n            binary_mask = binary_mask.astype(np.uint8)\n\n        # If the mask is in uint8 format, convert it to uint8.\n        if binary_mask.dtype == np.uint8:\n            binary_mask = binary_mask.astype(np.uint8)\n\n        # If the mask is in uint8 format, convert it to uint8.\n        if binary_mask.dtype == np.uint8:\n            binary_mask = binary"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances):\n        raise ValueError(f\"Input object is not of type Instances. Got {type(input)}\")\n    if not isinstance(other, Instances):\n        raise ValueError(f\"Other object is not of type Instances. Got {type(other)}\")\n\n    if size_as_tensor:\n        if not isinstance(input.image_size, torch.Tensor):\n            raise ValueError(\n                f\"Input image_size is not of type torch.Tensor. Got {type(input.image_size)}\"\n            )\n        if not isinstance(other.image_size, torch.Tensor):\n            raise ValueError(\n                f\"Other image_size is not of type torch.Tensor. Got {type(other.image_size)}\"\n            )\n        assert torch.allclose(\n            input.image_size, other.image_size, rtol=rtol, atol=1e-8\n        ), f\"{msg}Mismatch in image_size: {input.image_size} != {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Mismatch in image_size\"\n\n    for input_field, other_field in zip(input.get_fields(), other.get_fields()):\n        assert input_field == other_field, f\"{msg}Mismatch in field name: {input_field} != {other_field}\"\n        assert type(input.get(input_field)) == type(\n            other.get(other_field)\n        ), f\"{msg}Mismatch in field type: {type(input.get(input_field))} != {type(other.get(other_field))}\"\n\n        if isinstance(input.get(input_field), Boxes):\n            assert torch.allclose(\n                input.get(input_field).tensor,\n                other.get(other_field).tensor,\n                rtol=rtol,\n                atol=1e-8,\n            ), f\"{msg}"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box_areas = self.tensor[:, 2] * self.tensor[:, 3]\n        return box_areas\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Extract the ground truth from the proposals\n        gt_classes = [proposal.gt_classes for proposal in proposals]\n        gt_boxes = [proposal.gt_boxes for proposal in proposals]\n\n        # Extract the number of boxes from the ground truth\n        num_gt_boxes = [len(gt_class) for gt_class in gt_classes]\n\n        # Extract the number of boxes from the predictions\n        num_boxes = [len(box_delta) for box_delta in proposal_deltas]\n\n        # Extract the number of classes from the predictions\n        num_classes = [len(score) for score in scores]\n\n        # Calculate the classification loss\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight[\"loss_cls\"], num_boxes, num_classes)\n\n        # Calculate the box regression loss\n        loss_box_reg = smooth_l1_loss(\n            proposal_deltas,\n            gt_boxes,\n            num_boxes,\n            num_gt_boxes,\n            num_classes,\n            self.box2box_transform,\n            self.smooth_l1_beta,\n            self.box_reg_loss_type,\n            self.loss_weight[\"loss_box_reg\"],\n        )\n\n        # Return the losses\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    # Retrieve the tracker name from the configuration\n    name = cfg.TRACKER_NAME\n\n    # Build the tracker head\n    tracker = TRACKER_HEADS_REGISTRY.get(name)(cfg)\n\n    return tracker"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred_boxes\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(image, str):\n            image = cv2.imread(image)\n        if isinstance(image, np.ndarray):\n            image = image.astype(np.float32)\n            image = image.transpose(2, 0, 1)\n            image = image[np.newaxis, :]\n            image = image.astype(np.float32)\n        else:\n            raise Exception(f'Error image: {image}')\n\n        output = self.general_ins(image)\n        if anno_type is None:\n            return output\n        elif isinstance(anno_type, str):\n            assert anno_type in output.keys()\n            return output[anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in output.keys() for tp in anno_type)\n            return {tp: output[tp] for tp in anno_type}\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        query_keywords = query.split()\n        scores = defaultdict(float)\n        for kw in query_keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        clip_angle_threshold = float(clip_angle_threshold)\n        assert clip_angle_threshold >= 0.0\n\n        # Compute the x and y coordinates of the top-left and bottom-right corners\n        corners = torch.empty_like(self.tensor)\n        corners[:, 0] = self.tensor[:, 0] - self.tensor[:, 2] / 2.0\n        corners[:, 1] = self.tensor[:, 1] - self.tensor[:, 3] / 2.0\n        corners[:, 2] = self.tensor[:, 0] + self.tensor[:, 2] / 2.0\n        corners[:, 3] = self.tensor[:, 1] + self.tensor[:, 3] / 2.0\n        corners[:, 4] = self.tensor[:, 4]\n\n        # Identify the indices of the boxes that are nearly horizontal\n        abs_angle_diff = torch.abs(corners[:, 4])\n        is_nearly_horizontal = torch.logical_and(abs_angle_diff <= clip_angle_threshold, abs_angle_diff > 0.0)\n\n        # Clip the boxes\n        corners[:, 0].clamp_(min=0, max=box_size[1])\n        corners[:, 1].clamp_(min=0, max=box_size[0])\n        corners[:, 2].clamp_(min=0, max=box_size[1])\n        corners[:, 3].clamp_(min=0, max=box_size[0])\n\n        # Convert the boxes back to the original representation\n        self.tensor[:, 0] = (corners[:, 0] + corners[:, 2]) / 2.0\n        self.tensor[:, 1] = (corners[:, 1] + corners[:, 3]) / 2.0\n        self.tensor[:, 2] = (corners[:, 2] - corners[:, 0])"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMSEG_LOSSES.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg have been deprecated. Please specify '\n            'them in the model configuration file instead.')\n\n    if 'type' not in cfg:\n        raise TypeError('\"type\" field must be specified in config dict')\n    if 'train_cfg' in cfg and train_cfg is None:\n        warnings.warn(\n            'train_cfg has been deprecated. Please specify it in the model '\n            'configuration file instead.', DeprecationWarning)\n        train_cfg = cfg.pop('train_cfg')\n    if 'test_cfg' in cfg and test_cfg is None:\n        warnings.warn(\n            'test_cfg has been deprecated. Please specify it in the model '\n            'configuration file instead.', DeprecationWarning)\n        test_cfg = cfg.pop('test_cfg')\n\n    assert 'type' in cfg\n    assert cfg['type'] in MMDET_DETECTORS.module_dict or cfg['type'] in DETECTORS.module_dict\n    if cfg['type'] in DETECTORS.module_dict:\n        return DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(gt_annos) == len(dt_annos)\n    num_imgs = len(gt_annos)\n    assert num_imgs > 0\n\n    cat_names = list(label2cat.keys())\n    cat_ids = list(label2cat.values())\n\n    # initialize a dict to store the evaluation results\n    ret_dict = {}\n    for metric in metric:\n        ret_dict['mAP_{}'.format(str(metric))] = 0.0\n        ret_dict['mAR_{}'.format(str(metric))] = 0.0\n\n    for class_name in cat_names:\n        ret_dict['mAP_{}/{}'.format(str(metric), class_name)] = 0.0\n        ret_dict['mAR_{}/{}'.format(str(metric), class_name)] = 0.0\n\n    for i in range(num_imgs):\n        gt_annos[i]['boxes'] = gt_annos[i]['boxes'].astype(np.float32)\n        dt_annos[i]['boxes'] = dt_annos[i]['boxes'].astype(np.float32)\n\n    # compute the AP and AR for each class\n    for class_id in cat_ids:\n        # get the ground truth and detection results for the class\n        gt_class = [gt for gt in gt_annos if gt['class'] == class_id]\n        dt_class = [dt for dt in dt_annos if dt['class'] == class_id]\n\n        # get the number of ground truths and detections\n        num_gt = len(gt_class)\n        num_dt = len(dt_class)\n\n        # if there are no detections or ground truths, skip the class\n        if num_dt == 0 or num_gt == 0:\n            continue\n\n        # sort the detections by confidence\n        dt_class.sort(key=lambda x: x['score'], reverse=True)\n\n        # get the IoU threshold for the class"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_cls = LiDARInstance3DBoxes\n        box_mode = Box3DMode.LIDAR\n    elif box_type == \"Camera\":\n        box_cls = CameraInstance3DBoxes\n        box_mode = Box3DMode.CAM\n    elif box_type == \"Depth\":\n        box_cls = DepthInstance3DBoxes\n        box_mode = Box3DMode.DEPTH\n    else:\n        raise ValueError(f\"Unsupported box_type '{box_type}'\")\n\n    return box_cls, box_mode\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      raise RequestError('must provide messages')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, Message):\n        try:\n          message = Message(**message)\n        except Exception as e:\n          raise TypeError('messages must be a list of Message or dict-like objects') from e\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [message.dict() for message in messages],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model is required')\n\n    if context is None:\n      context = []\n\n    if images is None:\n      images = []\n\n    if options is None:\n      options = {}\n\n    if keep_alive is None:\n      keep_alive = ''\n\n    if isinstance(keep_alive, (int, float)):\n      keep_alive = str(keep_alive)\n\n    if not isinstance(context, Sequence):\n      raise TypeError('context must be a sequence')\n\n    if not isinstance(images, Sequence):\n      raise TypeError('images must be a sequence')\n\n    if not isinstance(options, Mapping):\n      raise TypeError('options must be a mapping')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be an empty string or \"json\"')\n\n    if not isinstance(keep_alive, str):\n      raise TypeError('keep_alive must be a string')\n\n    if not isinstance(prompt, str):\n      raise TypeError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise TypeError('system must be a string')\n\n    if not isinstance(template, str):\n      raise TypeError('template must be a string')\n\n    if not isinstance(model, str):\n      raise TypeError('model must be a string')\n\n    if not isinstance(images, Sequence):\n      raise TypeError('images must be a sequence')\n\n    if not isinstance(options, Mapping):\n      raise TypeError('options must be a mapping')\n\n    if not isinstance(keep_alive, str):\n      raise TypeError('keep_alive must be a string')\n\n    if not isinstance(prompt, str):\n      raise TypeError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise TypeError('system must be a string')\n\n    if not isinstance(template, str):\n      raise TypeError('template must be a string')\n\n    if not isinstance(model, str):\n      raise TypeError('model must be a"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide a model file or path')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': _encode_file(modelfile),\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n    if not path.exists():\n      raise FileNotFoundError(f'{path} does not exist')\n\n    with open(path, 'rb') as f:\n      digest = sha256(f.read()).hexdigest()\n\n    url = f'{self._client.base_url}/api/blobs/{digest}'\n    response = self._client.head(url)\n    if response.status_code == 200:\n      return f'sha256:{digest}'\n\n    with open(path, 'rb') as f:\n      response = self._client.post(url, data=f)\n\n    if response.status_code != 201:\n      raise ResponseError(response.text, response.status_code)\n\n    return f'sha256:{digest}'\n\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the user code\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", encoding=\"utf-8\", delete=False\n        ) as user_code_file:\n            user_code_file.write(user_code)\n            user_code_file.close()\n\n        # Create a temporary file to store the test code\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", encoding=\"utf-8\", delete=False\n        ) as test_code_file:\n            test_code_file.write(test_code)\n            test_code_file.close()\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", encoding=\"utf-8\", delete=False\n        ) as combined_code_file:\n            combined_code_file.write(user_code)\n            combined_code_file.write(\"\\n\")\n            combined_code_file.write(test_code)\n            combined_code_file.close()\n\n        # Run Pyright on the combined code\n        pyright_process = subprocess.run(\n            [\"pyright\", combined_code_file.name],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            encoding=\"utf-8\",\n        )\n\n        # Clean up the temporary files\n        user_code_file.close()\n        test_code_file.close()\n        combined_code_file.close()\n        Path(user_code_file.name).unlink()\n        Path(test_code_file.name).unlink()\n        Path(combined_code_file.name).unlink()\n\n        # Parse the output of Pyright\n        pyright_output = pyright_process.stdout\n        pyright_error_messages = []\n        for line in pyright_output.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match."}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, get_compiler_fn(\"Forward pass:\"))\n    else:\n        return aot_function(fn, get_compiler_fn(\"Forward pass:\"))\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary file\n    summary_df = load_summary_file(trial_path)\n\n    # Load the config file\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    # Convert the summary file to a dictionary\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    # Save the config file\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n\n    return config_dict\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Check if the function is a module\n    if isinstance(func, torch.nn.Module):\n        # If so, use the module's forward method\n        func = func.forward\n\n    # Define the wrapper function\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Acquire the lock\n        with wrapper.lock:\n            # Check if the function is in the cache\n            if wrapper.cache.get(args, None) is None:\n                # If not, trace the function\n                traced_module = better_trace(func, args, **kwargs_)\n                # If a compiler is provided, use it to compile the traced module\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module)\n                # Add the traced module to the cache\n                wrapper.cache[args] = traced_module\n            else:\n                # If it is, get it from the cache\n                traced_module = wrapper.cache[args]\n\n        # Return the traced module\n        return traced_module\n\n    # Initialize the cache\n    wrapper.cache = {}\n    # Initialize the lock\n    wrapper.lock = threading.Lock()\n\n    # Return the wrapper function\n    return wrapper\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        summary_path = os.path.join(trial_path, 'summary.csv')\n        if not os.path.exists(summary_path):\n            raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n        trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            config_dict = yaml.safe_load(f)\n        yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n        return cls(yaml_dict, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it does not exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a list of tuples of the form (module, params)\n    module_tuples = list(zip(modules, module_params))\n\n    # Create a list of tuples of the form (module, params, speed_threshold)\n    module_tuples_with_thresholds = []\n    for module, params in module_tuples:\n        if \"speed_threshold\" in strategies:\n            module_tuples_with_thresholds.append((module, params, strategies[\"speed_threshold\"]))\n        else:\n            module_tuples_with_thresholds.append((module, params, None))\n\n    # Create a list of tuples of the form (module, params, speed_threshold, speed_threshold_name)\n    module_tuples_with_thresholds_and_names = []\n    for module, params, speed_threshold in module_tuples_with_thresholds:\n        if \"speed_threshold_names\" in strategies:\n            module_tuples_with_thresholds_and_names.append((module, params, speed_threshold, strategies[\"speed_threshold_names\"]))\n        else:\n            module_tuples_with_thresholds_and_names.append((module, params, speed_threshold, None))\n\n    # Create a list of tuples of the form (module, params, speed_threshold, speed_threshold_name, speed_threshold_result)\n    module_tuples_with_thresholds_and_names_and_results = []\n    for module, params, speed_threshold, speed_threshold_name in module_tuples_with_thresholds_and_names:\n        if \"speed_threshold_results\" in strategies:\n            module_tuples_with_thresholds_and_names_and_results.append((module, params, speed_threshold, speed_threshold_name, strategies[\""}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # create a directory for the current node line\n    node_line_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # create a directory for the current node line\n    node_line_dir = os.path.join(node_line_dir, \"results\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # create a directory for the current node line\n    node_line_dir = os.path.join(node_line_dir, \"summary\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # create a directory for the current node line\n    node_line_dir = os.path.join(node_line_dir, \"plots\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # create a directory for the current node line\n    node_line_dir = os.path.join(node_line_dir, \"images\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # create a directory for the current node line\n    node_line_dir = os.path.join(node_line_dir, \"tables\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # create a directory for the current node line\n    node_line_dir = os.path.join(node_line_dir, \"figures\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # create a directory for the current node line\n    node_line_dir = os.path.join(node_line_dir, \"figures_2\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'results'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'results', 'raw'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'results', 'processed'), exist_ok=True)\n\n    # create a list of combinations of modules and their parameters\n    module_combinations = make_combinations(modules, module_params)\n\n    # create a list of combinations of strategies and their parameters\n    strategy_combinations = make_combinations(strategies['strategies'], strategies['strategy_params'])\n\n    # create a list of combinations of module and strategy combinations\n    combinations = make_combinations(module_combinations, strategy_combinations)\n\n    # create a list of all possible combinations\n    all_combinations = explode(combinations)\n\n    # create a list of all possible combinations of module and strategy combinations\n    all_combinations = explode(all_combinations)\n\n    # create a list of all possible combinations of module and strategy combinations\n    all_combinations = explode(all_combinations)\n\n    # create a list of all possible combinations of module and strategy combinations\n    all_combinations = explode(all_combinations)\n\n    # create a list of all possible combinations of module and strategy combinations\n    all_combinations = explode(all_combinations)\n\n    # create a list of all possible combinations of module and strategy combinations\n    all_combinations = explode(all_combinations)\n\n    # create a list of all possible combinations of module and strategy combinations\n    all_combinations = explode(all_combinations)\n\n    # create a list of all possible combinations of module and strategy combinations\n    all_combinations = explode(all_combinations)\n\n    # create a list"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        values.extend(extract_values(node, key))\n    return list(set(values))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_model('all-mpnet-base-v2')\n\n    gt_embeddings = list(map(lambda x: embedding_model.encode(x), generation_gt))\n    pred_embedding = embedding_model.encode(pred)\n\n    return max(list(map(lambda x: calculate_cosine_similarity(x, pred_embedding), gt_embeddings)))\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n\n    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up. Returning original image.\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except errors.ModelError:\n        logger.warning(\"GFPGAN face restorer failed. Returning original image.\")\n        return np_image\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except errors.ModelError as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except errors.ModelError as e:\n        logger.error(f\"Error setting up GFPGAN face restorer: {e}\")\n        gfpgan_face_restorer = None\n    except Exception as e:\n        logger.error(f\"Error setting up GFPGAN face restorer: {e}\")\n        gfpgan_face_restorer = None\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector into a quaternion\n  v_quat = jnp.concatenate([jnp.zeros(v.shape[:-1]), v], axis=-1)\n\n  # Apply the rotation\n  v_rotated = multiply(multiply(q, v_quat), conjugate(q))\n\n  # Return the rotated vector\n  return v_rotated[Ellipsis, 1:]\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n  axis = axis / jnp.linalg.norm(axis)\n  s = jnp.sin(angle / 2)\n  c = jnp.cos(angle / 2)\n  return jnp.concatenate([axis * s, [c]], axis=-1)\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    while True:\n        logit_bias[idx] = high\n        topk_words = model.topk(prefix, logit_bias)\n        if idx in topk_words:\n            break\n        high *= 2\n        num_calls += k\n\n    # binary search\n    low = 0\n    mid = (high + low) / 2\n    while high >= low + 1e-8:\n        logit_bias[idx] = mid\n        if idx in model.topk(prefix, logit_bias):\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if method == 'TRILINEAR':\n    return resample_3d_trilinear(data, locations, edge_behavior, constant_values, coordinate_order, half_pixel_center)\n  elif method == 'NEAREST':\n    return resample_3d_nearest(data, locations, edge_behavior, constant_values, coordinate_order, half_pixel_center)\n  else:\n    raise ValueError('Unsupported interpolation method: %s' % method)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, x)\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (-max_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, grad_fn=lambda x, y, x_dot: x_dot / y, x_range=(tiny_val, max_val)(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot * y / x,\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 2:\n    return np.inf\n  elif p == 3:\n    return (np.sqrt(3) + 1) / 2\n  elif p == 4:\n    return np.sqrt(2)\n  elif p == 6:\n    return (np.sqrt(6) + 2) / 3\n  elif p == 8:\n    return np.sqrt(2) * np.sqrt(2)\n  elif p == 12:\n    return (np.sqrt(12) + 3) / 4\n  elif p == 20:\n    return (np.sqrt(20) + 4) / 5\n  elif p == 30:\n    return (np.sqrt(30) + 5) / 6\n  elif p == 60:\n    return (np.sqrt(60) + 10) / 15\n  elif p == 120:\n    return (np.sqrt(120) + 20) / 35\n  elif p == 240:\n    return (np.sqrt(240) + 40) / 75\n  elif p == 480:\n    return (np.sqrt(480) + 80) / 150\n  elif p == 1000:\n    return (np.sqrt(1000) + 200) / 500\n  elif p == 2000:\n    return (np.sqrt(2000) + 400) / 1000\n  elif p == 4000:\n    return (np.sqrt(4000) + 800) / 2000\n  elif p == 8000:\n    return (np.sqrt(8000) + 1600) / 4000\n  elif p == 16000:\n    return (np.sqrt(16000) + 3200)"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n      [1, 1, 1],\n      [-1, -1, 1],\n      [1, -1, -1],\n      [-1, 1, -1]\n    ])\n    base_faces = np.array([\n      [0, 1, 2],\n      [0, 2, 3],\n      [0, 3, 1],\n      [1, 3, 2]\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n      [0, 1, 1],\n      [0, -1, 1],\n      [1, 0, 1],\n      [-1, 0, 1],\n      [1, 1, 0],\n      [1, -1, 0],\n      [-1, 1, 0],\n      [-1, -1, 0],\n      [0, 1, -1],\n      [0, -1, -1],\n      [1, 0, -1],\n      [-1, 0, -1]\n    ])\n    base_faces = np.array([\n      [0, 1, 2],\n      [0, 2, 3],\n      [0, 3, 4],\n      [0, 4, 5],\n      [0, 5, 1],\n      [1, 5, 6],\n      [1, 6, 2],\n      [2, 6, 7],\n      [2, 7, 3],\n      [3, 7, 8],\n      [3, 8, 4],\n      [4, 8, 9],\n      [4, 9, 5],\n      [5, 9, 10],\n      [5, 10, 6],\n      [6, 10, 7],\n      [7, 10, 8],\n      [8, 10, 9"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + jnp.abs(x)),\n      (-tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder", "completion": "  # Handle special cases\n  if p == 1:\n    return x\n  elif p == 0:\n    return jnp.ones_like(x)\n  elif p == -jnp.inf:\n    return jnp.sign(x)\n  elif p == jnp.inf:\n    return jnp.sign(x) * power_ladder_max_output(p)\n\n  # Compute the power ladder\n  x = jnp.abs(x)\n  x = jnp.where(x < 1, x, 1)\n  x = jnp.where(x < 1 / p, x ** p, x - (1 - p) / 2)\n  x = jnp.where(x < 1 / p, x * p, x ** p)\n\n  # Apply pre- and post-multiplications\n  if premult is not None:\n    x = x * premult\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # If a delay is specified, apply the delay to the learning rate\n  if lr_delay_steps > 0:\n    # If the current step is before the delay period, apply a multiplier to the learning rate\n    if step < lr_delay_steps:\n      lr = lr_init * lr_delay_mult\n    # If the current step is after the delay period, apply the normal learning rate\n    else:\n      lr = lr_init\n  # If no delay is specified, apply the normal learning rate\n  else:\n    lr = lr_init\n\n  # Calculate the learning rate decay based on the progression of steps\n  lr = lr * (lr_final / lr_init) ** (step / max_steps)\n\n  return lr\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=jax.random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the translation and rotation components from the screw axis\n  v = screw_axis[0:3]\n  theta = jnp.linalg.norm(screw_axis[0:3])\n  w = screw_axis[3:6]\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = jnp.eye(4)\n  X_taylor = X_taylor.at[:3, :3].add(spin_math.exp_so3(w, eps))\n  X_taylor = X_taylor.at[:3, 3].add(v)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  v_safe = jnp.where(theta > eps, v, 0.0)\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  axis = w_safe / theta_safe\n  W = skew(axis)\n  X = (\n      jnp.eye(4)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n  X = X.at[:3, :3].add(spin_math.exp_so3(w_safe, eps))\n  X = X.at[:3, 3].add(v_safe)\n\n  return jnp.where(theta > eps, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Convert the input axis-angle to a unit vector\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.array([[jnp.cos(axis_angle[2]), -jnp.sin(axis_angle[2]), 0],\n                 [jnp.sin(axis_angle[2]), jnp.cos(axis_angle[2]), 0], [0, 0, 1]])\n\n  # Compute the skew-symmetric matrix\n  W = skew(axis_angle)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = R + jnp.sin(axis_angle[2]) * W + (1 - jnp.cos(axis_angle[2])) * jnp.matmul(W, W)\n\n  return R\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  if pixtocam_ndc is not None:\n    # Convert to NDC space.\n    pix_x_ndc = (pix_x_int - (pixtocam_ndc[0, 2] + 1.0)) / pixtocam_ndc[0, 0]\n    pix_y_ndc = (pix_y_int - (pixtocam_ndc[1, 2] + 1.0)) / pixtocam_ndc[1, 1]\n    pix_z_ndc = xnp.ones_like(pix_x_ndc)\n    pix_x_ndc = xnp.stack([pix_x_ndc, pix_y_ndc, pix_z_ndc], axis=-1)\n    pix_y_ndc = xnp.stack([pix_y_ndc, pix_x_ndc, pix_z_ndc], axis=-1)\n    pix_z_ndc = xnp.stack([pix_z_ndc, pix_z_ndc, pix_x_ndc], axis=-1)\n    pix_x_int = xnp.stack([pix_x_ndc, pix_y_ndc, pix_z_ndc], axis=-1)\n    pix_y_int = xnp.stack([pix_y_ndc, pix_x_ndc, pix_z_ndc], axis=-1)\n\n  pix_x_int = xnp.reshape(pix_x_int, (-1, 1))\n  pix_y_int = xnp.reshape(pix_y_int, (-1, 1))\n  pix_x_int = xnp.broadcast_to(pix_x_int, (len(pix_x_int), 3))\n  pix_y_int = xnp.broadcast_to(pix_y_int, (len(pix_y_int), 3))\n\n  # Convert camera coordinates to world coordinates.\n "}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Compute the bin centers.\n  t_center = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n  # Compute the bin widths.\n  t_width = jnp.diff(t)\n  # Compute the inverse CDF.\n  u = jax.random.uniform(rng, (num_samples,), minval=eps, maxval=1 - eps)\n  t_new = invert_cdf(u, t, w_logits)\n  # Compute the bin indices.\n  i = jnp.searchsorted(cw, t_new)\n  # Compute the bin widths.\n  t_width = jnp.diff(t)\n  # Compute the bin centers.\n  t_center = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n  # Compute the bin widths.\n  t_width = jnp.diff(t)\n  # Compute the bin indices.\n  i = jnp.searchsorted(cw, t_new)\n  # Compute the bin widths.\n  t_width = jnp.diff(t)\n  # Compute the bin centers.\n  t_center = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n  # Compute the bin widths.\n  t_width = jnp.diff(t)\n  # Compute the bin indices.\n  i = jnp.searchsorted(cw, t_new)\n  # Compute the bin widths.\n  t_width = jnp.diff(t)\n  # Compute the bin centers.\n  t_center = (t[Ellipsis, 1:] + t["}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Invert the CDF to get the samples.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Calculate the midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate([jnp.array([domain[0]]), t_midpoints, jnp.array([domain[1]])], axis=-1)\n\n  return t_samples\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  ps = jnp.array(ps)\n  ps = jnp.clip(ps, 0, 100)\n  ps = ps / 100\n  cw = integrate_weights(w)\n  t_new = math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n  return t_new\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF.\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF.\n  pdf_blurred = linspline.blur(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to the new time points.\n  w_resampled = resample(tq, t, pdf_blurred)\n\n  return w_resampled\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the vectors to homogeneous coordinates.\n  homogeneous_vectors = to_homogeneous(vectors)\n\n  # Transform the points.\n  transformed_vectors = matmul(transform, homogeneous_vectors)\n\n  # Convert the transformed points back to non-homogeneous coordinates.\n  return from_homogeneous(transformed_vectors)\n\n"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  utils.assert_valid_stepfun(t, vp)\n\n  # Compute the width of each interval in t.\n  dt = jnp.diff(t)\n\n  # Compute the width of each interval in tp.\n  dtp = jnp.diff(tp)\n\n  # Compute the cumulative sum of the widths of the intervals in tp.\n  cdtp = jnp.cumsum(dtp)\n\n  # Compute the cumulative sum of the widths of the intervals in t.\n  cd = jnp.cumsum(dt)\n\n  # Compute the indices of the intervals in tp that are contained in t.\n  i = jnp.searchsorted(cdtp, cd)\n\n  # Compute the width of the intervals in tp that are contained in t.\n  w = jnp.take(dtp, i)\n\n  # Compute the width of the intervals in t that are contained in tp.\n  wtp = jnp.take(dt, i)\n\n  # Compute the values of the step function at the intervals in tp that are contained in t.\n  v = jnp.take(vp, i)\n\n  # Compute the values of the step function at the intervals in t that are contained in tp.\n  vtp = jnp.take(vp, i - 1)\n\n  # Compute the weighted average of the values of the step function at the intervals in tp that are contained in t.\n  if use_avg:\n    v = v + vtp\n    w = w + wtp\n\n  # Compute the weighted average of the values of the step function at the intervals in t that are contained in tp.\n  v = jnp.where(w > 0, jnp.divide(v, w), 0)\n\n  return v\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the input coordinates.\n  scaled_mean = mean * (2 ** min_deg)\n  scaled_var = var * (2 ** (2 * min_deg))\n\n  # Concatenate the scaled mean and variance.\n  scaled_coords = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply sinusoidal encoding.\n  encoded_coords = math.pos_enc(scaled_coords, min_deg, max_deg)\n\n  return encoded_coords\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    Function returning directional encoding (DE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting DE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis=-1)\n\n  return dir_enc_fn\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(lines)\n    # print(len(lines))\n    # print(len(lines[0]))\n    # print(lines[0])\n    # print(len(lines[1]))\n    # print(lines[1])\n    # print(len(lines[2]))\n    # print(lines[2])\n    # print(len(lines[3]))\n    # print(lines[3])\n    # print(len(lines[4]))\n    # print(lines[4])\n    # print(len(lines[5]))\n    # print(lines[5])\n    # print(len(lines[6]))\n    # print(lines[6])\n    # print(len(lines[7]))\n    # print(lines[7])\n    # print(len(lines[8]))\n    # print(lines[8])\n    # print(len(lines[9]))\n    # print(lines[9])\n    # print(len(lines[10]))\n    # print(lines[10])\n    # print(len(lines[11]))\n    # print(lines[11])\n    # print(len(lines[12]))\n    # print(lines[12])\n    # print(len(lines[13]))\n    # print(lines[13])\n    # print(len(lines[14]))\n    # print(lines[14])\n    # print(len(lines[15]))\n    # print(lines[15])\n    # print(len(lines[16]))\n    # print(lines[16])\n    # print(len(lines[17]))\n    # print(lines[17])\n    # print(len(lines[18]))\n    # print(lines[18])\n    # print(len(lines[19]))\n    # print(lines[19])\n    # print(len(lines[20]))\n    # print(lines[20])\n    # print(len(lines[21]))\n    # print(lines[21])\n    # print(len(lines[22]))\n    # print(lines["}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If the input text is empty or None, return the input as is\n    if not org_texts:\n        return org_texts\n\n    # Normalize quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # Tokenize the text into sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # Apply the rules to the sentences\n    for i, sentence in enumerate(sentences):\n        # Apply the rules to the sentences\n        for rule, replaced in rules:\n            sentence = rule.sub(replaced, sentence)\n\n        # Remove any space between punctuations (.')\n        sentence = space_rule.sub(r'\\1', sentence)\n\n        # Remove any content inside brackets\n        sentence = bracket_rule.sub(\"\", sentence)\n\n        # Remove any space at the beginning of the sentence\n        sentence = sentence.lstrip()\n\n        # Remove any space at the end of the sentence\n        sentence = sentence.rstrip()\n\n        # Replace multiple spaces with a single space\n        sentence = re.sub(r\"\\s+\", \" \", sentence)\n\n        # Replace multiple new lines with a single new line\n        sentence = re.sub(r\"\\n+\", \"\\n\", sentence)\n\n        # Replace multiple tabs with a single tab\n        sentence = re.sub(r\"\\t+\", \"\\t\", sentence)\n\n        # Replace multiple commas with a single comma\n        sentence = re.sub(r\",+\", \",\", sentence)\n\n        # Replace multiple semicolons with a single semicolon\n        sentence = re.sub(r\";+\", \";\", sentence)\n\n        # Replace multiple colons with a single colon\n        sentence = re.sub(r\":+\", \":\", sentence)\n\n        # Replace multiple dashes with a single dash\n        sentence = re.sub(r\"-+\", \"-\", sentence)\n\n        # Replace multiple question marks with a single question mark\n        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n\n        # Replace multiple exclamation marks with a single exclamation mark\n        sentence = re.sub(r"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Check if token is a string\n        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n\n        # Check if token is in the term dictionary\n        try:\n            term_id = self.term_dict.get_term_id(token)\n        except TermMissingError:\n            return []\n\n        # If key is None, positions are searched across all documents\n        if key is None:\n            # If subset is True, positions are searched within the subset of rows\n            if self.term_mat.subset:\n                doc_ids, positions = self.posns.positions(term_id,\n                                                          doc_ids=self.term_mat.rows)\n                # If subset is False, positions are searched within all rows\n            else:\n                doc_ids, positions = self.posns.positions(term_id)\n\n            # If positions are found, return a list of numpy arrays where each array contains the positions of the given term\n            if len(positions) > 0:\n                return positions\n\n            # If positions are not found, return an empty list\n            else:\n                return []\n\n        # If key is not None, positions are searched within the specified document\n        else:\n            # Check if key is an integer\n            if not isinstance(key, int):\n                raise TypeError(\"Expected an integer\")\n\n            # Check if key is a valid document key\n            if key < 0 or key >= len(self):\n                raise ValueError(\"Invalid document key\")\n\n            # If subset is True, positions are searched within the subset of rows\n            if self.term_mat.subset:\n                if key in self.term_mat.rows:\n                    doc_ids, positions = self.posns.positions(term_id,\n                                                              doc_ids=[key],\n                                                              is_encoded=True)\n\n                    # If positions are found, return a list of numpy arrays where each array contains the positions of the given term\n                    if len(positions) > 0:\n                        return positions\n\n                    # If positions are not found, return an empty list\n                    else"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # If the 'mm' spec is an absolute number, return it.\n    if spec.isdigit():\n        return int(spec)\n\n    # If the 'mm' spec is a percentage, return the corresponding number of clauses.\n    if spec[-1] == '%':\n        return int(num_clauses * float(spec[:-1]) / 100)\n\n    # If the 'mm' spec is a conditional expression, parse it.\n    if spec[0] == '<':\n        # Parse the condition.\n        condition = spec[1:].split('<')\n        if len(condition) != 2:\n            raise ValueError(f'Invalid conditional expression: {spec}')\n        condition = [int(c) if c.isdigit() else c for c in condition]\n\n        # Parse the condition's value.\n        if condition[0].isdigit():\n            value = int(condition[0])\n        elif condition[0][-1] == '%':\n            value = int(num_clauses * float(condition[0][:-1]) / 100)\n        else:\n            raise ValueError(f'Invalid conditional expression: {spec}')\n\n        # Parse the condition's comparison operator.\n        if condition[1] == 'SHOULD':\n            operator = '>='\n        elif condition[1] == 'MUST':\n            operator = '>'\n        else:\n            raise ValueError(f'Invalid conditional expression: {spec}')\n\n        # Calculate the minimum number of clauses that must match.\n        return eval(f'{value} {operator} num_clauses')\n\n    raise ValueError(f'Invalid \"min should match\" specification: {spec}')\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If slop is 1 and all tokens are unique, we can directly calculate the phrase frequencies using the positions of terms.\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_slop_1_unique_tokens(tokens)\n\n        # Otherwise, we delegate the calculation to another method that handles different slopes or non-unique tokens.\n        return self._phrase_freq_slop_not_1_or_unique_tokens(tokens, slop)\n\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check input arguments\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n        if not callable(tokenizer):\n            raise TypeError(\"Expected a callable object, got {}\".format(type(tokenizer)))\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected a boolean, got {}\".format(type(truncate)))\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected an integer, got {}\".format(type(batch_size)))\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected a boolean, got {}\".format(type(avoid_copies)))\n\n        # Check if the array is too large to fit in memory\n        if not truncate:\n            if len(array) > 1000000000:\n                raise MemoryError(\"Array is too large to fit in memory. \"\n                                  \"Consider using the truncate option.\")\n\n        # Check if the array is too large to fit in memory\n        if not truncate:\n            if len(array) > 1000000000:\n                raise MemoryError(\"Array is too large to fit in memory. \"\n                                  \"Consider using the truncate option.\")\n\n        # Check if the batch size is too large to fit in memory\n        if not truncate:\n            if batch_size > 1000000000:\n                raise MemoryError(\"Batch size is too large to fit in memory. \"\n                                  \"Consider using a smaller batch size.\")\n\n        # Check if the batch size is too large to fit in memory\n        if not truncate:\n            if batch_size > 1000000000:\n                raise MemoryError(\"Batch size is too large to fit in memory. \"\n                                  \"Consider using a smaller batch size.\")\n\n        # Check if the batch size is too large to fit in memory\n        if not truncate:\n            if batch_size > 10000"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.logger)\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr = np.array(arr, dtype=np.uint64)\n    # arr ="}
{"namespace": "searcharray.solr.edismax", "completion": "    if not isinstance(frame, pd.DataFrame):\n        raise ValueError(\"frame must be a pandas DataFrame\")\n\n    if not isinstance(q, str):\n        raise ValueError(\"q must be a string\")\n\n    if not isinstance(qf, list):\n        raise ValueError(\"qf must be a list\")\n\n    if mm is not None and not isinstance(mm, str):\n        raise ValueError(\"mm must be a string\")\n\n    if pf is not None and not isinstance(pf, list):\n        raise ValueError(\"pf must be a list\")\n\n    if pf2 is not None and not isinstance(pf2, list):\n        raise ValueError(\"pf2 must be a list\")\n\n    if pf3 is not None and not isinstance(pf3, list):\n        raise ValueError(\"pf3 must be a list\")\n\n    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(\"q_op must be 'OR' or 'AND'\")\n\n    if not isinstance(similarity, Similarity):\n        raise ValueError(\"similarity must be a Similarity object\")\n\n    if not qf:\n        raise ValueError(\"qf must be a non-empty list\")\n\n    if pf is None:\n        pf = []\n\n    if pf2 is None:\n        pf2 = []\n\n    if pf3 is None:\n        pf3 = []\n\n    if mm is None:\n        mm = \"100%\"\n\n    if q_op == \"OR\":\n        q_op = \"|\"\n    elif q_op == \"AND\":\n        q_op = \"&\"\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    # Parse field boosts\n    query_fields = parse_field_boosts(qf)\n    phrase_fields = parse_field_boosts(pf)\n    bigram_fields = parse_field_boosts(pf2)\n    trigram_fields = parse"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if message.type == MessageType.SEND:\n            self.lock.acquire()\n            try:\n                connection = self.connections.get(message.connection_id, None)\n                if connection is None:\n                    connection = Connection(\n                        multiple_connections=self.config['multipleConnections'],\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                    self.connections[message.connection_id] = connection\n                connection.c2s(message.data)\n            finally:\n                self.lock.release()\n\n        elif message.type == MessageType.RECV:\n            self.lock.acquire()\n            try:\n                connection = self.connections.get(message.connection_id, None)\n                if connection is None:\n                    connection = Connection(\n                        multiple_connections=self.config['multipleConnections'],\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                    self.connections[message.connection_id] = connection\n                message.data = connection.s2c()\n            finally:\n                self.lock.release()\n\n        elif message.type == MessageType.CLOSE:\n            self.lock.acquire()\n            try:\n                connection = self.connections.get(message.connection_id, None)\n                if connection is not None:\n                    connection.close()\n                    del self.connections[message.connection_id]\n            finally:\n                self.lock.release()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying ProxifierMessageInterceptor')\n        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n            self.connections.clear()\n        self.server.stop()\n"}
