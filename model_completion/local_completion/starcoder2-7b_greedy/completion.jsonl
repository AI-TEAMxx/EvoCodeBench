{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table to store the function outputs if it doesn't already exist\n            c.execute(f\"CREATE TABLE IF NOT EXISTS {func_name} (key TEXT PRIMARY KEY, value BLOB)\")\n\n            # Create a hash of the function input arguments\n            key = hashlib.sha256(json.dumps(args + tuple(kwargs.items())).encode()).hexdigest()\n\n            # Check if the function output is already stored in the database\n            c.execute(f\"SELECT value FROM {func_name} WHERE key = ?\", (key,))\n            result = c.fetchone()\n\n            # If the function output is not already stored in the database, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(f\"INSERT INTO {func_name} VALUES (?, ?)\", (key, sqlite3.Binary(json.dumps(result))))\n                conn.commit()\n\n            # Return the function output\n            return json.loads(result[0])\n\n        return wrapper\n\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: Bounding box values are invalid. x_min must be less than x_max and y_min must be less than y_max. Received {values}\")\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of each column in `mat0` and `mat1`\n  sq_norm0 = np.sum(mat0**2, axis=0)\n  sq_norm1 = np.sum(mat1**2, axis=0)\n\n  # Compute the dot product between each column in `mat0` and `mat1`\n  dot_prod = np.dot(mat0, mat1.T)\n\n  # Compute the squared Euclidean distance between each pair of columns in `mat0` and `mat1`\n  sq_dist = sq_norm0[:, None] + sq_norm1[None, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"The 'assets_names' argument must be provided when 'items' is a dictionary.\"\n            )\n        if len(assets_names) != len(items):\n            raise ValueError(\n                f\"The length of 'assets_names' ({len(assets_names)}) must match the number of keys in 'items' ({len(items)}).\"\n            )\n        items = [items.get(asset_name, fill_value) for asset_name in assets_names]\n\n    if not isinstance(items, npt.ArrayLike):\n        raise TypeError(f\"The 'items' argument must be an array-like structure, got {type(items)}.\")\n\n    items = np.asarray(items)\n\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(f\"The 'items' argument must be a 1D array, got {items.ndim}D.\")\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"The 'items' argument must have shape ({n_assets},) for dim=1, got {items.shape}.\"\n            )\n    elif dim == 2:\n        if items.ndim != 2:\n            raise ValueError(f\"The 'items' argument must be a 2D array, got {items.ndim}D.\")\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"The 'items' argument must have shape ({items.shape[0]}, {n_assets}) for dim=2, got {items.shape}.\"\n            )\n    else:\n        raise ValueError(f\"The 'dim' argument must be either 1 or 2, got {dim}.\")\n\n    return items\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data[\"purpose_embedding\"]\n        if isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb0 = 0.0031308 * srgb\n  srgb1 = 1.055 * xnp.maximum(eps, srgb) ** (1 / 2.4) - 0.055\n  return xnp.where(srgb <= 0.04045, srgb0, srgb1)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust spline degree to be at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Interpolate the signal x using a spline of the given degree and smoothness.\n  tck, u_keyframes = scipy.interpolate.splprep(\n      np.stack([t_input, x], axis=-1), k=spline_degree, s=smoothness\n  )\n\n  # Query the interpolated signal at the output times.\n  x_output = scipy.interpolate.splev(t_output, tck)\n\n  return x_output\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if word is all caps\n    if word.isupper():\n        return word\n    # if word is all lowercase\n    elif word.islower():\n        return word\n    # if word is mixed case\n    else:\n        # if first letter is capitalized\n        if word[0].isupper():\n            # if second letter is capitalized\n            if word[1].isupper():\n                # if third letter is capitalized\n                if word[2].isupper():\n                    # if fourth letter is capitalized\n                    if word[3].isupper():\n                        # if fifth letter is capitalized\n                        if word[4].isupper():\n                            # if sixth letter is capitalized\n                            if word[5].isupper():\n                                # if seventh letter is capitalized\n                                if word[6].isupper():\n                                    # if eighth letter is capitalized\n                                    if word[7].isupper():\n                                        # if ninth letter is capitalized\n                                        if word[8].isupper():\n                                            # if tenth letter is capitalized\n                                            if word[9].isupper():\n                                                # if eleventh letter is capitalized\n                                                if word[10].isupper():\n                                                    # if twelfth letter is capitalized\n                                                    if word[11].isupper():\n                                                        # if thirteenth letter is capitalized\n                                                        if word[12].isupper():\n                                                            # if fourteenth letter is capitalized\n                                                            if word[13].isupper():\n                                                                # if fifteenth letter is capitalized\n                                                                if word[14].isupper():\n                                                                    # if sixteenth letter is capitalized\n                                                                    if word[15].isupper():\n                                                                        # if seventeenth letter is capitalized\n                                                                        if word[16].isupper():\n                                                                            # if eighteenth letter is capitalized\n                                                                            if word[17].isupper():\n                                                                                # if nineteenth letter is capitalized\n                                                                                if word[18].isupper():\n                                                                                    # if twentieth letter is capitalized\n                                                                                    if word[19].isupper():\n                                                                                        # if twenty-first letter is capitalized\n                                                                                        if word[20].isupper():\n                                                                                            # if twenty-second letter is capitalized\n                                                                                            if word[21].isupper():\n                                                                                                # if twenty-third letter is capitalized\n                                                                                                if word[22].isupper():\n                                                                                                   "}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary.\")\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(ast.literal_eval)\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the covariance matrix.\n    det = jnp.linalg.det(cov)\n    # Check if the determinant is valid.\n    if det <= 0:\n      raise ValueError('Invalid determinant: {}'.format(det))\n    # Compute the square root of the determinant.\n    sqrt_det = jnp.sqrt(det)\n    # Compute the isotropic covariance matrix.\n    isotropic_cov = cov / sqrt_det\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    # Check if the logarithm of the determinant is valid.\n    if log_det <= -1e-6:\n      raise ValueError('Invalid logarithm of determinant: {}'.format(log_det))\n    # Compute the isotropic covariance matrix.\n    isotropic_cov = jnp.exp(log_det / 2) * cov\n  else:\n    raise ValueError('Invalid mode: {}'.format(mode))\n  return isotropic_cov\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent: A Framework for Task Execution and Planning\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", type=bool, default=False, help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", type=bool, help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file, specifying where to find the configuration settings.\")\n\n    return parser.parse_args"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape != (None, 2):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    char_set = string.ascii_lowercase + string.digits + \"_\"\n\n    # Convert the integer to a binary string\n    binary_str = bin(n)[2:]\n\n    # Pad the binary string with zeros to make it a multiple of 8 bits\n    padded_binary_str = binary_str.rjust(8, \"0\")\n\n    # Convert the padded binary string to a list of characters\n    char_list = [char_set[int(padded_binary_str[i:i + 3], 2)] for i in range(0, len(padded_binary_str), 3)]\n\n    # Join the list of characters into a single string\n    encoded_str = \"\".join(char_list)\n\n    return encoded_str\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    # Initialize the chunk index and current index for each worker\n    chunk_index = {}\n    current_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_index[worker_idx] = 0\n        current_index[worker_idx] = 0\n\n    # Iterate through each worker's intervals\n    for worker_idx in range(len(workers_intervals)):\n        intervals = workers_intervals[worker_idx]\n        for interval in intervals:\n            # Calculate the size of the interval\n            interval_size = interval[1] - interval[0]\n\n            # Update the current index for the worker\n            current_index[worker_idx] += indexes[worker_idx]\n\n            # Update the chunk index for the worker\n            if current_index[worker_idx] >= interval_size:\n                chunk_index[worker_idx] += 1\n                current_index[worker_idx] -= interval_size\n\n    return chunk_index, current_index"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}. Only \"grid\" or \"hash\" are supported.')\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid.\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the integer weights for each vertex of the triangle.\n  weights = np.array([\n    [0, 0, 0],\n    [0, 0, v],\n    [0, v, 0],\n    [0, v, v],\n    [v, 0, 0],\n    [v, 0, v],\n    [v, v, 0],\n    [v, v, v]\n  ])\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights, axis=1, keepdims=True)\n\n  return weights\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(jnp.all(t[Ellipsis, 0] == 0), 'Splines must all start with 0.')\n  checkify.check(jnp.all(t[Ellipsis, -1] == 0), 'Splines must all end with 0.')\n\n  # Find the index of the first time point that is greater than or equal to the query point.\n  # This is the index of the knot that the query point is in.\n  # If the query point is greater than the last time point, then the index is the last time point.\n  # If the query point is less than the first time point, then the index is the first time point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point.\n  # This is done by finding the index of the first time point that is greater than or equal to the query point."}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for value in v:\n            if value <= 0:\n                raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert the origins to NDC.\n  origins_ndc = xnp.matmul(pixtocam, xnp.concatenate([origins, xnp.ones((origins.shape[0], 1))], axis=1).T).T\n  origins_ndc = origins_ndc[:, :3] / origins_ndc[:, 3:]\n\n  # Calculate the directions in NDC.\n  directions_ndc = xnp.matmul(pixtocam, xnp.concatenate([directions, xnp.zeros((directions.shape[0], 1))], axis=1).T).T\n  directions_ndc = directions_ndc[:, :3] / directions_ndc[:, 3:]\n\n  # Adjust the origins to the near plane.\n  origins_ndc = origins_ndc - near * directions_ndc\n\n  return origins_ndc, directions_ndc\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors.\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors.\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Consider a small epsilon to account for numerical precision issues.\n  epsilon = 1e-6\n\n  # Check if the dot product is close to 1 (parallel).\n  is_parallel = jnp.isclose(dot_product, 1.0, atol=epsilon)\n\n  return is_parallel\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = list(jieba.cut(continuation))\n    reference_tokens = list(jieba.cut(reference))\n\n    # Compute the BLEU score\n    bleu_score = evaluate.bleu([continuation_tokens], [reference_tokens], max_n=4, weights=[1, 0, 0, 0])\n\n    # Apply the brevity penalty if requested\n    if with_penalty:\n        brevity_penalty = min(1, math.exp(1 - len(reference_tokens) / len(continuation_tokens)))\n        bleu_score *= brevity_penalty\n\n    return bleu_score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Convert the input vector t into a numpy array.\n  t = np.array(t)\n\n  # Convert the input vector w into a numpy array.\n  w = np.array(w)\n\n  # Calculate the difference between consecutive elements in the input vector t.\n  diff = np.diff(t)\n\n  # Divide the input vector w by the difference between consecutive elements in the input vector t.\n  pdf = w / diff\n\n  # Return the resulting PDF that integrates to 1.\n  return pdf\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for filename in filenames:\n            try:\n                total_size += os.path.getsize(os.path.join(dirpath, filename))\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if val.ndim == 0:\n        val = val.item()\n\n    if val < -offset * period:\n        val += (1 + np.floor((offset * period - val) / period)) * period\n    elif val > (1 - offset) * period:\n        val -= (np.floor((val - (1 - offset) * period) / period)) * period\n\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            purpose_embedding = agent.purpose_embedding.tolist()\n        else:\n            purpose_embedding = agent.purpose_embedding\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is valid\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be a positive integer.\")\n\n    # Check if the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must match.\")\n\n    # Check if the weights are valid\n    for weight in weights:\n        if weight <= 0:\n            raise ValueError(\"Each weight must be a positive integer.\")\n\n    # Create a dictionary to store the items in each bin\n    bins = defaultdict(list)\n\n    # Create a dictionary to store the total weights of each bin\n    bin_weights = defaultdict(int)\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Distribute the items into the bins\n    for item, weight in sorted_items:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n        # Add the item to the bin\n        bins[min_bin].append(item)\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, tuple(sorted(kwargs.items())))\n        return hashlib.sha256(json.dumps(data, sort_keys=True).encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Initialize the total length to zero\n    total_length = 0\n\n    # Iterate over the distances and add them to the total length if they are below the maximum distance\n    for distance in distances:\n        if distance <= max_point_distance:\n            total_length += distance\n\n    return total_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n\n    # Get the largest polygon's area\n    max_area = max(area(polygon) for polygon in polygons)\n\n    # Filter out polygons whose area is below the relative threshold\n    filtered_polygons = [\n        polygon for polygon in polygons if area(polygon) >= max_area * rel_tr or area(polygon) >= abs_tr\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should process\n    samples_per_worker = num_samples_yielded // num_workers\n    samples_remaining = num_samples_yielded % num_workers\n\n    # Create a dictionary to store the number of samples each worker has processed\n    samples_processed = {i: 0 for i in range(num_workers)}\n\n    # Distribute the remaining samples evenly among the workers\n    for i in range(samples_remaining):\n        samples_processed[i] += 1\n\n    # Distribute the remaining samples evenly among the workers\n    for i in range(samples_remaining, num_workers):\n        samples_processed[i] = samples_per_worker\n\n    # Calculate the number of batches each worker should process\n    batches_per_worker = samples_per_worker // batch_size\n    batches_remaining = samples_per_worker % batch_size\n\n    # Distribute the remaining batches evenly among the workers\n    for i in range(batches_remaining):\n        samples_processed[i] += batch_size\n\n    # Distribute the remaining batches evenly among the workers\n    for i in range(batches_remaining, num_workers):\n        samples_processed[i] = batches_per_worker * batch_size\n\n    return samples_processed\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape != (2,):\n        raise ValueError(\"Input array must have shape (_, 2), where _ is the number of points in the polygon.\")\n\n    x = array[0]\n    y = array[1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: [B, M]\n\n    # a: [B, N]\n    # v: [B, M, N]\n    # idx_lo: [B, M]\n    # idx_hi: ["}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input points\n  mag_sq = jnp.sum(x**2, axis=-1)\n\n  # Calculate the scaling factor\n  scaling_factor = 1 / (1 + mag_sq)\n\n  # Scale the input points towards the origin\n  return x * scaling_factor[..., None]\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1000:\n        return f\"{num_bytes} B\"\n    for suffix in _FORMAT_TO_RATIO:\n        if num_bytes < _FORMAT_TO_RATIO[suffix]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[suffix]} {suffix}\"\n    return f\"{num_bytes / _FORMAT_TO_RATIO['pb']} PB\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that array has `nb_dimensions` dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return validator\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arctan2(jnp.sqrt(x**2 + y**2), z)\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']['f']\n    return score\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate the object using a fallback method.\n    try:\n        return pydoc.locate(name.replace(\".\", \"_\"))\n    except ImportError:\n        pass\n\n    # If the object cannot be located by either method, raise an exception.\n    raise ImportError(f\"Could not locate object with name {name}.\")"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the weights are valid\n    if not isinstance(weights, tuple):\n        raise TypeError(\"Weights must be a tuple of floats.\")\n    if len(weights) != len(ids) or len(weights) != len(scores):\n        raise ValueError(\"The length of the weights tuple must match the length of the ids and scores tuples.\")\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Check if the top_k is valid\n    if not isinstance(top_k, int):\n        raise TypeError(\"Top_k must be an integer.\")\n    if top_k <= 0:\n        raise ValueError(\"Top_k must be greater than 0.\")\n\n    # Check if the ids and scores are valid\n    if not isinstance(ids, tuple):\n        raise TypeError(\"Ids must be a tuple of lists.\")\n    if not isinstance(scores, tuple):\n        raise TypeError(\"Scores must be a tuple of lists.\")\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of the ids and scores tuples must match.\")\n    for id_list in ids:\n        if not isinstance(id_list, list):\n            raise TypeError(\"Each list in the ids tuple must be a list.\")\n    for score_list in scores:\n        if not isinstance(score_list, list):\n            raise TypeError(\"Each list in the scores tuple must be a list.\")\n\n    # Check if the ids and scores are valid\n    for id_list in ids:\n        if len(id_list) != len(scores[0]):\n            raise ValueError(\"The length of each list in the ids tuple must match the length of the first list in the scores tuple.\")\n    for score_list in scores:\n        if len(score_list) != len(ids[0]):\n            raise ValueError(\"The length of each list in the scores tuple must match the length of the first list in the ids tuple.\")\n\n    # Normalize the scores\n    normalized_scores = []\n    for score_list in scores:\n        normalized_scores.append([score / sum(score_list) for score in score_list])\n\n    # Perform the fusion\n    fused_scores = []\n    for i in range(len(normalized_scores[0])):\n        fused_scores.append(sum([normalized_scores[j][i] * weights[j] for j in range"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n        return f\"{x:.2f}%\"\n    if x == 0:\n        return \"0\"\n    if x == 1:\n        return \"1\"\n    if x == -1:\n        return \"-1\"\n    if x == 0.5:\n        return \"0.5\"\n    if x == -0.5:\n        return \"-0.5\"\n    if x == 0.25:\n        return \"0.25\"\n    if x == -0.25:\n        return \"-0.25\"\n    if x == 0.125:\n        return \"0.125\"\n    if x == -0.125:\n        return \"-0.125\"\n    if x == 0.0625:\n        return \"0.0625\"\n    if x == -0.0625:\n        return \"-0.0625\"\n    if x == 0.03125:\n        return \"0.03125\"\n    if x == -0.03125:\n        return \"-0.03125\"\n    if x == 0.015625:\n        return \"0.015625\"\n    if x == -0.015625:\n        return \"-0.015625\"\n    if x == 0.0078125:\n        return \"0.0078125\"\n    if x == -0.0078125:\n        return \"-0.0078125\"\n    if x == 0.00390625:\n        return \"0.00390625\"\n    if x == -0.00390625:\n        return \"-0.00390625\"\n    if x == 0.001953125:\n        return \"0.001953125\"\n    if x == -0.001953125:\n        return \"-0.00195312"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / 1024 ** 3\n        if free_space_in_gb < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\"\\n\", \" \")\n    line_text = line_text.replace(\"\\t\", \" \")\n    line_text = line_text.strip()\n    line_text = re.sub(r\"\\s+\", \" \", line_text)\n    line_text = line_text.split(\" \")\n    return line_text\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the total number of weights.\")\n\n    weights = np.random.rand(n)\n    weights /= np.sum(weights)\n\n    if zeros > 0:\n        indices = np.random.choice(n, zeros, replace=False)\n        weights[indices] = 0\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box coordinates from the instance annotation\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n\n    # Convert the bounding box coordinates to the desired mode\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the dimensions of the bounding box\n    width = bbox[2] - bbox[0]\n    height = bbox[3] - bbox[1]\n\n    # Calculate the desired crop size based on the aspect ratio of the bounding box\n    desired_aspect_ratio = crop_size[0] / crop_size[1]\n    desired_width = int(desired_aspect_ratio * height)\n\n    # Calculate the top-left corner of the crop\n    crop_x = int(center_x - desired_width / 2)\n    crop_y = int(center_y - height / 2)\n\n    # Adjust the crop coordinates to ensure they fit within the image boundaries\n    crop_x = max(0, crop_x)\n    crop_y = max(0, crop_y)\n    crop_x = min(image_size[0] - crop_size[0], crop_x)\n    crop_y = min(image_size[1] - crop_size[1], crop_y)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_x, crop_y, crop_size[0], crop_size[1])\n\n    return crop_transform\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of the input vector along the last axis.\n  norm_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the norm to a minimum value to prevent division by zero during the backward pass.\n  norm_sq = jnp.maximum(norm_sq, grad_eps)\n\n  # Calculate the inverse of the norm.\n  inv_norm = 1.0 / jnp.sqrt(norm_sq)\n\n  # Normalize the input vector along the last axis.\n  return x * inv_norm\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Extract the agent information from the response string\n        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n\n        # Split the agent information into the agent name and input text (if present)\n        agent_name, input_text = agent_info.split(\":\") if \":\" in agent_info else (agent_info, \"\")\n\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if not annos:\n        return Instances(image_size)\n\n    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            # PolygonMasks\n            masks = [\n                PolygonMasks(obj[\"segmentation\"], image_size) for obj in annos if obj[\"segmentation\"]\n            ]\n        elif mask_format == \"bitmask\":\n            # BitMasks\n            masks = [\n                BitMasks(\n                    polygons_to_bitmask(obj[\"segmentation\"], image_size), image_size\n                )\n                for obj in annos\n                if obj[\"segmentation\"]\n            ]\n        else:\n            raise ValueError(\n                \"mask_format must be either 'polygon' or 'bitmask', got {}\".format(mask_format)\n            )\n    else:\n        masks = None\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [\n            Keypoints(obj[\"keypoints\"], image_size) for obj in annos if obj[\"keypoints\"]\n        ]\n    else:\n        keypoints = None\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    instances = Instances(image_size, boxes=boxes, classes=classes, masks=masks, keypoints=keypoints)\n    instances.set(\"gt_classes\", torch.as_tensor(classes))\n    instances.set(\"gt_boxes\", boxes)\n    instances.set(\"gt_masks\", masks)\n    instances.set(\"gt_keypoints\", keypoints)\n    return instances\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\")\n        if data_home is None:\n            data_home = os.path.join(os.path.expanduser(\"~\"), \"skfolio_data\")\n\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n    assert_is_positive_definite(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(module):\n        if hasattr(module, \"training\"):\n            module.training = torch.jit.Final[bool](True)\n\n    model.apply(_freeze_training_mode)\n    yield\n    model.apply(_freeze_training_mode)"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[Any]]) -> Dict[str, List[Any]]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # If the input is a list of strings, it is assumed to be a list of metric names.\n    if isinstance(metrics, list) and isinstance(metrics[0], str):\n        metric_names = metrics\n        metric_params = []\n    # If the input is a list of dictionaries, it is assumed to be a list of metric parameters.\n    elif isinstance(metrics, list) and isinstance(metrics[0], dict):\n        metric_names = [metric['name'] for metric in metrics]\n        metric_params = metrics\n    # If the input is not a list, it is assumed to be a single metric name or metric parameter.\n    else:\n        metric_names = [metrics] if isinstance(metrics, str) else [metric['name'] for metric in metrics]\n        metric_params = metrics if isinstance(metrics, dict) else metrics\n\n    return metric_names, metric_params\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Attempt to automatically determine the inverse of the `fn` function.\n    if fn is contract:\n      fn_inv = inv_contract\n    elif fn is contract3_isoscale:\n      fn_inv = lambda x: contract3_isoscale(x)\n    else:\n      raise ValueError(f'Cannot automatically determine the inverse of the `fn` function. Please provide the `fn_inv` argument.')\n\n  def t_to_s(t):\n    \"\"\"Maps metric distances to normalized distances in the range [0, 1].\"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    return fn(t)\n\n  def s_to_t(s):\n    \"\"\"Maps normalized distances back to metric distances.\"\"\"\n    return fn_inv(s)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n\n  return jnp.stack([x, y, z], axis=-1)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sum = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            if ids[i][j] not in weighted_sum:\n                weighted_sum[ids[i][j]] = 0\n            weighted_sum[ids[i][j]] += scores[i][j] * weights[i]\n\n    # Normalize the weighted sums\n    normalized_weighted_sum = {}\n    for id, weighted_sum in weighted_sum.items():\n        normalized_weighted_sum[id] = weighted_sum / sum(weighted_sum.values())\n\n    # Sort the normalized weighted sums in descending order\n    sorted_weighted_sum = sorted(normalized_weighted_sum.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding normalized weighted sums\n    top_ids = [id for id, _ in sorted_weighted_sum[:top_k]]\n    top_scores = [score for _, score in sorted_weighted_sum[:top_k]]\n\n    return top_ids, top_scores"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jac = jax.jacfwd(fn)(mean)\n\n  # Compute the covariance of the function at the mean\n  fn_cov = jnp.matmul(jac, cov)\n  fn_cov = jnp.matmul(fn_cov, jac.T)\n\n  # Compute the mean of the function at the mean\n  fn_mean = fn(mean)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The input matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Generate the scales\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Reshape the input array to add a new dimension\n  x_reshaped = jnp.reshape(x, x.shape + (1,))\n\n  # Apply the sine function to the input array\n  sin_x = jnp.sin(x_reshaped)\n\n  # Apply the scaling to the sine function\n  scaled_sin_x = jnp.reshape(sin_x * scales, x.shape + (scales.shape[0],))\n\n  # Concatenate the original input with the scaled sine function\n  if append_identity:\n    encoded_x = jnp.concatenate([x_reshaped, scaled_sin_x], axis=-1)\n  else:\n    encoded_x = scaled_sin_x\n\n  return encoded_x\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch.\")\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new configuration object for the Nomic model\n    nomic_config = NomicBertConfig()\n\n    # Copy the original configuration to the new configuration\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.hidden_act = bert_config.hidden_act\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n    nomic_config.pad_token_id = bert_config.pad_token_id\n    nomic_config.bos_token_id = bert_config.bos_token_id\n    nomic_config.eos_token_id = bert_config.eos_token_id\n    nomic_config.sep_token_id = bert_config.sep_token_id\n    nomic_config.cls_token_id = bert_config.cls_token_id\n    nomic_config.mask_token_id = bert_config.mask_token_id\n    nomic_config.pad_token_type_id = bert_config.pad_token_type_id\n    nomic_config.sep_token_type_id = bert_config.sep_token_type_id\n    nomic_config.cls_token_type_id = bert_config.cls_token_type_id\n    nomic_config.mask_"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Set up the shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            use_gl_program(self.point_program)\n        else:\n            use_gl_program(self.mesh_program)\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * 2, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl.GL_UNSIGNED_INT, None)\n        else:\n            raise RuntimeError(f'Unsupported render type: {self.render_type}')\n\n        # Unbind the VAO\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # If the input is a PyTorch tensor, convert it to a numpy array first.\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # If the input is a numpy array, check if it has the correct shape.\n        if isinstance(ptr, np.ndarray):\n            if ptr.ndim == 2:\n                ptr = ptr[..., np.newaxis]\n            if ptr.shape[-1] == 1:\n                ptr = np.concatenate([ptr, ptr, ptr, ptr], axis=-1)  # add alpha channel\n            if ptr.shape[-1] == 3:\n                ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n\n        # If the input is a numpy array, check if it has the correct shape.\n        if isinstance(ptr, np.ndarray):\n            if ptr.ndim != 3:\n                raise ValueError(f'Input array must have 3 dimensions, got {ptr.ndim}')\n            if ptr.shape[-1] not in [3, 4]:\n                raise ValueError(f'Input array must have 3 or 4 channels, got {ptr.shape[-1]}')\n\n        # If the input is a numpy array, check if it has the correct shape.\n        if isinstance(ptr, np.ndarray):\n            if ptr.shape[0] != self.H or ptr.shape[1] != self.W:\n                raise ValueError(f'Input array must have the same shape as the texture, got {ptr.shape}')\n\n        # If the input is a numpy array, check if it has the correct shape.\n        if isinstance(ptr, np.ndarray):\n            if ptr.shape[-1] == 3:\n                ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n\n        # If the input is a numpy array, check if it has the correct shape.\n        if isinstance(ptr, np.ndarray):\n            if ptr.shape[-1] == 3:\n                ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n\n        # If the input is a"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate the input shapes and values\n    assert R.ndim == 3 and R.shape[-2:] == (3, 3), \"R must be a batch of rotation matrices of size (*, 3, 3)\"\n    assert tvec.ndim == 2 and tvec.shape[-1] == 3, \"tvec must be a batch of translation vectors of size (*, 3)\"\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-2:] == (3, 3), \"camera_matrix must be a batch of camera intrinsic matrices of size (*, 3, 3)\"\n    assert image_size.ndim == 2 and image_size.shape[-1] == 2, \"image_size must be a batch of image sizes of size (*, 2)\"\n\n    # Compute the camera position\n    camera_position = -torch.bmm(R, tvec.unsqueeze(-1)).squeeze(-1)\n\n    # Compute the camera rotation\n    camera_rotation = matrix_to_rotation_6d(R)\n\n    # Compute the focal length\n    focal_length = camera_matrix[..., 0, 0] / 2.0\n\n    # Compute the principal point offsets\n    principal_point_offset_x = (image_size[..., 0] - camera_matrix[..., 0, 2]) / 2.0\n    principal_point_offset_y = (image_size[..., 1] - camera_matrix[..., 1, 2]) / 2.0\n\n    # Compute the sensor width\n    sensor_width = camera_matrix[..., 0, 0] * image_size[..., 0] / camera_matrix[..., 0, 0]\n\n    # Compute the near clipping plane distance\n    near_clip_distance = znear\n\n    # Create a tensor to store the camera parameters\n    camera_parameters = torch.zeros(R.shape[0], 10, device=R.device)\n\n    # Set the camera position\n    camera_parameters[..., :3] = camera_position.unsqueeze(-1)\n\n    # Set the camera rotation\n    camera_parameters[..., 3:9] = camera_rotation\n\n    # Set the focal length\n    camera_parameters[..., 9] = focal_length\n\n    # Set the principal point offsets\n    camera_parameters[..., 10"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box for rendering\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program and bind the texture\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)  # number of vertices\n        gl.glBindVertexArray(0)\n\n        # Restore the viewport and scissor box to their original sizes\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # default framebuffer\n        gl.glBlitFramebuffer(0, 0, self.W, self.H, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)  # default framebuffer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values\n    sorted_indices = torch.argsort(t1)\n    t1 = t1[sorted_indices]\n    y1 = y1[sorted_indices]\n\n    # Compute the cumulative sums\n    csum_t1 = torch.cumsum(t1, dim=0)\n    csum_y1 = torch.cumsum(y1, dim=0)\n\n    # Compute the inner and outer measures\n    inner = torch.zeros_like(t0)\n    outer = torch.zeros_like(t0)\n    for i in range(len(t0)):\n        t0_i = t0[i]\n        inner[i] = csum_y1[searchsorted(t1, t0_i)]\n        outer[i] = csum_y1[searchsorted(t1, t0_i) + 1]\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights\n    w_env_up = torch.cat([torch.zeros_like(w_env[..., :1]), torch.cumsum(w_env, dim=-1)], dim=-1)  # 129\n    idx_lo, idx_hi = searchsorted(t_env, t)\n\n    w_env_up_lo = torch.take_along_dim(w_env_up, idx_lo, dim=-1)  # 128\n    w_env_up_hi = torch.take_along_dim(w_env_up, idx_hi, dim=-1)\n\n    w_env_up_lo = torch.where(idx_hi[..., :-1] <= idx_lo[..., 1:], w_env_up_lo[..., 1:], 0)\n    w_env_up_hi = torch.where(idx_hi[..., :-1] <= idx_lo[..., 1:], w_env_up_hi[..., :-1], 0)\n\n    # Calculate the difference between the target weights and the upper envelope weights\n    w_diff = w - w_env_up_lo + w_env_up_hi\n\n    # Calculate the half-quadratic loss\n    loss = 0.5 * w_diff ** 2 / (w + eps)\n\n    # Return the half-quadratic loss\n    return loss\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Calculate the inter-interval loss\n    inter_interval_loss = torch.sum(torch.abs(torch.diff(t, dim=-1)))\n\n    # Calculate the intra-interval loss\n    intra_interval_loss = torch.sum(torch.abs(torch.diff(w, dim=-1)))\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(ps, t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check if the input tensors are valid\n    if t.ndim != w.ndim + 1:\n        raise ValueError(\"t and w must have the same number of dimensions.\")\n    if t.shape[-1] != w.shape[-1] + 1:\n        raise ValueError(\"The last dimension of t must be one greater than the last dimension of w.\")\n    if t.shape[-1] < 2:\n        raise ValueError(\"t must have at least two elements.\")\n    if w.shape[-1] < 1:\n        raise ValueError(\"w must have at least one element.\")\n\n    # Compute the PDF and CDF for each weight vector.\n    pdf = w / torch.sum(w, dim=-1, keepdim=True)\n    cdf = integrate_weights(pdf)\n\n    # Generate samples from the uniform distribution.\n    u = torch.rand(num_samples, device=t.device)\n\n    # Interpolate into the inverse CDF.\n    t_new = interpolate(u, cdf, t)\n\n    # Apply perturbation if requested.\n    if perturb:\n        # Compute the bin widths.\n        bin_widths = t[..., 1:] - t[..., :-1]\n\n        # Generate random perturbations.\n        perturbations = torch.rand_like(t_new) * bin_widths\n\n        # Apply perturbations to the samples.\n        t_new = t_new + perturbations\n\n    # Apply jitter if requested.\n    if single_jitter:\n        # Generate random jitters.\n        jitters = torch.rand_like(t_new) * bin_widths\n\n        # Apply jitters to the samples.\n        t_new = t_new + jitters\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps.\n    dilated_t = torch.nn.functional.max_pool1d(t.unsqueeze(-1), kernel_size=dilation, stride=dilation).squeeze(-1)\n\n    # Clip the dilated time steps to the specified domain.\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the weights corresponding to the dilated time steps.\n    dilated_w = torch.nn.functional.max_pool1d(w.unsqueeze(-1), kernel_size=dilation, stride=dilation).squeeze(-1)\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices of the query times that match a step change time.\n    idx_match = torch.where(torch.isclose(tq, t))[0]\n\n    # Interpolate the values at the query times that don't match a step change time.\n    idx_interp = torch.where(~torch.isclose(tq, t))[0]\n    y_interp = interpolate(tq[idx_interp], t, y)\n\n    # Return the interpolated values and the outside values.\n    return torch.cat([y_interp, torch.full_like(y[idx_match], outside_value)], dim=0)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Check if the weights tensor is a 1D or 2D tensor\n    if w.ndim == 1:\n        # If the weights tensor is 1D, reshape it to a 2D tensor with a single column\n        w = w.reshape(-1, 1)\n\n    # Check if the time tensor is a 1D or 2D tensor\n    if t.ndim == 1:\n        # If the time tensor is 1D, reshape it to a 2D tensor with a single column\n        t = t.reshape(-1, 1)\n\n    # Check if the time tensor and weights tensor have the same number of rows\n    if t.shape[0] != w.shape[0]:\n        raise ValueError(\"The time tensor and weights tensor must have the same number of rows.\")\n\n    # Check if the time tensor and weights tensor have the same number of columns\n    if t.shape[1] != w.shape[1]:\n        raise ValueError(\"The time tensor and weights tensor must have the same number of columns.\")\n\n    # Check if the training fraction is a float between 0 and 1\n    if not isinstance(train_frac, float) or train_frac < 0 or train_frac > 1:\n        raise ValueError(\"The training fraction must be a float between 0 and 1.\")\n\n    # Check if the annealing slope is a positive float\n    if not isinstance(anneal_slope, float) or anneal_slope <= 0:\n        raise ValueError(\"The annealing slope must be a positive float.\")\n\n    # Check if the epsilon value is a float\n    if not isinstance(eps, float):\n        raise ValueError(\"The epsilon value must be a float.\")\n\n    # Calculate the annealing factor based on the training fraction and annealing slope\n    anneal_factor = 1 / (1 + torch.exp(-anneal_slope * (train_frac - 0.5)))\n\n    # Adjust the weights based on the annealing factor\n    adjusted_weights = w * anneal_factor\n\n    # Apply a softmax operation to the adjusted weights to ensure stability\n    adjusted_weights = torch.softmax(adjusted_weights, dim=-1)\n\n    # Add the epsilon value to prevent division by zero and log of zero in computations\n    adjusted_weights += eps\n\n    # Apply a log operation to the"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        return type(batch)(to_cuda(x, device, ignore_list) for x in batch)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # Adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor if necessary\n    if v.ndim != f.ndim:\n        f = f.expand(v.shape[:-1] + f.shape[1:])\n\n    # Gather the vertices corresponding to each face\n    v_gathered = multi_gather(v, f, dim)\n\n    # Compute the cross product of each pair of adjacent vertices to obtain the face normals\n    v_diff = v_gathered[:, :, 1:] - v_gathered[:, :, :-1]\n    v_cross = torch.cross(v_diff[:, :, 0], v_diff[:, :, 1], dim=dim)\n\n    # Reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return v_cross.reshape(v_cross.shape[:-2] + f.shape[1:])\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = torch.as_tensor(batch)[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert camera parameters to tensors\n        H = torch.tensor([self.H], dtype=torch.int32)\n        W = torch.tensor([self.W], dtype=torch.int32)\n        K = torch.tensor(self.K.to_list(), dtype=torch.float32)\n        R = torch.tensor(self.R.to_list(), dtype=torch.float32)\n        T = torch.tensor(self.T.to_list(), dtype=torch.float32)\n        n = torch.tensor([self.n], dtype=torch.float32)\n        f = torch.tensor([self.f], dtype=torch.float32)\n        t = torch.tensor([self.t], dtype=torch.float32)\n        v = torch.tensor([self.v], dtype=torch.float32)\n        bounds = torch.tensor(self.bounds.to_list(), dtype=torch.float32)\n\n        # Convert GUI related elements to tensors\n        origin = torch.tensor(self.origin.to_list(), dtype=torch.float32)\n        world_up = torch.tensor(self.world_up.to_list(), dtype=torch.float32)\n        movement_speed = torch.tensor([self.movement_speed], dtype=torch.float32)\n        movement_force = torch.tensor([self.movement_force], dtype=torch.float32)\n        drag_coeff_mult = torch.tensor([self.drag_coeff_mult], dtype=torch.float32)\n        constant_drag = torch.tensor([self.constant_drag], dtype=torch.float32)\n        mass = torch.tensor([self.mass], dtype=torch.float32)\n        moment_of_inertia = torch.tensor([self.moment_of_inertia], dtype=torch.float32)\n        movement_torque = torch.tensor([self.movement_torque], dtype=torch.float32)\n        angular_friction = torch.tensor([self.angular_friction], dtype=torch.float32)\n        constant_torque = torch.tensor([self.constant_torque], dtype=torch.float32)\n        min_interval = torch.tensor([self"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            agent_dict = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(agent_dict)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarity_scores = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            if len(similarity_scores) == 0:\n                return None, -np.inf\n            else:\n                return self.agents[np.argmax(similarity_scores)], np.max(similarity_scores)\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            self.openai_wrapper,\n            PRIME_PROMPT,\n            PRIME_NAME,\n            PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n        else:\n            return None\n\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: A list of deserialized agents.\n        \"\"\"\n        serialized_agents = self.persistence.load_all_agents()\n        return [AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper) for serialized_agent in serialized_agents]\n\n    def load_all_working_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all working agents from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for serialized_agent in self.persistence.fetch_all_agents():\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent {agent.id} with purpose {agent.purpose}: {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT.format(goal=goal, sample_input=sample_input)\n            completion = self.openai_wrapper.get_completion(prompt)\n            return completion.choices[0].text\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT * FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is None:\n                return None\n            else:\n                return json.loads(row[2])"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = [row[0] for row in cursor.fetchall()]\n            return purposes"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments.\n    CONFIG.update(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal.\n    if quiet_mode:\n        with open(os.path.join(CONFIG.get('record_dir'), 'output.txt'), 'w') as f:\n            with redirect_stdout(f):\n                execute_command_line(args)\n    else:\n        execute_command_line(args)\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                response = json.loads(str(response))\n            else:\n                raise e\n\n        return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method cannot be called from a DataLoader worker process. \"\n                \"Please call the `state_dict` method from the main process.\"\n            )\n\n        if self.distributed_env.world_size > 1:\n            raise RuntimeError(\n                \"The `state_dict` method cannot be called from a distributed environment. \"\n                \"Please call the `state_dict` method from the main process.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path if self.input_dir.path else self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.input_dir.path = state_dict[\"input_dir_path\"]\n        self.input_dir.url = state_dict[\"input_dir_url\"]\n        self.item_loader = BaseItemLoader.from_state_dict(state_dict[\"item_loader\"]) if state_dict[\"item_loader\"] else None\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.distributed_env = _DistributedEnv(state_dict[\"world_size\"])\n        self.shuffle = state_dict[\"shuffle\"]\n\n        self.cache = self._create_cache(worker_env=_WorkerEnv.detect())\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self.worker_env = _WorkerEnv.detect()\n        self.worker_chunks = []\n        self.worker_intervals = []\n\n        chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n            self.distributed_env, self.current_epoch\n        )\n        chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n        intervals_replica = intervals_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n\n        # Handle restart\n        if self._state_dict:\n            self._resume(chunks_replica, intervals_replica)\n        else:\n            chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n                self.distributed_env, self.current_epoch\n            )\n            chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n            intervals_replica = intervals_per_replica[\n                self.distributed_env.global_rank % self.distributed_env.world_size\n            ]\n\n            self.worker_chunks = []\n            self.worker_intervals = []\n\n            for i, (chunk_index, chunk_interval) in enumerate(zip"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is not empty\n        if not self._state_dict:\n            raise ValueError(\"The state dictionary is empty. Please ensure that the state dictionary is not empty before calling this function.\")\n\n        # Check if the state dictionary contains the required keys\n        required_keys = [\"num_samples_yielded\", \"num_workers\", \"batch_size\", \"current_epoch\", \"input_dir_path\", \"input_dir_url\", \"item_loader\", \"drop_last\", \"seed\", \"world_size\", \"shuffle\"]\n        if not all(key in self._state_dict for key in required_keys):\n            raise ValueError(\"The state dictionary does not contain all the required keys. Please ensure that the state dictionary contains all the required keys before calling this function.\")\n\n        # Check if the input directory path matches the state dictionary\n        if self.input_dir.path != self._state_dict[\"input_dir_path\"]:\n            raise ValueError(\"The input directory path in the state dictionary does not match the current input directory path of the StreamingDataset instance. Please ensure that the input directory path in the state dictionary matches the current input directory path of the StreamingDataset instance before calling this function.\")\n\n        # Check if the input directory URL matches the state dictionary\n        if self.input_dir.url != self._state_dict[\"input_dir_url\"]:\n            raise ValueError(\"The input directory URL in the state dictionary does not match the current input directory URL of the StreamingDataset instance. Please ensure that the input directory URL in the state dictionary matches the current input directory URL of the StreamingDataset instance before calling this function.\")\n\n        # Check if the shuffle flag matches the state dictionary\n        if self.shuffle != self._state_dict[\"shuffle\"]:\n            raise ValueError(\"The shuffle flag in the state dictionary does not match the current shuffle flag of the StreamingDataset instance. Please ensure that the shuffle flag in the state dictionary matches the current shuffle flag of the StreamingDataset instance before calling this function.\")\n\n        # Check if the drop_last flag matches the state dictionary\n        if self.drop_last != self._state_dict[\"drop_last\"]:\n            raise ValueError(\"The drop_last flag in the state dictionary does not match the current drop_last flag of the StreamingDataset instance. Please ensure that the drop_last flag in the state dictionary matches the current drop_last flag of the StreamingDataset instance before calling"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        return None\n\n    # If the environment variable DATA_OPTIMIZER_CACHE_DIR is set, use it as the cache directory.\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    # Otherwise, use a default cache directory.\n    else:\n        cache_dir = _DEFAULT_CACHE_DIR\n\n    # Create the cache directory if it does not exist.\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Hash the input directory and create a unique cache directory name.\n    cache_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n    cache_dir_path = os.path.join(cache_dir, cache_dir_name)\n\n    # Create the cache directory if it does not exist.\n    if not os.path.exists(cache_dir_path):\n        os.makedirs(cache_dir_path)\n\n    return cache_dir_path\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Parse the remote file path to get the scheme, netloc, and path\n        parsed_url = parse.urlparse(remote_filepath)\n\n        # Check if the scheme is \"s3\"\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"The remote file path {remote_filepath} is not an S3 URL.\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Create a file lock to prevent multiple processes from downloading the same file\n        lock_path = os.path.join(os.path.dirname(local_filepath), \".lock\")\n        lock = FileLock(lock_path, timeout=10)\n\n        # Attempt to acquire the file lock\n        try:\n            with lock.acquire(timeout=10):\n                # Check if the file already exists\n                if os.path.exists(local_filepath):\n                    return\n\n                # Check if s5cmd is available\n                if self._s5cmd_available:\n                    # Download the file using s5cmd\n                    s5cmd_command = f\"s5cmd cp {remote_filepath} {local_filepath}\"\n                    subprocess.run(s5cmd_command, shell=True, check=True)\n                else:\n                    # Download the file using boto3\n                    self._client.download_file(parsed_url.netloc, parsed_url.path, local_filepath)\n        except Timeout:\n            raise Timeout(\"Failed to acquire file lock within the specified timeout.\")\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize the dictionaries to store the assigned chunks and intervals for each worker.\n    chunks_per_worker = {i: [] for i in range(num_workers)}\n    intervals_per_worker = {i: [] for i in range(num_workers)}\n\n    # Distribute the chunks and intervals across the workers based on the worker index and the total world size.\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        chunks_per_worker[worker_index].append(chunk_index)\n        intervals_per_worker[worker_index].append(chunk_interval)\n\n    return chunks_per_worker, intervals_per_worker\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image):\n            raise TypeError(f\"Expected PIL Image, got {type(item)}\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(f\"Expected PIL Image mode to be a string, got {type(item.mode)}\")\n\n        if not isinstance(item.width, int):\n            raise TypeError(f\"Expected PIL Image width to be an integer, got {type(item.width)}\")\n\n        if not isinstance(item.height, int):\n            raise TypeError(f\"Expected PIL Image height to be an integer, got {type(item.height)}\")\n\n        if not isinstance(item.size, tuple):\n            raise TypeError(f\"Expected PIL Image size to be a tuple, got {type(item.size)}\")\n\n        if not isinstance(item.info, dict):\n            raise TypeError(f\"Expected PIL Image info to be a dict, got {type(item.info)}\")\n\n        if not isinstance(item.encoderinfo, dict):\n            raise TypeError(f\"Expected PIL Image encoderinfo to be a dict, got {type(item.encoderinfo)}\")\n\n        if not isinstance(item.decoderconfig, dict):\n            raise TypeError(f\"Expected PIL Image decoderconfig to be a dict, got {type(item.decoderconfig)}\")\n\n        if not isinstance(item.encoderconfig, dict):\n            raise TypeError(f\"Expected PIL Image encoderconfig to be a dict, got {type(item.encoderconfig)}\")\n\n        if not isinstance(item.mode_size, int):\n            raise TypeError(f\"Expected PIL Image mode_size to be an integer, got {type(item.mode_size)}\")\n\n        if not isinstance(item.mode_bands, int):\n            raise TypeError(f\"Expected PIL Image mode_bands to be an integer, got {type(item.mode_bands)}\")\n\n        if not isinstance(item.mode_mask, int):\n            raise TypeError(f\"Expected PIL Image mode_mask to be an integer, got {type(item.mode_mask)}\")\n\n        if not isinstance(item.mode_alpha, int):\n            raise TypeError(f\"Expected PIL Image mode_alpha to be an integer, got {type(item.mode_alpha)}\")\n\n        if not isinstance(item.mode_palette, int):\n            raise TypeError(f\"Expected PIL"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            # If the item is a JPEG and has a defined filename that exists, it reads the file directly.\n            if item.filename:\n                with open(item.filename, \"rb\") as f:\n                    data = f.read()\n            else:\n                # Otherwise, it converts the item into JPEG format in memory.\n                data = item.tobytes()\n        elif isinstance(item, Image.Image):\n            # If the item is an Image, it converts it into JPEG format in memory.\n            data = item.tobytes()\n        else:\n            # If the item is not a supported image type, it raises an error.\n            raise TypeError(\"Unsupported image type: {}\".format(type(item)))\n\n        return data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_len = ints\n        mode = data[12:12 + mode_len].decode(\"utf-8\")\n        raw = data[12 + mode_len:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array.\n        dtype_indice = np.frombuffer(data[:4], dtype=np.uint32)[0]\n        shape_size = np.frombuffer(data[4:8], dtype=np.uint32)[0]\n        shape = np.frombuffer(data[8:8 + 4 * shape_size], dtype=np.uint32)\n\n        # Reconstruct the tensor from the remaining bytes.\n        tensor_data = np.frombuffer(data[8 + 4 * shape_size:], dtype=_TORCH_DTYPES_MAPPING[dtype_indice])\n        tensor = torch.tensor(tensor_data.reshape(shape), dtype=_TORCH_DTYPES_MAPPING[dtype_indice])\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        if not isinstance(item, torch.Tensor):\n            raise TypeError(f\"The provided item should be of type {torch.Tensor}. Found {item}.\")\n\n        dtype = item.dtype\n        shape = item.shape\n        data = item.numpy()\n\n        # Convert the dtype to an integer index\n        dtype_index = self._dtype_to_indices[dtype]\n\n        # Convert the shape to a tuple of integers\n        shape_tuple = tuple(shape)\n\n        # Convert the data to a bytes object\n        data_bytes = data.tobytes()\n\n        # Create a buffer to store the serialized data\n        buffer = io.BytesIO()\n\n        # Write the dtype index to the buffer\n        buffer.write(dtype_index.to_bytes(1, \"little\"))\n\n        # Write the shape to the buffer\n        buffer.write(len(shape_tuple).to_bytes(4, \"little\"))\n        for dim in shape_tuple:\n            buffer.write(dim.to_bytes(4, \"little\"))\n\n        # Write the data to the buffer\n        buffer.write(data_bytes)\n\n        # Get the serialized data from the buffer\n        serialized_data = buffer.getvalue()\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            try:\n                return Image.open(io.BytesIO(data))\n            except OSError:\n                pass\n\n        raise TypeError(f\"The provided data should be of type bytes. Found {type(data)}.\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data format has not been set. Please call setup() before calling serialize().\")\n\n        tensor_data = item.numpy()\n        tensor_data_bytes = tensor_data.tobytes(order=\"C\")\n        tensor_data_size = len(tensor_data_bytes)\n        tensor_data_size_bytes = np.array([tensor_data_size], dtype=np.uint32).tobytes()\n        tensor_data_size_bytes_size = len(tensor_data_size_bytes)\n        tensor_data_size_bytes_size_bytes = np.array([tensor_data_size_bytes_size], dtype=np.uint32).tobytes()\n        tensor_data_size_bytes_size_bytes_size = len(tensor_data_size_bytes_size_bytes)\n        tensor_data_size_bytes_size_bytes_size_bytes = np.array([tensor_data_size_bytes_size_bytes_size], dtype=np.uint32).tobytes()\n        tensor_data_size_bytes_size_bytes_size_bytes_size = len(tensor_data_size_bytes_size_bytes_size_bytes)\n        tensor_data_size_bytes_size_bytes_size_bytes_size_bytes = np.array([tensor_data_size_bytes_size_bytes_size_bytes_size], dtype=np.uint32).tobytes()\n        tensor_data_size_bytes_size_bytes_size_bytes_size_bytes_size = len(tensor_data_size_bytes_size_bytes_size_bytes_size_bytes)\n        tensor_data_size_bytes_size_bytes_size_bytes_size_bytes_size_bytes = np.array([tensor_data_size_bytes_size_bytes_size_bytes_size_bytes_size], dtype=np.uint32).tobytes()\n        tensor_data_size_bytes_size_bytes_size_bytes_size_bytes_size_bytes_size = len(tensor_data_size_bytes_size_bytes_size_bytes_size_bytes_size_bytes)\n        tensor_data_size_bytes_size_bytes_size"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        tensor = torch.frombuffer(data, dtype=self._dtype)\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = super().state_dict()\n\n        if isinstance(self.dataset, StreamingDataset):\n            state_dict[\"num_samples_yielded\"] = self._num_samples_yielded_streaming\n        else:\n            state_dict[\"num_samples_yielded\"] = self._num_samples_yielded_combined\n\n        state_dict[\"latest_worker_idx\"] = self._latest_worker_idx\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is required to deserialize videos. Please install it using 'pip install av'.\")\n\n        # Create a temporary file to write the video data\n        with tempfile.NamedTemporaryFile(mode=\"wb\") as temp_file:\n            temp_file.write(data)\n            temp_file.flush()\n\n            # Read the video file using torchvision's read_video function\n            video = torch.utils.data.datasets.vision.read_video(temp_file.name)\n\n        return video\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        while self._should_write():\n            self.write_chunk()\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, f) for f in os.listdir(self._cache_dir) if f.endswith(\".bin\")]"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"], self.batch_size)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"], self.batch_size, obj[\"num_samples_yielded\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.current_epoch = obj[\"current_epoch\"]\n        self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return {\n                __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n                __SAMPLES_KEY__: [\n                    dataset.state_dict(num_workers, batch_size) for dataset in self._datasets\n                ],\n            }\n\n        return self._iterator.state_dict(num_workers, batch_size)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)\n            return\n\n        self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__, None)\n        self._datasets = [\n            d.load_state_dict(state_dict[__SAMPLES_KEY__][i]) for i, d in enumerate(self._datasets)\n        ]\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://projects/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://spaces/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://models/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://datasets/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://logs/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The output_dir must be a Dir object, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"The output_dir must start with 's3://', got: {output_dir.url}\")\n\n    if not append and not overwrite:\n        raise ValueError(f\"The append and overwrite arguments must be True or False, got: {append} and {overwrite}\")\n\n    if append:\n        raise NotImplementedError(\"Appending data to the directory is not currently supported.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the directory is not currently supported.\")\n\n    if output_dir.path is None:\n        raise ValueError(f\"The output_dir must have a path, got: {output_dir}\")\n\n    if not os.path.exists(output_dir.path):\n        raise ValueError(f\"The output_dir path does not exist, got: {output_dir.path}\")\n\n    if os.listdir(output_dir.path):\n        raise ValueError(f\"The output_dir path is not empty, got: {output_dir.path}\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    if objects[\"KeyCount\"] > 0:\n        for obj in objects[\"Contents\"]:\n            if obj[\"Key\"] == \"index.json\":\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n    # Delete all objects within the specified prefix in the bucket.\n    if objects[\"KeyCount\"] > 0:\n        s3.delete_objects(\n            Bucket=obj.netloc,\n            Delete={\n                \"Objects\": [\n                    {\"Key\": obj[\"Key\"]} for obj in objects[\"Contents\"] if obj[\"Key\"] != \"index.json\"\n                ],\n                \"Quiet\": False,\n            },\n        )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Check if the index files are available\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        if len(index_files) < num_workers:\n            # Wait for all index files to be available\n            while len(index_files) < num_workers:\n                sleep(1)\n                index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n\n        # Check if the index files are available\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        if len(index_files) < num_workers:\n            raise RuntimeError(\n                f\"The index files are not available. Expected {num_workers} but found {len(index_files)}.\"\n            )\n\n        # Check if the index files are available\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        if len(index_files) < num_workers:\n            raise RuntimeError(\n                f\"The index files are not available. Expected {num_workers} but found {len(index_files)}.\"\n            )\n\n        # Check if the index files are available\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        if len(index_files) < num_workers:\n            raise RuntimeError(\n                f\"The index files are not available. Expected {num_workers} but found {len(index_files)}.\"\n            )\n\n        # Check if the index files are available\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        if len(index_files) < num_workers:\n            raise RuntimeError(\n                f\"The index files are not available. Expected {num_workers} but found {len(index_files)}.\"\n            )\n\n        # Check if the index files are available\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        if len(index_files) < num_workers:\n            raise RuntimeError(\n                f\"The index"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"The `lightning_sdk` package is required to use the `execute` method. \"\n            \"Please install it with `pip install lightning_sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\n            \"The `boto3` package is required to use the `execute` method. \"\n            \"Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(num_nodes=num_nodes)\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && env\"\n\n    job = machine.execute(name=name, command=command)\n\n    print(f\"Job started: {job.url}\")\n\n    while True:\n        job = job.refresh()\n\n        if job.status == \"running\":\n            print(f\"Job running: {job.url}\")\n            sleep(10)\n        elif job.status == \"failed\":\n            raise RuntimeError(f\"Job failed: {job.url}\")\n        elif job.status == \"succeeded\":\n            print(f\"Job succeeded: {job.url}\")\n            break\n        else:\n            raise RuntimeError(f\"Job status unknown: {job.url}\")\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n        )\n\n        if config.is_valid():\n            self._config = config\n            return config\n        return None\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.put(chunk_indexes)\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not set. Please set it before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be a ChunkedIndex, got {type(index)}\")\n\n        if self._config is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check whether the chunk is already downloaded\n        chunk_index = self._get_chunk_index_from_index(index)\n        if chunk_index == self._last_chunk_index:\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(chunk_index):\n            self._item_loader.pre_load_chunk(chunk_index, self._config[chunk_index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(chunk_index):\n            self._item_loader.pre_load_chunk(chunk_index, self._config[chunk_index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(chunk_index):\n            self._item_loader.pre_load_chunk(chunk_index, self._config[chunk_index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(chunk_index):\n            self._item_loader.pre_load_chunk(chunk_index, self._config[chunk_index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(chunk_index):\n            self._item_loader.pre_load_chunk(chunk_index, self._config[chunk_index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(chunk_index):\n            self._item_loader.pre_load_chunk(chunk_index, self._config[chunk_index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    return _ImmutableDistributedMap().set_and_get(key, obj)\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes and the world size\n    num_nodes = distributed_env.num_nodes\n    world_size = distributed_env.world_size\n\n    # Get the number of chunks per node\n    num_chunks_per_node = len(chunks_per_ranks[0])\n\n    # Create a list of shuffled chunk indexes for each node\n    shuffled_chunks_per_node = []\n    for node_id in range(num_nodes):\n        # Get the chunk indexes for the current node\n        node_chunks = chunks_per_ranks[node_id]\n\n        # Create a list of shuffled chunk indexes for the current node\n        shuffled_node_chunks = []\n\n        # Set the seed for the current node\n        np.random.seed(seed + node_id + current_epoch * world_size)\n\n        # Shuffle the chunk indexes for the current node\n        np.random.shuffle(node_chunks)\n\n        # Add the shuffled chunk indexes to the list\n        shuffled_node_chunks.extend(node_chunks)\n\n        # Add the shuffled chunk indexes for the current node to the list of shuffled chunk indexes for all nodes\n        shuffled_chunks_per_node.append(shuffled_node_chunks)\n\n    # Flatten the list of shuffled chunk indexes for all nodes\n    shuffled_chunks = [chunk for node_chunks in shuffled_chunks_per_node for chunk in node_chunks]\n\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = {}\n\n    for input_ in inputs:\n        if isinstance(input_, str) and os.path.exists(input_):\n            indexed_paths = _get_indexed_paths(input_)\n            break\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 1:\n        raise ValueError(\n            \"Multiple input directories found. Please specify a single input directory.\"\n        )\n\n    input_dir = list(indexed_paths.values())[0]\n\n    if not os.path.exists(input_dir):\n        raise ValueError(f\"Input directory {input_dir} does not exist.\")\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        try:\n            # Enable DNS optimization\n            os.environ[\"DATA_OPTIMIZER_DNS_OPTIMIZATION\"] = \"1\"\n            yield\n        finally:\n            # Disable DNS optimization\n            os.environ[\"DATA_OPTIMIZER_DNS_OPTIMIZATION\"] = \"0\"\n    else:\n        yield\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank - (len(indexes) % distributed_env.world_size)\n\n    # distribute the chunks and their intervals across the ranks\n    chunks_per_ranks = []\n    chunk_intervals_per_ranks = []\n    for rank in range(distributed_env.world_size):\n        chunks_per_rank = []\n        chunk_intervals_per_rank = []\n        for i in range(num_items_per_rank):\n            chunk_index = indexes[rank * num_items_per_rank + i]\n            chunk_interval = chunk_intervals[chunk_index]\n            chunks_per_rank.append(chunk_index)\n            chunk_intervals_per_rank.append(chunk_interval)\n        chunks_per_ranks.append(chunks_per_rank)\n        chunk_intervals_per_ranks.append(chunk_intervals_per_rank)\n\n    return chunks_per_ranks, chunk_intervals_per_ranks\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = self._device if self._device else \"cpu\"\n            self._fn(item_metadata, output_dir, device=device, is_last=is_last)\n        elif self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.netloc, Key=obj.path[1:])\n            return response\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File {obj.path} not found. Retrying in {sleep_time} seconds.\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not isinstance(inputs, (list, tuple)):\n        raise ValueError(f\"The provided inputs {inputs} isn't supported.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs {inputs} is empty.\")\n\n    if not isinstance(output_dir, (str, Dir)):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't supported.\")\n\n    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if not isinstance(fn, (FunctionType, partial)):\n        raise ValueError(f\"The provided fn {fn} isn't supported.\")\n\n    if not isinstance(weights, (list, tuple)) and weights is not None:\n        raise ValueError(f\"The provided weights {weights} isn't supported.\")\n\n    if not isinstance(num_workers, int) and num_workers is not None:\n        raise ValueError(f\"The provided num_workers {num_workers} isn't supported.\")\n\n    if not isinstance(fast_dev_run, bool) and not isinstance(fast_dev_run, int):\n        raise ValueError(f\"The provided fast_dev_run {fast_dev_run} isn't supported.\")\n\n    if not isinstance(num_nodes, int) and num_nodes is not None:\n        raise ValueError(f\"The provided num_nodes {num_nodes} isn't supported.\")\n\n    if not isinstance(machine, str) and machine is not None:\n        raise ValueError(f\"The provided machine {machine} isn't supported.\")\n\n    if not isinstance(num_downloaders, int) and num_downloaders is not None:\n        raise ValueError(f\"The provided num_downloaders {num_downloaders} isn't supported.\")\n\n    if not isinstance(num_uploaders, int) and num_uploaders is not None:\n        raise ValueError(f\"The provided num_uploaders {num_uploaders} isn't supported.\")\n\n    if not isinstance(reorder_files, bool):\n        raise ValueError(f\"The provided reorder_files {reorder_files} isn't supported.\")\n\n    if not isinstance(error_when_not_empty, bool):\n        raise ValueError(f\"The provided error_when_not_empty {error_when_not_empty} isn't supported.\")\n\n    if not isinstance(reader, BaseReader) and reader"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir)\n\n    while True:\n        try:\n            task = queue_in.get(timeout=1)\n        except Empty:\n            continue\n\n        if task is None:\n            break\n\n        task_index, files = task\n\n        for file in files:\n            obj = parse.urlparse(file)\n            if obj.scheme == \"s3\":\n                _wait_for_file_to_exist(s3, obj)\n            else:\n                _wait_for_disk_usage_higher_than_threshold(input_dir, 25)\n\n            if not os.path.exists(os.path.join(cache_dir, obj.path.lstrip(\"/\"))):\n                s3.download(obj.path.lstrip(\"/\"), os.path.join(cache_dir, obj.path.lstrip(\"/\")))\n\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[Tuple[int, List[str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        index, paths = r\n\n        # 4. Check whether all the files are already uploaded\n        if output_dir.path and all(\n            os.path.exists(p.replace(cache_dir, output_dir.path) if output_dir else p) for p in paths\n        ):\n            remove_queue.put(paths)\n            continue\n\n        # 5. Upload all the required paths to unblock the current index\n        for path in paths:\n            if output_dir.path:\n                local_path = path.replace(cache_dir, output_dir.path)\n\n            if output_dir.url and output_dir.path:\n                path = path.replace(cache_dir, output_dir.url)\n\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                dirpath = os.path.dirname(local_path)\n\n                os.makedirs(dirpath, exist_ok=True)\n\n                with open(path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n            elif os.path.isfile(path):\n                if not path.startswith(\"/teamspace/studios/this_studio\"):\n                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                    shutil.copyfile(path, local_path)\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 6. Inform the worker the current files are available\n        remove_queue.put(paths)\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Associate the items to the workers based on number of nodes and node rank.\n    weights = [1] * len(user_items) if weights is None else weights\n\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for workers on the current node.\n    print(f\"Worker distribution on node {node_rank} of {num_nodes} nodes:\")\n    for i, worker_id in enumerate(worker_ids_this_node):\n        print(f\"Worker {worker_id}: {len(worker_items[i])} items, total weight: {sum(worker_weights[i])}\")\n\n    # Shuffle the items for each worker.\n    for i in range(len(worker_items)):\n        random.shuffle(worker_items[i])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * _get_num_nodes()\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n\n    # Calculate the remainder\n    remainder = len(user_items) % total_workers\n\n    # Create a list of lists to store the items assigned to each worker\n    worker_items = [[] for _ in range(total_workers)]\n\n    # Distribute the items to the workers\n    for i, item in enumerate(user_items):\n        worker_index = i % total_workers\n        worker_items[worker_index].append(item)\n\n    # Adjust for the remainder\n    for i in range(remainder):\n        worker_items[-(i + 1)].append(user_items[i + items_per_worker * remainder])\n\n    # Check if the output list has the expected length\n    if len(worker_items) != total_workers:\n        raise RuntimeError(f\"The output list has an unexpected length: {len(worker_items)} instead of {total_workers}.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "/litdata/processing/utilities/packing.py\nimport math\nimport random\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\n\nfrom litdata.constants import _TORCH_GREATER_EQUAL_2_1_0\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch import Tensor\nelse:\n    from torch import Tensor\n\n\ndef _pack_greedy(items: List[Any], weights: List[int], num_bins: int) -> Tuple[List[int], Dict[int, int]]:\n    \"\"\"This function packs the items into the given number of bins using a greedy algorithm.\n\n    Arguments:\n        items: The list of items to be packed.\n        weights: The list of weights associated with each item.\n        num_bins: The number of bins into which the items should be packed.\n\n    Returns:\n        A tuple containing the list of bin indices and the dictionary of bin sizes.\n\n    \"\"\"\n    bin_indices = [0] * len(items)\n    bin_sizes = {0: 0}\n\n    for i, item in enumerate(items):\n        best_bin = 0\n        best_bin_size = bin_sizes[0]\n\n        for j, bin_size in bin_sizes.items():\n            if bin_size + weights[i] <= num_bins:\n                if bin_size + weights[i] > best_bin_size:\n                    best_bin = j\n                    best_bin_size = bin_size + weights[i]\n\n        bin_indices[i] = best_bin\n        bin_sizes[best_bin] = best_bin_size\n\n    return bin_indices, bin_sizes\n\n\ndef _pack_greedy_with_random_ties(items: List[Any], weights: List[int], num_bins: int) -> Tuple[List[int], Dict[int, int]]:\n    \"\"\"This function packs the items into the given number of bins using a greedy algorithm with random ties.\n\n    Arguments:\n        items: The list of items to be packed.\n        weights: The list of weights associated with each item.\n        num_bins: The number of bins into which the items should be packed.\n\n    Returns:\n        A tuple containing the list of bin indices and the"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n        element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0.\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0.\"\n\n        if self.tcnn:\n            if n_neurons < 1000:\n                return self._get_tinycudann_network(\n                    n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation\n                )\n            else:\n                return self._get_tinycudann_network_large(\n                    n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation\n                )\n        else:\n            return self._get_pytorch_network(\n                n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation\n            )\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the rolling median of the signal by shifting it by a range defined by the kernel offset, computing the median of these shifted signals, and then trimming the resulting median array to account for edge effects introduced by the shifting process.\n        median_signal = np.array([np.median(signal[i : i + 2 * kernel_offset + 1]) for i in range(len(signal) - 2 * kernel_offset)])\n\n        # Trim the median signal array to remove edge effects introduced by the shifting process.\n        return median_signal[kernel_offset : -kernel_offset]"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if not isinstance(template_probe, IrisTemplate):\n        raise MatcherError(\"template_probe must be an IrisTemplate\")\n    if not isinstance(template_gallery, IrisTemplate):\n        raise MatcherError(\"template_gallery must be an IrisTemplate\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"template_probe and template_gallery must have the same codesize\")\n\n    if template_probe.codesize % 2 != 0:\n        raise MatcherError(\"template_probe and template_gallery must have an even codesize\")\n\n    if template_probe.codesize < 16:\n        raise MatcherError(\"template_probe and template_gallery must have a codesize of at least 16\")\n\n    if template_probe.codesize > 128:\n        raise MatcherError(\"template_probe and template_gallery must have a codesize of at most 128\")\n\n    if template_probe.codesize % 4 != 0:\n        raise MatcherError(\"template_probe and template_gallery must have a codesize that is a multiple of 4\")\n\n    if template_probe.codesize < 16 or template_probe.codesize > 128:\n        raise MatcherError(\"template_probe and template_gallery must have a codesize of at least 16 and at most 128\")\n\n    if template_probe.codesize % 4 != 0:\n        raise MatcherError(\"template_probe and template_gallery must have a codesize that is a multiple of 4\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"template_probe and template_gallery must have the same codesize\")\n\n    if template_probe.codesize % 2 != 0:\n        raise MatcherError(\"template_probe and template_gallery must have an even codesize\")\n\n    if template_probe.codesize < 16:\n        raise MatcherError(\"template_probe and template_gallery must have a codesize of at least 16\")\n\n    if template_probe.codesize > 128:\n        raise MatcherError(\"template_probe and template_gallery must have a codesize of at most 128\")\n\n    if template_probe.codesize % 4 != 0:\n        raise Matcher"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the first and second bisectors arrays\n        first_bisectors = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors = np.zeros((self.params.num_bisectors, 2))\n\n        # Initialize the number of iterations\n        iterations = 0\n\n        # Loop until the number of iterations exceeds the maximum allowed\n        while iterations < self.params.max_iterations:\n\n            # Randomly select two points from the polygon\n            point_1 = polygon[np.random.randint(0, polygon.shape[0])]\n            point_2 = polygon[np.random.randint(0, polygon.shape[0])]\n\n            # Calculate the perpendicular bisector of the two points\n            bisector_start = (point_1 + point_2) / 2\n            bisector_end = np.array([bisector_start[1], bisector_start[0]])\n\n            # Check if the distance between the two points is greater than the minimum distance\n            if np.linalg.norm(point_1 - point_2) > min_distance_between_sector_points_in_px:\n\n                # If the distance is greater than the minimum, add the bisector to the first bisectors array\n                first_bisectors[iterations] = bisector_start\n                second_bisectors[iterations] = bisector_end\n\n                # Increment the number of iterations\n                iterations += 1\n\n        # If the number of iterations exceeds the maximum allowed, raise an exception\n        if iterations == self.params.max_iterations:\n            raise EyeCentersEstimationError(\n                f\"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed ({self.params.max_iterations}).\"\n            )\n\n        return first_bisectors, second_bisectors\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature and type hints\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Get the function's name and docstring\n        func_name = func_object.__name__\n        func_docstring = func_object.__doc__\n\n        # Initialize the function description\n        function_description = FunctionDescription(func_name, func_docstring)\n\n        # Get the function's input and output type hints\n        input_type_hints = {}\n        output_type_hints = {}\n        for name, type_hint in type_hints.items():\n            if name in signature.parameters:\n                input_type_hints[name] = type_hint\n            else:\n                output_type_hints[name] = type_hint\n\n        # Get the function's input and output class definitions\n        input_class_definitions = {}\n        output_class_definitions = {}\n        for name, type_hint in input_type_hints.items():\n            input_class_definitions[name] = get_class_definition(type_hint)\n        for name, type_hint in output_type_hints.items():\n            output_class_definitions[name] = get_class_definition(type_hint)\n\n        # Set the function's input and output class definitions\n        function_description.input_class_definitions = input_class_definitions\n        function_description.output_class_definitions = output_class_definitions\n\n        # Set the function's input and output type hints\n        function_description.input_type_hints = input_type_hints\n        function_description.output_type_hints = output_type_hints\n\n        # Set the function's source code\n        function_description.source_code = get_source(func_object)\n\n        # Set the function's type\n        output_type_hint = output_type_hints[list(output_type_hints.keys())[0]]\n        if issubclass(output_type_hint, Embedding):\n            function_description.type = FunctionType.EMBEDDABLE\n        elif get_origin(output_type_hint) == Union:\n            function_description.type = FunctionType.SYMBOLIC\n        else:\n            function_description.type = FunctionType.SYMBOLIC\n\n        return function_description\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] = seed\n\n        self.persistence.save(self.bit_array, self.indices)\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) != self.size:\n            logging.warning(f\"Loaded bit array length {len(loaded_bit_array)} does not match expected length {self.size}. Reinitializing bit array and indices.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if not self.bit_array[index]:\n                return False\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[CURRENT_MODEL_STATS]\n        self.last_training_run = json_dict[LAST_TRAINING_RUN]\n        self.current_training_run = json_dict[CURRENT_TRAINING_RUN]\n        self.nr_of_training_runs = json_dict[NR_OF_TRAINING_RUNS]\n        self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[TEACHER_MODELS]]\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate the model configuration\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration. Expected an OpenAIConfig instance, but got a {}.\".format(type(model)))\n\n        # Validate the system message\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message. Expected a string, but got a {}.\".format(type(system_message)))\n\n        # Validate the prompt\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt. Expected a string, but got a {}.\".format(type(prompt)))\n\n        # Validate the additional parameters\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(\"Invalid parameter. Expected one of {}, but got {}.\".format(LLM_GENERATION_PARAMETERS, key))\n            if not isinstance(value, (int, float)):\n                raise ValueError(\"Invalid parameter value. Expected an integer or float, but got a {}.\".format(type(value)))\n\n        # Set the default parameters\n        default_parameters = {\n            \"temperature\": 0.7,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"max_new_tokens\": 100,\n        }\n\n        # Merge the default parameters with the provided parameters\n        parameters = {**default_parameters, **kwargs}\n\n        # Set the model name\n        model_name = model.model_name\n\n        # Set the system message\n        system_message = system_message.strip()\n\n        # Set the prompt\n        prompt = prompt.strip()\n\n        # Set the parameters\n        parameters = {k: v for k, v in parameters.items() if v is not None}\n\n        # Set the headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\",\n        }\n\n        # Set the payload\n        payload = {\n            \"model\": model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            **parameters,\n        }\n\n        # Send the request\n        response = requests.post(OPENAI_URL, json=payload, headers=headers)\n\n        # Check"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have zero diagonal elements\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\n                \"examples\": [],\n                \"model\": \"\"\n            }\n        # check if the function is already initialized and has examples\n        if func_hash in self.initialized_functions and len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n            # check if the function is already initialized and has examples\n            if func_hash in self.initialized_functions and len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n                # check if the function is already initialized and has examples\n                if func_hash in self.initialized_functions and len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n                    # check if the function is already initialized and has examples\n                    if func_hash in self.initialized_functions and len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n                        # check if the function is already initialized and has examples\n                        if func_hash in self.initialized_functions and len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n                            # check if the function is already initialized and has examples\n                            if func_hash in self.initialized_functions and len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n                                # check if the function is already initialized and has examples\n                                if func_hash in self.initialized_functions and len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n                                    # check if the function is already initialized and has examples\n                                    if func_hash in self.initialized_functions and len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n                                        # check if the function is already initialized and has examples\n                                        if func_hash in self.initialized_functions and len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n                                            # check if the function is already initialized and has examples\n                                            if func_hash in self.initialized_functions and len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n                                                # check if the function is already initialized and has examples\n                                                if func_hash in self.initialized_functions and len(self.initialized_functions[func"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n\n    if higham:\n        cov = higham_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = clip_nearest(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if the input is a list of strings\n    if not isinstance(equations, list):\n        raise TypeError(f\"The {names[1]} parameter must be a list of strings.\")\n\n    # Check if the input is a list of strings\n    if not isinstance(groups, list):\n        raise TypeError(f\"The {names[0]} parameter must be a list of strings.\")\n\n    # Check if the input is a list of strings\n    if not isinstance(sum_to_one, bool):\n        raise TypeError(f\"The sum_to_one parameter must be a boolean.\")\n\n    # Check if the input is a list of strings\n    if not isinstance(raise_if_group_missing, bool):\n        raise TypeError(f\"The raise_if_group_missing parameter must be a boolean.\")\n\n    # Check if the input is a list of strings\n    if not isinstance(names, tuple):\n        raise TypeError(f\"The names parameter must be a tuple.\")\n\n    # Check if the input is a list of strings\n    if not isinstance(names[0], str):\n        raise TypeError(f\"The names[0] parameter must be a string.\")\n\n    # Check if the input is a list of strings\n    if not isinstance(names[1], str):\n        raise TypeError(f\"The names[1] parameter must be a string.\")\n\n    # Check if the input is a list of strings\n    if len(names) != 2:\n        raise ValueError(f\"The names parameter must be a tuple of length 2.\")\n\n    # Check if the input is a list of strings\n    if len(names[0]) == 0:\n        raise ValueError(f\"The names[0] parameter must be a non-empty string.\")\n\n    # Check if the input is a list of strings\n    if len(names[1]) == 0:\n        raise ValueError(f\"The names[1] parameter must be a non-empty string.\")\n\n    # Check if the input is a list of strings\n    if not isinstance(groups, list):\n        raise TypeError(f\"The {names[0]} parameter must be a list of strings.\")\n\n    # Check if the input is a list of strings\n    if not isinstance(equations, list):\n        raise TypeError(f\"The {names[1]} parameter must be a list of strings.\")\n\n    # Check if the input is a list of strings\n    if"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    cls_name = f\"Instances{_counter}\"\n    cls_path = f\"detectron2.structures.instances_patch{_counter}\"\n    cls_path_module = f\"detectron2.structures.instances_patch{_counter}.__init__\"\n\n    # create a new class\n    newInstances = type(cls_name, (Instances,), {})\n    for field in fields:\n        newInstances.__annotations__[field[0]] = field[1]\n\n    # add from_instances method\n    _add_instances_conversion_methods(newInstances)\n\n    # write the class to a temporary file\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n        f.write(f\"class {cls_name}({Instances.__name__}):\\n\")\n        for field in fields:\n            f.write(f\"    {field[0]}: {field[1].__name__} = None\\n\")\n        f.write(f\"    def __init__(self, image_size: Tuple[int, int]):\\n\")\n        f.write(f\"        super({cls_name}, self).__init__(image_size)\\n\")\n        f.write(f\"        self.fields = {{\\n\")\n        for field in fields:\n            f.write(f\"            '{field[0]}': {field[1].__name__},\\n\")\n        f.write(f\"        }}\\n\")\n        f.write(f\"    def get_fields(self): return self.fields\\n\")\n        f.write(f\"    def __repr__(self): return f'{cls_name}(image_size=self.image_size)'\\n\")\n        f.write(f\"    def __str__(self): return f'{cls_name}(image_size=self.image_size)'\\n\")\n        f.write(f\"    def __len__(self): return len(self.pred_boxes)\\n\")\n        f.write(f\"    def __getitem__(self, item): return self.select(item)\\n\")\n        f.write(f\"    def select(self, item):\\n\")\n        f.write(f\"        ret = {cls_name}(self."}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding box\n    if \"bbox\" in annotation:\n        bbox = BoxMode.convert(\n            annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS\n        )\n        bbox = transforms.apply_box(bbox)\n        bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n        annotation[\"bbox\"] = bbox\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if \"polygons\" in annotation[\"segmentation\"]:\n            polygons = annotation[\"segmentation\"][\"polygons\"]\n            polygons = transforms.apply_polygons(polygons)\n            polygons = PolygonMasks(polygons, image_size)\n            annotation[\"segmentation\"][\"polygons\"] = polygons\n        elif \"rle\" in annotation[\"segmentation\"]:\n            rle = annotation[\"segmentation\"][\"rle\"]\n            rle = transforms.apply_segmentation(rle)\n            rle = mask_util.frPyObjects(rle, image_size[0], image_size[1])\n            annotation[\"segmentation\"][\"rle\"] = rle\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transforms.apply_keypoint(keypoints, image_size, keypoint_hflip_indices)\n        annotation[\"keypoints\"] = keypoints\n\n    # Set bbox_mode to XYXY_ABS\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = coords.astype(np.float32)\n        coords = np.hstack((coords, np.ones((coords.shape[0], 1), dtype=np.float32)))\n        coords = np.dot(coords, self.rm_coords.T)\n        coords[:, 0] += self.center[0] - self.bound_w / 2\n        coords[:, 1] += self.center[1] - self.bound_h / 2\n        return coords\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a FlopCountAnalysis object to compute the flops\n    analysis = FlopCountAnalysis(model, inputs)\n\n    # Compute the flops\n    flops = analysis.get_flops()\n\n    # Return the flops\n    return flops\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] == (self.h, self.w):\n            if self.angle == 0:\n                return img\n            if self.angle % 360 == 0:\n                return img\n            if interp is None:\n                interp = self.interp\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(self.angle, resample=interp, expand=self.expand)\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                Image.NEAREST: \"nearest\",\n                Image.BILINEAR: \"bilinear\",\n                Image.BICUBIC: \"bicubic\",\n            }\n            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp]\n            align_corners = None if mode == \"nearest\" else False\n            img = F.rotate(img, self.angle, resample=mode, align_corners=align_corners)\n            shape[:2] = (self.bound_h, self.bound_w)\n            ret = img.permute(2, 3, 0, 1).view(shape).numpy()  # nchw -> hw(c)\n\n        return ret\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        masks = predictions.pred_masks if predictions.has(\"pred_masks\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if not any([boxes, scores, classes, masks, keypoints]):\n            return self.output\n\n        if boxes is not None and not isinstance(boxes, (Boxes, RotatedBoxes)):\n            raise ValueError(\n                \"Expected boxes to be a Boxes or RotatedBoxes, but got {}\".format(type(boxes))\n            )\n        if scores is not None and not isinstance(scores, torch.Tensor):\n            raise ValueError(\n                \"Expected scores to be a tensor, but got {}\".format(type(scores))\n            )\n        if classes is not None and not isinstance(classes, torch.Tensor):\n            raise ValueError(\n                \"Expected classes to be a tensor, but got {}\".format(type(classes))\n            )\n        if masks is not None and not isinstance(masks, (BitMasks, PolygonMasks)):\n            raise ValueError(\n                \"Expected masks to be a BitMasks or PolygonMasks, but got {}\".format(type(masks))\n            )\n        if keypoints is not None and not isinstance(keypoints, Keypoints):\n            raise ValueError(\n                \"Expected keypoints to be a Keypoints, but got {}\".format(type(keypoints))\n            )\n\n        if boxes is not None:\n            self.draw_boxes(boxes, scores, classes, masks, keypoints)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # self.canvas.draw()\n        # width, height = self.fig.get_size_inches() * self.fig.get_dpi()\n        # width, height = int(width), int(height)\n        # width, height = self.width, self.height\n        # print(width, height)\n        # print(self.canvas.get_width_height())\n        # print(self.canvas.get_width_height()[0], self.canvas.get_width_height()[1])\n        # print(self.canvas.get_width_height()[0] / self.canvas.get_width_height()[1])\n        # print(self.canvas.get_width_height()[1] / self.canvas.get_width_height()[0])\n        # print(self.canvas.get_width_height()[0] / self.canvas.get_width_height()[1])\n        # print(self.canvas.get_width_height()[1] / self.canvas.get_width_height()[0])\n        # print(self.canvas.get_width_height()[0] / self.canvas.get_width_height()[1])\n        # print(self.canvas.get_width_height()[1] / self.canvas.get_width_height()[0])\n        # print(self.canvas.get_width_height()[0] / self.canvas.get_width_height()[1])\n        # print(self.canvas.get_width_height()[1] / self.canvas.get_width_height()[0])\n        # print(self.canvas.get_width_height()[0] / self.canvas.get_width_height()[1])\n        # print(self.canvas.get_width_height()[1] / self.canvas.get_width_height()[0])\n        # print(self.canvas.get_width_height()[0] / self.canvas.get_width_height()[1])\n        # print(self.canvas.get_width_height()[1] / self.canvas.get_width_height()[0])\n        # print(self.canvas.get_width_height()[0] / self.canvas.get_width_height()[1])\n        # print(self.canvas.get_width_height"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Check if the input dictionary contains any annotations.\n        if \"annotations\" in dic:\n            # If annotations are present, draw them on the image.\n            self.overlay_dataset_dict(dic)\n\n        # Check if the input dictionary contains any semantic segmentation masks.\n        if \"sem_seg\" in dic:\n            # If semantic segmentation masks are present, draw them on the image.\n            self.draw_sem_seg(dic[\"sem_seg\"])\n\n        # Check if the input dictionary contains any panoptic segmentation masks.\n        if \"panoptic_seg\" in dic:\n            # If panoptic segmentation masks are present, draw them on the image.\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"])\n\n        # Return the modified image object.\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = _OFF_WHITE\n\n        if text is not None:\n            text_color = self._change_color_brightness(color, brightness_factor=0.7)\n            font_size = self._default_font_size * 0.5\n            self.draw_text(text, (0, 0), color=text_color, font_size=font_size)\n\n        if binary_mask.dtype == np.uint8:\n            binary_mask = binary_mask.astype(np.bool)\n\n        if binary_mask.dtype != np.bool:\n            raise ValueError(\n                \"binary_mask must be a numpy array of type bool or uint8, but got {}\".format(\n                    type(binary_mask)\n                )\n            )\n\n        if binary_mask.ndim != 2:\n            raise ValueError(\n                \"binary_mask must be a 2D array, but got {}\".format(binary_mask.ndim)\n            )\n\n        if binary_mask.shape[0] != self.output.height or binary_mask.shape[1] != self.output.width:\n            raise ValueError(\n                \"binary_mask must have the same shape as the image, but got {} and {}\".format(\n                    binary_mask.shape, (self.output.height, self.output.width)\n                )\n            )\n\n        if binary_mask.dtype == np.bool:\n            # draw regular masks as polygons\n            if binary_mask.sum() == 0:\n                return self.output\n\n            # find connected components\n            labels = measure.label(binary_mask, connectivity=1)\n            # find the area of each component\n            areas = [r.area for r in measure.regionprops(labels)]\n            # find the corresponding bounding boxes\n            bboxes = [r.bbox for r in measure.regionprops(labels)]\n            # remove small components\n            areas = np.array(areas)\n            bboxes = np.array(bboxes)\n            keep = np.where(areas > area_threshold)[0]\n            labels = labels[keep]\n            areas = areas[keep]\n            bboxes = bboxes[keep]\n            # draw bounding boxes\n            for label, area, bbox in zip(labels,"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are the same\n    if size_as_tensor:\n        assert input.image_size.allclose(other.image_size), f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n\n    # Check if the fields are the same\n    for field in input._fields:\n        if field == \"image_size\":\n            continue\n        val1 = getattr(input, field)\n        val2 = getattr(other, field)\n        if isinstance(val1, Boxes):\n            assert val1.tensor.allclose(val2.tensor, rtol=rtol), f\"{msg}Field {field} does not match: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.allclose(val2.tensor, rtol=rtol), f\"{msg}Field {field} does not match: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, torch.Tensor):\n            assert val1.allclose(val2, rtol=rtol), f\"{msg}Field {field} does not match: {val1} vs {val2}\"\n        else:\n            assert val1 == val2, f\"{msg}Field {field} does not match: {val1} vs {val2}\"\n\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 3] * self.tensor[:, 2]\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(cfg.MODEL.PROPOSAL_GENERATOR.NAME)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # classification loss\n        loss_cls = cross_entropy(scores, cat(gt_classes, dim=0), self.num_classes)\n\n        # box regression loss\n        loss_box_reg = _dense_box_regression_loss(\n            proposal_deltas,\n            cat(gt_boxes, dim=0),\n            self.box2box_transform,\n            self.smooth_l1_beta,\n            self.box_reg_loss_type,\n        )\n\n        losses = {}\n        losses[\"loss_cls\"] = loss_cls * self.loss_weight[\"loss_cls\"]\n        losses[\"loss_box_reg\"] = loss_box_reg * self.loss_weight[\"loss_box_reg\"]\n\n        return losses\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.NAME\n    tracker_class = TRACKER_HEADS_REGISTRY.get(tracker_name)\n    if tracker_class is None:\n        raise KeyError(\"Unknown tracker: {}\".format(tracker_name))\n    return tracker_class(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            anno_type = self.anno_type_map.keys()\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        image = np.array(image)\n        image, k = resize_image(image, 64)\n        image = image.transpose(2, 0, 1)\n        image = image.astype(np.float32)\n        image = image / 255.0\n        image = image[np.newaxis, :, :, :]\n        image = torch.from_numpy(image)\n        image = image.to(we.device_id)\n\n        output = self.general_ins(image)\n        output = output.cpu().numpy()\n        output = output[0, :, :, :]\n        output = output.transpose(1, 2, 0)\n        output = output * 255.0\n        output = output.astype(np.uint8)\n        output = resize_image_ori(image.shape[1], image.shape[2], output, k)\n\n        if len(anno_type) == 1:\n            return output[anno_type[0]]\n        else:\n            return {tp: output[tp] for tp in anno_type}"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        result = {}\n        for kw in keywords:\n            result = update_url_scores(result, self.bm25(kw))\n        return result\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within the range (-180, 180] degrees\n        self.normalize_angles()\n\n        # Identify indices of boxes that are nearly horizontal\n        clip_angle_threshold = math.radians(clip_angle_threshold)\n        horizontal_indices = (\n            torch.abs(self.tensor[:, 4]) <= clip_angle_threshold\n        ).nonzero().squeeze(1)\n\n        # Convert boxes to (x1, y1, x2, y2) representation\n        boxes = self.tensor[horizontal_indices]\n        boxes[:, 0] = boxes[:, 0] - boxes[:, 2] / 2\n        boxes[:, 1] = boxes[:, 1] - boxes[:, 3] / 2\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n\n        # Clip x and y coordinates to ensure they do not exceed the specified box_size limits\n        boxes[:, 0] = torch.clamp(boxes[:, 0], 0, box_size[1])\n        boxes[:, 1] = torch.clamp(boxes[:, 1], 0, box_size[0])\n        boxes[:, 2] = torch.clamp(boxes[:, 2], 0, box_size[1])\n        boxes[:, 3] = torch.clamp(boxes[:, 3], 0, box_size[0])\n\n        # Convert boxes back to (center x, center y, width, height, angle) representation\n        boxes[:, 0] = boxes[:, 0] + boxes[:, 2] / 2\n        boxes[:, 1] = boxes[:, 1] + boxes[:, 3] / 2\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n\n        # Update the tensor attribute of the RotatedBoxes instance\n        self.tensor[horizontal_indices] = boxes\n\n        # Convert angles back to the range (-180, 180) degrees\n        self.tensor[:, 4] = (self.tensor[:, 4] + 180.0) % 360.0 - 180.0\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary with keys for each type\n        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate over the data\n        for item in self.data:\n            # Get the type of the current item\n            item_type = item['type']\n            # Increment the count of the corresponding type in the statistics dictionary\n            statistics[item_type] += 1\n\n        # Return the statistics dictionary\n        return statistics"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMSEG_SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None:\n        warnings.warn('train_cfg is deprecated, please use model.train_cfg instead.')\n    if test_cfg is not None:\n        warnings.warn('test_cfg is deprecated, please use model.test_cfg instead.')\n\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg)"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # Initialize the dictionary to store the evaluation results\n    eval_results = {}\n\n    # Create a dictionary to store the class-wise AP and AR for each IoU threshold\n    for iou_threshold in metric:\n        eval_results[f'AP_{iou_threshold}'] = {}\n        eval_results[f'AR_{iou_threshold}'] = {}\n\n    # Create a dictionary to store the overall mAP and mAR\n    eval_results['mAP'] = {}\n    eval_results['mAR'] = {}\n\n    # Create a dictionary to store the class-wise AP and AR for each IoU threshold\n    for iou_threshold in metric:\n        eval_results[f'AP_{iou_threshold}'] = {}\n        eval_results[f'AR_{iou_threshold}'] = {}\n\n    # Create a dictionary to store the overall mAP and mAR\n    eval_results['mAP'] = {}\n    eval_results['mAR'] = {}\n\n    # Create a dictionary to store the class-wise AP and AR for each IoU threshold\n    for iou_threshold in metric:\n        eval_results[f'AP_{iou_threshold}'] = {}\n        eval_results[f'AR_{iou_threshold}'] = {}\n\n    # Create a dictionary to store the overall mAP and mAR\n    eval_results['mAP'] = {}\n    eval_results['mAR'] = {}\n\n    # Create a dictionary to store the class-wise AP and AR for each IoU threshold\n    for iou_threshold in metric:\n        eval_results[f'AP_{iou_threshold}'] = {}\n        eval_results[f'AR_{iou_threshold}'] = {}\n\n    # Create a dictionary to store the overall mAP and mAR\n    eval_results['mAP'] = {}\n    eval_results['mAR'] = {}\n\n    # Create a dictionary to store the class-wise AP and AR for each IoU threshold\n    for iou_threshold in metric:\n        eval_results[f'AP_{iou_threshold}'] = {}\n        eval_results[f'AR_{iou_threshold}'] = {}\n\n    # Create a dictionary to store the overall mAP and mAR\n    eval_results['mAP'] = {}\n    eval_results['mAR'] = {}\n\n    # Create a dictionary to store the class-wise AP and AR for each IoU threshold\n    for iou_"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_type = box_type.lower()\n    if box_type == 'lidar':\n        return LidarBox, 'LIDAR'\n    elif box_type == 'camera':\n        return CameraBox, 'CAMERA'\n    elif box_type == 'depth':\n        return DepthBox, 'DEPTH'\n    else:\n        raise ValueError(f'Unknown box type: {box_type}')\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      raise RequestError('must provide messages')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(message, dict) for message in messages):\n      raise TypeError('messages must be a list of dict-like objects')\n\n    if not all(message.get('role') in ('system', 'user', 'assistant') for message in messages):\n      raise RequestError('messages must contain a role')\n\n    if not all(message.get('content') for message in messages):\n      raise RequestError('messages must contain a content')\n\n    if any(message.get('images') for message in messages):\n      messages = [_encode_images(message) for message in messages]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('Model is required')\n\n    if not prompt:\n      raise ValueError('Prompt is required')\n\n    if not system:\n      raise ValueError('System is required')\n\n    if not template:\n      raise ValueError('Template is required')\n\n    if not context:\n      context = []\n\n    if not options:\n      options = {}\n\n    if keep_alive:\n      options['keep_alive'] = keep_alive\n\n    if images:\n      options['images'] = images\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'options': options,\n    }\n\n    if raw:\n      data['raw'] = True\n\n    if format:\n      data['format'] = format\n\n    return self._request_stream(\n      'POST',\n      '/generate',\n      json=data,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide a model file or path')\n\n    if path:\n      path = Path(path)\n      if not path.exists():\n        raise RequestError(f'path {path} does not exist')\n      modelfile = path.read_text()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = _as_path(path)\n    digest = sha256()\n    with path.open('rb') as f:\n      for chunk in iter(lambda: f.read(4096), b''):\n        digest.update(chunk)\n\n    digest = digest.hexdigest()\n    response = self._client.head(f'/api/blobs/{digest}')\n    if response.status_code == 404:\n      with path.open('rb') as f:\n        self._client.post(f'/api/blobs/{digest}', data=f)\n\n    return f'sha256:{digest}'\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory to store the user code and test code\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Create a temporary file to store the user code\n            user_code_file = Path(temp_dir) / \"user_code.py\"\n            user_code_file.write_text(user_code)\n\n            # Create a temporary file to store the test code\n            test_code_file = Path(temp_dir) / \"test_code.py\"\n            test_code_file.write_text(test_code)\n\n            # Run Pyright to type check the user code and test code\n            pyright_command = [\"pyright\", \"--no-config\", \"--output-format\", \"json\", user_code_file, test_code_file]\n            pyright_output = subprocess.run(pyright_command, capture_output=True, text=True)\n\n            # Parse the output of Pyright to identify lines with expected type errors\n            expected_error_lines = []\n            for line in pyright_output.stdout.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no = int(match.group(1))\n                    error_message = match.group(2)\n                    if cls.EXPECT_ERROR_COMMENT in error_message:\n                        expected_error_lines.append(line_no)\n\n            # Create a TypeCheckResult object to store the result of the type check\n            result = TypeCheckResult(\n                message=pyright_output.stdout,\n                passed=len(expected_error_lines) == 0,\n                debug_info={\"expected_error_lines\": expected_error_lines},\n            )\n\n        return result\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, get_compiler_fn(\"AOT Module\"))\n    else:\n        return aot_function(fn, get_compiler_fn(\"AOT Function\"))"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml')))\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('The output path must have a .yaml or .yml extension.')\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Check if the function is a PyTorch module or a function\n    if isinstance(func, torch.nn.Module):\n        is_module = True\n        func_name = func.__class__.__name__\n        func_forward = func.forward\n    else:\n        is_module = False\n        func_name = func.__name__\n        func_forward = func\n\n    # Check if the function has already been traced\n    if func_name in _traced_functions:\n        # If the function has already been traced, return the cached version\n        return _traced_functions[func_name]\n\n    # If the function has not been traced, trace it\n    if is_module:\n        # If the function is a module, trace its forward method\n        traced_module, untraced_module = trace_with_kwargs(\n            func_forward, example_inputs=(), example_kwarg_inputs={})\n    else:\n        # If the function is not a module, trace it directly\n        traced_module, untraced_module = trace_with_kwargs(\n            func, example_inputs=(), example_kwarg_inputs={})\n\n    # If a compiler function is provided, compile the traced module or its call helper\n    if ts_compiler is not None:\n        traced_module = ts_compiler(traced_module, **kwargs_)\n        untraced_module = ts_compiler(untraced_module, **kwargs_)\n\n    # Cache the traced module\n    with _traced_functions_lock:\n        _traced_functions[func_name] = untraced_module\n\n    # Return the cached version of the traced module\n    return _traced_functions[func_name]\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a summary file for this node line\n    summary_file = os.path.join(node_line_dir, \"summary.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join(node_line_dir, \"results.csv\")\n\n    # Create a results file for this node line\n    results_file = os.path.join"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Get the list of query expansion modules from the support modules\n    query_expansion_modules = get_support_modules(modules, \"query_expansion\")\n\n    # Get the list of query expansion module names\n    query_expansion_module_names = [module.__name__ for module in query_expansion_modules]\n\n    # Get the list of query expansion module parameters\n    query_expansion_module_params = [module_params[i] for i in range(len(query_expansion_modules))]\n\n    # Get the list of query expansion module names and parameters\n    query_expansion_module_names_and_params = make_combinations(query_expansion_module_names, query_expansion_module_params)\n\n    # Get the list of query expansion module names and parameters as a list of dictionaries\n    query_expansion_module_names_and_params_list = [dict(zip(query_expansion_module_names, params)) for params in query_expansion_module_names_and_params]\n\n    # Get the list of query expansion module names and parameters as a list of tuples\n    query_expansion_module_names_and_params_tuples = [(name, params) for name, params in query_expansion_module_names_and_params_list]\n\n    # Get the list of query expansion module names and parameters as a list of strings\n    query_expansion_module_names_and_params_strings = [f\"{name}({str(params)})\" for name, params in query_expansion_module_names_and_params_tuples]\n\n    # Get the list of query expansion module names and parameters as a list of strings\n    query_expansion_module_names_and_params_strings = [f\"{name}({str(params)})\" for name, params in query_expansion_module_names_and_params_tuples]\n\n    # Get the list of query expansion module names and parameters as a list of strings\n    query_expansion_module_names_and_params_strings = [f\"{name}({str(params)})\" for name, params in query_expansion_module_names_and_params_tuples]\n\n    # Get the list of query expansion module names and parameters as a list of strings\n    query_expansion_module_names_and_params_strings = [f\"{name}({str(params)})\" for name"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create the node's output directory\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a list of prompt maker module names\n    module_names = [module.__name__ for module in modules]\n\n    # Create a list of prompt maker module parameters\n    module_params_list = [module_params[i] for i in range(len(modules))]\n\n    # Create a list of prompt maker module execution times\n    module_times = []\n\n    # Create a list of prompt maker module evaluation metrics\n    module_metrics = []\n\n    # Create a list of prompt maker module results\n    module_results = []\n\n    # Create a list of prompt maker module summaries\n    module_summaries = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list_str = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list_str_dict = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list_str_dict_str = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list_str_dict_str_dict = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list_str_dict_str_dict_str = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list_str_dict_str_dict_str_dict = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list_str_dict_str_dict_str_dict_str = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list_str_dict_str_dict_str_dict_str_dict = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list_str_dict_str_dict_str_dict_str_dict_str = []\n\n    # Create a list of prompt maker module combinations\n    module_combinations_list_str_dict_str_dict_str_"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Convert the ground truth strings into embeddings\n    gt_embeddings = embedding_model.embed_documents(generation_gt)\n\n    # Convert the prediction string into an embedding\n    pred_embedding = embedding_model.embed_query(pred)\n\n    # Calculate the maximum cosine similarity between the prediction and each ground truth string\n    max_cosine_similarity = max(calculate_cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings)\n\n    return max_cosine_similarity\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n\n    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up. Faces will not be restored.\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except errors.FaceRestorationError as e:\n        logger.warning(f\"Failed to restore faces using GFPGAN: {e}\")\n        return np_image\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        logger.info(f\"Initialized CodeFormer face restorer with model from {dirname}\")\n    except errors.ModelLoadingError as e:\n        logger.error(f\"Failed to initialize CodeFormer face restorer with model from {dirname}: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        # Patch the facexlib with the given directory\n        shared.patch_facexlib(dirname)\n\n        # Initialize the GFPGAN face restorer with the model located in the specified directory\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n\n    except Exception as e:\n        # Report any exceptions that occur during the setup process\n        logger.error(f\"Error occurred while setting up GFPGAN face restorer: {e}\")\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  v_quat = jnp.concatenate([v, jnp.zeros_like(v)], axis=-1)\n\n  # Apply the rotation\n  rotated_quat = multiply(multiply(q, v_quat), inverse(q))\n\n  # Convert the rotated quaternion back to a vector\n  rotated_vector = rotated_quat[Ellipsis, :3]\n\n  return rotated_vector\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis = axis_angle[0:3]\n  angle = axis_angle[3]\n  axis = axis / linalg.norm(axis, axis=-1, keepdims=True)\n  half_angle = angle / 2.0\n  sin_half_angle = jnp.sin(half_angle)\n  cos_half_angle = jnp.cos(half_angle)\n  return jnp.concatenate(\n      [axis * sin_half_angle, cos_half_angle], axis=-1\n  )\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = 0\n    mid = (high + low) / 2\n    while high >= low + 1e-8:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(f'Invalid edge_behavior: {edge_behavior}. Must be one of \"CONSTANT_OUTSIDE\" or \"CLAMP\".')\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(f'Invalid method: {method}. Must be one of \"TRILINEAR\" or \"NEAREST\".')\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(f'Invalid coordinate_order: {coordinate_order}. Must be one of \"xyz\" or \"zyx\".')\n\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  if method == 'TRILINEAR':\n    return trilinear_interpolation_3d(\n        data, locations, edge_behavior, constant_values, coordinate_order)\n  elif method == 'NEAREST':\n    return nearest_neighbor_interpolation_3d(\n        data, locations, edge_behavior, constant_values, coordinate_order)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, max_val))\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, jnp.exp, (min_val, max_val))(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  @jax.custom_jvp\n  def safe_sqrt_fn(x):\n    \"\"\"\n    This function is a safe version of the square root function that can handle edge cases more gracefully. It uses a helper function to generate this safe square root function, which includes a custom gradient for backpropagation in automatic differentiation contexts. The safe version ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    return jnp.sqrt(jnp.clip(x, 0, max_val))\n\n  @safe_sqrt_fn.defjvp\n  def safe_sqrt_fn_jvp(primals, tangents):\n    \"\"\"\n    This function is a custom gradient for the safe square root function. It is used to compute the gradient of the safe square root function with respect to its input. The gradient is computed by applying the chain rule to the square root function and its custom gradient.\n\n    Input-Output Arguments\n    :param primals: The primal values of the safe square root function.\n    :param tangents: The tangent values of the safe square root function.\n    :return: The gradient of the safe square root function with respect to its input.\n    \"\"\"\n    (x,) = primals\n    (x_dot,) = tangents\n    y = safe_sqrt_fn(x)\n    y_dot = 0.5 * x_dot / y\n    return y, y_dot\n\n  return safe_sqrt_fn(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p == 1:\n    return 0\n  elif p == 2:\n    return 1\n  elif p == 3:\n    return 0\n  elif p == 4:\n    return 1\n  elif p == 5:\n    return 0\n  elif p == 6:\n    return 1\n  elif p == 7:\n    return 0\n  elif p == 8:\n    return 1\n  elif p == 9:\n    return 0\n  elif p == 10:\n    return 1\n  elif p == 11:\n    return 0\n  elif p == 12:\n    return 1\n  elif p == 13:\n    return 0\n  elif p == 14:\n    return 1\n  elif p == 15:\n    return 0\n  elif p == 16:\n    return 1\n  elif p == 17:\n    return 0\n  elif p == 18:\n    return 1\n  elif p == 19:\n    return 0\n  elif p == 20:\n    return 1\n  elif p == 21:\n    return 0\n  elif p == 22:\n    return 1\n  elif p == 23:\n    return 0\n  elif p == 24:\n    return 1\n  elif p == 25:\n    return 0\n  elif p == 26:\n    return 1\n  elif p == 27:\n    return 0\n  elif p == 28:\n    return 1\n  elif p == 29:\n    return 0\n  elif p == 30:\n    return 1\n  elif p == 31:\n    return 0\n  elif p == 32:\n    return 1\n  elif p == 33:\n    return 0\n  elif p == 34:\n    return 1\n  elif p == 35:\n    return 0\n  elif p == 36:\n    return 1\n  elif p == 37:\n    return 0\n  elif p == 38:\n    return 1\n  elif p == 39:\n    return 0\n  elif p == 40:\n    return 1\n  elif p == 41:\n    return 0\n  elif p == 4"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [-1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array(\n        [[0.5257311121191336, 0.0, 0.8506508083520399],\n         [0.0, 0.8506508083520399, 0.5257311121191336],\n         [-0.5257311121191336, 0.0, 0.8506508083520399],\n         [0.5257311121191336, 0.0, -0.8506508083520399],\n         [0.0, 0.8506508083520399, -0.5257311121191336],\n         [-0.5257311121191336, 0.0, -0.8506508083520399],\n         [0.5257311121191336, -0.8506508083520399, 0.0],\n         [-0.5257311121191336, -0.8506508083520399, 0.0],\n         [0.0, -0.8506"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.sign(x)\n  elif p == -jnp.inf:\n    y = jnp.where(x > 0, 1, 0)\n  elif p == jnp.inf:\n    y = jnp.where(x > 0, jnp.inf, 0)\n  else:\n    y = safe_div(safe_pow(x, p) - 1, p)\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y / premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(yp)),\n          (p == -jnp.inf, -safe_expm1(-yp)),\n          (p == jnp.inf, safe_expm1(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x / postmult\n  return x\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # Check if the delay period is specified\n  if lr_delay_steps > 0:\n    # Check if the current step is within the delay period\n    if step < lr_delay_steps:\n      # Apply the delay multiplier to the initial learning rate\n      lr_init *= lr_delay_mult\n      # Calculate the learning rate based on the delay period\n      lr = lr_init + (lr_final - lr_init) * step / lr_delay_steps\n    else:\n      # Calculate the learning rate based on the progression after the delay period\n      lr = lr_init + (lr_final - lr_init) * (step - lr_delay_steps) / (\n          max_steps - lr_delay_steps\n      )\n  else:\n    # Calculate the learning rate based on the progression without delay\n    lr = lr_init + (lr_final - lr_init) * step / max_steps\n\n  return lr\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=0.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Apply camera rotation matrices.\n  points_cam = matmul(camtoworlds[Ellipsis, :3, :3], points)\n  # Apply camera translation.\n  points_cam = points_cam + camtoworlds[Ellipsis, :3, -1]\n\n  # Apply inverse intrinsic matrices.\n  points_pix = matmul(pixtocams, points_cam)\n\n  # Apply distortion correction.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_undistort(\n        points_pix[Ellipsis, 0],\n        points_pix[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_pix = xnp.stack([x, y, points_pix[Ellipsis, 2]], axis=-1)\n\n  # Apply camera projection.\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points_pix[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points_pix = xnp.stack(\n        [\n            points_pix[Ellipsis, 0] * sin_theta_over_theta,\n            points_pix[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points_pix[Ellipsis, 0]\n    phi = points_pix[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    points_pix = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi),\n            -xnp.sin(phi) * xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Flip from OpenCV to OpenGL coordinate system.\n  points_pix = matmul(points_pix, xnp.diag(xnp.array([1.0, -1.0, -1.0"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the rotation and translation components from the screw axis.\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the exponential map for the rotation component.\n  R = exp_so3(w, eps)\n\n  # Compute the exponential map for the translation component.\n  v_hat = skew(v)\n  v_hat_exp = jnp.eye(3) + v_hat + (1.0 / 2.0) * v_hat @ v_hat\n\n  # Compute the exponential map for the screw axis.\n  S = jnp.block([[R, v_hat_exp], [jnp.array([[0.0, 0.0, 0.0, 1.0]])]])\n\n  return S\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Normalize the axis-angle vector\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Compute the sine and cosine of the angle of rotation\n  sin_theta = jnp.sin(jnp.linalg.norm(axis_angle))\n  cos_theta = jnp.cos(jnp.linalg.norm(axis_angle))\n\n  # Compute the cross product matrix of the axis of rotation\n  cross_product_matrix = skew(axis_angle)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  rotation_matrix = jnp.eye(3) + cross_product_matrix * sin_theta + cross_product_matrix @ cross_product_matrix * (1 - cos_theta)\n\n  # Return the rotation matrix\n  return rotation_matrix\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean and covariance of the Gaussian distribution.\n  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n  # Scale the covariance matrix based on the base radius.\n  cov *= base_radius**2\n\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean and variance of the Gaussian distribution based on the cylinder's axis, start and end distances, and radius.\n  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0) ** 2 / 12\n  r_var = radius ** 2\n\n  # Lift the Gaussian distribution to 3D coordinates.\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n  return mean, cov\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  cam_x, cam_y = xnp.meshgrid(pix_x_int, pix_y_int, indexing='xy')\n  cam_x = cam_x.flatten()\n  cam_y = cam_y.flatten()\n  cam_z = xnp.ones_like(cam_x)\n  cam_coords = xnp.stack([cam_x, cam_y, cam_z], axis=-1)\n  cam_coords = xnp.matmul(cam_coords, pixtocams)\n\n  # Convert camera coordinates to world coordinates.\n  world_coords = xnp.matmul(cam_coords, camtoworlds)\n\n  # Compute ray origins and directions.\n  origins = world_coords[:, :3]\n  directions = world_coords[:, 3:]\n\n  # Normalize ray directions.\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute differential radii.\n  radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute image plane coordinates.\n  imageplane = xnp.matmul(cam_coords, pixtocam_ndc)\n  imageplane = imageplane[:, :2] / imageplane[:, 2:]\n\n  # Apply lens distortion correction if provided.\n  if distortion_params is not None:\n    k1 = distortion_params.get('k1', 0.0)\n    k2 = distortion_params.get('k2', 0.0)\n    k3 = distortion_params.get('k3', 0.0)\n    k4 = distortion_params.get('k4', 0.0)\n    p1 = distortion_params.get('p1', 0.0)\n    p2 = distortion_params.get('p2', 0.0)\n\n    if camtype == ProjectionType.FISHEYE:\n      # Apply radial and tangential distortion correction for fisheye cameras.\n      cam_x, cam_y = _radial_and_tangential_undistort(\n          xd=cam_coords[:, 0],\n          yd=cam_coords[:, 1],\n          k1=k1,\n          k2"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  weights = compute_alpha_weights_helper(density_delta)\n\n  return weights\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Check input arguments\n  utils.assert_valid_stepfun(t, w_logits)\n  if rng is None:\n    if single_jitter:\n      raise ValueError(\"Cannot use single_jitter with deterministic sampling.\")\n    if deterministic_center:\n      raise ValueError(\"Cannot use deterministic_center with deterministic sampling.\")\n  else:\n    if deterministic_center:\n      raise ValueError(\"Cannot use deterministic_center with random sampling.\")\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Compute the endpoints of the intervals in the step function.\n  t_endpoints = t[Ellipsis, :-1]\n  t_endpoints = jnp.concatenate([t_endpoints, t[Ellipsis, -1:]], axis=-1)\n\n  # Compute the endpoints of the intervals in the PDF.\n  pdf_endpoints = jnp.diff(t_endpoints)\n\n  # Compute the endpoints of the intervals in the CDF.\n  cdf_endpoints = jnp.diff(cw)\n\n  # Compute the endpoints of the intervals in the inverse CDF.\n  inv_cdf_endpoints = cdf_endpoints / pdf_endpoints\n\n  # Compute the endpoints of the intervals in the inverse CDF.\n  inv_cdf_endpoints = jnp.concatenate([jnp.zeros_like(inv_cdf_endpoints[..., :1]), inv_cdf_endpoints, jnp.ones_like(inv_cdf_endpoints[..., -1:])], axis=-1)\n\n  # Compute the endpoints of the intervals in the inverse CDF.\n  inv_cdf_endpoints = jnp.concatenate([jnp.zeros_like(inv_cdf_endpoints[..., :1]), inv_cdf_endpoints, jnp.ones_like(inv_cdf_endpoints[..., -1:])], axis=-1)\n\n  # Compute the endpoints of the intervals in the inverse CDF.\n  inv_cdf_endpoints = jnp.concatenate([jnp.zeros_like(inv_cdf_endpoints[..., :1]), inv_cdf_endpoints, jnp.ones_like(inv_cdf_endpoints[..., -1:])], axis=-1)\n\n  # Compute the endpoints of the intervals in the inverse CDF.\n  inv_cdf_endpoints = jnp.concatenate([jnp.zeros_"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_midpoints = jnp.concatenate([jnp.array([domain[0]]), t_midpoints, jnp.array([domain[1]])], axis=-1)\n\n  return t_midpoints\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w)\n\n  # Compute the integrated weights.\n  cw = integrate_weights(w)\n\n  # Interpolate the integrated weights to the given percentiles.\n  ps = ps / 100\n  t_new = math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n\n  return t_new\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF\n  p_blurred = linspline.blur_pdf(t, p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  w_blurred = pdf_to_weight(tq, p_blurred)\n\n  return w_blurred\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return matmul(transform, to_homogeneous(vectors))\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input arguments are valid.\n  if t.ndim != 1:\n    raise ValueError(f'The input t must be a 1D tensor, but got {t.ndim}D.')\n  if tp.ndim != 1:\n    raise ValueError(f'The input tp must be a 1D tensor, but got {tp.ndim}D.')\n  if vp.ndim != 1:\n    raise ValueError(f'The input vp must be a 1D tensor, but got {vp.ndim}D.')\n  if t.shape[0] != tp.shape[0] + 1:\n    raise ValueError(\n        f'The number of elements in t must be one more than the number of elements in tp, but got {t.shape[0]} and {tp.shape[0]} respectively.'\n    )\n  if t.shape[0] != vp.shape[0]:\n    raise ValueError(\n        f'The number of elements in t must be the same as the number of elements in vp, but got {t.shape[0]} and {vp.shape[0]} respectively.'\n    )\n\n  # Compute the widths of the intervals in `t`.\n  widths = jnp.diff(t)\n\n  # Compute the indices of the intervals in `t` that contain the original time points in `tp`.\n  indices = jnp.searchsorted(t, tp)\n\n  # Resample the values of the step function at the original time points.\n  if use_avg:\n    # Compute the average value of the step function for each interval in `t`.\n    vp_avg = jnp.zeros_like(t)\n    vp_avg[1:-1] = (vp[indices[:-1]] + vp[indices[1:]]) / 2\n    vp_avg[0] = vp[0]\n    vp_avg[-1] = vp[-1]\n  else:\n    # Sum the values of the step function for each interval in `t`.\n    vp_avg = jnp.zeros_like(t)\n    vp_avg[1:-1] = vp[indices[:-1]] + vp[indices[1:]]\n    vp_avg[0] = vp[0]\n    vp_avg[-1] = vp[-1]\n\n  # Resample the values of the step function at the new intervals defined by `"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for each degree in the range.\n  scale_factors = 2 ** jnp.arange(min_deg, max_deg)\n\n  # Scale the mean and variance by the scaling factors.\n  scaled_mean = mean * scale_factors\n  scaled_var = var * scale_factors ** 2\n\n  # Concatenate the scaled mean and variance.\n  concat_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding to the concatenated mean and variance.\n  encoded_variables = math.safe_sin(concat_mean_var)\n\n  return encoded_variables\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding (DE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting DE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis=-1)\n\n  return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # remove duplicate lines\n    lines = list(dict.fromkeys(lines))\n\n    # remove empty lines\n    lines = [line for line in lines if len(line) > 0]\n\n    # remove lines with only numbers\n    lines = [line for line in lines if not line.isnumeric()]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with only floating characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n\n    # remove lines with"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # Normalize quotation marks\n    org_texts = quotation_pattern.sub('\"', org_texts)\n\n    # Normalize abbreviations\n    for rule, replaced in rules:\n        org_texts = rule.sub(replaced, org_texts)\n\n    # Normalize brackets\n    org_texts = bracket_rule.sub(r\"\\1\", org_texts)\n\n    # Normalize spaces\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # Tokenize sentences\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    return sentences\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Check if the token is a string or a list of strings\n        if isinstance(token, str):\n            token = [token]\n\n        # Check if the key is a valid document key\n        if key is not None and key < 0:\n            key += len(self)\n\n        # Initialize an empty list to store the positions\n        positions = []\n\n        # Iterate over each token in the list\n        for term in token:\n            # Check if the term is present in the term dictionary\n            if term in self.term_dict:\n                # Get the term ID from the term dictionary\n                term_id = self.term_dict.get_term_id(term)\n\n                # Get the positions for the term\n                if key is not None:\n                    # If a key is provided, get the positions for the given document\n                    positions.append(self.posns.positions(term_id, key))\n                else:\n                    # If no key is provided, get the positions across all documents\n                    positions.append(self.posns.positions(term_id))\n            else:\n                # If the term is not present in the term dictionary, add an empty array to the positions list\n                positions.append(np.array([]))\n\n        return positions\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # If the spec is a number, return the number as-is\n    if spec.isdigit():\n        return int(spec)\n\n    # If the spec is a percentage, calculate the minimum number of clauses that must match based on the percentage\n    elif spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(np.ceil(num_clauses * percentage / 100))\n\n    # If the spec is a conditional expression, calculate the minimum number of clauses that must match based on the conditional expression\n    elif '<' in spec:\n        parts = spec.split('<')\n        if len(parts) != 2:\n            raise ValueError(f\"Invalid 'min should match' specification: {spec}. The 'min should match' specification must be in the format 'num_clauses<min_should_match', where 'num_clauses' is the total number of clauses in the query and 'min_should_match' is the minimum number of clauses that must match.\")\n        num_clauses_str, min_should_match_str = parts\n        num_clauses_str = num_clauses_str.strip()\n        min_should_match_str = min_should_match_str.strip()\n        if not num_clauses_str.isdigit():\n            raise ValueError(f\"Invalid 'min should match' specification: {spec}. The 'num_clauses' part of the 'min should match' specification must be a positive integer.\")\n        if not min_should_match_str.isdigit():\n            raise ValueError(f\"Invalid 'min should match' specification: {spec}. The 'min_should_match' part of the 'min should match' specification must be a positive integer.\")\n        num_clauses = int(num_clauses_str)\n        min_should_match = int(min_should_match_str)\n        return min(num_clauses, min_should_match)\n\n    # If the spec is not a number, percentage, or conditional expression, raise an error\n    else:\n        raise ValueError(f\"Invalid 'min should match' specification: {spec}. The 'min should match' specification must be a positive integer, a percentage, or a conditional expression in the format 'num_clauses<min_should_match'.\")\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # Check if the tokens are unique\n        unique_tokens = set(tokens)\n        if len(unique_tokens) == len(tokens):\n            # If the tokens are unique, we can directly calculate the phrase frequencies using the positions of terms\n            return self._phrase_freq_unique_tokens(tokens, slop)\n        else:\n            # If the tokens are not unique, we delegate the calculation to another method that handles different slops or non-unique tokens\n            return self._phrase_freq_non_unique_tokens(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check dtype, raise TypeError\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check batch_size, raise TypeError\n        if not isinstance(batch_size, numbers.Integral):\n            raise TypeError(\"Expected int, got {}\".format(type(batch_size)))\n\n        # Check truncate, raise TypeError\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(truncate)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['proxyHost'], self.config['proxyPort'], self.config['serverHost'], self.config['serverPort'], self.config['strategies'], self.config['autoCloseConnections'], self.config['multipleConnections'])\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server.start()\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.bitwise_and(arr, mask)\n    arr = np.bitwise_or(arr, np.right_shift(arr, _1))\n    arr = np.bitwise_and(arr, s55)\n    arr = np.bitwise_or(arr, np.right_shift(arr, _2))\n    arr = np.bitwise_and(arr, s33)\n    arr = np.bitwise_or(arr, np.right_shift(arr, _4))\n    arr = np.bitwise_and(arr, s0F)\n    arr = np.bitwise_or(arr, np.right_shift(arr, _8))\n    arr = np.bitwise_and(arr, s01)\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            self.lock.acquire()\n            if message.connection_id not in self.connections:\n                self.connections[message.connection_id] = Connection(\n                    connection_id=message.connection_id,\n                    server=self.server,\n                    logger=self.logger\n                )\n            self.lock.release()\n            message.data = self.connections[message.connection_id].c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            self.lock.acquire()\n            if message.connection_id not in self.connections:\n                self.connections[message.connection_id] = Connection(\n                    connection_id=message.connection_id,\n                    server=self.server,\n                    logger=self.logger\n                )\n            self.lock.release()\n            message.data = self.connections[message.connection_id].s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self.lock.acquire()\n            if message.connection_id in self.connections:\n                self.connections[message.connection_id].close()\n                if self.config['autoCloseConnections']:\n                    del self.connections[message.connection_id]\n            self.lock.release()\n        else:\n            pass"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n            self.connections.clear()\n\n        self.server.stop()\n"}
