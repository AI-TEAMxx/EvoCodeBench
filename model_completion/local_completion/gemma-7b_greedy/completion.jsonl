{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        cholesky = np.linalg.cholesky(x, lower=True)\n    except np.linalg.LinAlgError:\n        return False\n    return True\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.sum(z**2, axis=-1, keepdims=True)\n  scale = (1 - jnp.sqrt(1 - z_mag_sq)) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    \n    def decorator(func):\n    \n        def wrapper(*args, **kwargs):\n            \n            # Get the database connection\n            conn = sqlite3.connect(filename)\n            \n            # Get the dispensition of the function name\n            cur = conn.cursor()\n            cur.execute(\"SELECT dispensition FROM dispensition WHERE dispensition = ?\", (func_name,))\n            \n            # If dispensition is dispensition, pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessitimately pessimistimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessitimately pessi"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] > values[\"x_max\"] or values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: The bounding box is invalid. The minimum x and y values must be less than the maximum x and y values, respectively.\"\n        )\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0 = np.atleast_2d(mat0)\n  mat1 = np.atleast_2d(mat1)\n  if mat0.shape[1] != mat1.shape[1]:\n    raise ValueError(\"Matrices must have the same number of columns.\")\n  if mat0.shape[1] == 1:\n    return mat0\n  return (mat0 * mat0).sum(axis=-1) - 2 * mat1.dot(mat0.T) + mat1 * mat1\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return True\n    if path.startswith(\"file://\") or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\") or path.startswith(\"wasb://\") or path.startswith(\"wasbs://\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if dim == 1:\n        if isinstance(items, dict):\n            if assets_names is None:\n                raise ValueError(\n                    f\"When {name} is a dictionary, the 'assets_names' argument is required.\"\n                )\n            if not isinstance(assets_names, np.ndarray):\n                raise TypeError(\n                    f\"'assets_names' must be a numpy array, got {type(assets_names)}\"\n                )\n            if len(assets_names) != n_assets:\n                raise ValueError(\n                    f\"'assets_names' must have {n_assets} elements, got {len(assets_names)}\"\n                )\n            items = {\n                name: items.get(name, fill_value) for name in assets_names\n            }\n        if not isinstance(items, np.ndarray):\n            items = np. mozaic_array(items, n_assets, fill_value)\n        if items.ndim != 1:\n            raise ValueError(\n                f\"When {name} is a dictionary or a 1D array, the 'items' argument must be a 1D array, got {items.ndim}D\"\n            )\n        if items.shape != (n_assets,):\n            raise ValueError(\n                f\"When {name} is a dictionary or a 1D array, the 'items' argument must have {n_assets} elements, got {items.shape[0]}.\"\n            )\n    elif dim == 2:\n        if not isinstance(items, np.ndarray):\n            items = np.array(items)\n        if items.ndim != 2:\n            raise ValueError(\n                f\"When {name} is a 2D array, the 'items' argument must be a 2D array, got {items.ndim}D\"\n            )\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"When {name} is a 2D array, the 'items' argument must have {n_assets} columns, got {items.shape[1]}.\"\n            )\n    else:\n        raise ValueError(\n            f\"The 'dim' argument must be either 1 or 2,"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        if \"purpose_embedding\" in data:\n            purpose_embedding = data[\"purpose_embedding\"]\n        else:\n            purpose_embedding = None\n\n        if \"id\" in data:\n            id = data[\"id\"]\n        else:\n            id = None\n\n        if \"parent_id\" in data:\n            parent_id = data[\"parent_id\"]\n        else:\n            parent_id = None\n\n        if \"working_agent\" in data:\n            working_agent = data[\"working_agent\"]\n        else:\n            working_agent = None\n\n        if \"is_prime\" in data:\n            is_prime = data[\"is_prime\"]\n        else:\n            is_prime = None\n\n        if \"evolve_count\" in data:\n            evolve_count = data[\"evolve_count\"]\n        else:\n            evolve_count = None\n\n        if \"number_of_code_executions\" in data:\n            number_of_code_executions = data[\"number_of_code_executions\"]\n        else:\n            number_of_code_executions = None\n\n        if \"last_input\" in data:\n            last_input = data[\"last_input\"]\n        else:\n            last_input = None\n\n        if \"usage_count\" in data:\n            usage_count = data[\"usage_count\"]\n        else:\n            usage_count = None\n\n        if \"depth\" in data:\n            depth = data[\"depth\"]\n        else:\n            depth = None\n\n        if \"max_depth\" in data:\n            max_depth = data[\"max_depth\"]\n        else:\n            max_depth = None\n\n        if \"dynamic_prompt\" in data:\n            dynamic_prompt = data[\"dynamic_prompt\"]\n        else:\n            dynamic_prompt = None\n\n        if \"purpose\" in data:\n            purpose = data[\"purpose\"]\n        else:\n            purpose = None\n\n        return MicroAgent(\n            agent_lifecycle,\n            openai_wrapper,\n            purpose_embedding,\n            id,\n            parent_id,\n            working_agent,\n            is_prime,\n            evolve_count,\n            number_of_code_execu"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb * 12.92\n  linear1 = (srgb + 0.055) * 1.055 - 0.055\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  x = np.array(x)\n  t_input = np.array(t_input)\n  t_output = np.array(t_output)\n  k = min(spline_degree, len(t_input) - 1)\n  tck, _ = scipy.interpolate.splprep(t_input, k=k, s=smoothness)\n  return np.array(scipy.interpolate.splev(t_output, tck))\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    if word.isupper():\n        return word\n    if word[0].islower() and word[1].islower():\n        return word.lower()\n    if word[0].isupper() and word[1].islower():\n        return word.title()\n    if word[0].islower() and word[1].isupper():\n        return word.title()\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.all(v.dtype == np.bool):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array.\"\n        )\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag = jnp.sqrt(jnp.sum(x**2, axis=-1))\n  return x * (2 * jnp.sqrt(x_mag) - x_mag)\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if not os.path.exists(summary_path):\n        raise ValueError(f\"The summary file {summary_path} does not exist.\")\n    summary_df = pd.read_csv(summary_path, index_col=0, header=0, sep='\\t')\n    if dict_columns is not None:\n        for column in dict_columns:\n            summary_df[column] = summary_df[column].apply(ast.literal_eval)\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if not jnp.all(jnp.isfinite(cov)):\n    raise ValueError('Covariance matrix is not finite')\n  if mode == 'fast':\n    sqrtm_mat = sqrtm(cov)\n  else:\n    log_det = jnp.log(jnp.abs(jnp.linalg.det(cov)))\n    if jnp.any(jnp.isnan(log_det)):\n      raise ValueError('Covariance matrix is not positive definite')\n    sqrtm_mat = sqrtm(jnp.exp(log_det / 2), return_eigs=True)[0]\n  return sqrtm_mat\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n    parser.add_argument('--task', type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument('--upload-files', nargs='*', type=str, help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument('--model', type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument('--record-dir', type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument('--mode', type=str, help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\", default=\"auto\")\n    parser.add_argument('--quiet', action='store_true', help=\"If set, the program runs in quiet mode with minimal output.\", default=False)\n    parser.add_argument('--max-subtask-chain-length', type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\", default=None)\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help=\"Flag to enable asking for human assistance during task execution.\", default=False)\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\", default=None)\n    parser.add_argument('--max-plan-tree-depth', type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\", default=None)\n    parser.add_argument('--max-plan-tree-width', type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\", default=None)\n    parser.add_argument('--max-retry-times', type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\", default=None)\n    parser.add_argument('--config-file', type=str, help=\"Path to the configuration file, specifying where to find the configuration settings.\","}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape != (2,):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}\"\n        )\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    if n < 0:\n        raise ValueError(\"The integer to be encoded must be non-negative\")\n    if n == 0:\n        return \"0\"\n    if n < 10:\n        return str(n)\n    if n < 36:\n        return string.ascii_lowercase[n - 10]\n    if n < 62:\n        return string.digits[n - 10 - 26]\n    if n < 62 * 2:\n        return string.ascii_lowercase[n - 62 - 10]\n    if n < 62 * 3:\n        return string.digits[n - 10 - 26 - 2]\n    if n < 62 * 4:\n        return string.ascii_uppercase[n - 62 * 3 - 10 - 10]\n    if n < 62 * 5:\n        return string.digits[n - 10 - 26 - 2 - 2]\n    if n < 62 * 6:\n        return string.ascii_uppercase[n - 10 - 26 - 10 - 10]\n    if n < 62 * 7:\n        return string.digits[n - 10 - 26 - 10 - 2 - 2]\n    if n < 62 * 8:\n        return string.ascii_uppercase[n - 10 - 26 - 10 - 10 - 10 - 10]\n    if n < 62 * 9:\n        return string.digits[n - 10 - 26 - 10 - 10 - 10 - 10 - 2 - 2]\n    if n < 62 * 10:\n        return string.ascii_uppercase[n - 10 - 26 - 10 - 10 - 10 - 10 - 10 - 10 - 10]\n    if n < 62 * 11:\n        return string.digits[n - 1"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.clip(x, eps, jnp.inf))\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_index[worker_idx] = 0\n    for worker_idx in range(len(workers_intervals)):\n        interval = workers_intervals[worker_idx][chunk_index[worker_idx]]\n        indexes[worker_idx] = indexes[worker_idx] + interval[1]\n        chunk_index[worker_idx] += 1\n\n    return chunk_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilinear_resample(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilinear_resample(values, coordinates)\n  else:\n    raise ValueError('datastructure must be either \"grid\" or \"hash\"')\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the weights for the vertices of the triangle.\n  w1 = np.array([1.0, 1.0, 1.0])\n  w2 = np.array([v**2 - 1.0, v**2 - 1.0, 1.0])\n  w3 = np.array([1.0, v**2 - 1.0, v**2 - 1.0])\n  # Compute the weights for the barycentric coordinates of the tessellated triangle.\n  w1 = w1 / np.sum(w1)\n  w2 = w2 / np.sum(w2)\n  w3 = w3 / np.sum(w3)\n  return np.array([w1, w2, w3])\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(\n      jnp.all(tq >= t[0]) and jnp.all(tq <= t[-1]),\n      'Query points must be within the range of the given time points.')\n  checkify.check(\n      len(tq.shape) == len(t.shape),\n      'The number of dimensions of the query points must match the number of '\n      'dimensions of the time points.')\n  checkify.check(\n      len(tq.shape) == len(v.shape),\n      'The number of dimensions of the query points must match the number of '\n      'dimensions of the values.')\n  checkify.check(\n      len(t.shape) == len(v.shape),\n      'The number of dimensions of the time points must match the number of '\n      'dimensions of the values.')\n  checkify.check(\n      t.shape == v.shape,\n      'The time points and values must have the same shape.')\n  checkify.check(\n      jnp.all(t[:-1] < t[1:]),\n      'The time points must be sorted in increasing order.')\n  checkify.check(\n      jnp.all(tq[:-1] < tq[1:]),\n      'The query points must be sorted in increasing order.')\n  checkify.check(\n      jnp.all(tq[:-1] < t),\n      'The query points must be within the range of the time points.')\n  checkify.check(\n      jnp.all(t < tq[Ellipsis, -1]),\n      'The query points must be within the range of the time points.')\n\n  # The query points are in the same order as the time points.\n  tq = jnp.where(tq < t[Ellipsis, -1], tq, t[Ellipsis, -1])\n  tq = jnp.where(tq > t[Ellipsis, 0], tq, t[Ellipsis, 0])\n  return (\n      tq - t[Ellipsis, :-1]) / (t[Ellipsis, 1:] - t[Ellipsis, :-1]) * (v[Ellipsis, 1:] - v[Ellipsis, :-1])"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, (int, float)) and v < 0:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be positive. Got {v}\"\n        )\n    if isinstance(v, (list, np.ndarray)) and any(x < 0 for x in v):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be positive. Got {v}\"\n        )\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.asarray(origins)\n  directions = xnp.asarray(directions)\n  pixtocam = xnp.asarray(pixtocam)\n  if not (origins.shape == directions.shape or origins.shape == (\n      origins.shape[0], 1, 3)):\n    raise ValueError(\n        f\"origins and directions must have the same shape or be a 1-by-3 array, \"\n        f\"but got {origins.shape} and {directions.shape}.\")\n  if not (pixtocam.shape == (3, 3) or pixtocam.shape == (3, 4)):\n    raise ValueError(\"pixtocam must be a 3-by-3 or 3-by-4 array, but got \"\n                     f\"{pixtocam.shape}.\")\n  if not (origins.dtype == directions.dtype or\n          origins.dtype == xnp.float32 or\n          directions.dtype == xnp.float32):\n    raise ValueError(\n        f\"origins and directions must have the same data type or be float32, \"\n        f\"but got {origins.dtype} and {directions.dtype}.\")\n  if not (pixtocam.dtype == xnp.float32 or\n          pixtocam.dtype == xnp.float32):\n    raise ValueError(\n        f\"pixtocam must be float32, but got {pixtocam.dtype}.\")\n  if not (origins.size > 0 and directions.size > 0 and\n          pixtocam.size > 0):\n    raise ValueError(\"origins, directions and pixtocam must be non-empty.\")\n  if not (origins.ndim == 2 and directions.ndim == 2 and\n          pixtocam.ndim == 2):\n    raise ValueError(\n        f\"origins, directions and pixtocam must be 2D arrays, but got \"\n        f\"{origins.ndim}, {directions.ndim} and {pixtocam.ndim}.\")\n  if not (origins.shape[1] == directions.shape[1] == pixtocam.shape[0]):\n    raise ValueError(\n        f\"origins, directions and pixtocam must have"}
{"namespace": "geometry.are_lines_parallel", "completion": "  return jnp.isclose(jnp.sum(dir1 * dir2, axis=-1), 1.0)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda x: jieba.cut(x)\n    return evaluate.bleu(\n        continuation,\n        reference,\n        with_penalty=with_penalty,\n        tokenizer=tokenizer\n    )\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(\n      x < eps,\n      value_at_zero,\n      jnp.sqrt(x))\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / (t[1:] - t[:-1])\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for (dirpath, dirnames, filenames) in os.walk(path):\n        for f in filenames:\n            try:\n                total_size += os.path.getsize(os.path.join(dirpath, f))\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if not isinstance(val, (np.ndarray, torch.Tensor)):\n        raise ValueError(\n            f'The input value {val} is not a ndarray or a tensor')\n\n    if not isinstance(offset, (np.ndarray, torch.Tensor)):\n        offset = np.array([offset])\n    if not isinstance(period, (np.ndarray, torch.Tensor)):\n        period = np.array([period])\n\n    if not isinstance(val, np.ndarray):\n        val = val.cpu().numpy()\n    if not isinstance(offset, np.ndarray):\n        offset = offset.cpu().numpy()\n    if not isinstance(period, np.ndarray):\n        period = period.cpu().numpy()\n    if not (isinstance(val, np.ndarray) and isinstance(offset, np.ndarray) and isinstance(period, np.ndarray)):\n        raise ValueError(\n            f'The input value {val}, offset {offset}, and period {period} are not all ndarray')\n\n    if not (val.ndim == offset.ndim == period.ndim == 1):\n        raise ValueError(\n            f'The input value {val}, offset {offset}, and period {period} are not all 1-D')\n\n    if not (val.size == offset.size == period.size):\n        raise ValueError(\n            f'The input value {val}, offset {offset}, and period {period} are not all of the same size')\n\n    if not (val.size == 1 or (val.size == 2 and val.size == offset.size == period.size == 1)):\n        raise ValueError(\n            f'The input value {val}, offset {offset}, and period {period} are not all of size 1 or 2')\n\n    if not (val.size == 1 or (val.size == 2 and val.size == offset.size == period.size == 1)):\n        raise ValueError(\n            f'The input value {val}, offset {offset}, and period {period} are not all of size 1 or 2')\n\n    if not (val.size == 1 or (val.size == 2 and val.size == offset.size == period.size == 1)):\n        raise ValueError"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent.id if agent.working_agent is not None else None,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # TODO: Implement the function\n    #\n    # Hints:\n    # - The function should pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistically pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately pessimistimately"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # The data to be hashed is the concatenation of the function name, the\n        # function arguments, and the function keyword arguments.\n        data = func_name.encode()\n        for arg in args:\n            data += str(arg).encode()\n        for kwarg in kwargs:\n            data += kwarg.encode()\n            data += str(kwargs[kwarg]).encode()\n\n        # Compute the hash\n        return hashlib.sha256(data).hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(\n            f\"Unable to determine the area of a polygon with shape {polygon.shape}. Expecting (_, 2).\"\n        )\n\n    if max_point_distance < 1:\n        raise ValueError(\"The maximum point distance must be greater than or equal to 1.\")\n\n    xs, ys = polygon.T\n    distances = np.sqrt(np.square(xs[1:] - xs[:-1]) + np.square(ys[1:] - ys[:-1]))\n    distances = np.concatenate(\n        [distances, np.array([max_point_distance])]\n    )  # add a max_point_distance to the end of the array to avoid the last point to be ignored\n    return np.sum(np.minimum(distances, max_point_distance))"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n    if not all([type(rel_tr) == NonNegativeFloat and rel_tr >= 0 for _ in range(len(polygons))]):\n        raise VectorizationError(\"rel_tr is not a non-negative float\")\n    if not all([type(abs_tr) == NonNegativeFloat and abs_tr >= 0 for _ in range(len(polygons))]):\n        raise VectorizationError(\"abs_tr is not a non-negative float\")\n    if not all([type(polygons[i]) == np.ndarray for i in range(len(polygons))]):\n        raise VectorizationError(\"polygons is not a list of numpy arrays\")\n    if not all([polygons[i].shape == ( poligon.shape for poligon in polygons if i != 0]):\n        raise VectorizationError(\"polygons are not of the same dimention\")\n    if not all([type(polygons[i].dtype) == poligon.dtype for poligon in polygons if i != 0]):\n        raise VectorizationError(\"polygons are not of the same data type\")\n    if not all([polygons[i].size == poligon.size for poligon in polygons if i != 0]):\n        raise VectorizationError(\"polygons are not of the same dimention\")\n    if not all([polygons[i].dtype == poligon.dtype for poligon in polygons if i != 0]):\n        raise VectorizationError(\"polygons are not of the same data type\")\n    if not all([polygons[i].ndim == poligon.ndim for poligon in polygons if i != 0]):\n        raise VectorizationError(\"polygons are not of the same dimention\")\n    if not all([polygons[i].size == poligon.size for poligon in polygons if i != 0]):\n        raise VectorizationError(\"polygons are not of the same dimention\")\n    if not all([polygons[i].size == poligon.size for poligon in polygons if i != 0]):\n        raise VectorizationError(\"polygons are not of the same dimention\")\n    if not all([polygons["}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    if num_samples_yielded == 0:\n        return {i: 0 for i in range(num_workers)}\n\n    # Calculate the number of samples that should be distributed evenly among the workers\n    num_samples_per_worker = num_samples_yielded // num_workers\n    # Calculate the number of samples that should be distributed evenly among the workers, but with the batch size\n    num_samples_per_worker_with_batch_size = num_samples_per_worker // batch_size\n    # Calculate the number of workers that have processed the maximum number of samples\n    num_workers_max_samples = num_samples_per_worker_with_batch_size * batch_size\n    # Calculate the number of samples that have been processed by the workers that have not reached the maximum number of samples\n    num_samples_processed_by_other_workers = num_samples_yielded - num_workers_max_samples\n    # Calculate the number of samples that should be distributed evenly among the workers that have not reached the maximum number of samples\n    num_samples_per_worker_other_workers = num_samples_processed_by_other_workers // (num_workers - num_workers_max_samples)\n    # Calculate the number of samples that should be distributed evenly among the workers that have not reached the maximum number of samples, but with the batch size\n    num_samples_per_worker_other_workers_with_batch_size = num_samples_per_worker_other_workers // batch_size\n    # Calculate the number of workers that have not reached the maximum number of samples\n    num_workers_other_workers = num_samples_per_worker_other_workers_with_batch_size * batch_size\n    # Calculate the number of samples that have been processed by the workers that have not reached the maximum number of samples\n    num_samples_other_workers = num_samples_per_worker_other_workers * (num_workers - num_workers_max_samples)\n    # Calculate the number of samples that should be distributed evenly among the workers that have not reached the maximum number of samples\n    num_samples_per_worker_other_workers = num_samples_processed_by_other_workers - num_"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not value or not results or not threshold:\n        return [], []\n    if not metadatas:\n        metadatas = [None] * len(results)\n    return [result for result, value in zip(results, value) if value <= threshold], [\n        metadatas[index] for index, value in enumerate(value) if value <= threshold\n    ]\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape != (2,):\n        raise ValueError(\"The input array must have the shape (_, 2), indicating it represents a list of polygon points\")\n    return 0.5 * np.sum(array[0] * array[1][:-1] - array[1] * array[0][:-1])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    if a.size(-1) != v.size(-1):\n        raise Exception('a and v must have the same number of dimensions')\n    if a.size() < v.size():\n        raise Exception('a must be at least as large as v')\n\n    a_shape = a.size()\n    v_shape = v.size()\n    a_size = a.size(-1)\n    v_size = v.size(-1)\n    a_flat = a.view(-1)\n    v_flat = v.view(-1)\n    idx_lo = torch.searchsorted(a_flat, v_flat)\n    idx_hi = idx_lo + a_size\n    idx_lo = idx_lo.view(a_shape + (v_size,))\n    idx_hi = idx_hi.view(a_shape + (v_size,))\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n\n"}
{"namespace": "coord.contract", "completion": "  return x / (1 + jnp.logaddexp(1, jnp.sum(x * x, axis=-1, keepdims=True)))\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1000:\n        return f\"{num_bytes} B\"\n    for suffix in _FORMAT_TO_RATIO:\n        if num_bytes < _FORMAT_TO_RATIO[suffix]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[suffix]:.2f} {suffix}\"\n        num_bytes /= _FORMAT_TO_RATIO[suffix]\n    return f\"{num_bytes:.1f} PB\""}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\"\n            )\n\n        return v\n\n    return is_array_n_dimensions"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(jnp.clip(cartesian_vector[..., 2], -1 + eps, 1 - eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rouge-l']\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return __import__(name)\n    except (ImportError, AttributeError):\n        pass\n\n    # The object is not importable, so we try to locate it using a fallback\n    # method.\n    try:\n        return pydoc.import_object(name)\n    except (ImportError, AttributeError):\n        raise ImportError(\n            f\"The object of name '{name}' is not importable\"\n        )\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    return torch.jit.load(buf)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    if not isinstance(ids, tuple elektra.core.types.List):\n        raise elektra.core.exceptions.InvalidInputException(\n            \"The ids input must be a tuple of lists.\")\n    if not isinstance(scores, tuple elektra.core.types.List):\n        raise elektra.core.exceptions.InvalidInputException(\n            \"The scores input must be a tuple of lists.\")\n    if not isinstance(top_k, int):\n        raise elektra.core.exceptions.InvalidInputException(\n            \"The top_k input must be an int.\")\n    if not isinstance(weights, tuple elektra.core.types.List):\n        raise elektra.core.exceptions.InvalidInputException(\n            \"The weights input must be a tuple of floats.\")\n    if not all(isinstance(weight, float) for weight in weights):\n        raise elektra.core.exceptions.InvalidInputException(\n            \"The weights input must be a tuple of floats.\")\n    if not all(len(ids) == len(scores) == len(weights)):\n        raise elektra.core.exceptions.InvalidInputException(\n            \"The ids, scores, and weights inputs must have the same length.\")\n    if not all(sum(weights) == 1):\n        raise elektra.core.exceptions.InvalidInputException(\n            \"The sum of the weights must be 1.\")\n\n    # Normalize the scores\n    for i in range(len(weights)):\n        scores[i] = [score / sum(scores[i]) for score in scores[i]]\n\n    # Calculate the combined scores\n    scores = [\n        sum([weight * score for weight, score in zip(weights, score_list)])\n        for weight, score_list in zip(weights, scores)]\n    # Select the top_k results\n    return elektra.core.types.List(\n        [id_list[top_k - 1] for id_list in ids],\n        [score for score in scores[top_k - 1]])"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        return f\"{x * 100:.2f}%\"\n    return f\"{x:.2f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        try:\n            free_space = os.statvfs(input_dir).f_bavail\n        except FileNotFoundError:\n            return\n        if free_space < threshold_in_gb * 1024 * 1024 * 1024:\n            return\n        sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if n < zeros:\n        raise ValueError(\"The number of zeros must not exceed the number of weights.\")\n    weights = np.random.dirichlet(np.ones(n))\n    if zeros > 0:\n        zeros_idx = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[zeros_idx] = 0\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        return cls(module_type=module_dict['module_type'], module_param=module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    if \"bbox\" in instance:\n        bbox = instance[\"bbox\"]\n    else:\n        bbox = instance[\"bbox_mode\"].convert(instance[\"keypoint_2d\"], instance[\"keypoint_mode\"], BoxMode.XYXY_ABS)\n    bbox = bbox.to_xyxy()\n    bbox = bbox.clip(min=0)\n    bbox = bbox.to_xyxy()\n    center = bbox.center()\n    center = center.to_xy()\n    center = center.to_tuple()\n    center = (\n        center[0] - crop_size[1] // 2,\n        center[1] - crop_size[0] // 2,\n    )\n    center = np.array(center)\n    center = center.astype(\"int\")\n    center = center.tolist()\n    return T.CropTransform(\n        center,\n        crop_size,\n        image_size,\n    )\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  return x / (jnp.sqrt(jnp.sum(x * x, axis=-1, keepdims=True)) * (\n      jnp.where(jnp.isclose(jnp.sum(x * x, axis=-1, keepdims=True), 0.0),\n                grad_eps,\n                1.0))\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split(\"Use Agent[\")[-1].split(\"]\")[0]\n        input_text = response.split(\"Use Agent[\" + agent_name + \"]:\")[-1]\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if \"annotations\" in annos:\n        annos = annos[\"annotations\"]\n    if \"image_id\" in annos:\n        annos = annos[\"image_id\"]\n\n    if isinstance(annos, int):\n        annos = [annos]\n    elif not isinstance(annos, list):\n        raise TypeError(\n            \"annos must be a list of dicts or a single int, got type {}\".format(\n                type(annos)\n            )\n        )\n\n    if mask_format not in [\"polygon\", \"bitmask\"]:\n        raise ValueError(\n            \"mask_format must be 'polygon' or 'bitmask', got {}\"\n            \"\".format(mask_format)\n        )\n\n    # create a new instance for each image\n    num_images = len(annos)\n    if not all(isinstance(a, dict) for a in annos):\n        raise ValueError(\"annos must be a list of dicts\")\n\n    instances = [\n        Instances(image_size, num_classes=len(MetadataCatalog.get_classes()))\n        for _ in range(num_images)\n    ]\n\n    for i, anno in enumerate(annos):\n        if \"bbox\" in anno:\n            anno_box = anno[\"bbox\"]\n            if anno_box is not None:\n                anno_box = np.array(anno_box, dtype=np.float32)\n                anno_box = anno_box.reshape((1, -1))\n                anno_box = anno_box.copy()\n                anno_box = anno_box.reshape((4, 2))\n        else:\n            anno_box = None\n        if \"area\" in anno:\n            anno_area = anno[\"area\"]\n        else:\n            anno_area = None\n        if \"iscrowd\" in anno:\n            anno_iscrowd = anno[\"iscrowd\"]\n        else:\n            anno_iscrowd = None\n        if \"iscrowd\" in anno:\n            anno_iscrowd = anno[\"iscrowd\"]\n        else:\n            anno_iscrowd = None\n        if \"isdifficult\" in anno:\n            anno_isdifficult = anno[\"isdifficult\"]"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.path.join(\n            os.path.expanduser(\"~\"), \"skfolio_data\"\n        )\n    if not os.path.exists(data_home):\n        os.mkdir(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert cov.ndim == 2\n    if cov.size == 1:\n        return cov, np.ones(cov.size)\n    return np.corrcoef(cov), np.std(cov, axis=1)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: this is a bit of a hack, but the only way to get the\n    # submodules of a model in detectron2\n    # (https://github.com/facebookresearch/detectron2/issues/111)\n    if not hasattr(model, \"modules\"):\n        model.modules = model.children()\n\n    def _freeze_training_mode(m):\n        if hasattr(m, \"training\") and m.training is not None:\n            m.training = False\n\n    with ExitStack() as stack:\n        ctxs = [stack.enter_context(_freeze_training_mode) for m in model.modules]\n        yield"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[Any]]) -> Dict[str, List[Any]]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1].shape) != len(values[field2].shape):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shapes mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not isinstance(metrics, list):\n        metrics = [metrics]\n\n    if not all(isinstance(metric, (str, dict)) for metric in metrics):\n        raise ValueError(\"The input must be a list of strings or dictionaries.\")\n\n    if not all(isinstance(metric, str) for metric in metrics if isinstance(metric, str)):\n        if not all(isinstance(metric, dict) for metric in metrics if isinstance(metric, dict)):\n            raise ValueError(\"The input must be a list of strings or dictionaries.\")\n        else:\n            metrics = [metric[\"name\"] for metric in metrics]\n\n    return metrics, [metric for metric in metrics if isinstance(metric, dict)]\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == geopoly.contract:\n      fn_inv = geopoly.inv_contract\n    elif fn == contract:\n      fn_inv = inv_contract\n  else:\n    fn_inv = jax.jit(fn_inv)\n  t_near, t_far = t_near[..., 0], t_far[..., 0]\n  t_range = t_far - t_near\n  t_to_s = (fn - t_near) / t_range\n  s_to_t = t_range * t_to_s + t_near\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  return r * jnp.array([\n    jnp.cos(phi) * jnp.cos(theta),\n    jnp.sin(phi) * jnp.cos(theta),\n    jnp.sin(theta),\n  ])\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(t, w)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_scores = [\n        sum([weight * score for weight, score in zip(weights, scores)])\n        for id_list, score_list in zip(ids, scores)]\n    # Normalize the weighted scores\n    normalized_scores = [\n        score / sum(weighted_scores)\n        for score in weighted_scores]\n    # Get the top K IDs and their corresponding scores\n    top_k_ids = [\n        id_list[:top_k]\n        for id_list, score in zip(ids, normalized_scores)\n    ]\n    top_k_scores = [\n        score[:top_k]\n        for id_list, score in zip(ids, normalized_scores)\n    ]\n    return top_k_ids, top_k_scores"}
{"namespace": "coord.track_linearize", "completion": "  # TODO(b/204010111): This is a very naive implementation.\n  # TODO(b/204010111): This is a very naive implementation.\n  fn_mean = fn(mean)\n  fn_jac = jax.jacfwd(fn, mean)\n  fn_jac_cov = jnp.tensordot(fn_jac, cov, axes=[-1, -1])\n  fn_jac_cov_fn_jac = jax.jacfwd(fn_jac, mean)\n  fn_cov = jnp.tensordot(fn_jac_cov, fn_jac_cov_fn_jac, axes=[-1, -1])\n  return fn_mean, fn_cov\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) == 1:\n            yield [i]\n        else:\n            yield [i[: int(len(i) / 2)], [i[int(len(i) / 2) :]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # TODO: this is a bit of a hack.\n  if not append_identity:\n    min_deg = 1\n  if not (min_deg < max_deg):\n    raise ValueError('min_deg must be less than max_deg')\n  if x.shape[-1] != 3:\n    raise ValueError('x must be 3D')\n  if x.shape[-1] != 3:\n    raise ValueError('x must be 3D')\n  if not (min_deg < max_deg):\n    raise ValueError('min_deg must be less than max_deg')\n  if x.shape[-1] != 3:\n    raise ValueError('x must be 3D')\n  x_scale = jnp.sqrt(jnp.sum(x**2, axis=-1))\n  x_sin = jnp.sin(x_scale * 2.0 * jnp.pi)\n  x_cos = jnp.cos(x_scale * 2.0 * jnp.pi)\n  if append_identity:\n    return jnp.concatenate([x, x_sin, x_cos], axis=-1)\n  else:\n    return jnp.concatenate([x_sin, x_cos], axis=-1)\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch.\")\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(eglctx, camera)\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Initialize the new config\n    nomic_config = NomicBertConfig()\n\n    # Copy the inherited settings\n    for attr in dir(bert_config):\n        if not re.match(\"^(is_type_id|is_position_id|is_token_type_id|is_label_id|is_mask_id|is_input_ids|is_attention_mask|is_token_type_ids|is_position_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_input_ids|is_labels|is_"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Select the appropriate shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_POINTS, len(self.faces), gl.GL_UNSIGNED_INT, self.ebo)\n            else:\n                gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            if self.faces is not None:\n                if self.render_type == Mesh.RenderType.LINES:\n                    gl.glDrawElements(gl.GL_LINES, len(self.faces), gl.GL_UNSIGNED_INT, self.ebo)\n                elif self.render_type == Mesh.RenderType.TRIS:\n                    gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, self.ebo)\n                elif self.render_type == Mesh.RenderType.QUADS:\n                    gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, self.ebo)\n                elif self.render_type == Mesh.RenderType.STRIPS:\n                    gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, self.ebo)\n            else:\n                if self.render_type == Mesh.RenderType.LINES:\n                    gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n                elif self.render_type == Mesh.RenderType.TRIS:\n                    gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n                elif self.render_type == Mesh.RenderType.QUADS:\n                    "}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if w == 0: w = self.W\n        if h == 0: h = self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n        if ptr.ndim == 1:\n            ptr = np.expand_dims(ptr, axis=1)\n        if ptr.ndim == 2:\n            ptr = np.expand_dims(ptr, axis=0)\n        if ptr.shape[-1] == 1:\n            ptr = np.repeat(ptr, 4, axis=-1)\n        if ptr.shape[-1] == 2:\n            ptr = np.concatenate([ptr, ptr, ptr], axis=-1)\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1], ptr.dtype)], axis=-1)\n        if ptr.shape[-1] == 4:\n            pass\n        else:\n            raise Exception(\"The input data must have 1, 2, or 3 channels.\")\n        if ptr.shape[-1] == 4:\n            ptr = np.ascontiguousarray(ptr)\n        else:\n            ptr = np.ascontiguousarray(ptr[..., :4])\n        if ptr.shape[-1] == 4:\n            ptr = np.ascontiguousarray(ptr)\n        else:\n            ptr = np.ascontiguousarray(ptr[..., :4])\n        ptr = ptr.astype(np.uint8)\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check input shapes\n    if R.ndimension() == 1:\n        R = R.unsqueeze(0)\n    if tvec.ndimension() == 1:\n        tvec = tvec.unsqueeze(0)\n    if camera_matrix.ndimension() == 1:\n        camera_matrix = camera_matrix.unsqueeze(0)\n    if image_size.ndimension() == 1:\n        image_size = image_size.unsqueeze(0)\n    if R.size() != tvec.size() or tvec.size() != camera_matrix.size() or camera_matrix.size() != image_size.size():\n        raise ValueError(\"Input shapes do not match\")\n\n    # Check input values\n    if not (R.size(-1) == 3 and R.size() == tvec.size() and tvec.size() == camera_matrix.size() and camera_matrix.size() == image_size.size()):\n        raise ValueError(\"Input must be a batch of 3x3 rotation matrices, 3x1 translation vectors, and 3x3 camera matrices\")\n    if not (R.min() >= 0 and R.max() <= 1 and tvec.min() >= 0 and tvec.max() <= 1 and camera_matrix.min() >= 0 and camera_matrix.max() <= 1 and image_size.min() >= 0 and image_size.max() <= 1):\n        raise ValueError(\"Input values must be between 0 and 1\")\n    if not (R.min() >= 1 and R.max() <= 1 and tvec.min() >= 1 and tvec.max() <= 1 and camera_matrix.min() >= 1 and camera_matrix.max() <= 1 and image_size.min() >= 1 and image_size.max() <= 1):\n        raise ValueError(\"Input values must be between 1 and 1\")\n    if not (R.min() >= 0 and R.max() <= 1 and tvec.min() >= 0 and tvec.max() <= 1 and camera_matrix.min() >= 0 and camera_matrix.max()"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set the viewport and scissor box\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glScissor(x, y, w, h)\n\n        # Bind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glUseProgram(0)\n\n        # Restore the viewport and scissor box\n        gl.glScissor(*old_scissor)\n        gl.glViewport(*old_viewport)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # R and T are applied on the right (requires a transposed R from OpenCV camera format)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # However, the correction has to be down on both T and R... (instead of just R)\n    R = R.permute(0, 2, 1)\n    T = T.permute(0, 2, 1)\n    C = C.permute(0, 2, 1)\n    # R = R.permute(0, 2, 1)\n    # T = T.permute(0, 2, 1)\n    # C = C.permute(0, 2, 1)\n    # C = -batch.R.mT @ batch.T  # B, 3, 1\n    # C = C.permute(0, 2, 1)\n    # C = -batch.R.mT @ batch.T  # B, 3, 1\n    # C = C.permute(0, 2, 1)\n    # C = -batch.R.mT @ batch.T  # B, 3, 1\n    # C = C.permute(0, 2, 1)\n    # C = -batch.R.mT @ batch.T  # B, 3, 1\n    # C = C.permute(0, 2, 1)\n    # C = -batch.R.mT @ batch.T  # B, 3, 1\n    # C = C.permute(0, 2, 1)\n    # C = -batch.R.mT @ batch.T  # B, 3, 1\n    # C = C.permute(0, 2, 1)\n    #"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        if not self.use_quad_draw:\n            return\n\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        old = gl.glGetInteger(gl.GL_DRAW_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,\n            x, y, w, h,\n            gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, old)\n\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t1 = t1.view(-1, 1)\n    y1 = y1.view(-1, 1)\n    t0 = t0.view(-1, 1)\n    t1_cumsum = torch.cumsum(t1, dim=0)\n    y1_cumsum = torch.cumsum(y1, dim=0)\n    t0_cumsum = torch.cumsum(t0, dim=0)\n    inner = t1_cumsum - t0_cumsum\n    outer = y1_cumsum - y1_cumsum.gather(0, t0_cumsum.to(t1_cumsum))\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t_env = t_env.unsqueeze(-1)\n    # t = t.unsqueeze(-1)\n    # t_env = t_env.expand_as(t)\n    # t = t.expand_as(t_env)\n    # t_env = t_env.to(t.device)\n    # t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t = t.to(t.device)\n    t_env = t_env.to(t.device)\n    t ="}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    _, w_outer = inner_outer(t, t[..., :-1], w)\n    return (w - w_outer).pow(2).mean()\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(ps, cw, t)"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the CDF.\n    cw = integrate_weights(w)\n    # Sample the CDF.\n    u = torch.rand(num_samples, t.shape[-1], device=t.device)\n    t_new = invert_cdf(u, t, cw)\n    # Apply perturbation.\n    if perturb:\n        if single_jitter:\n            t_new = t_new + torch.rand(num_samples, t.shape[-1], device=t.device)\n        else:\n            t_new = t_new + torch.rand(num_samples, t.shape[-1], device=t.device)\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t_dilated = torch.max(t * dilation, domain[0])\n    t_dilated = torch.min(t_dilated, domain[1])\n    w_dilated = w * (t_dilated - t)\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    if tq.ndim == t.ndim - 1:\n        tq = tq[None]\n\n    # If the query time is a step change time, return the outside value.\n    is_step_change = torch.any(torch.eq(tq, t), dim=-1)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq.to(t)\n    tq = tq."}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    t_new = t[..., :-1]\n    t_new = t_new + (1 - t_new) * train_frac\n    w_new = w * torch.exp(anneal_slope * (t_new - t_new.min()))\n    w_new = w_new / (torch.sum(w_new, dim=-1, keepdim=True).clip(eps))\n    return w_new"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if not ignore_list and \"meta\" in batch:\n        return batch\n    if isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    if isinstance(batch, (list, tuple)):\n        return [to_cuda(v, device, ignore_list) for v in batch]\n    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f = f.unsqueeze(-1)\n    if f.shape[dim] == 1:\n        f ="}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch.unsqueeze(0)\n    else:\n        batch = torch.as_tensor(batch).unsqueeze(0)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        return batch\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_prime:\n            if agent.is_working:\n                self.persistence.save_agent(agent.to_dict())\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) == 0:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n            if similarities is None or similarities < 0:\n                return None, -np.inf\n            return self.agents[np.argmax(similarities)], similarities\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(\n            MicroAgent(\n                self.openai_wrapper,\n                PRIME_PROMPT,\n                PRIME_NAME,\n                PRIME_AGENT_WEIGHT,\n                is_prime=True,\n                is_unspecified=True,\n            )\n        )\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    def load_agent(self, purpose):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(\n                serialized_agent\n            )\n        return None\n\n    def get_all_agent_purposes(self):\n        return self.persistence.get_all_agent_purposes()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": ""}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error while saving agent {agent.name}.\")\n            logger.error(e)\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        \n        # Generate the prompt\n        prompt = f\"{PROMPT_ENGINEERING_SYSTEM_PROMPT}{goal}{PROMPT_ENGINEERING_TEMPLATE}{sample_input}{EXAMPLES}\"\n\n        # Attempt to get a chat completion from the OpenAI wrapper\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"INSERT OR REPLACE INTO agents VALUES ( ?, ?, ? )\",\n                         (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            for row in cursor:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        data = json.dumps(result)\n        cursor.execute(\"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\", (arg_hash, data))\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    if not quiet_mode:\n        with redirect_stdout(open(os.path.join(os.path.join(os.path.join(os. frau('XAgent', 'logs'), 'command_line'), 'command_line_process.log'), 'w')):\n            print(args)\n    else:\n        print(args)\n\n    if args.config_file is not None:\n        CONFIG.load_config(args.config_file)\n    if args.model is not None:\n        CONFIG.set_model(args.model)\n    if args.record_dir is not None:\n        CONFIG.set_record_dir(args.record_dir)\n    if args.max_subtask_chain_length is not None:\n        CONFIG.set_max_subtask_chain_length(args.max_subtask_chain_length)\n    if args.enable_ask_human_for_help is not None:\n        CONFIG.set_enable_ask_human_for_help(args.enable_ask_human_for_help)\n    if args.max_plan_refine_chain_length is not None:\n        CONFIG.set_max_plan_refine_chain_length(args.max_plan_refine_chain_length)\n    if args.max_plan_tree_depth is not None:\n        CONFIG.set_max_plan_tree_depth(args.max_plan_tree_depth)\n    if args.max_plan_tree_width is not None:\n        CONFIG.set_max_plan_tree_width(args.max_plan_tree_width)\n    if args.max_retry_times is not None:\n        CONFIG.set_max_retry_times(args.max_retry_times)\n    if args.mode is not None:\n        CONFIG.set_mode(args.mode)\n    if args.upload_files is not None:\n        for file in args.upload_files:\n            CONFIG.set_upload_file(file)\n\n    if CONFIG.get_mode() == \"auto\":\n        print(\"Executing the command line process in 'auto' mode.\")\n    else:\n        print(\"Executing the command line"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai.ChatCompletion.create(**chatcompletion_kwargs)"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"The state_dict method should not be called from a DataLoader worker process.\")\n\n        if self.shuffler is None:\n            self.worker_env = _WorkerEnv.detect()\n            self.cache = self._create_cache(worker_env=self.worker_env)\n            self.shuffler = self._create_shuffler(self.cache)\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n        }\n\n        if self.item_loader is not None:\n            state[\"item_loader\"] = self.item_loader.state_dict()\n\n        if self.input_dir is not None:\n            state[\"input_dir\"] = self.input_dir.path\n\n        state[\"drop_last\"] = self.drop_last\n        state[\"seed\"] = self.seed\n        state[\"current_epoch\"] = self.current_epoch\n        state[\"world_size\"] = self.distributed_env.world_size\n        state[\"shuffle\"] = self.shuffle\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n        if self._state_dict is not None:\n            return\n        self._state_dict = state_dict\n        self._validate_state_dict()\n        self._state_dict = None\n        self.set_epoch(self.current_epoch)\n        self.set_shuffle(self.shuffle)\n        self.set_item_loader(self.item_loader)\n        self.set_input_dir(self.input_dir)\n        self.set_drop_last(self.drop_last)\n        self.set_seed(self.seed)\n        self.set_max_cache_size(self.max_cache_size)\n        self.set_serializers(self.serializers)\n        self.set_min_items_per_replica(self.min_items_per_replica)\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path in the state dictionary is {self._state_dict['input_dir_path']}, \"\n                f\"but the current input directory path is {self.input_dir.path}. \"\n            )\n        if self._state_dict[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL in the state dictionary is {self._state_dict['input_dir_url']}, \"\n                f\"but the current input directory URL is {self.input_dir.url}. \"\n            )\n        if self._state_dict[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"The item loader in the state dictionary is {self._state_dict['item_loader']}, \"\n                f\"but the current item loader is {self.item_loader}. \"\n            )\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last flag in the state dictionary is {self._state_dict['drop_last']}, \"\n                f\"but the current drop_last flag is {self.drop_last}. \"\n            )\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed in the state dictionary is {self._state_dict['seed']}, \"\n                f\"but the current seed is {self.seed}. \"\n            )\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle flag in the state dictionary is {self._state_dict['shuffle']}, \"\n                f\"but the current shuffle flag is {self.shuffle}. \"\n            )\n        if self._state_dict[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The number of workers in the state dictionary is {self._state_dict['num_workers']}, \"\n                f\"but the current number of"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if not input_dir:\n        input_dir = \"\"\n\n    if not os.path.exists(input_dir):\n        return None\n\n    if not os.path.isdir(input_dir):\n        return None\n\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), input_dir)\n    else:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, input_dir)\n\n    if not os.path.exists(cache_dir):\n        os.mkdir(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"The remote file path must use the 's3' scheme\")\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            self._download_file_using_s5cmd(remote_filepath, local_filepath)\n        else:\n            self._download_file_using_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Associate the chunks to the workers\n    num_chunks = len(chunks_replica)\n    workers_chunks = [[] for _ in range(num_workers)]\n    for i in range(num_workers):\n        for j in range(num_chunks):\n            if i == worker_env.rank:\n                workers_chunks[i].append(chunks_replica[j])\n\n    # Associate the intervals to the workers\n    num_chunks_per_worker = [\n        len(workers_chunks[i]) for i in range(num_workers)\n    ]  # number of chunks per worker\n    num_intervals_per_worker = [\n        len(intervals_replica[i]) for i in range(num_workers)\n    ]  # number of intervals per worker\n    num_chunks_per_interval = [\n        sum(num_chunks_per_worker) for i in range(num_workers)\n    ]  # number of chunks per interval\n    num_chunks_per_interval_per_worker = [\n        sum(num_chunks_per_interval[:i + 1]) for i in range(num_workers)\n    ]  # number of chunks per interval per worker\n    workers_intervals = [[] for _ in range(num_workers)]\n    for i in range(num_workers):\n        for j in range(num_intervals_per_worker[i]):\n            for k in range(num_chunks_per_interval[i]):\n                if i == worker_env.rank:\n                    workers_intervals[i].append(\n                        intervals_replica[j][\n                            num_chunks_per_interval_per_worker[i] + k : num_chunks_per_interval_per_worker[i] + k + 1\n                        ]\n                    )\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if \"local:\" in remote_filepath:\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not _PIL_AVAILABLE:\n            raise ValueError(\"PIL is not available\")\n\n        if not isinstance(item, Image.Image):\n            raise ValueError(\"The item must be a PIL Image object\")\n\n        if not item.mode in _NUMPY_DTYPES_MAPPING:\n            raise ValueError(\n                f\"The image mode {item.mode} is not supported. Supported modes are {', '.join(_NUMPY_DTYPES_MAPPING.keys())}\"\n            )\n\n        if item.mode == \"P\":\n            raise ValueError(\"The image mode P is not supported\")\n\n        if item.mode == \"I\":\n            raise ValueError(\"The image mode I is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")\n\n        if item.mode == \"I;16\":\n            raise ValueError(\"The image mode I;16 is not supported\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not _PIL_AVAILABLE:\n            raise Exception(\"PIL is not available\")\n\n        if isinstance(item, JpegImageFile):\n            return item.tobytes(), None\n        else:\n            return decode_jpeg(item).tobytes(), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        data = data[4:]\n        size = np.frombuffer(data[:4], np.uint32)[0]\n        data = data[4:]\n        shape = [int.from_bytes(data[:4], \"big\") for _ in range(size)]\n        data = data[4:]\n        return torch.frombuffer(data, _TORCH_DTYPES_MAPPING[dtype_indice], shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = item.dtype\n        if not isinstance(dtype, torch.dtype):\n            dtype = torch.as_tensor(dtype)\n        dtype = self._dtype_to_indices[dtype]\n        return dtype.tobytes() + item.tobytes()\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except Exception:\n                pass\n        if _PIL_AVAILABLE:\n            return PILSerializer().deserialize(data)\n        else:\n            raise ValueError(\"The data is not a JPEG image.\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if not self._dtype:\n            raise ValueError(\"The data type of the tensor must be specified before serializing.\")\n        data = item.to(self._dtype).numpy().tobytes(order=\"C\")\n        return data, f\"no_header_tensor:{self._dtype.value}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise Exception(\"The NoHeaderTensorSerializer must have a predefined data type for the tensor.\")\n        return torch.frombuffer(data, self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        return np.frombuffer(data[8 + 4 * (shape_size + 1) : len(data)], dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(order=\"C\"), f\"no_header_numpy:{self._dtype.num}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": self.dataset,\n            \"current_epoch\": self.current_epoch,\n        }\n        if isinstance(self.dataset, StreamingDataset):\n            state_dict[\"num_samples_yielded\"] = self._num_samples_yielded_streaming\n        else:\n            state_dict[\"num_samples_yielded\"] = self._num_samples_yielded_combined\n        state_dict[\"latest_worker_idx\"] = self._latest_worker_idx\n        return state_dict\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise Exception(\"The 'av' library is not installed. Please install it to use the VideoSerializer class.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise Exception(\"The 'torchvision' library is not installed. Please install it to use the VideoSerializer class.\")\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return torch.io.read_video(f.name)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        if self._rank is None:\n            self.write_chunks_index()\n            return []\n\n        while not self._should_write():\n            sleep(0.1)\n        self.write_chunks_index()\n        self._is_done = True\n        return [self.write_chunk(on_done=True)]\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.restore = True\n        self.current_epoch = obj[\"current_epoch\"]\n        self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n        self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(\n                self._num_samples_yielded_streaming, self.num_workers, self.batch_size\n            )\n        else:\n            self.dataset._set_use_streaming_dataloader(True)\n            self.dataset.load_state_dict(self.num_workers, self.batch_size, self._num_samples_yielded_combined)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n        else:\n            return {\n                __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n                __SAMPLES_KEY__: [dataset.state_dict(num_workers, batch_size) for dataset in self._datasets],\n            }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if not _is_state_dict(state_dict):\n            raise ValueError(\"The state_dict is not a valid state dict.\")\n\n        if \"num_samples_yielded\" in state_dict:\n            self._num_samples_yielded = state_dict[\"num_samples_yielded\"]\n        else:\n            self._num_samples_yielded = None\n\n        if \"use_streaming_dataloader\" in state_dict:\n            self._use_streaming_dataloader = state_dict[\"use_streaming_dataloader\"]\n\n        if \"current_epoch\" in state_dict:\n            self._current_epoch = state_dict[\"current_epoch\"]\n        else:\n            self._current_epoch = 0\n\n        for dataset in self._datasets:\n            if \"state_dict\" in state_dict:\n                dataset.load_state_dict(state_dict[\"state_dict\"])\n            if \"num_samples_yielded\" in state_dict:\n                dataset.load_state_dict(state_dict[\"num_samples_yielded\"])\n            if \"use_streaming_dataloader\" in state_dict:\n                dataset.load_state_dict(state_dict[\"use_streaming_dataloader\"])\n            if \"current_epoch\" in state_dict:\n                dataset.load_state_dict(state_dict[\"current_epoch\"])\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://project/\"):\n        return Dir(path=dir"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The `output_dir` must be a Dir object\")\n\n    if not output_dir.path.startswith(\"s3://\"):\n        raise ValueError(\"The `output_dir` must start with s3://\")\n\n    if not append and not overwrite:\n        if os.path.exists(output_dir.path):\n            raise ValueError(\n                f\"The directory {output_dir.path} already exists and neither the 'append' nor the 'overwrite' flag is set.\"\n            )\n    else:\n        raise ValueError(\n            f\"The 'append' and 'overwrite' flags are not yet implemented. Please set them to False.\"\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        raise ValueError(\n            f\"The provided output_dir `{output_dir.path}` is not a valid S3 bucket directory.\"\n        )\n\n    if \"KeyCount\" not in objects:\n        raise ValueError(\n            f\"The provided output_dir `{output_dir.path}` is not a valid S3 bucket directory.\"\n        )\n\n    if \"Name\" not in objects[\"Contents\"][0]:\n        raise ValueError(\n            f\"The provided output_dir `{output_dir.path}` is not a valid S3 bucket directory.\"\n        )\n\n    if \"Size\" not in objects[\"Contents\"][0]:\n        raise ValueError(\n            f\"The provided output_dir `{output_dir.path}` is not a valid S3 bucket directory.\"\n        )\n\n    if \"LastModified\" not in objects[\"Contents\"][0]:\n        raise ValueError(\n            f\"The provided output_dir `{output_dir.path}` is not a valid S3 bucket directory.\"\n        )\n\n    if \"ETag\" not in objects[\"Contents\"][0]:\n        raise ValueError(\n            f\"The provided output_dir `{output_dir.path}` is not a valid S3 bucket directory.\"\n        )\n\n    if \"StorageClass\" not in objects[\"Contents\"][0]:\n        raise ValueError(\n            f\"The provided output_dir `{output_dir.path}` is not a valid S3 bucket directory.\"\n        )\n\n    if \"Owner\" not in objects[\"Contents\"][0]:\n        raise ValueError(\n            "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all the index files to be available\n        if node_rank is None:\n            node_rank = self.rank\n        if node_rank == 0:\n            while len(self._chunks_info) < num_workers:\n                sleep(0.1)\n\n        # Merge the index files\n        if node_rank == 0:\n            self._merge_index_files()\n        else:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\")):\n                sleep(0.1)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The `lightning_sdk` is not available. Please install it with `pip install lightning-sdk`.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The `boto3` is not available. Please install it with `pip install boto3`.\")\n\n    if machine is None:\n        machine = Machine()\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.getenv('LIGHTNING_CLOUD_PROJECT_ID', '')}\"\n\n    if machine.name == \"local\":\n        os.system(command)\n    else:\n        if not _LIGHTNING_SDK_AVAILABLE:\n            raise RuntimeError(\"The `lightning_sdk` is not available. Please install it with `pip install lightning-sdk`.\")\n        if not _BOTO3_AVAILABLE:\n            raise RuntimeError(\"The `boto3` is not available. Please install it with `pip install boto3`.\")\n\n        client = Studio(\n            url=_get_lightning_cloud_url(),\n            project_id=os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", \"\"),\n            cluster_id=os.getenv(\"LIGHTNING_CLUSTER_ID\", \"\"),\n        )\n        job = client.create_job(\n            name=name,\n            num_nodes=num_nodes,\n            machine=machine,\n            command=command,\n        )\n\n        print(f\"Job {job.id} started. You can check the job status at {job.url}.\")\n        while not job.is_done():\n            sleep(1)\n            job = client.get_job(job.id)\n        if not job.is_success():\n            raise RuntimeError(f\"Job {job.id} failed. Check the job status at {job.url}.\")\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        if self._rank is None:\n            self._rank = _WorkerEnv.get_rank()\n\n        if self._rank == 0:\n            self._config = ChunksConfig(\n                self._cache_dir,\n                self._remote_input_dir,\n                self._compression,\n                self._serializers,\n                self._item_loader,\n            )\n        else:\n            self._config = ChunksConfig(\n                self._cache_dir,\n                self._remote_input_dir,\n                self._compression,\n                self._serializers,\n                self._item_loader,\n                self._rank,\n            )\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config is not set. Please call the `load_config()` method first.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        if self._prepare_thread is None:\n            self._prepare_thread = PrepareChunksThread(\n                self._config,\n                self._item_loader,\n                self._distributed_env,\n                self._max_cache_size,\n            )\n            self._prepare_thread.start()\n        if self._prepare_thread.is_alive():\n            self._prepare_thread.download([self._get_chunk_index_from_index(index.chunk_index)])\n            self._prepare_thread.join()\n\n        if self._config is None:\n            raise Exception(\"The config should be defined.\")\n\n        chunk_index = self._get_chunk_index_from_index(index.chunk_index)\n        if self._last_chunk_index is None or self._last_chunk_index != chunk_index:\n            self._last_chunk_index = chunk_index\n            self._prepare_thread.download([chunk_index])\n\n        if self._config is None:\n            raise Exception(\"The config should be defined.\")\n\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        return self._item_loader.read(index.index, chunk_filepath)\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if \"LIGHTNING_APP_EXTERNAL_URL\" in os.environ:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Shuffle the chunks\n    for i in range(distributed_env.num_nodes):\n        if i == distributed_env.rank:\n            np.random.seed(seed + current_epoch)\n        np.random.shuffle(chunks_per_ranks[i])\n    return [\n        int(item)\n        for sublist in chunks_per_ranks\n        for item in sublist\n    ]"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # get the first two elements of the input sequence\n    first_element = inputs[0]\n    second_element = inputs[1]\n\n    # get the input directory from the first element\n    input_dir = _get_input_dir_from_first_element(first_element)\n\n    # if the first element is a file path, the second element must also be a file path\n    if isinstance(input_dir, str):\n        if not isinstance(second_element, str):\n            raise ValueError(\n                f\"The second element of the input sequence must be a file path, but it is a {type(second_element)}.\"\n            )\n        if not os.path.exists(second_element):\n            raise ValueError(\n                f\"The second element of the input sequence is a file path, but the file does not exist at {second_element}.\"\n            )\n\n    # if the first element is a directory, the second element must be a file path\n    if not isinstance(input_dir, str):\n        if isinstance(second_element, str):\n            raise ValueError(\n                f\"The first element of the input sequence is a directory, but the second element is a file path.\"\n            )\n        if not os.path.exists(second_element):\n            raise ValueError(\n                f\"The first element of the input sequence is a directory, but the second element is not a file path.\"\n            )\n\n    # if the first element is a directory, the second element must be a file path\n    if isinstance(input_dir, str):\n        if not isinstance(second_element, str):\n            raise ValueError(\n                f\"The first element of the input sequence is a directory, but the second element is not a file path.\"\n            )\n        if not os.path.exists(second_element):\n            raise ValueError(\n                f\"The first element of the input sequence is a directory, but the second element is not a file path.\"\n            )\n\n    # if the first element is a directory, the second element must be a file path\n    if not isinstance(input_dir, str):\n        if isinstance(second_element, str):\n            raise ValueError(\n                f\"The first element of the input sequence is"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        os.system(\"echo 1 > /proc/sys/net/ipv4/tcp_no_check_source\")\n    else:\n        os.system(\"echo 0 > /proc/sys/net/ipv4/tcp_no_check_source\")\n    try:\n        yield\n    finally:\n        os.system(\"echo 0 > /proc/sys/net/ipv4/tcp_no_check_source\")\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    if drop_last:\n        if distributed_env.world_size > 1:\n            num_items = len(indexes)\n            num_items_per_rank = (num_items - 1) // (distributed_env.world_size - 1)\n            num_items_per_rank = (\n                num_items_per_rank + 1\n            ) if num_items % (distributed_env.world_size - 1) else num_items_per_rank\n            indexes = indexes[:num_items_per_rank * (distributed_env.world_size - 1)]\n    else:\n        num_items_per_rank = len(indexes) // distributed_env.world_size\n        num_items_per_rank = (\n            num_items_per_rank + 1\n        ) if len(indexes) % distributed_env.world_size else num_items_per_rank\n    if distributed_env.world_size == 1:\n        return [indexes], [chunk_intervals]\n    else:\n        chunk_indexes_per_rank = [\n            [] for _ in range(distributed_env.world_size)\n        ]\n        chunk_intervals_per_rank = [[] for _ in range(distributed_env.world_size)]\n        for i in range(distributed_env.world_size):\n            chunk_indexes_per_rank[i] = indexes[\n                i * num_items_per_rank : (i + 1) * num_items_per_rank\n            ]\n            chunk_intervals_per_rank[i] = [\n                chunk_intervals[i * num_items_per_rank],\n                chunk_intervals[i * num_items_per_rank + num_items_per_rank - 1],\n            ]\n        return chunk_indexes_per_rank, chunk_intervals_per_rank"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.get(\"bucket\"), obj.get(\"path\"))\n        except botocore.error.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n                sleep(sleep_time)\n            else:\n                raise\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch"}
{"namespace": "litdata.processing.functions.map", "completion": "    if _IS_IN_STUDIO:\n        if num_nodes is not None:\n            raise ValueError(\n                f\"The 'num_nodes' argument is not supported in the 'map' function when using the 'litdata' library in the 'litdata.studio' environment.\"\n            )\n        if machine is not None:\n            raise ValueError(\n                f\"The 'machine' argument is not supported in the 'map' function when using the 'litdata' library in the 'litdata.studio' environment.\"\n            )\n        if num_downloaders is not None:\n            raise ValueError(\n                f\"The 'num_downloaders' argument is not supported in the 'map' function when using the 'litdata' library in the 'litdata.studio' environment.\"\n            )\n        if num_uploaders is not None:\n            raise ValueError(\n                f\"The 'num_uploaders' argument is not supported in the 'map' function when using the 'litdata' library in the 'litdata.studio' environment.\"\n            )\n\n    if not isinstance(output_dir, Dir):\n        output_dir = Dir(output_dir)\n\n    if not isinstance(inputs, (list, tuple)):\n        raise ValueError(\n            f\"The 'inputs' argument must be a list or tuple. The provided type is {type(inputs)}.\"\n        )\n\n    if not isinstance(fn, (FunctionType, partial)):\n        raise ValueError(\n            f\"The 'fn' argument must be a function or a partial function. The provided type is {type(fn)}.\"\n        )\n\n    if isinstance(fast_dev_run, (int, bool)) and fast_dev_run:\n        if not _IS_IN_STUDIO:\n            raise ValueError(\n                f\"The 'fast_dev_run' argument can only be used in the 'litdata' library in the 'litdata.studio' environment.\"\n            )\n        if not isinstance(inputs, (list, tuple)):\n            raise ValueError(\n                f\"The 'inputs' argument must be a list or tuple when using the 'fast_dev_run' argument. The provided type is {type(inputs)}.\"\n            )\n        if not isinstance(fn, (FunctionType, partial"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        try:\n            task = queue_in.get(block=True, timeout=1)\n        except Empty:\n            break\n        else:\n            if not _download_data_task(input_dir, cache_dir, task):\n                queue_out.put(task[\"index\"])\n            else:\n                logger.info(\n                    f\"The data for index {task['index']} is already downloaded. Skipping download.\"\n                )\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Get the next item from the queue\n        item = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if item is None:\n            return\n\n        # 3. Unpack\n        if isinstance(item, str):\n            path = item\n        else:\n            path = item[1]\n        dirpath = item[0]\n\n        # 4. Upload the file\n        if output_dir.path and not path.startswith(output_dir.path):\n            path = path.replace(cache_dir, output_dir.path)\n\n        if output_dir.url:\n            if output_dir.url.startswith(\"s3://\"):\n                s3.client.upload_file(path, output_dir.url.lstrip(\"s3://\"))\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n        elif output_dir.path:\n            shutil.move(path, output_dir.path)\n        else:\n            raise ValueError(\"The provided output directory is not supported.\")\n\n        # 5. Remove the file from the cache\n        if dirpath is not None:\n            remove_queue.put(dirpath)\n\n        # 5. Inform the worker the current files are available\n        upload_queue.put(None)\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        import os\n        import stat\n\n        if not _IS_IN_STUDIO:\n            raise ValueError(\"The file_size option is only available in the studio.\")\n\n        if not _TORCH_GREATER_EQUAL_2_1_0:\n            raise ValueError(\"The file_size option is only available with PyTorch >= 2.1.0\")\n\n        if not isinstance(user_items, list):\n            raise ValueError(\"The file_size option is only available for lists.\")\n\n        if not all(isinstance(item, str) for item in user_items):\n            raise ValueError(\"The file_size option is only available for lists of strings.\")\n\n        if not all(os.path.exists(item) for item in user_items):\n            raise ValueError(\"The file_size option is only available for existing files.\")\n\n        if not all(os.path.isfile(item) for item in user_items):\n            raise ValueError(\"The file_size option is only available for files.\")\n\n        if not all(os.path.isabs(item) for item in user_items):\n            raise ValueError(\"The file_size option is only available for absolute paths.\")\n\n    if not _IS_IN_STUDIO:\n        raise ValueError(\"The weighted option is only available in the studio.\")\n\n    if not _TORCH_GREATER_EQUAL_2_1_0:\n        raise ValueError(\"The weighted option is only available with PyTorch >= 2.1.0\")\n\n    if not isinstance(user_items, list):\n        raise ValueError(\"The weighted option is only available for lists.\")\n\n    if not all(isinstance(item, str) for item in user_items):\n        raise ValueError(\"The weighted option is only available for lists of strings.\")\n\n    if not all(os.path.exists(item) for item in user_items):\n        raise ValueError(\"The weighted option is only available for existing files.\")\n\n    if not all(os.path.isfile(item) for item in user_items):\n        raise ValueError(\"The weighted option is only available for files.\")\n\n    if not all(os.path.isabs(item) for item in user_items):\n        raise ValueError(\"The weighted option is only available for absolute paths.\")"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    if num_workers is None:\n        num_workers = _get_num_nodes()\n\n    if _get_node_rank() == 0:\n        num_items = len(user_items)\n        num_workers_per_node = num_workers\n        num_nodes = _get_num_nodes()\n        num_workers = num_workers_per_node * num_nodes\n        num_items_per_worker = int(num_items / num_workers)\n        num_remainder = num_items % num_workers\n        start_indices = [i * num_items_per_worker for i in range(num_nodes)]\n        end_indices = [\n            start_indices[i + 1] - 1 if i < num_nodes - 1 else num_items\n        ]  # -1 to get the last item\n        end_indices = [\n            end_indices[i] + num_remainder if i < num_nodes - 1 else end_indices[i]\n        ]  # add the remainder to the last workers\n        return [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    else:\n        return [user_items]\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "def _get_num_nodes() -> int:\n    \"\"\"Returns the number of nodes in the current cluster.\n\n    This function is used to determine the number of nodes in the current cluster.\n\n    Input-Output Arguments\n    :return: The number of nodes in the current cluster.\n    \"\"\"\n    return int(os.environ[\"DATA_OPTIMIZER_NUM_WORKERS\"])\n\n\ndef _get_node_rank() -> int:\n    \"\"\"Returns the rank of the current node in the cluster.\n\n    This function is used to determine the rank of the current node in the cluster.\n\n    Input-Output Arguments\n    :return: The rank of the current node in the cluster.\n    \"\"\"\n    return int(os.environ[\"DATA_OPTIMIZER_GLOBAL_RANK\"])\n\n\ndef _get_fast_dev_run() -> Optional[Union[int, bool]]:\n    \"\"\"Returns the fast dev run value.\n\n    This function is used to determine the fast dev run value.\n\n    Input-Output Arguments\n    :return: The fast dev run value.\n    \"\"\"\n    return int(os.environ[\"DATA_OPTIMIZER_FAST_DEV_RUN\"])\n\n\ndef _get_cache_data_dir() -> str:\n    \"\"\"Returns the path to the cache data directory.\n\n    This function is used to determine the path to the cache data directory.\n\n    Input-Output Arguments\n    :return: The path to the cache data directory.\n    \"\"\"\n    return os.path.join(_get_cache_dir(), \"data\")\n\n\ndef _get_cache_dir() -> str:\n    \"\"\"Returns the path to the cache directory.\n\n    This function is used to determine the path to the cache directory.\n\n    Input-Output Arguments\n    :return: The path to the cache directory.\n    \"\"\"\n    return os.path.join(os.path.dirname(__file__), \"cache\")\n\n\ndef _get_item_filesizes(user_items: List[Any], base_path: Optional[str]) -> List[int]:\n    \"\"\"Returns the file sizes of the items in the user items list.\n\n    This function is used to determine the file sizes of the items in the user items list.\n\n    Input-Output Arguments\n    :param user_items: The list of"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return os.path.exists(_to_path(element))\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        if self.tcnn:\n            return self._get_tcnn_network(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation,\n                output_activation,\n            )\n        else:\n            return self._get_torch_network(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation,\n                output_activation,\n            )\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        if len(signal) < 1:\n            return signal\n\n        if kernel_offset < 1:\n            return signal\n\n        if len(signal) < 2 * kernel_offset:\n            return np.array([np.nan] * len(signal))\n\n        signal_size = len(signal)\n        signal_left = signal[:kernel_offset]\n        signal_right = signal[-(kernel_offset + 1) :]\n        signal_center = signal[kernel_offset - 1 : -kernel_offset]\n        signal_center_size = len(signal_center)\n        signal_left_size = len(signal_left)\n        signal_right_size = len(signal_right)\n        signal_left_right_size = signal_left_size + signal_right_size\n\n        signal_left_right = np.concatenate((signal_left, signal_right))\n        signal_left_right_size = len(signal_left_right)\n        signal_left_right_center = np.concatenate((signal_left_right, signal_center))\n        signal_left_right_center_size = len(signal_left_right_center)\n\n        # Calculate the rolling median\n        rolling_median = np.median(signal_left_right_center, axis=0)\n\n        # Trim the rolling median to remove edge effects\n        rolling_median = rolling_median[kernel_offset - 1 : -kernel_offset]\n\n        # If the signal is shorter than the kernel size, then the rolling median is the same as the signal\n        if signal_center_size < signal_left_right_center_size:\n            return rolling_median\n\n        # If the signal is longer than the kernel size, then the rolling median is the same as the signal for the first and last kernel offsets\n        if signal_center_size > signal_left_right_center_size:\n            return np.concatenate((signal[:kernel_offset], rolling_median, signal[-kernel_offset:]))\n\n        # If the signal is the same size as the kernel, then the rolling median is the same as the signal\n        return rolling_median"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the iriscode is a numpy array\n    if not isinstance(template_probe.iriscode, np.ndarray):\n        raise MatcherError(\"The iriscode in the template_probe must be a numpy array.\")\n    if not isinstance(template_gallery.iriscode, np.ndarray):\n        raise MatcherError(\"The iriscode in the template_gallery must be a numpy array.\")\n\n    # Check if the iriscode is a 2D array\n    if template_probe.iriscode.ndim != 2:\n        raise MatcherError(\"The iriscode in the template_probe must be a 2D array.\")\n    if template_gallery.iriscode.ndim != 2:\n        raise MatcherError(\"The iriscode in the template_gallery must be a 2D array.\")\n\n    # Check if the iriscode is a 2D array\n    if template_probe.iriscode.shape[1] != template_gallery.iriscode.shape[1]:\n        raise MatcherError(\"The iriscode in the template_probe and the template_gallery must have the same number of columns.\")\n    if template_probe.iriscode.shape[1] != 128:\n        raise MatcherError(\"The iriscode in the template_probe and the template_gallery must have 128 columns.\")\n\n    # Check if the iriscode is a 2D array\n    if template_probe.maskcode.ndim != 2:\n        raise MatcherError(\"The maskcode in the template_probe must be a 2D array.\")\n    if template_gallery.maskcode.ndim != 2:\n        raise MatcherError(\"The maskcode in the template_gallery must be a 2D array.\")\n\n    # Check if the iriscode is a 2D array\n    if template_probe.maskcode.shape[1] != template_gallery.maskcode.shape[1]:\n        raise MatcherError(\"The maskcode in the template_probe and the template_gallery must have the same number of columns.\")\n    if template_probe.maskcode.shape[1] != 128:\n        raise MatcherError(\"The maskcode in the template_probe and the template_gallery must have 128 columns.\")"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n        bisectors_points = np.empty((num_bisectors, 1, 2))\n        bisectors_points_end = np.empty((num_bisectors, 1, 2))\n        for i in range(num_bisectors):\n            try:\n                first_point = self._get_random_point(polygon)\n                second_point = self._get_random_point(polygon, first_point, min_distance_between_sector_points_in_px)\n                bisectors_points[i] = first_point\n                bisectors_points_end[i] = self._get_perpendicular_bisector(\n                    first_point, second_point, polygon\n                )\n            except EyeCentersEstimationError as e:\n                if i == max_iterations - 1:\n                    raise e\n                else:\n                    continue\n        return bisectors_points, bisectors_points_end\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.on_before_run(self, *args, **kwargs)\n        result = self._run(*args, **kwargs)\n        for callback in self._callbacks:\n            callback.on_after_run(self, *args, **kwargs)\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        if type_definition is None:\n            return True\n        if not isinstance(type_definition, type):\n            return self.validate_type(output, type_definition)\n        if is_dataclass(type_definition):\n            return self.validate_dataclass(output, type_definition)\n        if type(type_definition) in self.dict_like_types:\n            return self.validate_dict(output, type_definition)\n        if type(type_definition) in self.set_like_types:\n            return self.validate_set(output, type_definition)\n        if type(type_definition) in self.tuple_like_types:\n            return self.validate_tuple(output, type_definition)\n        if type(type_definition) in self.list_like_types:\n            return self.validate_list(output, type_definition)\n        return self.validate_type(output, type_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        if not isinstance(func_object, Callable):\n            raise ValueError(\"The function object must be a callable\")\n\n        func_name = func_object.__name__\n        func_docstring = func_object.__doc__\n        func_signature = inspect.signature(func_object)\n        input_type_hints = get_type_hints(func_object)\n        output_type_hints = input_type_hints.pop(\"return\")\n        input_type_class_definitions = {\n            key: get_class_definition(value) for key, value in input_type_hints.items()\n        }\n        output_type_class_definition = get_class_definition(output_type_hints)\n\n        if not output_type_class_definition:\n            raise ValueError(\"The function must have an output type hint\")\n\n        if not issubclass(output_type_class_definition, Embedding):\n            if issubclass(output_type_class_definition, Union):\n                function_type = FunctionType.EMBEDDABLE\n                output_type_class_definition = get_class_definition(\n                    get_type_from_union(output_type_class_definition))\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.EMBEDDABLE\n\n        return FunctionDescription(\n            name=func_name,\n            docstring=func_docstring,\n            input_type_class_definitions=input_type_class_definitions,\n            output_type_class_definition=output_type_class_definition,\n            function_type=function_type\n        )\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] = string\n        self.persistence.write(self.bit_array, self.indices)\n        return\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            logging.warning(\"Loaded bit array has a different length than expected. Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        if not isinstance(string, str):\n            raise ValueError(\"The string must be a string\")\n        if not self.persistence.is_open():\n            self.persistence.open()\n        if self.persistence.is_open():\n            self.bit_array = self.persistence.get_bit_array()\n            self.indices = self.persistence.get_indices()\n        if not self.bit_array:\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if not self.bit_array[index]:\n                return False\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs and not isinstance(kwargs[param], (int, float)):\n                raise ValueError(f\"The {param} parameter must be an integer or float\")\n        \n        # set default parameters\n        if \"temperature\" not in kwargs:\n            kwargs[\"temperature\"] = model.temperature\n        if \"top_p\" not in kwargs:\n            kwargs[\"top_p\"] = model.top_p\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = model.max_new_tokens\n        if \"frequency_penalty\" not in kwargs:\n            kwargs[\"frequency_penalty\"] = model.frequency_penalty\n        if \"presence_penalty\" not in kwargs:\n            kwargs[\"presence_penalty\"] = model.presence_penalty\n\n        # set the model name\n        model_name = model.model_name\n        if model.parsing_helper_tokens is not None:\n            model_name += f\" {model.parsing_helper_tokens}\"\n\n        # set the promp\n        prompt = f\"{system_message} {prompt}\"\n\n        # set the promp\n        response = self.client.completions.create(\n            model=model_name,\n            prompt=prompt,\n            **kwargs\n        )\n        \n        # process the response\n        text = response.get(\"choices\", [\"\"])[0].get(\"text\", \"\")\n        if model.parsing_helper_tokens is not None:\n            text = text.split(model.parsing_helper_tokens)[0]\n        return text\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    if not np.all(np.isclose(x, x.T)):\n        raise ValueError(\"The matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(x.diagonal(), 0):\n        raise ValueError(\"The matrix must have a zero diagonal\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        model = self.get_model(function_description.model_type, llm_parameters)\n        # get the prompt\n        prompt = self.get_prompt(args, kwargs, function_description, model)\n        # get the save_to_finetune\n        save_to_finetune = self.get_save_to_finetune(function_description, model.model_name)\n        # get the is_distilled_model\n        is_distilled_model = self.get_is_distilled_model(function_description, model.model_name)\n        # get the is_initialized\n        is_initialized = self.get_is_initialized(func_hash, function_description.name)\n        return prompt, model, save_to_finetune, is_distilled_model\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return higham_cov(cov, higham_max_iteration)\n    return cov_clipping(cov)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home, ignore_errors=True)\n    os.makedirs(data_home, exist_ok=True)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        if not isinstance(obj, (ListSchema, TupleSchema)):\n            return ListSchema.flatten(obj)\n        else:\n            return obj\n    elif isinstance(obj, dict):\n        if not isinstance(obj, DictSchema):\n            return DictSchema.flatten(obj)\n        else:\n            return obj\n    elif isinstance(obj, (Instances, Boxes, ROIMasks)):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (nn.Module, nn.ModuleList, nn.ModuleDict)):\n        return (obj,), IdentitySchema()\n    else:\n        return (obj,), TensorWrapSchema(type(obj))\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if not isinstance(groups, np. mozaic):\n        groups = mozaic.from_array(groups)\n    if not isinstance(equations, mozaic.mozaic):\n        equations = mozaic.from_array(equations)\n    if not isinstance(sum_to_one, mozaic.mozaic):\n        sum_to_one = mozaic.from_array(sum_to_one)\n    if not isinstance(raise_if_group_missing, mozaic.mozaic):\n        raise_if_group_missing = mozaic.from_array(raise_if_group_missing)\n    if not isinstance(names, mozaic.mozaic):\n        names = mozaic.from_array(names)\n\n    if mozaic.is_empty(groups):\n        return mozaic.array(None), mozaic.array(None)\n    if mozaic.is_empty(equations):\n        return mozaic.array(None), mozaic.array(None)\n    if mozaic.is_empty(sum_to_one):\n        sum_to_one = mozaic.array(False)\n    if mozaic.is_empty(raise_if_group_missing):\n        raise_if_group_missing = mozaic.array(True)\n    if mozaic.is_empty(names):\n        names = mozaic.array((\"groups\", \"equations\"))\n\n    # Check if the groups are mosaic\n    if mozaic.is_empty(groups):\n        raise mozaic.mozaic.MozaicError(\n            f\"The {names[0]} array must be mosaic.\"\n        )\n    # Check if the equations are mosaic\n    if mozaic.is_empty(equations):\n        raise mozaic.mozaic.MozaicError(\n            f\"The {names[1]} array must be mosaic.\"\n        )\n    # Check if the sum_to_one is mosaic\n    if mozaic.is_empty(sum_to_one):\n        raise mozaic.mozaic.MozaicError(\n            f\"The {names[2]} array must be mosaic.\"\n        )\n    # Check if the"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    name = f\"Instances_{_counter}\"\n    _counter += 1\n    with tempfile.NamedTemporaryFile(prefix=name, dir=tempfile.gettempdir()) as f:\n        f.write(\n            f\"class {name}(nn.Module):\\n\"\n            f\"    def __init__(self, image_size):\\n\"\n            f\"        self.image_size = image_size\\n\"\n        )\n        for field in fields:\n            if isinstance(field, tuple):\n                name, type_ = field\n            else:\n                name, type_ = field, field\n            f.write(f\"        self.{name} = {type_}\\n\")\n        f.write(f\"    def get_fields(self):\\n\")\n        f.write(f\"        return {name}.get_fields()\\n\")\n        f.write(f\"    def to(self, *args, **kwargs):\\n\")\n        f.write(f\"        return {name}.to(*args, **kwargs)\\n\")\n        f.write(f\"    def cpu(self):\\n\")\n        f.write(f\"        return {name}.cpu()\\n\")\n        f.write(f\"    def cuda(self, device=None):\\n\")\n        f.write(f\"        return {name}.cuda(device)\\n\")\n        f.write(f\"    def to(self, *args, **kwargs):\\n\")\n        f.write(f\"        return {name}.to(*args, **kwargs)\\n\")\n        f.write(f\"    def cuda(self, device=None):\\n\")\n        f.write(f\"        return {name}.cuda(device)\\n\")\n    _import_file(f.name)\n    _add_instances_conversion_methods(eval(name))\n    _clear_jit_cache()\n    oldInstances = Instances\n    Instances = eval(name)\n    yield Instances\n    Instances = oldInstances\n    os.remove(f.name)\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    image = Image.open(file_name)\n    image = _apply_exif_orientation(image)\n    return convert_PIL_to_numpy(image, format)\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"bbox\" in annotation:\n        annotation[\"bbox\"] = transforms.apply_box(\n            BoxMode.convert(\n                annotation[\"bbox\"],\n                BoxMode.XYXY_ABS,\n                BoxMode.convert(\n                    annotation[\"bbox_mode\"],\n                    BoxMode.XYXY_ABS,\n                    BoxMode.XYXY_ABS,\n                ),\n            )\n        )\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            annotation[\"segmentation\"] = [\n                transforms.apply_polygons(\n                    polygons_to_bitmask(annotation[\"segmentation\"], image_size),\n                    image_size,\n                )\n            ]\n        else:\n            annotation[\"segmentation\"] = transforms.apply_mask(\n                annotation[\"segmentation\"], image_size\n            )\n    if \"keypoints\" in annotation:\n        if keypoint_hflip_indices is None:\n            keypoint_hflip_indices = create_keypoint_hflip_indices(\n                image_size\n            )\n        annotation[\"keypoints\"] = transforms.apply_keypoint(\n            annotation[\"keypoints\"], image_size, keypoint_hflip_indices\n        )\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if self.angle % 360 == 0 or self.angle == 0:\n            return coords\n        return np.dot(coords, self.rm_coords)\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # TODO: this is a hack to get the model to work with detectron2\n    if not isinstance(model, nn.Module):\n        model = model.module\n\n    # TODO: this is a hack to get detectron2 to work with the model\n    if not isinstance(inputs, list):\n        inputs = [inputs]\n\n    # TODO: this is a hack to get detectron2 to work with the model\n    if not isinstance(inputs[0], dict):\n        inputs = [{\"image\": inputs}]\n\n    return flop_count(model, inputs)\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if not self.rm_image.all() == 1.0:\n            if self.expand:\n                if len(img.shape) > 2 and img.shape[2] == 1:\n                    pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n                else:\n                    pil_image = Image.fromarray(img)\n                pil_image = pil_image.transform(\n                    size=(self.bound_h, self.bound_w),\n                    method=Image.AFFINE,\n                    data=self.rm_image,\n                    resample=interp if interp else self.interp,\n                )\n                ret = np.asarray(pil_image)\n                if len(img.shape) > 2 and img.shape[2] == 1:\n                    ret = np.expand_dims(ret, -1)\n            else:\n                ret = cv2.warpAffine(\n                    img,\n                    self.rm_image,\n                    dsize=(self.bound_h, self.bound_w),\n                    flags=cv2.INTER_LINEAR,\n                )\n        else:\n            ret = img\n        return ret\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if not isinstance(predictions, torch.Tensor):\n            predictions = predictions.to(self.cpu_device)\n        if \"pred_boxes\" in predictions.fields:\n            self.draw_boxes(predictions.pred_boxes, predictions.pred_classes, predictions.scores)\n        if \"pred_masks\" in predictions.fields:\n            self.draw_masks(predictions.pred_masks, predictions.pred_classes, predictions.scores)\n        if \"keypoints\" in predictions.fields:\n            self.draw_keypoints(predictions.keypoints, predictions.pred_classes, predictions.scores)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        return self.canvas.tostring_rgb()\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"image\" in dic:\n            self.output.reset_image(dic[\"image\"])\n        if \"keypoints\" in dic:\n            self.draw_keypoints(dic[\"keypoints\"], dic[\"keypoints_info\"])\n        if \"boxes\" in dic:\n            self.draw_boxes(dic[\"boxes\"], dic[\"labels\"], dic[\"scores\"])\n        if \"masks\" in dic:\n            self.draw_masks(dic[\"masks\"], dic[\"labels\"])\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], area_threshold=dic[\"sem_seg_area_threshold\"])\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(\n                dic[\"panoptic_seg\"],\n                dic[\"panoptic_seg_info\"],\n                area_threshold=dic[\"panoptic_seg_area_threshold\"],\n            )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if not color:\n            color = self._jitter([random.random() for _ in range(3)])\n        if not edge_color:\n            edge_color = _OFF_WHITE\n        if not text:\n            text = None\n        if not binary_mask.any():\n            return self.output\n        # Find connected components\n        if binary_mask.max() == 1:\n            # The mask is a regular mask\n            components = binary_mask\n        else:\n            # The mask has holes\n            components = bw_components(binary_mask)\n        # Draw the mask\n        for component in components:\n            if component.area() < area_threshold:\n                continue\n            self.draw_polygon(component.to_list(), color, edge_color=edge_color, alpha=alpha)\n        if text is not None:\n            self.draw_text(text, (0.5, 0.5), color=color, alpha=alpha, rotation=90)\n        return self.output\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances):\n        raise ValueError(\n            f\"The first input must be an instance of Instances. Got {type(input)}.\"\n        )\n    if not isinstance(other, Instances):\n        raise ValueError(\n            f\"The second input must be an instance of Instances. Got {type(other)}.\"\n        )\n    if input.image_size != other.image_size:\n        if size_as_tensor:\n            if not torch.allclose(\n                input.image_size,\n                other.image_size,\n                atol=1,\n                rtol=rtol,\n            ):\n                raise AssertionError(\n                    f\"{msg}The image sizes of the two instances are not equal. \"\n                    f\"Got {input.image_size} and {other.image_size}.\"\n                )\n        else:\n            if not input.image_size == other.image_size:\n                raise AssertionError(\n                    f\"{msg}The image sizes of the two instances are not equal. \"\n                    f\"Got {input.image_size} and {other.image_size}.\"\n                )\n    for name in input._field_names:\n        input_val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(input_val, Boxes):\n            if not input_val.is_valid():\n                raise ValueError(\n                    f\"The first input {name} is not a valid Boxes object.\"\n                )\n            if not other_val.is_valid():\n                raise ValueError(\n                    f\"The second input {name} is not a valid Boxes object.\"\n                )\n            if not input_val.allclose(other_val):\n                raise AssertionError(\n                    f\"{msg}The {name} fields of the two instances are not equal. \"\n                    f\"Got {input_val} and {other_val}.\"\n                )\n        elif isinstance(input_val, ROIMasks):\n            if not input_val.is_valid():\n                raise ValueError(\n                    f\"The first input {name} is not a valid ROIMasks object.\"\n                )\n            if not other_val.is_valid():"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 2] * self.tensor[:, 3]\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(cfg.PROPOSAL_GENERATOR.NAME)(\n        cfg, input_shape\n    )"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        if self.training:\n            return self._fast_rcnn_losses(predictions, proposals)\n        else:\n            return self._fast_rcnn_inference(predictions, proposals)\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.pop(\"name\")\n    return TRACKER_HEADS_REGISTRY.get(name)(**cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(\n            deltas, torch.Tensor\n        ), \"The input deltas should be a torch.Tensor.\"\n        assert isinstance(\n            boxes, torch.Tensor\n        ), \"The input boxes should be a torch.Tensor.\"\n        assert deltas.size(\n            1\n        ) == 4, \"The number of deltas should be 4.\"\n        assert boxes.size(\n            1\n        ) == 4, \"The number of boxes should be 4.\"\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n        #\n        # dx, dy, dw, dh = deltas[:, 0], deltas[:, 1], deltas[:, 2], deltas[:, 3]\n        #\n        # new_ctr_x = dx * widths + ctr_x\n        # new_ctr_y = dy * heights + ctr_y\n        # new_w = torch.exp(dw) * widths\n        # new_h = torch.exp(dh) * heights\n        #\n        # new_boxes = torch.stack((new_ctr_x - 0.5 * new_w, new_ctr_y - 0.5 * new_h,\n        #                          new_ctr_x + 0.5 * new_w, new_ctr_y + 0.5 * new_h),\n        #                         dim=1)\n        #\n        # return new_boxes\n        new_ctr_x = deltas[:, 0] * widths + ctr_x\n        new_ctr_y = deltas[:, 1] * heights + ctr_y\n        new_w = torch.exp(deltas[:, 2]) * widths\n        new_h = torch.exp(deltas[:, 3]) * heights\n\n        new_boxes = torch.stack((new_ctr_x - 0.5 * new_w, new_ctr_y - 0.5 * new_h,\n                                 new_ctr_x + 0.5 * new_w,"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins.run(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            if all(tp in self.anno_type_map.keys() for tp in anno_type):\n                out = self.general_ins.run(image)\n                return {tp: out[tp] for tp in anno_type if tp in out}\n            else:\n                raise Exception(\n                    f'Error anno_type: {anno_type}, not in {self.anno_type_map.keys()}')"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        return {\n            url: sum(self.bm25(kw) for kw in keywords) for url, _ in self._index.items()\n        }\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        box = self.tensor\n        box_size = torch.as_tensor(box_size, dtype=box.dtype, device=box.device)\n        if box.numel() == 0 or box_size.numel() == 0:\n            return\n\n        # Normalize angles\n        self.normalize_angles()\n\n        # Get the indices of the nearly horizontal boxes\n        is_horizontal = torch.abs(box[:, 4]) < clip_angle_threshold\n        is_horizontal = is_horizontal.nonzero().view(-1)\n\n        # Convert the nearly horizontal boxes to (x1, y1, x2, y2)\n        box = box[is_horizontal]\n        box = self.to_xyxy(box)\n\n        # Clip the x and y coordinates\n        box[:, 0].clamp_(min=0.0, max=box_size[1])\n        box[:, 1].clamp_(min=0.0, max=box_size[0])\n        box[:, 2].clamp_(min=0.0, max=box_size[1])\n        box[:, 3].clamp_(min=0.0, max=box_size[0])\n\n        # Convert the clipped boxes back to the original representation\n        box = self.to_xyxy_to_xyxy_angle(box)\n        self.tensor[is_horizontal] = box\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for item in self.data:\n            if 'type' in item:\n                if item['type'] == 'doc':\n                    statistics['doc'] += 1\n                elif item['type'] == 'gen':\n                    statistics['gen'] += 1\n                elif item['type'] == 'kno':\n                    statistics['kno'] += 1\n                elif item['type'] == 'num':\n                    statistics['num'] += 1\n        return statistics"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if 'type' not in cfg:\n        raise klu.config. komik_error('The model type is obligatorily required.')\n    if (\n        train_cfg is not None patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patroon patro"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if logger is None:\n        logger = print_log\n    if not isinstance(metric, list):\n        metric = [metric]\n    if not isinstance(label2cat, dict):\n        raise Exception('label2cat must be a dict')\n    if box_type_3d is not None and box_mode_3d is None:\n        raise Exception('box_mode_3d must be specified if box_type_3d is not None')\n    if box_type_3d is not None and box_mode_3d is not None:\n        if not isinstance(box_type_3d, type(box_mode_3d)):\n            raise Exception('box_type_3d and box_mode_3d must be of the same type')\n    if box_type_3d is not None:\n        if not isinstance(box_type_3d, type(box_mode_3d)):\n            raise Exception('box_type_3d and box_mode_3d must be of the same type')\n    if box_type_3d is not None and box_mode_3d is not None:\n        if not isinstance(box_type_3d, type(box_mode_3d)):\n            raise Exception('box_type_3d and box_mode_3d must be of the same type')\n\n    #\n    # 1. Convert the ground truth and detection annotations to the same format\n    #\n    if box_type_3d is not None:\n        gt_annos = box_type_3d.to_3d(gt_annos, box_mode_3d)\n    dt_annos = box_type_3d.to_3d(dt_annos, box_mode_3d)\n\n    # 2. Calculate the mAP and mAR for each class\n    #\n    ap = {}\n    ar = {}\n    for iou in metric:\n        ap[iou] = {}\n        ar[iou] = {}\n    for cat in label2cat.values():\n        ap[iou].update({cat: 0.})\n        ar[iou].update({cat: 0.})\n    for iou in metric"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        return 'LiDAR3DBox', 'LiDAR'\n    elif box_type == 'Camera':\n        return 'Camera3DBox', 'Camera'\n    elif box_type == 'Depth':\n        return 'Depth3DBox', 'Depth'\n    else:\n        raise ValueError(\n            f'box type {box_type} is not supported')\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not isinstance(messages, (list, tuple)):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          _encode_message(message)\n          for message in messages or []\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('Model is required')\n\n    if not prompt and not images:\n      raise RequestError('Prompt or images are required')\n\n    if not isinstance(context, (list, tuple)):\n      context = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not options:\n      options = Options()\n    else:\n      if not isinstance(options, Options):\n        options = Options(**options)\n\n    if not keep_alive:\n      keep_alive = None\n\n    return self._request_stream(\n      'POST',\n      f'/api/v1/models/{model}/generate',\n      json={\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n      },\n      stream=stream,\n      raw=raw,\n      format=format,\n      images=images,\n      options=options,\n      keep_alive=keep_alive,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not model:\n      raise RequestError('must provide a model')\n    if not (path or modelfile):\n      raise RequestError('must provide a path or modelfile')\n    if path and not isinstance(path, (str, Path)):\n      raise TypeError('path must be a string or a path-like object')\n    if modelfile and not isinstance(modelfile, str):\n      raise TypeError('modelfile must be a string')\n\n    if path:\n      if not modelfile:\n        modelfile = Path(path).read_text()\n    else:\n      path = io.StringIO(modelfile)\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'path': path.name,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    if not path.is_file():\n      raise RequestError('path must be a file')\n\n    with open(path, 'rb') as f:\n      digest = sha256(f.read()).hexdigest()\n    url = f'https://{self._client.base_url.netloc}/api/blobs/sha256/{digest}'\n    if self._client.head(url).status_code == 200:\n      return digest\n    return self._client.post(url, data=f.read()).text\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # TODO: This is a bit of a hack.\n        # We use a temporary file to run Pyright, because it is not possible to\n        # run it in a subprocess with a string as input.\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(user_code.encode(\"utf-8\"))\n            f.write(b\"\\n\")\n            f.write(test_code.encode(\"utf-8\"))\n            f.flush()\n            result = cls._run_pyright(f.name)\n        return cls._parse_pyright_result(result)\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, get_compiler_fn(\"Module\"))\n    else:\n        return aot_function(fn, get_compiler_fn(\"Function\"))"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml'),\n                                  Loader=yaml.FullLoader)\n                                 )\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output file must have a .yaml or .yml extension\")\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(config_dict, f, Dumper=yaml.Full prodigy.Dumper)\n    return config_dict\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is None:\n        ts_compiler = lambda m: m\n    if isinstance(func, torch.nn.Module):\n        return _lazy_trace_module(func, ts_compiler, **kwargs_)\n    else:\n        return _lazy_trace_function(func, ts_compiler, **kwargs_)\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        return cls.from_yaml(os.path.join(trial_path, 'config.yaml'), project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Get the name of the retrieval module\n    module_name = modules[0].__name__\n\n    # Get the retrieval result\n    retriever_result = fundament_retrieval_node(modules, module_params, previous_result)\n\n    # Get the fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament fundament"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # TODO: Add a way to save the best result to a unil intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit intit"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for the node\n    node_dir = os.path.join(node_line_dir, 'node')\n    os.mkdir(node_dir)\n\n    # Create a directory for the best prompt maker\n    best_dir = os.path.join(node_dir, 'best')\n    os.mkdir(best_dir)\n\n    # Create a directory for the best prompt maker's output\n    best_output_dir = os.path.join(best_dir, 'output')\n    os.mkdir(best_output_dir)\n\n    # Create a directory for the best prompt maker's summary\n    best_summary_dir = os.path.join(best_dir, 'summary')\n    os.mkdir(best_summary_dir)\n\n    # Create a directory for the best prompt maker's intitals\n    best_inits_dir = os.path.join(best_dir, 'inits')\n    os.mkdir(best_inits_dir)\n\n    # Create a directory for the best prompt maker's intitals' output\n    best_inits_output_dir = os.path.join(best_inits_dir, 'output')\n    os.mkdir(best_inits_output_dir)\n\n    # Create intitals for intitals\n    inits = [\n        {\n            'module': intitals,\n        }\n    ]\n\n    # Create intitals for intitals intitals\n    inits_inits = [\n        {\n            'module': intitals_inits,\n        }\n    ]\n\n    # Create intitals intitals intitals\n    inits_inits_inits = [\n        {\n            'module': intitals_inits_inits,\n        }\n    ]\n\n    # Create intitals intitals intitals intitals\n    inits_inits_inits_inits = [\n        {\n            'module': intitals_inits_inits_inits,\n        }\n    ]\n\n    # intitals intitals intitals intitals intitals\n    inits_inits_inits_inits_inits = [\n        {\n            'module': intitals_inits_in"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    return list(set(list(itertools.chain.from_iterable(list(map(lambda x: extract_values(x, key), nodes)))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_model('all-mpnet-base-v2')\n    return max(list(map(lambda x: calculate_cosine_similarity(embedding_model, x, pred), generation_gt)))\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n    if gfpgan_face_restorer is None:\n        try:\n            gfpgan_face_restorer = FaceRestorerGFPGAN(\n                model_path=shared.get_model_path(\n                    model_name=\"GFPGAN\",\n                )\n            )\n        except errors.ModelNotFoundError as e:\n            logger.warning(\n                \"GFPGAN face restorer is not set up. \"\n                \"The image is not restored. \"\n            )\n            return np_image\n    else:\n        return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except errors.NoModelError as e:\n        logger.error(e.message)\n    except Exception as e:\n        logger.error(e)\n    else:\n        codeformer.add_to_list()"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(model_path=dirname)\n    except errors.ModelError as e:\n        logger.error(\n            \"Error while initializing GFPGAN face restorer: %s\", e\n        )\n    except Exception as e:\n        logger.error(\"Error while initializing GFPGAN face restorer: %s\", e)"}
{"namespace": "quaternion.rotate", "completion": "  return jnp.dot(normalize(multiply(q, jnp.concatenate([v, jnp.zeros(v.shape[Ellipsis], v.dtype)]),\n                           keepdims=True),\n                 conjugate(q))\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = axis_angle[Ellipsis, :3]\n  angle = axis_angle[Ellipsis, 3]\n  return jnp.stack((jnp.sin(angle / 2) * axis, jnp.cos(angle / 2)), axis=-1)"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    if model.argmax(prefix) == idx:\n        return 0, 1\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, 1\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if method == 'TRILINEAR':\n    return trilinear_resample(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center,\n    )\n  elif method == 'NEAREST':\n    return nearest_resample(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center,\n    )\n  else:\n    raise ValueError('Invalid method: ' + method)\n\n"}
{"namespace": "math.plus_eps", "completion": "  # The tiny threshold is the smallest representable value in the floating-point format of the input.\n  if x < tiny_val:\n    return tiny_val, tiny_val\n  else:\n    return x, jax.lax.next_up(x)\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: x_dot * jnp.exp(x),\n      (-max_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, jnp.log, (min_val, max_val))(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(x),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return jnp.log(tiny_val)\n  elif p == 0:\n    return jnp.log(tiny_val)\n  elif p == 1:\n    return jnp.log(min_val)\n  elif p == 2:\n    return jnp.log(max_val)\n  elif p == 3:\n    return jnp.log(min_val)\n  else:\n    return jnp.log(max_val)\n\n"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(\n        f'base_shape {base_shape} is not supported. '\n        f'Must be one of \"tetrahedron\", \"icosahedron\", \"octahedron\".')\n  if angular_tesselation < 1:\n    raise ValueError('angular_tesselation must be >= 1')\n\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [-1, -1, -1],\n        [-1, -1, 1],\n        [-1, 1, -1],\n        [1, 1, 1],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [-1, -1, -1],\n        [-1, -1, 1],\n        [-1, 1, -1],\n        [1, 1, 1],\n    ])\n  elif base_shape == 'octahedron':\n    base_verts = np.array([\n        [1, 0, 0],\n        [-1, 0, 0],\n        [0, 1, 0],\n        [0, -1, 0],\n        [0, 0, 1],\n        [0, 0, -1],\n    ])\n\n  base_faces = np.array([\n      [0, 1, 2],\n      [1, 2, 3],\n      [1, 0, 3],\n      [0, 2, 3],\n  ])\n\n  basis = tesselate_geodesic(base_verts, base_faces, angular_tesselation, eps)\n  if remove_symmetries:\n    basis = remove_symmetries_from_basis(basis)\n  return basis\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (-max_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder", "completion": "  if p == 1:\n    return jnp.log(x)\n  if p == 0:\n    return jnp.log1p(x)\n  if p == -np.inf:\n    return jnp.log(x + 1)\n  if p == np.inf:\n    return jnp.log1p(x)\n  if premult is not None:\n    x = premult * x\n  if p < 0:\n    return jnp.log(power_ladder_max_output(p) * x ** p)\n  return x ** p * power_ladder_max_output(p)\n  if postmult is not None:\n    return postmult * x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute (y * (p - 1) + 1)^(-1/p)\n  if premult is not None:\n    y = y * premult\n  if postmult is not None:\n    y = y * postmult\n  y = jnp.where(\n      jnp.abs(p) < tiny_val,\n      jnp.where(\n          p < 0,\n          safe_log1p(-y),\n          safe_expm1(-y),\n      ),\n      safe_div(y + 1, jnp.abs(p - 1)),\n  )\n  return y\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0 and step < lr_delay_steps:\n    return lr_init * lr_delay_mult\n  return log_lerp(\n      jnp.clip(step, lr_delay_steps, max_steps), lr_init, lr_final\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      None,\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.1,\n      radius_hi=0.5,\n      near_lo=0.1,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=10.0,\n      include_exposure_idx = include_exposure_idx,\n      include_exposure_values = include_exposure_values,\n      include_device_idx = include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(points[Ellipsis, 0], points[Ellipsis, 1]),\n          pix_to_dir(points[Ellipsis, 0] + 1, points[Ellipsis, 1]),\n          pix_to_dir(points[Ellipsis, 0], points[Ellipsis, 1] + 1),\n      ],\n      axis=0,\n  )\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = xnp.matmul(\n      pixtocams, pixel_dirs_stacked[Ellipsis, :3, :]\n  )\n  # Flip from OpenCV to OpenGL coordinate system.\n  camera_dirs_stacked = xnp.matmul(\n      camera_dirs_stacked, xnp.diag(xnp.array([1.0, -1.0, -1.0]))\n  )\n  # Apply camera rotation matrices.\n  directions_stacked = xnp.matmul(\n      camtoworlds[Ellipsis, :3, :3], camera_dirs_stacked\n  )\n  directions = directions_stacked[Ellipsis, :3, :]\n  # Extract the offset rays.\n  dx = directions_stacked[Ellipsis, 0, :]\n  dy = directions_stacked[Ellipsis, 1, :]\n  # Distance from each unit-norm direction vector to its neighbors.\n  dx_norm = xnp.linalg.norm(dx - directions, axis=-1)\n  dy_norm = xnp.linalg.norm(dy - directions, axis=-1)\n  # Cut the distance in half, multiply it to match the variance of a uniform\n  # distribution the size of a pixel (1/12, see paper).\n  radii = (0.5 * (dx_norm + dy_norm"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[:3]\n  v = screw_axis[3:]\n  theta = _safe_sqrt(jnp.sum(w**2))\n  w = w / theta\n  v = v / theta\n  R = exp_so3(w, eps)\n  p = v * theta\n  return rp_to_se3(R, p)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis = jnp.reshape(axis_angle, (3))\n  angle = jnp.linalg.norm(axis)\n  if jnp.isclose(angle, 0.0):\n    return jnp.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n  else:\n    axis = axis / _safe_sqrt(angle)\n    return jnp.array([\n        [1.0 - jnp.cos(angle), axis[2], -axis[1]],\n        [axis[2], 1.0 - jnp.cos(angle), axis[0]],\n        [-axis[1], axis[0], 1.0 - jnp.cos(angle)]\n    ]\n    ) + jnp.array([\n        [0.0, -axis[2], axis[1]],\n        [axis[2], 0.0, -axis[0]],\n        [-axis[1], axis[0], 0.0]]\n    ) * jnp.sin(angle)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Get the number of cameras.\n  num_cams = camtoworlds.shape[0]\n  if num_cams == 1:\n    pixtocams = xnp.broadcast_to(pixtocams, [1, 3, 3])\n  else:\n    pixtocams = xnp.broadcast_to(pixtocams, [num_cams, 3, 3])\n\n  # Get the number of pixels.\n  num_pix = pix_x_int.shape[0]\n  if num_pix == 1:\n    pix_x_int = xnp.broadcast_to(pix_x_int, [num_cams, num_pix])\n    pix_y_int = xnp.broadcast_to(pix_y_int, [num_cams, num_pix])\n  else:\n    pix_x_int = xnp.broadcast_to(pix_x_int, [num_cams, num_pix])\n    pix_y_int = xnp.broadcast_to(pix_y_int, [num_cams, num_pix])\n\n  # Get the undistorted pixel coordinates.\n  if distortion_params is not None:\n    pix_x_int, pix_y_int = _radial_and_tangential_undistort(\n        xd=pix_x_int,\n        yd=pix_y_int,\n        k1=distortion_params['k1'],\n        k2=distortion_params['k2'],\n        k3=distortion_params['k3'],\n        k4=distortion_params['k4'],\n        p1=distortion_params['p1'],\n        p2=distortion_params['p2'],\n        eps=1e-9,\n        max_iterations=10,\n        xnp=xnp,\n    )\n\n  # Get the camera coordinates.\n  pixtocam = xnp.linalg.inv(pixtocams)\n  cam_x = (pixtocam[:, :2, :2] * (pix_x_int + 0.5)) - 0.5\n  cam_y = (pixtocam[:, :2, :2]"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * jnp.linalg.norm(dirs, axis=-1, keepdims=True)\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  w = jax.nn.softmax(w_logits, axis=-1)\n  # Compute the PDF and CDF for each weight vector.\n  cw = integrate_weights(w)\n  # Sample from the CDF.\n  if rng is not None:\n    u = rng.uniform(shape=(num_samples,), minval=eps, maxval=1 - eps)\n  else:\n    u = jnp.linspace(eps, 1 - eps, num_samples)\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  # If we're doing deterministic sampling, we can do a bit better than\n  # just sampling from the PDF.\n  if deterministic_center:\n    t_new = math.select(\n        (t_new < t[Ellipsis, :-1],\n         t_new >= t[Ellipsis, 1:]),\n        t_new,\n        t[Ellipsis, 1:-1]\n    )\n  # Jittering is a little more expensive, so we only do it if the user\n  # requested it.\n  if single_jitter:\n    t_new = t_new + eps * jax.random.normal(\n        rng,\n        shape=t_new.shape,\n        dtype=t_new.dtype,\n        seed=None,\n        name='jittered_samples')\n  return t_new\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1 - eps, num_samples)\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Interpolate into the inverse CDF.\n  t_samples = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  # Adjust the first and last intervals to fit within the domain.\n  t_samples = t_samples.at[Ellipsis, 0].set(domain[0])\n  t_samples = t_samples.at[Ellipsis, -1].set(domain[1])\n\n  return t_samples\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w)\n  # Compute the integrated weights.\n  cw = jnp.cumsum(w)\n  # Interpolate into the inverse CDF.\n  return jnp.interp(ps, cw, t, left=t[Ellipsis, :1], right=t[Ellipsis, -1:])\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Convert the histogram to a PDF\n  pdf = weight_to_pdf(t, w)\n  # Blur the PDF\n  pdf_blurred = linspline.blur(pdf, blur_halfwidth)\n  # Resample the PDF\n  w_resampled = resample(tq, t, pdf_blurred)\n  return w_resampled\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  if jnp.ndim(transform) == 1:\n    transform = jnp.full_like(vectors, transform, jnp.float32)\n  return matmul(to_homogeneous(vectors), transform)\n\n"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  if not jnp.is_tensor(t):\n    t = jnp.array(t)\n  if not jnp.is_tensor(tp):\n    tp = jnp.array(tp)\n  if not jnp.is_tensor(vp):\n    vp = jnp.array(vp)\n  if t.shape != tp.shape or t.shape != vp.shape:\n    raise ValueError(\n        f't.shape = {t.shape}, tp.shape = {tp.shape}, vp.shape = {vp.shape}'\n    )\n  if not jnp.all(t > tp):\n    raise ValueError('t must be greater than tp')\n  if not jnp.all(t[:-1] < t[1:]):\n    raise ValueError('t must be sorted')\n  if use_avg and not jnp.all(t[:-1] < t[1:]):\n    raise ValueError('t must be sorted')\n  if not jnp.is_tensor(use_avg):\n    use_avg = jnp.array(use_avg)\n  if use_avg.shape != t.shape or use_avg.shape != tp.shape or use_avg.shape != vp.shape:\n    raise ValueError(\n        f'use_avg.shape = {use_avg.shape}, t.shape = {t.shape}, tp.shape = {tp.shape}, vp.shape = {vp.shape}'\n    )\n  if not jnp.all(use_avg):\n    raise ValueError('use_avg must be all True or all False')\n  if jnp.all(use_avg):\n    # If we're using the average method, we can do a simple linear interpolation.\n    return jnp.sum(\n        jnp.diff(t) * jnp.interp(t, tp, vp, left=0, right=0), axis=-1\n    )\n  else:\n    # If we're using the sum method, we need to do a piecewise constant interpolation.\n    # We're going to do this by interpolating the values of the step function\n    # at the"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # scale the mean and variance\n  scale = jnp.exp(jnp.arange(min_deg, max_deg))\n  scale = jnp.concatenate((scale, jnp.ones(mean.shape)), axis=-1)\n  scale = jnp.tile(scale, (mean.shape[0], 1))\n  mean = mean * scale\n  var = var * scale\n  # apply the encoding\n  return jnp.sin(mean) * jnp.exp(-0.5 * var)\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def directional_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n    return sph_harms\n\n  return directional_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    if not lines:\n        return []\n    if xml:\n        lines = [line for line in lines if not line.is_table_row()]\n    if not lines:\n        return []\n    if xml:\n        lines = [line for line in lines if not line.is_table_row()]\n    if not lines:\n        return []\n    lines = [line for line in lines if not should_skip(line, xml)]\n    if not lines:\n        return []\n    lines = [line for line in lines if not find_floating_chars(line)]\n    if not lines:\n        return []\n    lines = [line for line in lines if not line.is_table_row()]\n    if not lines:\n        return []\n    lines = [line for line in lines if not should_skip(line, xml)]\n    if not lines:\n        return []\n    lines = [line for line in lines if not find_floating_chars(line)]\n    if not lines:\n        return []\n    lines = [line for line in lines if not line.is_table_row()]\n    if not lines:\n        return []\n    lines = [line for line in lines if not should_skip(line, xml)]\n    if not lines:\n        return []\n    lines = [line for line in lines if not find_floating_chars(line)]\n    if not lines:\n        return []\n    lines = [line for line in lines if not line.is_table_row()]\n    if not lines:\n        return []\n    lines = [line for line in lines if not should_skip(line, xml)]\n    if not lines:\n        return []\n    lines = [line for line in lines if not find_floating_chars(line)]\n    if not lines:\n        return []\n    lines = [line for line in lines if not line.is_table_row()]\n    if not lines:\n        return []\n    lines = [line for line in lines if not should_skip(line, xml)]\n    if not lines:\n        return []\n    lines = [line for line in lines if not find_floating_chars(line)]\n    if not lines:\n        return []"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n    else:\n        org_texts = org_texts.replace(\"  \", \" \")\n        org_texts = org_texts.replace(\"  \", \" \")\n        org_texts = org_texts.replace(\"  \", \" \")\n        org_texts = org_texts.replace(\"  \", \" \")\n        org_texts = org_texts.replace(\"  \", \" \")\n        org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    org_texts = org_texts.replace(\"  \", \" \")\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        if key is not None and not isinstance(key, int):\n            raise TypeError(\"Expected a document key\")\n        if key is not None and key < 0:\n            key += len(self)\n        if key is not None and key > len(self):\n            raise ValueError(\"Key out of range\")\n        if key is not None and key < 0:\n            key += len(self)\n        if key is not None and key > len(self):\n            raise ValueError(\"Key out of range\")\n        if key is None:\n            return self.posns.positions(self.term_dict.get_term_id(token))\n        else:\n            return self.posns.positions(self.term_dict.get_term_id(token), key)\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec is None:\n        return 1\n    if spec == 'all':\n        return num_clauses\n    if spec == 'all-but-one':\n        return num_clauses - 1\n    if spec == 'all-but-two':\n        return num_clauses - 2\n    if spec == 'all-but-three':\n        return num_clauses - 3\n    if spec == 'all-but-four':\n        return num_clauses - 4\n    if spec == 'all-but-five':\n        return num_clauses - 5\n    if spec == 'all-but-six':\n        return num_clauses - 6\n    if spec == 'all-but-seven':\n        return num_clauses - 7\n    if spec == 'all-but-eight':\n        return num_clauses - 8\n    if spec == 'all-but-nine':\n        return num_clauses - 9\n    if spec == 'all-but-ten':\n        return num_clauses - 10\n    if spec == 'all-but-eleven':\n        return num_clauses - 11\n    if spec == 'all-but-twelve':\n        return num_clauses - 12\n    if spec == 'all-but-thirteen':\n        return num_clauses - 13\n    if spec == 'all-but-fourteen':\n        return num_clauses - 14\n    if spec == 'all-but-fifteen':\n        return num_clauses - 15\n    if spec == 'all-but-sixteen':\n        return num_clauses - 16\n    if spec == 'all-but-seventeen':\n        return num_clauses - 17\n    if spec == 'all-but-eighteen':\n        return num_clauses - 18\n    if spec == 'all-but-nineteen':\n        return num_clauses - 19\n    if spec == 'all-but-twenty':\n        return num_clauses - 20\n    if spec == 'all-but-twenty-one':\n        return num_clauses - 21\n    if spec == 'all-but-twenty-"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_slop_1(tokens)\n        else:\n            return self.phrase_freq_slop_not_1(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n        if not isinstance(tokenizer, (type(ws_tokenizer), str)):\n            raise TypeError(\"Expected tokenizer to be a function or a string, got {}\".format(type(tokenizer)))\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool for truncate, got {}\".format(type(truncate)))\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected int for batch size, got {}\".format(type(batch_size)))\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool for avoid_copies, got {}\".format(type(avoid_copies)))\n\n        if batch_size < 1:\n            raise ValueError(\"batch_size must be a positive integer\")\n        if not isinstance(array, list):\n            array = list(array)\n        if not isinstance(array, list):\n            raise TypeError(\"Expected a list, got {}\".format(type(array)))\n        if not isinstance(tokenizer, (type(ws_tokenizer), str)):\n            raise TypeError(\"Expected tokenizer to be a function or a string, got {}\".format(type(tokenizer)))\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool for truncate, got {}\".format(type(truncate)))\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected int for batch size, got {}\".format(type(batch_size)))\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool for avoid_copies, got {}\".format(type(avoid_copies)))\n\n        if batch_size < 1:\n            raise ValueError(\"batch_size must be a positive integer\")\n        if not isinstance(array, list):\n            array = list(array)\n        if not isinstance(array, list):\n            raise TypeError(\"Expected a list, got {}\".format(type(array)))\n        if not isinstance(tokenizer, (type(ws_tokenizer), str)):\n            raise TypeError(\"Expected tokenizer to be a function or a string, got {}\".format(type(tokenizer)))\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool for truncate, got"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['strategies'][self.config['strategy']],\n            self.config['autoCloseConnections']\n        )\n        self.server.start()\n        self.connections = {DEFAULT_CONNECTION_ID: Connection(self.config['proxyHost'], self.config['proxyPort'])}\n        self.lock = threading.Lock()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.asanyarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.ascontiguousfarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)\n    arr = np.asfortranarray(arr)\n    arr = np.ascontiguousarray(arr)"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"qf is required\")\n    if not qf:\n        raise ValueError(\"q is required\")\n\n    # Parse the query\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, query=q, query_fields=qf)\n\n    # Parse the field boosts\n    query_fields = parse_field_boosts(qf)\n\n    if mm is None:\n        mm = \"100%\"\n\n    if pf is None:\n        pf = []\n    if pf2 is None:\n        pf2 = []\n    if pf3 is None:\n        pf3 = []\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if message.is_send_message():\n            self.send_message(process, message)\n        elif message.is_recv_message():\n            self.recv_message(process, message)\n        elif message.is_close_message():\n            self.close_message(process, message)\n        else:\n            self.logger.error('Unhandled message type')\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.server.stop()\n        for connection in self.connections.values():\n            connection.close()\n"}
