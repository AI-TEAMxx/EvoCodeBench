{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, output TEXT)\"\n            )\n            c.execute(\n                \"SELECT output FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\",\n                (func_name, json.dumps(args), json.dumps(kwargs)),\n            )\n            row = c.fetchone()\n            if row is None:\n                output = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, output) VALUES (?, ?, ?, ?)\",\n                    (func_name, json.dumps(args), json.dumps(kwargs), json.dumps(output)),\n                )\n                conn.commit()\n                return output\n            else:\n                return json.loads(row[0])\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_norm = np.sum(mat0**2, axis=0)\n  mat1_norm = np.sum(mat1**2, axis=0)\n  mat0_mat1_dot = np.dot(mat0.T, mat1)\n  sq_dist = mat0_norm[:, None] + mat1_norm[None, :] - 2 * mat0_mat1_dot\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return path is None or path == \"\" or path.startswith(\"file://\") or path.startswith(\"https://\")\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"The assets names must be provided when {name} is a dictionary\"\n            )\n        if dim == 1:\n            items = np.array([items.get(asset, fill_value) for asset in assets_names])\n        else:\n            items = np.array(\n                [\n                    [items.get(asset, fill_value) for asset in assets_names]\n                    for _ in range(n_assets)\n                ]\n            )\n    else:\n        items = np.asarray(items)\n        if dim == 1:\n            if items.shape[0] != n_assets:\n                raise ValueError(\n                    f\"The {name} must have shape (n_assets,) but got {items.shape}\"\n                )\n        else:\n            if items.shape[0] != n_assets or items.shape[1] != n_assets:\n                raise ValueError(\n                    f\"The {name} must have shape (n_assets, n_assets) but got\"\n                    f\" {items.shape}\"\n                )\n    return items\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 323\n  linear1 = ((srgb + 11) / 211) ** (12 / 5)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  tck, u_keyframes = scipy.interpolate.splprep(\n      t_input, k=spline_degree, s=smoothness, per=False\n  )\n  return scipy.interpolate.splev(t_output, tck)\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    if word.isupper():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.lower()\n    if word[0].islower() and word[1].islower():\n        return word.lower()\n    return word\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array (i.e., containing only True or False values).\"\n        )\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * x_mag - 1) / x_mag\n  z = scale * x\n  return z\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        if column in summary_df.columns:\n            summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    return cov / jnp.sqrt(det)\n  elif mode == 'accurate':\n    log_det = jnp.linalg.slogdet(cov)[1]\n    return cov / jnp.exp(log_det / 2)\n  else:\n    raise ValueError(f'Invalid mode: {mode}')\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', type=bool, default=False, help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', type=bool, help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be retried upon failure.')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file, specifying where to find the configuration settings.')\n\n    return parser.parse_args()\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n % len(charset)]\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        interval = workers_intervals[worker_idx]\n        chunk_index = indexes[worker_idx] // interval[1]\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    coordinates = coordinates.astype(jnp.float32)\n    coordinates = jnp.clip(coordinates, 0, 1)\n    coordinates = coordinates * (values.shape[:3] - 1)\n    coordinates = jnp.floor(coordinates).astype(jnp.int32)\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    coordinates = coordinates.astype(jnp.float32)\n    coordinates = jnp.clip(coordinates, 0, 1)\n    coordinates = coordinates * (values.shape[0] - 1)\n    coordinates = jnp.floor(coordinates).astype(jnp.int32)\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check that the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the integer weights for each vertex of the triangle\n  weights = np.array(list(itertools.product(range(v + 1), repeat=3)))\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=1, keepdims=True)\n\n  return weights\n\n"}
{"namespace": "linspline.query", "completion": "  # Check that the spline is valid\n  check_zero_endpoints(v)\n\n  # Find the indices of the knots that are closest to the query points\n  indices = jnp.searchsorted(t, tq, side='right') - 1\n\n  # Find the values at the query points using linear interpolation\n  values = jnp.where(\n      (indices >= 0) & (indices < t.shape[-1] - 1),\n      v[..., indices] + (v[..., indices + 1] - v[..., indices]) * (tq - t[..., indices]) / (t[..., indices + 1] - t[..., indices]),\n      0.0,\n  )\n\n  return values\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert the ray origins to NDC coordinates.\n  origins_ndc = xnp.matmul(pixtocam, xnp.vstack((origins, xnp.ones(origins.shape[1]))))\n  origins_ndc = origins_ndc / origins_ndc[2, :]\n  origins_ndc = origins_ndc[:2, :]\n\n  # Convert the ray directions to NDC coordinates.\n  directions_ndc = xnp.matmul(pixtocam, xnp.vstack((directions, xnp.zeros(directions.shape[1]))))\n  directions_ndc = directions_ndc / directions_ndc[2, :]\n  directions_ndc = directions_ndc[:2, :] - origins_ndc\n\n  # Adjust the ray origins to the near plane.\n  origins_ndc = origins_ndc * near / origins_ndc[2, :]\n\n  return origins_ndc, directions_ndc\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1e-6\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda x: jieba.cut(x)\n    bleu = evaluate.bleu(beam_size=5)\n    score = bleu.measure(continuation, reference, tokenizer=tokenizer)\n    if with_penalty:\n        score = bleu.penalty(score)\n    return score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.maximum(x, eps)) * jnp.sign(x) + value_at_zero * (1 - jnp.sign(x))\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_stepfun("}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - np.floor((val + offset * period) / period) * period\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent, MicroAgent):\n            return {\n                'dynamic_prompt': agent.dynamic_prompt,\n                'purpose': agent.purpose,\n                'purpose_embedding': agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n                'depth': agent.depth,\n                'max_depth': agent.max_depth,\n                'usage_count': agent.usage_count,\n                'id': agent.id,\n                'parent_id': agent.parent_id,\n                'working_agent': agent.working_agent,\n                'is_prime': agent.is_prime,\n                'evolve_count': agent.evolve_count,\n                'number_of_code_executions': agent.number_of_code_executions,\n                'last_input': agent.last_input\n            }\n        else:\n            raise TypeError('AgentSerializer.to_dict() expects a MicroAgent object as input.')\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Initialize the dictionaries that will be returned\n    bins = defaultdict(list)\n    weights_per_bin = defaultdict(int)\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_weight_bin = min(weights_per_bin, key=weights_per_bin.get)\n        bins[min_weight_bin].append(item)\n        weights_per_bin[min_weight_bin] += weight\n\n    return bins, weights_per_bin\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_json = json.dumps(data, sort_keys=True)\n        return hashlib.sha256(data_json.encode()).hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Check if the input is a 2D numpy array\n    if not isinstance(polygon, np.ndarray) or polygon.ndim != 2:\n        raise ValueError(\"The input polygon must be a 2D numpy array.\")\n\n    # Check if the input has at least 3 points\n    if polygon.shape[0] < 3:\n        raise ValueError(\"The input polygon must have at least 3 points.\")\n\n    # Check if the input has 2 columns\n    if polygon.shape[1] != 2:\n        raise ValueError(\"The input polygon must have 2 columns.\")\n\n    # Check if the input has a valid max_point_distance\n    if max_point_distance <= 0:\n        raise ValueError(\"The max_point_distance must be a positive integer.\")\n\n    # Compute the distance between consecutive points\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    polygon_length = np.sum(filtered_distances)\n\n    return polygon_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    rel_tr = max_area * rel_tr\n\n    return [polygon for polygon in polygons if area(polygon) > max(rel_tr, abs_tr)]\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_yielded % (num_workers * batch_size)\n        num_samples_yielded += batch_size\n\n    return indexes\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    filtered_results = []\n    filtered_metadatas = []\n    for result, metadata, val in zip(results, metadatas, value):\n        if val <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2).\")\n\n    return 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Expand the dimensions of v to match those of a\n    v = v.unsqueeze(-1).expand_as(a)\n\n    # Find the indices where v is greater than or equal to a\n    idx_lo = (v >= a).nonzero()\n\n    # Find the indices where v is greater than a\n    idx_hi = (v > a).nonzero()\n\n    # If no indices are found, set the lower bound to the first index of a\n    if idx_lo.numel() == 0:\n        idx_lo = torch.tensor([0], device=a.device)\n\n    # If no indices are found, set the upper bound to the last index of a\n    if idx_hi.numel() == 0:\n        idx_hi = torch.tensor([a.shape[-1] - 1], device=a.device)\n\n    # Return the lower and upper bounds\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # The intrinsic matrix is a 3x3 matrix that represents the camera's internal parameters.\n  # It's used to calibrate the camera and reconstruct 3D world space.\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n  # [0, 0, 1]\n  #\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n  # [0, 0, 1]\n  #\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n  # [0, 0, 1]\n  #\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n  # [0, 0, 1]\n  #\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n  # [0, 0, 1]\n  #\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n  # [0, 0, 1]\n  #\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n  # [0, 0, 1]\n  #\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n  # [0, 0, 1]\n  #\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n  # [0, 0, 1]\n  #\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n  # [0, 0, 1]\n  #\n  # The matrix is defined as follows:\n  #\n  # [fx, 0, cx]\n  # [0, fy, cy]\n "}
{"namespace": "coord.contract", "completion": "  return x / jnp.sqrt(1 + jnp.sum(x**2, axis=-1, keepdims=True))\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if abs(num_bytes) < 1000:\n            return f\"{num_bytes:.1f} {unit}\"\n        num_bytes /= 1000\n    return f\"{num_bytes:.1f} {unit}\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has the specified number of dimensions.\n\n        Args:\n            cls (type): Class type.\n            v (np.ndarray): Value to check.\n            field (fields.ModelField): Field descriptor.\n\n        Raises:\n            ValueError: Exception raised if array doesn't have the specified number of dimensions.\n\n        Returns:\n            np.ndarray: `v` sent for further processing.\n        \"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}.\"\n            )\n\n        return v\n\n    return validator"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Extract the x, y, and z coordinates from the input array\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate the radius (r)\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination (theta)\n  theta = jnp.arccos(z / (r + eps))\n\n  # Calculate the azimuth (phi)\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rouge1'].fmeasure\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # Try to locate the object using a fallback method.\n        # This method is used to locate objects that are not part of the standard library.\n        # It attempts to locate the object by importing the module and then accessing the object using its fully qualified name.\n        # If the object cannot be located using this method, an ImportError is raised.\n        module_name, _, object_name = name.rpartition(\".\")\n        module = __import__(module_name)\n        return getattr(module, object_name)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the length of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of the ids and scores tuples must match.\")\n\n    # Check if the length of the ids and scores tuples match\n    if len(ids) != len(weights):\n        raise ValueError(\"The length of the ids and scores tuples must match.\")\n\n    # Check if the sum of the weights is 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Normalize the scores\n    normalized_scores = []\n    for i in range(len(scores)):\n        scores_i = scores[i]\n        min_score = min(scores_i)\n        max_score = max(scores_i)\n        normalized_scores_i = [(score - min_score) / (max_score - min_score) for score in scores_i]\n        normalized_scores.append(normalized_scores_i)\n\n    # Combine the scores using the convex combination method\n    combined_scores = []\n    for i in range(len(normalized_scores[0])):\n        combined_score = 0\n        for j in range(len(normalized_scores)):\n            combined_score += weights[j] * normalized_scores[j][i]\n        combined_scores.append(combined_score)\n\n    # Sort the combined scores in descending order\n    sorted_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)\n\n    # Select the top_k ids and scores\n    top_k_ids = [ids[i][sorted_indices[i]] for i in range(top_k)]\n    top_k_scores = [combined_scores[sorted_indices[i]] for i in range(top_k)]\n\n    return top_k_ids, top_k_scores"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n        return f\"{x:.2f}%\"\n    if x < 1e-3:\n        return f\"{x:.2e}\"\n    if x < 1e-2:\n        return f\"{x:.3f}\"\n    if x < 1e-1:\n        return f\"{x:.4f}\"\n    if x < 1:\n        return f\"{x:.5f}\"\n    if x < 10:\n        return f\"{x:.4f}\"\n    if x < 100:\n        return f\"{x:.3f}\"\n    if x < 1000:\n        return f\"{x:.2f}\"\n    return f\"{x:.1f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024 * 1024 * 1024)\n        if free_space < threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\"\\n\", \" \")\n    line_text = line_text.replace(\"\\t\", \" \")\n    line_text = line_text.strip()\n    line_text_list = line_text.split()\n    return line_text, line_text_list\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n    weights = np.random.rand(n - zeros)\n    weights = weights / np.sum(weights)\n    if zeros > 0:\n        zeros_idx = np.random.choice(n, zeros, replace=False)\n        weights[zeros_idx] = 0\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop(\"module_type\")\n        return cls(module_type, module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    if \"bbox\" not in instance:\n        raise ValueError(\"Cannot crop without the bounding box of the instance!\")\n\n    # Ensure the crop fits within the image boundaries\n    crop_size = np.minimum(crop_size, image_size)\n\n    # Calculate the top-left corner of the crop\n    top_left = np.maximum(\n        np.minimum(instance[\"bbox\"], image_size - crop_size), np.zeros(2)\n    )\n\n    # Calculate the dimensions of the crop\n    dimensions = np.minimum(crop_size, image_size - top_left)\n\n    # Create the CropTransform object\n    return T.CropTransform(top_left, dimensions)\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of the input array along the last axis\n  squared_norm = jnp.sum(x * x, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent division by zero\n  squared_norm = jnp.maximum(squared_norm, jnp.finfo(jnp.float32).eps)\n\n  # Compute the normalized array by dividing the input array by the squared norm\n  normalized_x = x / jnp.sqrt(squared_norm)\n\n  # Clamp the squared norm to a minimum value during the backward pass to prevent exploding gradients\n  squared_norm = jnp.maximum(squared_norm, grad_eps)\n\n  # Define the gradient function for the normalized array\n  def grad_fn(grad):\n    # Compute the gradient of the squared norm with respect to the input array\n    grad_squared_norm = 2 * jnp.sum(x * grad, axis=-1, keepdims=True)\n\n    # Compute the gradient of the normalized array with respect to the input array\n    grad_normalized_x = (\n        grad - 0.5 * grad_squared_norm * normalized_x / squared_norm\n    )\n\n    return grad_normalized_x\n\n  # Attach the gradient function to the normalized array\n  normalized_x = jnp.array(normalized_x, grad_fn=grad_fn)\n\n  return normalized_x\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    assert isinstance(annos, (list, tuple))\n    assert isinstance(image_size, (list, tuple))\n    assert len(image_size) == 2\n\n    # If the annotations are empty, return an empty Instances object\n    if len(annos) == 0:\n        return Instances(image_size)\n\n    # Convert the annotations to a numpy array\n    if isinstance(annos[0], list):\n        annos = [np.array(a) for a in annos]\n    elif isinstance(annos[0], dict):\n        annos = [a for a in annos]\n    else:\n        logging.warning(f\"Invalid annotation type {type(annos[0])}\")\n        return None\n\n    # Extract the fields from the annotations\n    boxes = [BoxMode.convert(a[\"bbox\"], a[\"bbox_mode\"], BoxMode.XYXY_ABS) for a in annos]\n    classes = [a[\"category_id\"] for a in annos]\n    masks = [a.get(\"segmentation\", None) for a in annos]\n    keypoints = [a.get(\"keypoints\", None) for a in annos]\n\n    # Create an empty Instances object with the image size\n    instances = Instances(image_size)\n\n    # Add the fields to the Instances object\n    instances.gt_boxes = Boxes(boxes)\n    instances.gt_classes = torch.tensor(classes, dtype=torch.int64)\n    if mask_format == \"bitmask\":\n        instances.gt_masks = BitMasks(masks)\n    elif mask_format == \"polygon\":\n        instances.gt_masks = PolygonMasks(masks)\n    if keypoints is not None:\n        instances.gt_keypoints = Keypoints(keypoints)\n\n    return instances\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    if not data_home.exists():\n        data_home.mkdir(parents=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Check if the input is a 2D array\n    if not isinstance(cov, np.ndarray) or cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array.\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Check if the standard deviation is non-zero for all variables\n    if np.any(std == 0):\n        raise ValueError(\"Standard deviation is zero for at least one variable.\")\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(module):\n        if hasattr(module, \"training\"):\n            module.__class__.training = torch.jit.Final[bool]\n\n    model.apply(_freeze_training_mode)\n    yield\n    model.apply(_freeze_training_mode)\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if shape(field1) equals shape(field2).\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, []\n        elif isinstance(metrics[0], dict):\n            return [m[\"name\"] for m in metrics], metrics\n    elif isinstance(metrics, str):\n        return [metrics], []\n    elif isinstance(metrics, dict):\n        return [metrics[\"name\"]], [metrics]\n    else:\n        raise ValueError(\"Invalid metrics format.\")\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        contract: inv_contract,\n        geopoly.geopoly: geopoly.geopoly_inv,\n    }.get(fn)\n    if fn_inv is None:\n      raise ValueError(\n          f'No inverse function found for {fn}. Please provide one explicitly.'\n      )\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. Represents the metric distances to be mapped to normalized distances.\n    :return: Tensor. The normalized distances in the range [0, 1] corresponding to the input metric distances.\n    \"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    s = (fn(t) - fn(t_near)) / (fn(t_far) - fn(t_near))\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances in the range [0, 1] back to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. Represents the normalized distances in the range [0, 1] to be mapped back to metric distances.\n    :return: Tensor. The metric distances corresponding to the input normalized distances.\n    \"\"\"\n    t = fn_inv(s * (fn(t_far) - fn(t_near)) + fn(t_near))\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            if ids[i][j] not in weighted_sums:\n                weighted_sums[ids[i][j]] = 0\n            weighted_sums[ids[i][j]] += scores[i][j] * weights[i]\n\n    # Normalize the weighted sums\n    min_sum = min(weighted_sums.values())\n    max_sum = max(weighted_sums.values())\n    if max_sum != min_sum:\n        for key in weighted_sums:\n            weighted_sums[key] = (weighted_sums[key] - min_sum) / (max_sum - min_sum)\n\n    # Sort the IDs by their weighted sums in descending order\n    sorted_ids = sorted(weighted_sums.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding weighted sums\n    top_k_ids = [id for id, _ in sorted_ids[:top_k]]\n    top_k_scores = [weighted_sums[id] for id in top_k_ids]\n    return top_k_ids, top_k_scores"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  jac = jax.jacfwd(fn)(mean)\n\n  # Transform the covariances\n  fn_cov = jnp.einsum('...ij,...jk,...kl->...il', jac, cov, jac)\n\n  # Transform the means\n  fn_mean = fn(mean)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  if append_identity:\n    return jnp.concatenate([scaled_x, scaled_x + 0.5 * jnp.pi, x], axis=-1)\n  else:\n    return jnp.concatenate([scaled_x, scaled_x + 0.5 * jnp.pi], axis=-1)\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        output_hidden_states=bert_config.output_hidden_states,\n        output_attentions=bert_config.output_attentions,\n        output_past=True,\n        use_cache=True,\n        pruning_axis_names=bert_config.pruning_axis_names,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        use_token_type_ids=True,\n        use_position_ids=True,\n        use_input_mask=True,\n        use_labels=True,\n        use_loss=True,\n        use_logits=True,\n        use_hidden_states=True,\n        use_attentions=True,\n        use_past=True,\n        use_cache=True,\n        use_output_attentions=True,\n        use_output_hidden_states=True,\n        use_output_past=True,\n        use_output_loss=True,\n        use_output_logits=True,\n        use_output_hidden_states=True,\n        use_output_attentions=True,\n        use_output_past=True,\n        use_output"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        # Bind the appropriate program\n        if self.render_type == Mesh.RenderType.POINTS:\n            use_gl_program(self.point_program)\n        else:\n            use_gl_program(self.mesh_program)\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_SHORT, None)\n\n        # Unbind the VAO\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not self.use_quad_draw:\n            return\n\n        if not hasattr(self, 'tex'):\n            self.init_texture()\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr.data_ptr())\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure that all inputs are batched\n    if R.ndim == 2:\n        R = R[None]\n    if tvec.ndim == 2:\n        tvec = tvec[None]\n    if camera_matrix.ndim == 2:\n        camera_matrix = camera_matrix[None]\n    if image_size.ndim == 1:\n        image_size = image_size[None]\n\n    # Validate input shapes\n    assert R.shape == tvec.shape, \"Rotation and translation vectors must have the same shape.\"\n    assert R.shape[0] == camera_matrix.shape[0], \"Rotation and camera matrix must have the same batch size.\"\n    assert R.shape[0] == image_size.shape[0], \"Rotation and image size must have the same batch size.\"\n\n    # Compute camera position and rotation\n    camera_position = -torch.einsum(\"...ij,...j->...i\", R, tvec)\n    camera_rotation = matrix_to_rotation_6d(R)\n\n    # Compute intrinsic parameters\n    focal_lengths = camera_matrix[..., 0, 0]\n    principal_points = camera_matrix[..., 0, 2]\n    sensor_width = image_size[..., 1] * focal_lengths / principal_points\n\n    # Adjust principal point offsets and normalize focal length\n    principal_point_offsets = principal_points - image_size[..., 1] / 2\n    focal_lengths = focal_lengths * image_size[..., 1] / (2 * principal_points)\n\n    # Adjust focal length for near clipping plane\n    focal_lengths = focal_lengths / (znear - 1)\n\n    # Return camera parameters\n    return torch.cat(\n        [\n            camera_position,\n            camera_rotation,\n            focal_lengths[:, None],\n            principal_point_offsets[:, None],\n            sensor_width[:, None],\n        ],\n        dim=-1,\n    )\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.upload_to_texture(self.verts_data)  # MARK: SYNC\n            return\n\n        # Set up the viewport and scissor box\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glViewport(x, y, w or self.W, h or self.H)\n        gl.glScissor(x, y, w or self.W, h or self.H)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(old_viewport[0], old_viewport[1], old_viewport[2], old_viewport[3])\n        gl.glScissor(old_scissor[0], old_scissor[1], old_scissor[2], old_scissor[3])\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # Adjust rotation and translation matrices to match PyTorch3D's conventions\n    R = R.permute(0, 2, 1)  # B, 3, 3\n    T = -T  # B, 3, 1\n    # Compute the intrinsic matrix for NDC\n    K = get_ndc_perspective_matrix(K, H, W, n=1, f=1000)\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        old_fbo = gl.glGetIntegerv(gl.GL_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, old_fbo)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sums of the values (y1) at the source times (t1)\n    cumsum_y1 = torch.cumsum(y1, dim=0)\n\n    # Compute the inner measure by interpolating the cumulative sums at the target times (t0)\n    inner = torch.interp(t0, t1, cumsum_y1)\n\n    # Compute the outer measure by interpolating the values (y1) at the target times (t0)\n    outer = torch.interp(t0, t1, y1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_inner, w_outer = inner_outer(t, t_env, w_env)\n    w_inner = torch.clamp(w_inner, min=0)\n    w_outer = torch.clamp(w_outer, min=0)\n    w_inner = w_inner[..., :-1]\n    w_outer = w_outer[..., 1:]\n    w_inner = torch.where(w_inner > w, w_inner - w, 0)\n    w_outer = torch.where(w_outer > w, w_outer - w, 0)\n    w_inner = torch.sum(w_inner, dim=-1)\n    w_outer = torch.sum(w_outer, dim=-1)\n    loss = 0.5 * (w_inner ** 2 + w_outer ** 2) / (w_inner + w_outer + eps)\n    return loss\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    t_inner, w_inner = inner_outer(t, t, w)\n    t_outer, w_outer = inner_outer(t, t, w)\n    w_inner = torch.clip(w_inner, min=0.)\n    w_outer = torch.clip(w_outer, min=0.)\n    return (w_inner - w_outer).pow(2).mean()\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(ps, cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the CDF and PDF for each weight vector.\n    cw = integrate_weights(w)\n    # Sample uniformly from the CDF.\n    u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n    if perturb:\n        # Perturb the uniform samples to avoid sample clustering at bin boundaries.\n        u += torch.rand(t.shape[:-1] + (num_samples,), device=t.device) / num_samples\n    # Interpolate into the inverse CDF.\n    t_new = interpolate(u, cw, t)\n    if single_jitter:\n        # Apply the same jitter to every sample along each dimension.\n        t_new += torch.rand(t.shape[:-1] + (1,), device=t.device) / num_samples\n    else:\n        # Apply independent jitter to each sample.\n        t_new += torch.rand(t.shape[:-1] + (num_samples,), device=t.device) / num_samples\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t_dilated = torch.max(t[..., :-1], t[..., 1:] - dilation)\n    t_dilated = torch.clamp(t_dilated, domain[0], domain[1])\n    w_dilated = w * (t_dilated - t[..., :-1]) / (t[..., 1:] - t[..., :-1])\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Ensure that tq and t have the same shape\n    if tq.shape != t.shape:\n        raise ValueError(\"tq and t must have the same shape.\")\n\n    # Ensure that tq and y have compatible shapes\n    if tq.shape[:-1] != y.shape[:-1]:\n        raise ValueError(\"tq and y must have compatible shapes.\")\n\n    # Ensure that t and y have compatible shapes\n    if t.shape[:-1] != y.shape[:-1]:\n        raise ValueError(\"t and y must have compatible shapes.\")\n\n    # Ensure that t and y have the same number of time steps\n    if t.shape[-1] != y.shape[-1]:\n        raise ValueError(\"t and y must have the same number of time steps.\")\n\n    # Ensure that t is sorted in ascending order\n    if not torch.all(torch.diff(t, dim=-1) >= 0):\n        raise ValueError(\"t must be sorted in ascending order.\")\n\n    # Ensure that tq is within the range of t\n    if not torch.all(torch.logical_and(tq >= t[..., 0], tq <= t[..., -1])):\n        raise ValueError(\"tq must be within the range of t.\")\n\n    # Ensure that tq is a 1D tensor\n    if tq.ndim != 1:\n        raise ValueError(\"tq must be a 1D tensor.\")\n\n    # Ensure that t is a 2D tensor\n    if t.ndim != 2:\n        raise ValueError(\"t must be a 2D tensor.\")\n\n    # Ensure that y is a 2D tensor\n    if y.ndim != 2:\n        raise ValueError(\"y must be a 2D tensor.\")\n\n    # Ensure that tq and t have the same number of time steps\n    if tq.shape[-1] != t.shape[-1]:\n        raise ValueError(\"tq and t must have the same number of time steps.\")\n\n    # Ensure that tq and y have the same number of time steps\n    if tq.shape[-1] != y.shape[-1]:\n        raise ValueError(\"tq and y must have the same number of time steps.\")\n\n    # Ensure that tq and y have compatible shapes\n    if tq.shape[:-1] != y.shape[:-1]:\n        raise ValueError(\"tq and y must have compatible shapes.\")\n\n    # Ensure that t and y have compatible shapes\n    if"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the annealing factor.\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - t))\n    # Adjust the weights.\n    w_anneal = w * anneal_factor\n    # Ensure stability in the computation.\n    w_anneal = torch.where(\n        (t[..., 1:] - t[..., :-1]) < eps,\n        torch.zeros_like(w_anneal),\n        w_anneal,\n    )\n    # Prevent NaN values.\n    w_anneal = w_anneal / torch.sum(w_anneal, dim=-1, keepdim=True).clip(eps)\n    return w_anneal\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, (tuple, list)):\n        return type(batch)(to_cuda(b, device, ignore_list) for b in batch)\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # gather triangles from vertices and faces\n    # f: B, F, 3\n    # v: B, V, 3\n    # out: B, F, 3, 3\n    # out = multi_gather(v, f, dim)\n    # out = out.reshape(out.shape[:2] + (-1, 3))\n    # out = torch.cross(out[:, :, 1] - out[:, :, 0], out[:, :, 2] - out[:, :, 0], dim=-1)\n    # out = out / out.norm(dim=-1, keepdim=True)\n    # return out\n\n    # gather triangles from vertices and faces\n    # f: B, F, 3\n    # v: B, V, 3\n    # out: B, F, 3, 3\n    out = multi_gather(v, f, dim)\n    out = out.reshape(out.shape[:2] + (-1, 3))\n    out = torch.cross(out[:, :, 1] - out[:, :, 0], out[:, :, 2] - out[:, :, 0], dim=-1)\n    out = out / out.norm(dim=-1, keepdim=True)\n    return out\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = torch.as_tensor(batch).unsqueeze(0)\n    else:  # others, keep as is\n        batch = torch.as_tensor(batch).unsqueeze(0)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert camera parameters into a batch format\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        # Convert GUI related elements into a batch format\n        batch.origin, batch.world_up, batch.movement_speed, batch.movement_force, batch.drag_coeff_mult, batch.constant_drag, batch.pause_physics, batch.min_interval = self.origin, self.world_up, self.movement_speed, self.movement_force, self.drag_coeff_mult, self.constant_drag, self.pause_physics, self.min_interval\n\n        # Convert camera parameters into a batch format\n        batch.mass, batch.moment_of_inertia, batch.angular_speed, batch.angular_acc, batch.angular_friction, batch.constant_torque, batch.movement_torque = self.mass, self.moment_of_inertia, self.angular_speed, self.angular_acc, self.angular_friction, self.constant_torque, self.movement_torque\n\n        # Return the batch\n        return batch\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            self.persistence.save_agent(agent.id, AgentSerializer.serialize(agent))\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) == 0:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            max_similarity_index = similarities.index(max_similarity)\n            return self.agents[max_similarity_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            flag=True,\n            openai_wrapper=self.openai_wrapper,\n            agent_persistence_manager=self.agent_persistence\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n        return None\n\n    def get_all_agents(self):\n        \"\"\"\n        Returns a list of all agents in the database.\n        \"\"\"\n        return self.persistence.get_all_agents()\n\n    def get_all_agent_purposes(self):\n        \"\"\"\n        Returns a list of all agent purposes in the database.\n        \"\"\"\n        return self.persistence.get_all_agent_purposes()\n\n    def get_agent_by_id(self, agent_id):\n        \"\"\"\n        Returns an agent with the given ID from the database.\n        \"\"\"\n        return self.persistence.get_agent_by_id(agent_id)\n\n    def get_agent_by_purpose(self, purpose):\n        \"\"\"\n        Returns an agent with the given purpose from the database.\n        \"\"\"\n        return self.persistence.get_agent_by_purpose(purpose)\n\n    def get_agent_by_id_and_purpose(self, agent_id, purpose):\n        \"\"\"\n        Returns an agent with the given ID and purpose from the database.\n        \"\"\"\n        return self.persistence.get_agent_by_id_and_purpose(agent_id, purpose)\n\n    def get_agent_by_id_and_purpose_and_lifecycle(self, agent_id, purpose"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents()\n        if serialized_agents:\n            return [\n                AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n                for serialized_agent in serialized_agents\n            ]\n        return []\n\n    def load_all_agents_with_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database with the given purpose and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents_with_purpose(purpose)\n        if serialized_agents:\n            return [\n                AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n                for serialized_agent in serialized_agents\n            ]\n        return []\n\n    def load_all_agents_with_purpose_and_status(self, purpose, status, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database with the given purpose and status and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents_with_purpose_and_status(purpose, status)\n        if serialized_agents:\n            return [\n                AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n                for serialized_agent in serialized_agents\n            ]\n        return []\n\n    def load_all_agents_with_status(self, status, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database with the given status and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n       "}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT.format(\n                goal=goal,\n                sample_input=sample_input,\n                examples=EXAMPLES\n            )\n            prompt_embedding = self.openai_wrapper.get_embedding(prompt)\n            prompt_template = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                sample_input=sample_input,\n                examples=EXAMPLES\n            )\n            prompt_template_embedding = self.openai_wrapper.get_embedding(prompt_template)\n            prompt_template_similarity = self.openai_wrapper.get_similarity(prompt_embedding, prompt_template_embedding)\n            if prompt_template_similarity > 0.9:\n                return prompt_template\n            else:\n                return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,),\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.connection.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, \"output.txt\"), \"w\") as f:\n            with redirect_stdout(f):\n                CommandLine(CommandLineParam(args.task, args.upload_files, args.model, args.record_dir,\n                                             args.mode, args.quiet, args.max_subtask_chain_length,\n                                             args.enable_ask_human_for_help, args.max_plan_refine_chain_length,\n                                             args.max_plan_tree_depth, args.max_plan_tree_width,\n                                             args.max_retry_times, args.config_file)).run()\n    else:\n        CommandLine(CommandLineParam(args.task, args.upload_files, args.model, args.record_dir, args.mode, args.quiet,\n                                      args.max_subtask_chain_length, args.enable_ask_human_for_help,\n                                      args.max_plan_refine_chain_length, args.max_plan_tree_depth,\n                                      args.max_plan_tree_width, args.max_retry_times, args.config_file)).run()\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                response = json.loads(str(response))\n            else:\n                raise e\n\n        return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"Cannot call state_dict() in a worker process\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path if self.input_dir.path else self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state: Dict[str, Any] = self._state_dict\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The number of workers in the state dictionary ({state['num_workers']}) does not match the number of workers in the current StreamingDataset instance ({self.worker_env.world_size}).\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path in the state dictionary ({state['input_dir_path']}) does not match the input directory path in the current StreamingDataset instance ({self.input_dir.path}).\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL in the state dictionary ({state['input_dir_url']}) does not match the input directory URL in the current StreamingDataset instance ({self.input_dir.url}).\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed in the state dictionary ({state['seed']}) does not match the seed in the current StreamingDataset instance ({self.seed}).\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last flag in the state dictionary ({state['drop_last']}) does not match the drop_last flag in the current StreamingDataset instance ({self.drop_last}).\"\n            )\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle flag in the state dictionary ({state['shuffle']}) does not match the shuffle flag in the current StreamingDataset instance ({self.shuffle}).\"\n            )\n\n        if state[\"item_loader\"] is not None and self.item_loader is not None:\n            self.item_loader.load_state_dict(state[\"item_loader\"])\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.sha256(input_dir.encode()).hexdigest())\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not parse.urlparse(remote_filepath).scheme == \"s3\":\n            raise ValueError(f\"Remote file path {remote_filepath} is not an S3 URL.\")\n\n        if not os.path.exists(local_filepath):\n            with FileLock(f\"{local_filepath}.lock\", timeout=10):\n                if self._s5cmd_available:\n                    subprocess.run(\n                        [\"s5cmd\", \"cp\", remote_filepath, local_filepath],\n                        check=True,\n                        stdout=subprocess.DEVNULL,\n                        stderr=subprocess.DEVNULL,\n                    )\n                else:\n                    self._client.download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Handle restart\n    if num_workers == 0:\n        return [], []\n\n    # Handle restart\n    if num_workers == 1:\n        return [chunks_replica], [intervals_replica]\n\n    # Handle restart\n    if num_workers == 2:\n        return [chunks_replica[: len(chunks_replica) // 2], chunks_replica[len(chunks_replica) // 2 :]], [\n            intervals_replica[: len(intervals_replica) // 2],\n            intervals_replica[len(intervals_replica) // 2 :],\n        ]\n\n    # Handle restart\n    if num_workers == 3:\n        return [\n            chunks_replica[: len(chunks_replica) // 3],\n            chunks_replica[len(chunks_replica) // 3 : len(chunks_replica) // 3 * 2],\n            chunks_replica[len(chunks_replica) // 3 * 2 :],\n        ], [\n            intervals_replica[: len(intervals_replica) // 3],\n            intervals_replica[len(intervals_replica) // 3 : len(intervals_replica) // 3 * 2],\n            intervals_replica[len(intervals_replica) // 3 * 2 :],\n        ]\n\n    # Handle restart\n    if num_workers == 4:\n        return [\n            chunks_replica[: len(chunks_replica) // 4],\n            chunks_replica[len(chunks_replica) // 4 : len(chunks_replica) // 4 * 2],\n            chunks_replica[len(chunks_replica) // 4 * 2 : len(chunks_replica) // 4 * 3],\n            chunks_replica[len(chunks_replica) // 4 * 3 :],\n        ], [\n            intervals_replica[: len(intervals_replica) // 4],\n            intervals_replica[len(intervals_replica) // 4 : len(intervals_replica) // 4 * 2],\n            intervals_replica[len(intervals_replica) // 4 * 2 : len(intervals_replica) // 4 * 3],\n            intervals_replica[len(intervals_replica) // 4 * 3 :],\n        ]\n\n    # Handle restart\n    if num_workers == 5:\n        return [\n            chunks_replica[: len(chunks_replica)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's dimensions and mode\n        width, height = item.size\n        mode = item.mode\n\n        # Serialize the image's dimensions and mode\n        serialized_dimensions = width.to_bytes(4, \"little\") + height.to_bytes(4, \"little\")\n        serialized_mode = len(mode).to_bytes(4, \"little\") + mode.encode(\"utf-8\")\n\n        # Serialize the image's raw pixel data\n        with io.BytesIO() as f:\n            item.save(f, format=item.format)\n            serialized_data = f.getvalue()\n\n        # Return the serialized data and None\n        return serialized_dimensions + serialized_mode + serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile) and item.filename and os.path.exists(item.filename):\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n        elif isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_len = ints\n        mode = data[12 : 12 + mode_len].decode(\"utf-8\")\n        raw = data[12 + mode_len :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        idx = 4\n        dtype_indice = np.frombuffer(data[:idx], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        idx2 = idx + 4\n        ndim = np.frombuffer(data[idx:idx2], np.uint32)[0]\n        idx3 = idx2 + ndim * 4\n        shape = np.frombuffer(data[idx2:idx3], np.uint32)\n        raw = data[idx3:]\n        return torch.frombuffer(raw, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = item.dtype\n        shape = item.shape\n        raw = item.numpy().tobytes()\n        dtype_idx = self._dtype_to_indices[dtype]\n        ints = np.array([dtype_idx, *shape], np.uint32)\n        return ints.tobytes() + raw, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                # Try to decode the data as a JPEG image using torchvision\n                return decode_jpeg(data, \"RGB\")\n            except RuntimeError:\n                # If the decoding fails, fall back to using PIL to deserialize the data\n                pass\n\n        # If torchvision is not available or the decoding fails, use PIL to deserialize the data\n        item = Image.open(io.BytesIO(data))\n\n        if _TORCH_VISION_AVAILABLE:\n            # If torchvision is available, convert the PIL image to a PyTorch tensor\n            return pil_to_tensor(item)\n\n        return item\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        return np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = super().state_dict()\n\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"dataset\"] = self.dataset.state_dict()\n        else:\n            state[\"dataset\"] = self.dataset.state_dict()\n\n        state[\"current_epoch\"] = self.current_epoch\n        state[\"num_samples_yielded\"] = self._num_samples_yielded_streaming\n        state[\"latest_worker_idx\"] = self._latest_worker_idx\n\n        return state\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"torchvision is required to deserialize video data.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is required to deserialize video data.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            return torch.read_video(f.name)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        while self._should_write():\n            self.write_chunk()\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, filename) for filename in os.listdir(self._cache_dir)]\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            self.dataset.load_state_dict(\n                obj[\"dataset\"],\n                obj[\"num_samples_yielded\"],\n                self.num_workers,\n                self.batch_size,\n                obj[\"latest_worker_idx\"],\n            )\n        else:\n            self.dataset.load_state_dict(\n                obj[\"dataset\"],\n                self.num_workers,\n                self.batch_size,\n                obj[\"num_samples_yielded\"],\n                obj[\"latest_worker_idx\"],\n            )\n\n        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n        self.restore = True\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n                }\n        else:\n            return self._iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__, None)\n            return\n        self._iterator.load_state_dict(state_dict)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if not output_dir.url:\n        raise ValueError(f\"`output_dir` must be a `Dir` with a URL, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must be a `Dir` with a URL starting with `s3://`, got: {output_dir}\")\n\n    if append:\n        raise NotImplementedError(\"Appending data to an S3 directory is not supported.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in an S3 directory is not supported.\")\n\n    if _BOTO3_AVAILABLE:\n        s3_client = boto3.client(\"s3\")\n        try:\n            s3_client.list_objects_v2(Bucket=parse.urlparse(output_dir.url).netloc, Prefix=parse.urlparse(output_dir.url).path[1:])\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"NoSuchBucket\":\n                raise ValueError(f\"The S3 bucket `{parse.urlparse(output_dir.url).netloc}` does not exist.\")\n            else:\n                raise e\n        else:\n            raise ValueError(f\"The S3 directory `{output_dir.url}` is not empty.\")\n    else:\n        raise ImportError(\"The `boto3` package is not available. Please install it to use this function.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank == 0:\n            # Wait for all index files to be available\n            while True:\n                index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(1)\n\n            # Merge the index files\n            merged_index = {}\n            for index_file in index_files:\n                with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                    index = json.load(f)\n                    merged_index[\"chunks\"].extend(index[\"chunks\"])\n                    merged_index[\"config\"].update(index[\"config\"])\n\n            # Write the merged index file\n            with open(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\"), \"w\") as f:\n                json.dump(merged_index, f, sort_keys=True)\n        else:\n            # Wait for the merged index file to be available\n            while True:\n                index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == 1:\n                    break\n                sleep(1)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"The `lightning_sdk` package is required to execute the current operator. Please install it with `pip install lightning_sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\n            \"The `boto3` package is required to execute the current operator. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, memory=1)\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.getenv('LIGHTNING_CLOUD_COMMAND', 'python main.py')}\"\n\n    studio = Studio()\n\n    job = studio.create_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    job_url = f\"{_get_lightning_cloud_url()}/projects/{os.getenv('LIGHTNING_CLOUD_PROJECT_ID')}/jobs/{job.id}\"\n\n    print(f\"Job URL: {job_url}\")\n\n    while True:\n        job = studio.get_job(job.id)\n        if job.status == \"RUNNING\":\n            break\n        sleep(1)\n\n    print(f\"Job started: {job_url}\")\n\n    while True:\n        job = studio.get_job(job.id)\n        if job.status == \"FAILED\":\n            raise RuntimeError(f\"Job failed: {job_url}\")\n        if job.status == \"SUCCEEDED\":\n            break\n        sleep(1)\n\n    print(f\"Job succeeded: {job_url}\")\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            self._config = ChunksConfig(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._item_loader,\n                self._compression,\n            )\n            return self._config\n        except Exception as e:\n            logger.warning(f\"Failed to load the chunks configuration: {e}\")\n            return None\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be an instance of ChunkedIndex. Got {type(index)}\")\n\n        if self._config is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check if the chunk is already loaded\n        if chunk_index != self._last_chunk_index:\n            # If not, download the chunk\n            self._config.download_chunk_from_index(chunk_index)\n\n            # Preload item if possible\n            if self._prepare_thread is not None:\n                self._prepare_thread._pre_load_chunk(chunk_index)\n\n            # Delete the previous chunk\n            if self._last_chunk_index is not None:\n                self._item_loader.delete(self._last_chunk_index)\n\n            self._last_chunk_index = chunk_index\n\n        # Read the item\n        return self._item_loader.read(chunk_index, index.item_index)\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes in the distributed environment\n    num_nodes = distributed_env.num_nodes\n\n    # Get the world size of the distributed environment\n    world_size = distributed_env.world_size\n\n    # Get the rank of the current node in the distributed environment\n    rank = distributed_env.rank\n\n    # Get the number of chunks assigned to the current node\n    num_chunks = len(chunks_per_ranks[rank])\n\n    # Create a list of chunk indexes for the current node\n    chunk_indexes = list(range(num_chunks))\n\n    # Create a list of chunk indexes for all nodes\n    all_chunk_indexes = []\n\n    # Create a list of chunk indexes for each node\n    node_chunk_indexes = [[] for _ in range(num_nodes)]\n\n    # Create a list of chunk indexes for each rank\n    rank_chunk_indexes = [[] for _ in range(world_size)]\n\n    # Create a list of chunk indexes for each rank on each node\n    rank_chunk_indexes_per_node = [[] for _ in range(num_nodes)]\n\n    # Create a list of chunk indexes for each rank on each node\n    rank_chunk_indexes_per_node_per_epoch = [[] for _ in range(num_nodes)]\n\n    # Create a list of chunk indexes for each rank on each node\n    rank_chunk_indexes_per_node_per_epoch_per_seed = [[] for _ in range(num_nodes)]\n\n    # Create a list of chunk indexes for each rank on each node\n    rank_chunk_indexes_per_node_per_epoch_per_seed_per_chunk = [[] for _ in range(num_nodes)]\n\n    # Create a list of chunk indexes for each rank on each node\n    rank_chunk_indexes_per_node_per_epoch_per_seed_per_chunk_per_index = [[] for _ in range(num_nodes)]\n\n    # Create a list of chunk indexes for each rank on each node\n    rank_chunk_indexes_per_node_per_epoch_per_seed_per_chunk_per_index_per_node = [[] for _ in range(num_nodes)]\n\n    # Create a list of chunk indexes for each rank on each node\n    rank_chunk_indexes_per_node_per_epoch_"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            f\"Found {len(indexed_paths)} indexed paths in the input sequence. \"\n            \"Please provide a sequence of inputs with at most two indexed paths.\"\n        )\n\n    if len(indexed_paths) == 1:\n        return _resolve_dir(list(indexed_paths.values())[0])\n\n    if len(indexed_paths) == 2:\n        first_path = _resolve_dir(list(indexed_paths.values())[0])\n        second_path = _resolve_dir(list(indexed_paths.values())[1])\n\n        if first_path != second_path:\n            raise ValueError(\n                f\"Found two indexed paths in the input sequence, but they point to different directories: \"\n                f\"{first_path} and {second_path}. Please provide a sequence of inputs with at most two indexed paths \"\n                \"that point to the same directory.\"\n            )\n\n        return first_path\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        os.environ[\"DNS_RESOLVER_ENABLE\"] = \"1\"\n    else:\n        os.environ[\"DNS_RESOLVER_ENABLE\"] = \"0\"\n\n    try:\n        yield\n    finally:\n        os.environ[\"DNS_RESOLVER_ENABLE\"] = \"0\"\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items = len(indexes)\n    num_items_per_rank = num_items // distributed_env.world_size\n    if not drop_last:\n        num_items_per_rank += num_items % distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks: Any = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks: Any = [[] for _ in range(distributed_env.world_size)]\n    for i in range(distributed_env.world_size):\n        start = i * num_items_per_rank\n        end = (i + 1) * num_items_per_rank\n        if i == distributed_env.world_size - 1 and not drop_last:\n            end = num_items\n        chunks_per_ranks[i] = indexes[start:end]\n        chunk_intervals_per_ranks[i] = chunk_intervals[start:end]\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)  # type: ignore\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not isinstance(inputs, Sequence):\n        raise ValueError(f\"The provided {inputs} isn't supported.\")\n\n    if not isinstance(output_dir, (str, Dir)):\n        raise ValueError(f\"The provided {output_dir} isn't supported.\")\n\n    if not isinstance(fast_dev_run, (bool, int)):\n        raise ValueError(f\"The provided {fast_dev_run} isn't supported.\")\n\n    if not isinstance(error_when_not_empty, bool):\n        raise ValueError(f\"The provided {error_when_not_empty} isn't supported.\")\n\n    if not isinstance(reorder_files, bool):\n        raise ValueError(f\"The provided {reorder_files} isn't supported.\")\n\n    if not isinstance(batch_size, (int, type(None))):\n        raise ValueError(f\"The provided {batch_size} isn't supported.\")\n\n    if not isinstance(num_downloaders, (int, type(None))):\n        raise ValueError(f\"The provided {num_downloaders} isn't supported.\")\n\n    if not isinstance(num_uploaders, (int, type(None))):\n        raise ValueError(f\"The provided {num_uploaders} isn't supported.\")\n\n    if not isinstance(num_nodes, (int, type(None))):\n        raise ValueError(f\"The provided {num_nodes} isn't supported.\")\n\n    if not isinstance(num_workers, (int, type(None))):\n        raise ValueError(f\"The provided {num_workers} isn't supported.\")\n\n    if not isinstance(weights, (list, type(None))):\n        raise ValueError(f\"The provided {weights} isn't supported.\")\n\n    if not isinstance(reader, (BaseReader, type(None))):\n        raise ValueError(f\"The provided {reader} isn't supported.\")\n\n    if not isinstance(machine, (str, type(None))):\n        raise ValueError(f\"The provided {machine} isn't supported.\")\n\n    if not isinstance(fn, (FunctionType, partial)):\n        raise ValueError(f\"The provided {fn} isn't supported.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if error_when_not_empty:\n       "}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        try:\n            index, files = queue_in.get(timeout=1)\n        except Empty:\n            continue\n\n        for file in files:\n            if not os.path.exists(file):\n                input_dir.download(file, cache_dir)\n\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        if isinstance(r, tuple):\n            tmp_dir, path = r\n        else:\n            path = r\n\n        # 4. Upload the file\n        if output_dir.url is not None:\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 5. Remove the file\n        remove_queue.put(path)\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Associate the items to the workers based on number of nodes and node rank.\n    weights = [1] * len(user_items) if weights is None else weights\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for workers on the current node.\n    if file_size:\n        print(f\"Worker distribution on node {node_rank}:\")\n        for i, worker_id in enumerate(worker_ids_this_node):\n            print(f\"Worker {worker_id}: {worker_weights[i]} MB\")\n    else:\n        print(f\"Worker distribution on node {node_rank}:\")\n        for i, worker_id in enumerate(worker_ids_this_node):\n            print(f\"Worker {worker_id}: {worker_weights[i]} items\")\n\n    # Shuffle the items for each worker and return the list.\n    return [list(np.random.permutation(items)) for items in worker_items]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * _get_num_nodes()\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of items to be processed by the last worker\n    remainder = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + [num_items_per_worker] * (total_workers - 1))\n    end_indices = np.cumsum([num_items_per_worker] * total_workers)\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    if remainder > 0:\n        end_indices[-remainder:] += 1\n\n    # Assign the items to the workers\n    items_per_worker = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(items_per_worker) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    # Return the list of lists, where each sublist contains the items assigned to a worker\n    return items_per_worker\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return _IS_IN_STUDIO and input_dir is not None and element.startswith(input_dir) or os.path.exists(element)\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons == 1:\n                return torch.nn.Sequential(\n                    torch.nn.Linear(n_input_dims, n_output_dims),\n                    torch.nn.Sigmoid(),\n                )\n            else:\n                return torch.nn.Sequential(\n                    torch.nn.Linear(n_input_dims, n_neurons),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(n_neurons, n_output_dims),\n                    torch.nn.Sigmoid(),\n                )\n        else:\n            layers = []\n            layers.append(nn.Linear(n_input_dims, n_neurons))\n            if activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Unknown activation function: {activation}\")\n\n            for _ in range(n_layers - 2):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n                elif activation == \"None\":\n                    pass\n                else:\n                    raise ValueError(f\"Unknown activation function: {activation}\")\n\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n            elif output_activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Unknown activation function: {output_activation}\")\n\n            return nn.Sequential(*layers)"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Create a shifted version of the signal by shifting it by the kernel offset in both directions\n        shifted_signal_left = np.roll(signal, kernel_offset)\n        shifted_signal_right = np.roll(signal, -kernel_offset)\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(\n            [signal, shifted_signal_left, shifted_signal_right], axis=0\n        )\n\n        # Trim the median signal to remove edge effects introduced by the shifting process\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Convert rotation shift to columns\n    rotation_shift_columns = rotation_shift * template_probe.width // 360\n\n    # Get the iris codes from the probe and gallery templates\n    irisbits_probe = template_probe.irisbits\n    irisbits_gallery = template_gallery.irisbits\n\n    # Get the mask codes from the probe and gallery templates\n    maskbits_probe = template_probe.maskbits\n    maskbits_gallery = template_gallery.maskbits\n\n    # Get the half width of the iris codes\n    half_width = template_probe.half_width\n\n    # Get the total size of the iris codes\n    total_codesize = template_probe.total_codesize\n\n    # Get the square root of the total bit count\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        total_codesize, half_width, weights\n    )\n\n    # Initialize the minimum Hamming distance and corresponding rotation shift\n    min_HD = np.inf\n    min_rotation_shift = 0\n\n    # Loop over all possible rotation shifts\n    for rotation_shift_columns in range(rotation_shift_columns + 1):\n\n        # Shift the iris codes and mask codes of the gallery template\n        irisbits_gallery_shifted = np.roll(irisbits_gallery, rotation_shift_columns, axis=2)\n        maskbits_gallery_shifted = np.roll(maskbits_gallery, rotation_shift_columns, axis=2)\n\n        # Calculate the nonmatch bits for the top and bottom iris codes\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits_probe, maskbits_probe, half_width, weights\n        )\n        irisbitcount_top_shifted, maskbitcount_top_shifted, irisbitcount_bot_shifted, maskbitcount_bot_shifted = count_nonmatchbits(\n            irisbits_gallery_shifted, maskbits_gallery_shifted, half_width, weights\n        )\n\n        # Calculate the Hamming distance for the top and bottom iris codes\n        HD_top = normalized_HD(\n            irisbit"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n\n        for i in range(self.params.num_bisectors):\n            for j in range(self.params.max_iterations):\n                # Choose two random points from the polygon\n                point_1 = polygon[np.random.randint(0, len(polygon))]\n                point_2 = polygon[np.random.randint(0, len(polygon))]\n\n                # Calculate the distance between the two points\n                distance = np.linalg.norm(point_1 - point_2)\n\n                # Check if the distance is greater than the minimum distance\n                if distance > min_distance_between_sector_points_in_px:\n                    # Calculate the midpoint between the two points\n                    midpoint = (point_1 + point_2) / 2\n\n                    # Calculate the perpendicular bisector\n                    perpendicular_bisector = np.array([point_2[1] - point_1[1], point_1[0] - point_2[0]])\n\n                    # Calculate the starting and ending points of the perpendicular bisector\n                    first_bisectors_point[i] = midpoint + perpendicular_bisector / 2\n                    second_bisectors_point[i] = midpoint - perpendicular_bisector / 2\n\n                    break\n\n            else:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed. This indicates that it may not be possible to accurately estimate the center of the shape.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n        source = get_source(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param in signature.parameters.values():\n            if param.name in type_hints:\n                input_type_hints[param.name] = type_hints[param.name]\n\n        for param in signature.parameters.values():\n            if param.name in type_hints:\n                output_type_hints[param.name] = type_hints[param.name]\n\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for key, value in input_type_hints.items():\n            input_class_definitions[key] = get_class_definition(value)\n\n        for key, value in output_type_hints.items():\n            output_class_definitions[key] = get_class_definition(value)\n\n        output_type_hint = list(output_type_hints.values())[0]\n        if issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif issubclass(output_type_hint, Union):\n            if issubclass(output_type_hint.__args__[0], Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        output_class_definition = get_class_definition(output_type_hint)\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            type=function_type,\n            source=source\n        )\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n\n        self.persistence.save(self.bit_array, self.indices)\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        self.bit_array = self.persistence.load()\n        if len(self.bit_array) != self.size:\n            logging.warning(f\"Bit array length {len(self.bit_array)} does not match expected length {self.size}. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.load_from_dict(json_dict['distilled_model'])\n        self.current_model_stats = json_dict['current_model_stats']\n        self.last_training_run = json_dict['last_training_run']\n        self.current_training_run = json_dict['current_training_run']\n        self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        self.teacher_models = [config_factory.load_from_dict(teacher_model) for teacher_model in json_dict['teacher_models']]\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate the model name\n        if model.model_name is None:\n            raise ValueError(\"Model name is required\")\n\n        # Validate the system message\n        if system_message is None:\n            raise ValueError(\"System message is required\")\n\n        # Validate the prompt\n        if prompt is None:\n            raise ValueError(\"Prompt is required\")\n\n        # Validate the parameters\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # Set the default parameters\n        default_params = {\n            \"temperature\": 0.5,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"max_new_tokens\": 512,\n        }\n\n        # Merge the default parameters with the provided parameters\n        params = {**default_params, **kwargs}\n\n        # Set the retry count and backoff factor\n        retry_count = 5\n        backoff_factor = 1.5\n\n        # Loop until the response is received or the retry count is exceeded\n        for attempt in range(retry_count):\n            try:\n                # Create the OpenAI client\n                self.client = OpenAI(api_key=self.api_key)\n\n                # Create the completion request\n                completion_request = {\n                    \"model\": model.model_name,\n                    \"messages\": [\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    **params,\n                }\n\n                # Send the completion request\n                response = self.client.chat.completions.create(**completion_request)\n\n                # Check if the response is successful\n                if response.status == 200:\n                    # Process the response to remove any parsing helper tokens\n                    response_text = response.choices[0].message.content\n                    if model.parsing_helper_token is not None:\n                        response_text = response_text.replace(model.parsing_helper_token, \"\")\n                    return response_text\n                else:\n                    # Log the error\n                    logging.error(f\"Error occurred: {response.error}\")\n                    # Raise the error\n                    raise Exception(f\"Error occurred: {response.error}\")\n            except Exception as e:\n                # Log the error\n                logging.error(f"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have zeros on its diagonal\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # get the function's model and examples\n        function_model = self.initialized_functions[func_hash][\"model\"]\n        function_examples = self.initialized_functions[func_hash][\"examples\"]\n\n        # check if the function has a model\n        if function_model == \"\":\n            # if not, get the model for the function\n            function_model = self.function_modeler.get_model(function_description)\n\n        # get the model's config\n        model_config = self.api_provider[function_model.provider].get_model_config(function_model)\n\n        # check if the function has examples\n        if len(function_examples) == 0:\n            # if not, get the examples for the function\n            function_examples = self.function_modeler.get_examples(function_description)\n\n        # check if the function has examples\n        if len(function_examples) == 0:\n            # if not, get the examples for the function\n            function_examples = self.function_modeler.get_examples(function_description)\n\n        # check if the function has examples\n        if len(function_examples) == 0:\n            # if not, get the examples for the function\n            function_examples = self.function_modeler.get_examples(function_description)\n\n        # check if the function has examples\n        if len(function_examples) == 0:\n            # if not, get the examples for the function\n            function_examples = self.function_modeler.get_examples(function_description)\n\n        # check if the function has examples\n        if len(function_examples) == 0:\n            # if not, get the examples for the function\n            function_examples = self.function_modeler.get_examples(function_description)\n\n        # check if the function has examples\n        if len(function_examples) == 0:\n            # if not, get the examples for the function\n            function_examples = self.function_modeler.get_examples(function_description)\n\n        # check if the function has examples\n        if len(function_examples"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n\n    if higham:\n        # Higham & Nick (2002) algorithm\n        n = cov.shape[0]\n        eigval, eigvec = np.linalg.eigh(cov)\n        eigval[eigval < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov = np.dot(eigvec, np.dot(np.diag(eigval), eigvec.T))\n        count = 0\n        while not is_cholesky_dec(cov):\n            eigval[eigval < _CLIPPING_VALUE] *= 1.01\n            cov = np.dot(eigvec, np.dot(np.diag(eigval), eigvec.T))\n            count += 1\n            if count > higham_max_iteration:\n                raise ValueError(\n                    \"Higham & Nick (2002) algorithm did not converge after \"\n                    f\"{higham_max_iteration} iterations\"\n                )\n    else:\n        # Clipping eigenvalues\n        eigval, eigvec = np.linalg.eigh(cov)\n        eigval[eigval < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov = np.dot(eigvec, np.dot(np.diag(eigval), eigvec.T))\n\n    return cov\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.Module):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.Parameter):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ParameterList):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ParameterDict):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleList):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleDict):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.Sequential):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleList):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleDict):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.Sequential):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleList):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleDict):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.Sequential):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleList):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleDict):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.Sequential):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleList):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleDict):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.Sequential):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleList):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.ModuleDict):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.Sequential):\n        return (obj"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if the groups and equations are 1D arrays\n    if not (\n        isinstance(groups, np.ndarray)\n        and isinstance(equations, np.ndarray)\n        and groups.ndim == 2\n        and equations.ndim == 1\n    ):\n        raise EquationToMatrixError(\n            f\"The {names[0]} and {names[1]} parameters must be 1D arrays.\"\n        )\n\n    # Check if the groups and equations have the same length\n    if len(groups) != len(equations):\n        raise EquationToMatrixError(\n            f\"The {names[0]} and {names[1]} parameters must have the same length.\"\n        )\n\n    # Check if the groups and equations are not empty\n    if len(groups) == 0 or len(equations) == 0:\n        raise EquationToMatrixError(\n            f\"The {names[0]} and {names[1]} parameters must not be empty.\"\n        )\n\n    # Check if the groups and equations are not None\n    if groups is None or equations is None:\n        raise EquationToMatrixError(\n            f\"The {names[0]} and {names[1]} parameters must not be None.\"\n        )\n\n    # Check if the groups and equations are not empty\n    if len(groups) == 0 or len(equations) == 0:\n        raise EquationToMatrixError(\n            f\"The {names[0]} and {names[1]} parameters must not be empty.\"\n        )\n\n    # Check if the groups and equations are not None\n    if groups is None or equations is None:\n        raise EquationToMatrixError(\n            f\"The {names[0]} and {names[1]} parameters must not be None.\"\n        )\n\n    # Check if the groups and equations are not empty\n    if len(groups) == 0 or len(equations) == 0:\n        raise EquationToMatrixError(\n            f\"The {names[0]} and {names[1]} parameters must not be empty.\"\n        )\n\n    # Check if the groups and equations are not None\n    if groups is None or equations is None:\n        raise EquationToMatrixError(\n            f\"The {names[0]} and {names[1]} parameters must not be None.\"\n        )\n\n    # Check if the groups and equations are not empty\n    if len(groups)"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Create a new class for the scripted Instances\n    newInstances = type(\"Instances\", (Instances,), {})\n\n    # Add the fields to the new class\n    for field in fields:\n        newInstances.register_field(*field)\n\n    # Add from_instances methods to the scripted Instances class\n    _add_instances_conversion_methods(newInstances)\n\n    # Create a temporary file to store the new class\n    with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as f:\n        # Write the new class to the temporary file\n        f.write(\n            f\"\"\""}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"bbox\" in annotation:\n        # Apply transforms to box\n        bbox = annotation[\"bbox\"]\n        bbox_mode = annotation[\"bbox_mode\"]\n        bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n        bbox = transforms.apply_box([bbox])[0]\n        bbox_mode = BoxMode.XYXY_ABS\n        annotation[\"bbox_mode\"] = bbox_mode\n        annotation[\"bbox\"] = bbox\n\n    if \"segmentation\" in annotation:\n        # Apply transforms to polygons\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygons\n            polygons = annotation[\"segmentation\"]\n            polygons = transforms.apply_polygons(polygons)\n            polygons = [p for p in polygons if len(p) >= 6]\n            # Convert from polygons to rles. and vice versa\n            annotation[\"segmentation\"] = {\n                \"counts\": mask_util.encode(np.array(mask_util.frPyObjects(polygons, *image_size)))[0]\n            }\n        else:\n            # rle\n            annotation[\"segmentation\"][\"counts\"] = transforms.apply_segmentation(\n                [annotation[\"segmentation\"][\"counts\"]]\n            )[0]\n\n    if \"keypoints\" in annotation:\n        # Apply transforms to keypoints\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transforms.apply_keypoints(keypoints)\n        if keypoint_hflip_indices is not None:\n            keypoints = keypoints[:, keypoint_hflip_indices]\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return np.dot(coords, self.rm_coords.T)\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a FlopCountAnalysis object to analyze the model\n    analysis = FlopCountAnalysis(model, inputs)\n\n    # Get the operator names and their corresponding Gflop counts\n    op_names = list(analysis.op_names)\n    op_counts = list(analysis.op_flops)\n\n    # Create a dictionary to store the operator names and their corresponding Gflop counts\n    op_dict = dict(zip(op_names, op_counts))\n\n    # Convert the Gflop counts to Gflops\n    for op_name in op_dict:\n        op_dict[op_name] = op_dict[op_name] / 1e9\n\n    return op_dict\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[0] == 0 or img.shape[1] == 0 or self.angle % 360 == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        # the rotation center may not be at the center of the image\n        image_center = np.array((self.w / 2, self.h / 2))\n        rot_mat = self.rm_image\n        rot_move = (np.array((self.bound_w / 2.0, self.bound_h / 2.0)) - image_center).astype(\n            int\n        )\n        ret = cv2.warpAffine(\n            img,\n            rot_mat,\n            (self.bound_w, self.bound_h),\n            flags=interp,\n            borderMode=cv2.BORDER_CONSTANT,\n            borderValue=0,\n        )\n        ret = cv2.warpAffine(\n            ret,\n            np.array([[1, 0, -rot_move[0]], [0, 1, -rot_move[1]]], dtype=np.float32),\n            (self.bound_w, self.bound_h),\n            flags=interp,\n            borderMode=cv2.BORDER_CONSTANT,\n            borderValue=0,\n        )\n        return ret\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self._instance_mode == ColorMode.IMAGE:\n            self.output.img = self._create_colorized_image(\n                self.output.img,\n                masks=masks,\n                boxes=boxes,\n                labels=labels,\n            )\n        elif self._instance_mode == ColorMode.SEGMENTATION:\n            if masks is None:\n                raise ValueError(\"Prediction do not contain segmentation masks\")\n            self.output.img = self._create_segmentation_image(self.output.img, masks)\n        elif self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.img = self._create_grayscale_image(self.output.img, masks=masks)\n        else:\n            raise ValueError(\"Unsupported instance mode: {}\".format(self._instance_mode))\n\n        if boxes is not None or labels is not None:\n            self.output.img = self._create_text_labels(\n                self.output.img, boxes, labels, font_size=self._default_font_size\n            )\n\n        if keypoints is not None:\n            self.output.img = self._create_keypoint_image(self.output.img, keypoints)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n\n        # buf = io.BytesIO()\n        # canvas.print_rgba(buf)\n        # width, height = self.get_width_height()\n        # s = buf.getvalue()\n\n        buffer = np.frombuffer(s, dtype=\"uint8\")\n\n        img_rgba = buffer.reshape(height, width, 4)\n        rgb, alpha = np.split(img_rgba, [3], axis=2)\n\n        try:\n            import numexpr as ne  # faster than numpy\n\n            visualized_image = ne.evaluate(\"rgb * (alpha / 255.0) + 255 * (1 - (alpha / 255.0))\")\n        except ImportError:\n            alpha = alpha.astype(\"float32\") / 255.0\n            visualized_image = rgb * alpha + (1 - alpha) * 255\n        visualized_image = visualized_image.astype(\"uint8\")\n\n        return visualized_image\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        anns = dic.get(\"annotations\")\n        if anns:\n            if \"segmentation\" in anns[0]:\n                masks = [x[\"segmentation\"] for x in anns]\n                self.draw_binary_mask(masks)\n            if \"keypoints\" in anns[0]:\n                keypoints = [x[\"keypoints\"] for x in anns]\n                self.draw_keypoints(keypoints)\n            if \"bbox\" in anns[0]:\n                bboxes = [x[\"bbox\"] for x in anns]\n                self.draw_box(bboxes)\n            if \"category_id\" in anns[0]:\n                category_ids = [x[\"category_id\"] for x in anns]\n                try:\n                    colors = [\n                        self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n                        for c in category_ids\n                    ]\n                except AttributeError:\n                    colors = None\n                self.draw_box(bboxes, edge_color=colors)\n\n        sem_seg = dic.get(\"sem_seg\")\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg)\n\n        panoptic_seg = dic.get(\"panoptic_seg\")\n        if panoptic_seg is not None:\n            self.draw_panoptic_seg(panoptic_seg)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = color\n\n        if text is not None:\n            text_color = self._change_color_brightness(color, brightness_factor=0.7)\n\n        # draw mask\n        self.output.ax.imshow(binary_mask, cmap=\"gray\", alpha=alpha)\n\n        # draw polygon\n        for polygon in binary_mask.polygons:\n            self.draw_polygon(polygon.reshape(-1, 2), color=edge_color)\n\n        # draw text\n        if text is not None:\n            # get the center of the mask\n            cnt_x, cnt_y = np.median(binary_mask.mask.nonzero(), axis=1)\n            # get the area of the mask\n            area = binary_mask.area\n            # use thinner lines when the mask is small\n            linewidth = self._default_font_size / (\n                6 if area < area_threshold * self.output.scale else 3\n            )\n            font_size = (\n                np.clip((area / area_threshold - 0.02) / 0.08 + 1, 1.2, 2) * self._default_font_size\n            )\n            self.draw_text(text, (cnt_x, cnt_y), color=text_color, linewidth=linewidth, font_size=font_size)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"Expect an Instances object, but got {type(other)}!\"\n\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), f\"image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for name in input._fields:\n        val1 = getattr(input, name)\n        val2 = getattr(other, name)\n        if isinstance(val1, Boxes):\n            assert torch.allclose(val1.tensor, val2.tensor, rtol=rtol), f\"{msg} {name} mismatch: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, ROIMasks):\n            assert torch.allclose(val1.tensor, val2.tensor, rtol=rtol), f\"{msg} {name} mismatch: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, torch.Tensor):\n            assert torch.allclose(val1, val2, rtol=rtol), f\"{msg} {name} mismatch: {val1} vs {val2}\"\n        elif isinstance(val1, np.ndarray):\n            assert np.allclose(val1, val2, rtol=rtol), f\"{msg} {name} mismatch: {val1} vs {val2}\"\n        elif isinstance(val1, list):\n            assert len(val1) == len(val2), f\"{msg} {name} length mismatch: {len(val1)} vs {len(val2)}\"\n            for v1, v2 in zip(val1, val2):\n                assert_instances_allclose(v1, v2, rtol=rtol, msg=msg, size_as_tensor=size_as_tensor)\n        else:\n            raise ValueError(f\"Unknown field type {type(val1)} for field {name}\")"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 2] * self.tensor[:, 3]\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.proposal_generator.name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(cfg.proposal_generator.name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = [x.gt_classes for x in proposals]\n        gt_proposal_deltas = [x.gt_boxes.get_deltas(x.proposal_boxes, self.box2box_transform) for x in proposals]\n\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = _dense_box_regression_loss(\n                proposal_deltas,\n                gt_proposal_deltas,\n                self.box2box_transform.weights,\n                gt_classes,\n                self.smooth_l1_beta,\n            )\n        else:\n            loss_box_reg = _box_regression_loss(\n                proposal_deltas,\n                gt_proposal_deltas,\n                self.box2box_transform.weights,\n                gt_classes,\n                self.box_reg_loss_type,\n            )\n\n        loss_cls = F.cross_entropy(scores, gt_classes, reduction=\"none\")\n\n        if self.loss_weight is not None:\n            if isinstance(self.loss_weight, dict):\n                loss_cls *= self.loss_weight.get(\"loss_cls\", 1.0)\n                loss_box_reg *= self.loss_weight.get(\"loss_box_reg\", 1.0)\n            else:\n                loss_cls *= self.loss_weight\n                loss_box_reg *= self.loss_weight\n\n        losses = {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n        return losses\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER_HEADS.NAME\n    return TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(boxes, torch.Tensor), type(boxes)\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            anno_type = self.anno_type_map.keys()\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        img, k = resize_image(image, 512)\n        img = img.astype(np.float32) / 255.0\n        img = img.transpose(2, 0, 1)\n        img = np.expand_dims(img, 0)\n        img = np.ascontiguousarray(img)\n\n        with we.no_sync():\n            with we.device_ctx(we.device_id):\n                output = self.general_ins.run(img)\n\n        output = output[0]\n        output = resize_image_ori(image.shape[0], image.shape[1], output, k)\n        output = output.transpose(1, 2, 0)\n        output = output.astype(np.float32)\n\n        if len(anno_type) == 1:\n            return output[self.anno_type_map[anno_type[0]]['OUTPUT_KEYS'][0]]\n        else:\n            return {\n                tp: output[self.anno_type_map[tp]['OUTPUT_KEYS'][0]]\n                for tp in anno_type\n            }"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        # clip the boxes that are almost horizontal\n        # to avoid the complexities and ambiguities involved in clipping highly rotated boxes\n        # we only clip the boxes that are almost horizontal\n        # to maintain backward compatibility\n        # we use the following formula to determine if a box is almost horizontal:\n        # abs(angle) < clip_angle_threshold\n        # where angle is the angle of the box in degrees\n        # and clip_angle_threshold is a user-specified threshold\n        # we use the following formula to determine if a box is almost horizontal:\n        # abs(angle) < clip_angle_threshold\n        # where angle is the angle of the box in degrees\n        # and clip_angle_threshold is a user-specified threshold\n        # we use the following formula to determine if a box is almost horizontal:\n        # abs(angle) < clip_angle_threshold\n        # where angle is the angle of the box in degrees\n        # and clip_angle_threshold is a user-specified threshold\n        # we use the following formula to determine if a box is almost horizontal:\n        # abs(angle) < clip_angle_threshold\n        # where angle is the angle of the box in degrees\n        # and clip_angle_threshold is a user-specified threshold\n        # we use the following formula to determine if a box is almost horizontal:\n        # abs(angle) < clip_angle_threshold\n        # where angle is the angle of the box in degrees\n        # and clip_angle_threshold is a user-specified threshold\n        # we use the following formula to determine if a box is almost horizontal:\n        # abs(angle) < clip_angle_threshold\n        # where angle is the angle of the box in degrees\n        # and clip_angle_threshold is a user-specified threshold\n        # we use the following formula to determine if a box is almost horizontal:\n        # abs(angle) < clip_angle_threshold\n        # where angle is the angle of the box in degrees\n        # and clip_angle_threshold is a user-specified threshold\n        # we use the following formula to determine if a box is almost horizontal:\n        # abs(angle) < clip_angle_threshold\n        # where angle is the angle of the box in degrees\n        # and clip_angle_threshold is a user-specified threshold\n        # we use the following formula to determine if a box is almost horizontal:\n        #"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError(f'Loss {cfg[\"type\"]} is not implemented')\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMSEG_SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None:\n        warnings.warn('train_cfg is deprecated, please specify '\n                      'train cfg in model', UserWarning)\n    if test_cfg is not None:\n        warnings.warn('test_cfg is deprecated, please specify '\n                      'test cfg in model', UserWarning)\n    if train_cfg is not None:\n        assert 'train_cfg' not in cfg\n        cfg['train_cfg'] = train_cfg\n    if test_cfg is not None:\n        assert 'test_cfg' not in cfg\n        cfg['test_cfg'] = test_cfg\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg)\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # Initialize the evaluation results\n    eval_results = {}\n    for iou_thr in metric:\n        eval_results[f'AP_{iou_thr}'] = 0\n        eval_results[f'AR_{iou_thr}'] = 0\n\n    # Convert the annotations to the required format\n    gt_annos = [\n        {\n            'name': label2cat[anno['name']],\n            'bbox': anno['bbox'],\n            'score': 1.0\n        } for anno in gt_annos\n    ]\n    dt_annos = [\n        {\n            'name': label2cat[anno['name']],\n            'bbox': anno['bbox'],\n            'score': anno['score']\n        } for anno in dt_annos\n    ]\n\n    # Convert the annotations to the required format\n    if box_type_3d is not None:\n        gt_annos = [\n            {\n                'name': anno['name'],\n                'bbox': box_type_3d(anno['bbox'], box_mode_3d).convert(\n                    'xyxy').tensor.tolist(),\n                'score': 1.0\n            } for anno in gt_annos\n        ]\n        dt_annos = [\n            {\n                'name': anno['name'],\n                'bbox': box_type_3d(anno['bbox'], box_mode_3d).convert(\n                    'xyxy').tensor.tolist(),\n                'score': anno['score']\n            } for anno in dt_annos\n        ]\n\n    # Evaluate the detection results\n    recall, precision, ap = eval_map_recall(\n        {\n            '0': dt_annos\n        }, {\n            '0': gt_annos\n        }, metric)\n\n    # Calculate the mean Average Precision (mAP) and mean Average Recall (mAR)\n    for iou_idx, iou_thr in enumerate(metric):\n        eval_results[f'AP_{iou_thr}'] = ap[iou_idx]['0'].mean()\n        eval_results[f'AR_{iou_thr}'] = recall[iou_idx]['0'].mean()\n\n    # Print the evaluation results\n    if logger is not None:\n        print_log(f'Results of {len(gt_annos)} samples', logger)\n        table_data = [\n            ['Class', 'AP', 'AR'] + [f'AP"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        from mmdet3d.core.bbox.structures.lidar_bbox import LiDARInstance3DBoxes\n        box_class = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        from mmdet3d.core.bbox.structures.depth_bbox import DepthInstance3DBoxes\n        box_class = DepthInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        from mmdet3d.core.bbox.structures.depth_bbox import DepthInstance3DBoxes\n        box_class = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'Unsupported box type {box_type}')\n\n    return box_class, mode\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    for message in messages:\n      if not isinstance(message, (dict, Message)):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [_format_message(message) for message in messages],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    return self._request_stream(\n      'POST',\n      '/v1/generate',\n      json=data,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide a path or modelfile')\n\n    if path:\n      path = Path(path)\n      if not path.is_file():\n        raise RequestError('path must be a file')\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': b64encode(modelfile).decode('utf-8'),\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n    if not path.exists():\n      raise FileNotFoundError(f'file not found: {path}')\n\n    checksum = sha256(path.read_bytes()).hexdigest()\n    try:\n      self._client.head(f'/api/blobs/{checksum}')\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code == 404:\n        with path.open('rb') as f:\n          self._client.post(f'/api/blobs/{checksum}', data=f)\n      else:\n        raise\n\n    return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined user code and test code\n        with tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\") as temp_file:\n            temp_file.write(user_code + test_code)\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            pyright_process = subprocess.run(\n                [\"pyright\", temp_file.name],\n                capture_output=True,\n                text=True,\n                check=False,\n            )\n\n            # Parse the output of Pyright to identify lines with expected type errors\n            error_messages = []\n            for line in pyright_process.stderr.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_number = int(match.group(1))\n                    message = match.group(2)\n                    if cls.EXPECT_ERROR_COMMENT in message:\n                        error_messages.append(message)\n\n            # Determine if the type check passed or failed\n            passed = len(error_messages) == 0\n\n            # Create a debug dictionary to store the error messages and line numbers\n            debug_info = {\n                \"error_messages\": error_messages,\n                \"line_numbers\": [\n                    int(re.search(r\"line (\\d+)\", message).group(1))\n                    for message in error_messages\n                ],\n            }\n\n            # Create a TypeCheckResult object to return the result of the type check\n            return TypeCheckResult(\n                message=\"Type check passed\" if passed else \"Type check failed\",\n                passed=passed,\n                debug_info=debug_info,\n            )\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, get_compiler_fn(title=\"Forward Graph:\"), get_compiler_fn(title=\"Backward Graph:\"))\n    else:\n        return aot_function(fn, get_compiler_fn(title=\"Forward Graph:\"), get_compiler_fn(title=\"Backward Graph:\"))"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary file\n    summary_df = load_summary_file(trial_path)\n\n    # Load the configuration file\n    with open(os.path.join(trial_path, \"config.yaml\"), \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    # Convert the summary dataframe to a YAML dictionary\n    yaml_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    # Save the YAML dictionary to a file if an output path is provided\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.dump(yaml_dict, f)\n\n    return yaml_dict\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is None:\n        ts_compiler = lambda m: m\n\n    def wrapper(*args, **kwargs):\n        if not hasattr(wrapper, 'cache'):\n            wrapper.cache = {}\n        if not hasattr(wrapper, 'lock'):\n            wrapper.lock = threading.Lock()\n        with wrapper.lock:\n            if func not in wrapper.cache:\n                wrapper.cache[func] = trace_with_kwargs(\n                    func, *args, **kwargs)[0]\n            traced_module = wrapper.cache[func]\n        return ts_compiler(traced_module)(*args, **kwargs)\n\n    return wrapper\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        summary_path = os.path.join(trial_path, 'summary.csv')\n        if not os.path.exists(summary_path):\n            raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n        trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            config_dict = yaml.safe_load(f)\n        yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n        return cls(yaml_dict, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module with its parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(**params)\n        results.append(result)\n\n    # Measure the execution times of each module\n    times = measure_speed(results)\n\n    # Evaluate each module's result\n    metrics = evaluate_retrieval(results, previous_result)\n\n    # Combine the results, times, and metrics into a single DataFrame\n    combined_results = pd.concat(results, axis=1)\n    combined_results[\"time\"] = times\n    combined_results[\"metrics\"] = metrics\n\n    # Save the combined results to disk\n    combined_results.to_csv(os.path.join(node_line_dir, \"results.csv\"))\n\n    # Apply the strategies to select the best module\n    best_result = select_best_average(combined_results, strategies)\n\n    # Save the best result to disk\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"))\n\n    # Combine the previous result columns with the selected best result columns\n    best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Save the combined result to disk\n    best_result.to_csv(os.path.join(node_line_dir, \"combined_result.csv\"))\n\n    # Return the combined result\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Get the support modules\n    support_modules = get_support_modules()\n\n    # Get the module names\n    module_names = [module.__name__ for module in modules]\n\n    # Get the module parameters\n    module_params = [module_params[i] for i, module in enumerate(modules)]\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    # Get the module combinations\n    module_combinations = make_combinations(module_names, module_params)\n\n    #"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create the output directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Get the support modules\n    support_modules = get_support_modules()\n\n    # Get the generator module\n    generator_module = support_modules['generator']\n\n    # Get the generator module parameters\n    generator_module_params = support_modules['generator_params']\n\n    # Get the generator module name\n    generator_module_name = support_modules['generator_name']\n\n    # Get the generator module version\n    generator_module_version = support_modules['generator_version']\n\n    # Get the generator module description\n    generator_module_description = support_modules['generator_description']\n\n    # Get the generator module license\n    generator_module_license = support_modules['generator_license']\n\n    # Get the generator module author\n    generator_module_author = support_modules['generator_author']\n\n    # Get the generator module email\n    generator_module_email = support_modules['generator_email']\n\n    # Get the generator module url\n    generator_module_url = support_modules['generator_url']\n\n    # Get the generator module citation\n    generator_module_citation = support_modules['generator_citation']\n\n    # Get the generator module reference\n    generator_module_reference = support_modules['generator_reference']\n\n    # Get the generator module reference url\n    generator_module_reference_url = support_modules['generator_reference_url']\n\n    # Get the generator module reference doi\n    generator_module_reference_doi = support_modules['generator_reference_doi']\n\n    # Get the generator module reference year\n    generator_module_reference_year = support_modules['generator_reference_year']\n\n    # Get the generator module reference journal\n    generator_module_reference_journal = support_modules['generator_reference_journal']\n\n    # Get the generator module reference volume\n    generator_module_reference_volume = support_modules['generator_reference_volume']\n\n    # Get the generator module reference number\n    generator_module_reference_number = support_modules['generator_reference_number']\n\n    # Get the generator module reference pages\n    generator_module_reference_pages = support_modules['generator_reference_pages']\n\n    # Get the generator module reference arxiv\n    generator_module_reference_arxiv = support_modules"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n    return max(\n        list(map(lambda x: calculate_cosine_similarity(x, pred, embedding_model), generation_gt)))\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n    if gfpgan_face_restorer is None:\n        try:\n            gfpgan_face_restorer = FaceRestorerGFPGAN(\n                model_path=os.path.join(shared.data_path, \"models\", \"GFPGAN\"),\n            )\n        except errors.FaceRestorationError as e:\n            logger.warning(f\"Failed to load GFPGAN face restorer: {e}\")\n            return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except errors.FaceRestorationError as e:\n        logger.error(f\"Failed to load CodeFormer: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        face_restoration_utils.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except errors.FaceRestorationError as e:\n        logger.error(e)\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  v_quat = jnp.array([0.0, v[0], v[1], v[2]])\n\n  # Apply the rotation\n  rotated_v_quat = multiply(multiply(q, v_quat), conjugate(q))\n\n  # Convert the rotated quaternion back to a vector\n  rotated_v = rotated_v_quat[1:]\n\n  return rotated_v\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis, angle = jnp.split(axis_angle, 2, axis=-1)\n  angle = jnp.maximum(angle, eps * jnp.ones_like(angle))\n  half_angle = 0.5 * angle\n  sin_half_angle = jnp.sin(half_angle)\n  cos_half_angle = jnp.cos(half_angle)\n  return jnp.concatenate(\n      [axis * sin_half_angle, cos_half_angle], axis=-1\n  )\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    num_calls = k\n    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    while idx not in topk_words:\n        logit_bias[idx] *= 2\n        topk_words = model.topk(prefix, logit_bias)\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = 0\n    mid = (high + low) / 2\n    while high >= low + 1e-8:\n        logit_bias[idx] = mid\n        topk_words = model.topk(prefix, logit_bias)\n        if idx in topk_words:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data is a valid 3D tensor.\n  if len(data.shape) != 4:\n    raise ValueError('Input data must be a 3D tensor with shape [D, H, W, C].')\n\n  # Check if the input locations are a valid 3D tensor.\n  if len(locations.shape) < 3:\n    raise ValueError('Input locations must be a 3D tensor with shape [D, ..., 3].')\n\n  # Check if the edge_behavior is valid.\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Invalid edge_behavior. Must be either \"CONSTANT_OUTSIDE\" or \"CLAMP\".')\n\n  # Check if the method is valid.\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Invalid method. Must be either \"TRILINEAR\" or \"NEAREST\".')\n\n  # Check if the coordinate_order is valid.\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Invalid coordinate_order. Must be either \"xyz\" or \"zyx\".')\n\n  # Check if the half_pixel_center is valid.\n  if not isinstance(half_pixel_center, bool):\n    raise ValueError('Invalid half_pixel_center. Must be a boolean value.')\n\n  # Check if the constant_values is valid.\n  if not isinstance(constant_values, (int, float)):\n    raise ValueError('Invalid constant_values. Must be a numeric value.')\n\n  # Check if the data is on the CPU or GPU.\n  if jax.device_count() > 1:\n    raise ValueError('Input data must be on the CPU or GPU.')\n\n  # Check if the locations are on the CPU or GPU.\n  if jax.device_count() > 1:\n    raise ValueError('Input locations must be on the CPU or GPU.')\n\n  # Check if the data is on the CPU or GPU.\n  if jax.device_count() > 1:\n    raise ValueError('Input data must be on the CPU or GPU.')\n\n  # Check if the locations are on the CPU or GPU.\n  if jax.device_count() > 1:\n    raise ValueError('Input locations must be on the CPU or GPU.')\n\n  # Check if the data is on the CPU or GPU.\n  if"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, max_val))\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: x_dot * jnp.exp(x),\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / x, (tiny_val, None))(\n      x\n  )\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (0, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p == 1:\n    return jnp.inf\n  elif p == 2:\n    return jnp.inf\n  elif p == 3:\n    return jnp.inf\n  elif p == 4:\n    return jnp.inf\n  elif p == 5:\n    return jnp.inf\n  elif p == 6:\n    return jnp.inf\n  elif p == 7:\n    return jnp.inf\n  elif p == 8:\n    return jnp.inf\n  elif p == 9:\n    return jnp.inf\n  elif p == 10:\n    return jnp.inf\n  elif p == 11:\n    return jnp.inf\n  elif p == 12:\n    return jnp.inf\n  elif p == 13:\n    return jnp.inf\n  elif p == 14:\n    return jnp.inf\n  elif p == 15:\n    return jnp.inf\n  elif p == 16:\n    return jnp.inf\n  elif p == 17:\n    return jnp.inf\n  elif p == 18:\n    return jnp.inf\n  elif p == 19:\n    return jnp.inf\n  elif p == 20:\n    return jnp.inf\n  elif p == 21:\n    return jnp.inf\n  elif p == 22:\n    return jnp.inf\n  elif p == 23:\n    return jnp.inf\n  elif p == 24:\n    return jnp.inf\n  elif p == 25:\n    return jnp.inf\n  elif p == 26:\n    return jnp.inf\n  elif p == 27:\n    return jnp.inf\n  elif p == 28:\n    return jnp.inf\n  elif p == 29:\n    return jnp.inf\n  elif p == 30:\n    return jnp.inf\n  elif p == 31:\n    return jnp.inf\n  elif p == 32:\n    return jnp.inf\n  elif p == 33:\n    return jnp.inf\n  elif p == 34:\n    return jnp.inf\n  elif p == 35:\n    return jnp.inf\n  elif p == 36:"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [[0, 0, 1], [0, 1, 0], [1, 0, 0], [-1, -1, -1]]\n    ) / np.sqrt(3)\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array(\n        [\n            [0, 0, 1],\n            [0, 0, -1],\n            [0, 1, 0],\n            [0, -1, 0],\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0.5, 0.8660254037844386, 0.0],\n            [0.5, -0.8660254037844386, 0.0],\n            [-0.5, 0.8660254037844386, 0.0],\n            [-0.5, -0.8660254037844386, 0.0],\n            [0.8660254037844386, 0.5, 0.0],\n            [-0.8660254037844386, 0.5, 0.0],\n            [0.8660254037844386, -0.5, 0.0],\n            [-0.8660254037844386, -0.5, 0.0],\n            [0.0, 0.5, 0.8660254037844386],\n            [0.0, -0.5, 0.866025403784438"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    return x\n\n  if p == 0:\n    return jnp.where(x == 0, 0, jnp.sign(x) * jnp.log(jnp.abs(x)))\n\n  if p == -jnp.inf:\n    return jnp.where(x == 0, 0, jnp.sign(x) * jnp.exp(x))\n\n  if p == jnp.inf:\n    return jnp.where(x == 0, 0, jnp.sign(x) * jnp.exp(-x))\n\n  return jnp.sign(x) * jnp.abs(x)**p\n\n  if postmult is not None:\n    return x * postmult\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if postmult is not None:\n    y = y / postmult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * (ys ** (1 / p_safe) - 1)\n      ),\n  )\n  if premult is not None:\n    x = x / premult\n  return x\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    step = step - lr_delay_steps\n    if step < 0:\n      return lr_init\n\n  return log_lerp(step / max_steps, lr_init, lr_final)\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=jnp.array([-1.0, -1.0, -1.0]),\n      origin_hi=jnp.array([1.0, 1.0, 1.0]),\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Apply inverse intrinsics.\n  camera_points = xnp.matmul(pixtocams, points)\n\n  # Apply camera extrinsics.\n  camera_points = xnp.matmul(camtoworlds, camera_points)\n\n  # Apply distortion correction.\n  if distortion_params is not None:\n    camera_points = _radial_and_tangential_undistort(\n        camera_points[Ellipsis, 0],\n        camera_points[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n\n  # Apply fisheye projection.\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_points[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    camera_points = xnp.stack(\n        [\n            camera_points[Ellipsis, 0] * sin_theta_over_theta,\n            camera_points[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Apply panoramic projection.\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = camera_points[Ellipsis, 0]\n    phi = camera_points[Ellipsis, 1]\n    camera_points = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi),\n            -xnp.sin(phi) * xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Flip from OpenCV to OpenGL coordinate system.\n  camera_points = xnp.matmul(\n      camera_points, xnp.diag(xnp.array([1.0, -1.0, -1.0]))\n  )\n\n  # Extract 2D image plane (x, y) coordinates.\n  imageplane = camera_points[Ellipsis, :2]\n\n  # Extract depth values.\n  depth = camera_points[Ellipsis, 2]\n\n  return imageplane, depth"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = jnp.split(screw_axis, 2)\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n  theta_squared = jnp.sum(w**2, axis=-1)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = rp_to_se3(jnp.eye(3), v)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  W = skew(w_safe)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n  p = (\n      (jnp.eye(3) - R) @ spin_math.matmul(W, v)\n      + theta_safe * v\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, spin_math.matmul(W, v))\n  )\n  X = rp_to_se3(R, p)\n\n  return jnp.where(theta_squared > eps**2, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2))\n  if theta < eps:\n    return jnp.eye(3)\n  else:\n    axis = axis_angle / theta\n    return jnp.eye(3) + jnp.sin(theta) * skew(axis) + (1 - jnp.cos(theta)) * skew(\n        axis\n    ) @ skew(axis)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Check if the input camera projection type is fisheye.\n  fisheye = camtype == ProjectionType.FISHEYE\n\n  # Check if the input camera projection type is panoramic.\n  panoramic = camtype == ProjectionType.PANORAMIC\n\n  # Check if the input camera projection type is perspective.\n  perspective = camtype == ProjectionType.PERSPECTIVE\n\n  # Check if the input camera projection type is 360 pano.\n  pano = camtype == ProjectionType.PANORAMIC\n\n  # Check if the input camera projection type is not perspective.\n  not_perspective = not perspective\n\n  # Check if the input camera projection type is not fisheye.\n  not_fisheye = not fisheye\n\n  # Check if the input camera projection type is not panoramic.\n  not_panoramic = not panoramic\n\n  # Check if the input camera projection type is not pano.\n  not_pano = not pano\n\n  # Check if the input camera projection type is not 360 pano.\n  not_360_pano = not pano\n\n  # Check if the input camera projection type is not fisheye or panoramic.\n  not_fisheye_or_panoramic = not fisheye or not panoramic\n\n  # Check if the input camera projection type is not fisheye or panoramic.\n  not_fisheye_or_panoramic = not fisheye or not panoramic\n\n  # Check if the input camera projection type is not fisheye or panoramic or pano.\n  not_fisheye_or_panoramic_or_pano = not fisheye or not panoramic or not pano\n\n  # Check if the input camera projection type is not fisheye or panoramic or pano.\n  not_fisheye_or_panoramic_or_pano = not fisheye or not panoramic or not pano\n\n  # Check if the input camera projection type is not fisheye or panoramic or pano or perspective.\n  not_fisheye_or_panoramic_or_pano_or_perspective = (\n      not fisheye or"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * jnp.linalg.norm(dirs, axis=-1) * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Sample from the CDF.\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Center the samples in each interval.\n      t_new = (t[..., 1:] + t[..., :-1]) / 2\n    else:\n      # Spread the samples across the entire PDF.\n      t_new = jnp.linspace(t[..., 0], t[..., -1], num_samples + 1)\n    t_new = t_new[..., 1:]\n  else:\n    # Random sampling.\n    u = jax.random.uniform(rng, (t.shape[0], num_samples), minval=eps, maxval=1 - eps)\n    t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n    if single_jitter:\n      # Jitter all samples by the same amount.\n      jitter = jax.random.uniform(rng, (t.shape[0], 1), minval=eps, maxval=1 - eps)\n      t_new += jitter\n  return t_new\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Sample points from the step function.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_midpoints = jnp.concatenate(\n      [\n          jnp.broadcast_to(\n              jnp.maximum(domain[0], t_samples[..., 0]),\n              t_midpoints.shape[:-1] + (1,),\n          ),\n          t_midpoints,\n          jnp.broadcast_to(\n              jnp.minimum(domain[1], t_samples[..., -1]),\n              t_midpoints.shape[:-1] + (1,),\n          ),\n      ],\n      axis=-1,\n  )\n\n  return t_midpoints\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n  # Compute the integrated weights.\n  cw = integrate_weights(w)\n  # Interpolate into the integrated weights to find the corresponding x-values.\n  t_ps = math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n  return t_ps\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Convert the weights to a PDF.\n  p = weight_to_pdf(t, w)\n  # Blur the PDF.\n  p_blurred = linspline.blur(p, blur_halfwidth)\n  # Resample the PDF to match the new time points.\n  w_resampled = resample(tq, t, p_blurred, use_avg=True)\n  return w_resampled\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))\n\n"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  if use_avg:\n    return linspline.interp(t, tp, vp, utils.device_is_tpu())\n  else:\n    return linspline.interp_sum(t, tp, vp, utils.device_is_tpu())\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance by powers of 2 within the specified range of degrees.\n  scaled_mean = mean * 2 ** jnp.arange(min_deg, max_deg)\n  scaled_var = var * 2 ** jnp.arange(min_deg, max_deg)\n\n  # Concatenate the scaled mean and variance.\n  concat_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply sinusoidal encoding to the concatenated mean and variance.\n  sin_enc = jnp.sin(concat_mean_var)\n\n  return sin_enc\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding (DE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting DE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply attenuation function using the von Mises-Fisher distribution\n    # concentration parameter, kappa.\n    sigma = 0.5 * ml_array[1, :] * (ml_array[1, :] + 1)\n    ide = sph_harms * jnp.exp(-sigma * kappa_inv)\n\n    # Split"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    block_index = 0\n    block_type = None\n    block_start_index = 0\n    block_list = []\n    block_header_index = None\n    block_level = 0\n    block_list_index = 0\n    block_list_type = None\n    block_list_start_index = 0\n    block_list_level = 0\n    block_list_items = []\n    block_list_item_index = 0\n    block_list_item_type = None\n    block_list_item_start_index = 0\n    block_list_item_level = 0\n    block_list_item_text = \"\"\n    block_list_item_text_index = 0\n    block_list_item_text_type = None\n    block_list_item_text_start_index = 0\n    block_list_item_text_level = 0\n    block_list_item_text_text = \"\"\n    block_list_item_text_text_index = 0\n    block_list_item_text_text_type = None\n    block_list_item_text_text_start_index = 0\n    block_list_item_text_text_level = 0\n    block_list_item_text_text_text = \"\"\n    block_list_item_text_text_text_index = 0\n    block_list_item_text_text_text_type = None\n    block_list_item_text_text_text_start_index = 0\n    block_list_item_text_text_text_level = 0\n    block_list_item_text_text_text_text = \"\"\n    block_list_item_text_text_text_text_index = 0\n    block_list_item_text_text_text_text_type = None\n    block_list_item_text_text_text_text_start_index = 0\n    block_list_item_text_text_text_text_level = 0\n    block_list_item_text_text_text_text_text = \"\"\n    block_list_item_text_text_text_text_text_index = 0\n    block_list_item_text_text_text"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    texts = org_texts.strip()\n\n    # remove new line\n    texts = texts.replace(\"\\n\", \" \")\n\n    # remove space before punctuation\n    texts = space_rule.sub(r\"\\1\", texts)\n\n    # remove quotation marks\n    texts = quotation_pattern.sub(\"\", texts)\n\n    # remove brackets\n    texts = bracket_rule.sub(r\"\\1\", texts)\n\n    # tokenize\n    texts = nltk_tokenzier.tokenize(texts)\n\n    # apply rules\n    for rule, replaced in rules:\n        texts = [rule.sub(replaced, text) for text in texts]\n\n    return texts"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        token = self._check_token_arg(token)\n        if isinstance(token, list):\n            return self.phrase_positions(token, key=key)\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                doc_ids, term_posns = self.posns.positions(term_id)\n                return term_posns\n            else:\n                doc_ids, term_posns = self.posns.positions(term_id, doc_ids=key)\n                return term_posns\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif spec.startswith(\"<\"):\n        return int(float(spec[1:]) / 100 * num_clauses)\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # Get term freqs per token\n            tfs = np.asarray([self.termfreqs(token) for token in tokens])\n            # Get positions per token\n            posns = np.asarray([self.positions(token) for token in tokens])\n            # Get phrase freqs\n            phrase_freqs = compute_phrase_freqs(tfs, posns)\n            return phrase_freqs\n        else:\n            # Get term freqs per token\n            tfs = np.asarray([self.termfreqs(token) for token in tokens])\n            # Get positions per token\n            posns = np.asarray([self.positions(token) for token in tokens])\n            # Get phrase freqs\n            phrase_freqs = compute_phrase_freqs(tfs, posns, slop=slop)\n            return phrase_freqs\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if truncate:\n            array = array[:truncate]\n\n        if len(array) == 0:\n            return cls([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        if len(array) < batch_size:\n            return cls(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        # If we have a large array, process it in batches\n        # to avoid loading everything into memory at once\n        # (and to avoid memory errors)\n        batches = [array[i:i + batch_size] for i in range(0, len(array), batch_size)]\n        term_mats = []\n        posns = []\n        term_dicts = []\n        avg_doc_lengths = []\n        doc_lens = []\n        for batch in batches:\n            batch_sa = cls(batch, tokenizer=tokenizer, avoid_copies=avoid_copies)\n            term_mats.append(batch_sa.term_mat)\n            posns.append(batch_sa.posns)\n            term_dicts.append(batch_sa.term_dict)\n            avg_doc_lengths.append(batch_sa.avg_doc_length)\n            doc_lens.append(batch_sa.doc_lens)\n\n        # Concatenate the batches\n        term_mat = np.concatenate(term_mats)\n        posns = np.concatenate(posns)\n        term_dict = term_dicts[0]\n        avg_doc_length = np.mean(avg_doc_lengths)\n        doc_lens = np.concatenate(doc_lens)\n\n        return cls(term_mat, posns, term_dict, avg_doc_length, doc_lens, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server = Server(\n            host=self.config['serverHost'],\n            port=self.config['serverPort'],\n            logger=self.logger,\n            on_connect=self.on_connect,\n            on_disconnect=self.on_disconnect,\n            on_message=self.on_message,\n            on_error=self.on_error,\n        )\n\n        self.server.start()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.uint64(arr)\n    arr = np.uint64(arr - ((arr >> _1) & s55))\n    arr = np.uint64((arr & s33) + ((arr >> _2) & s33))\n    arr = np.uint64((arr + (arr >> _4)) & s0F)\n    arr = np.uint64(arr + (arr >> _8))\n    arr = np.uint64(arr + (arr >> _16))\n    arr = np.uint64(arr + (arr >> _32))\n    arr = np.uint64(arr & all_but_one_bit)\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            self.lock.acquire()\n            try:\n                connection = self.connections.get(process.id, None)\n                if connection is None:\n                    connection = Connection(\n                        process_id=process.id,\n                        strategy=self.server.strategy,\n                        strategy_config=self.server.strategy_config,\n                        logger=self.logger\n                    )\n                    self.connections[process.id] = connection\n                message.data = connection.c2s(message.data)\n            finally:\n                self.lock.release()\n        elif isinstance(message, RecvMessage):\n            self.lock.acquire()\n            try:\n                connection = self.connections.get(process.id, None)\n                if connection is None:\n                    connection = Connection(\n                        process_id=process.id,\n                        strategy=self.server.strategy,\n                        strategy_config=self.server.strategy_config,\n                        logger=self.logger\n                    )\n                    self.connections[process.id] = connection\n                message.data = connection.s2c(message.data)\n            finally:\n                self.lock.release()\n        elif isinstance(message, CloseMessage):\n            self.lock.acquire()\n            try:\n                connection = self.connections.get(process.id, None)\n                if connection is not None:\n                    connection.close()\n                    if self.config['autoCloseConnections']:\n                        del self.connections[process.id]\n            finally:\n                self.lock.release()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections.values():\n            connection.close()\n        self.server.stop()\n"}
