{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running: {cmd}\")\n    exit_status = os.system(cmd)\n    return exit_status\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(\"Running command:\", cmd)\n    exit_status = os.system(cmd)\n    return exit_status\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"\\nRunning command: {cmd}\\n\")\n    return os.system(cmd)\n\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = z / scale\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |z| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |z| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = z / scale\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |z| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 + jnp.sqrt(z_mag_sq)) / z_mag_sq\n  x = scale * z\n  return x\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (jnp.sqrt(z_mag_sq) + 1) / (2 * jnp.sqrt(z_mag_sq) - 1)\n  x = scale * z\n  return x\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = z / scale\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) + 1) / z_mag_sq\n  x = scale * z\n  return x\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) + 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = z / scale\n  return x\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 - jnp.sqrt(z_mag_sq)) / z_mag_sq\n  x = scale * z\n  return x\n\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = z / scale\n  return x\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.sum(z**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(z_mag_sq) + 1) / z_mag_sq\n  x = scale * z\n  return x\n\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 + 2 * jnp.sqrt(z_mag_sq)) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (jnp.sqrt(z_mag_sq) + 1) / (2 * jnp.sqrt(z_mag_sq) - 1)\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                f\"CREATE TABLE IF NOT EXISTS {func_name} (key TEXT PRIMARY KEY, value TEXT)\"\n            )\n            key = hashlib.sha256(\n                json.dumps(args, sort_keys=True).encode()\n                + json.dumps(kwargs, sort_keys=True).encode()\n            ).hexdigest()\n            c.execute(f\"SELECT value FROM {func_name} WHERE key = ?\", (key,))\n            result = c.fetchone()\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\n                    f\"INSERT INTO {func_name} (key, value) VALUES (?, ?)\", (key, result)\n                )\n                conn.commit()\n            else:\n                result = json.loads(result[0])\n            conn.close()\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                f\"\"\"\n                CREATE TABLE IF NOT EXISTS {func_name} (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    args TEXT,\n                    kwargs TEXT,\n                    result TEXT\n                )\n            \"\"\"\n            )\n            conn.commit()\n            c.execute(\n                f\"\"\"\n                SELECT * FROM {func_name} WHERE args = ? AND kwargs = ?\n            \"\"\",\n                (\n                    json.dumps(args),\n                    json.dumps(kwargs),\n                ),\n            )\n            result = c.fetchone()\n            if result:\n                return json.loads(result[3])\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\n                    f\"\"\"\n                    INSERT INTO {func_name} (args, kwargs, result) VALUES (?, ?, ?)\n                \"\"\",\n                    (\n                        json.dumps(args),\n                        json.dumps(kwargs),\n                        json.dumps(result),\n                    ),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create the table if it doesn't exist\n            c.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS cache (\n                    key TEXT PRIMARY KEY,\n                    value TEXT\n                )\n            \"\"\"\n            )\n\n            # Generate a unique key for the function call\n            key = hashlib.md5(\n                (func_name + json.dumps(args) + json.dumps(kwargs)).encode()\n            ).hexdigest()\n\n            # Check if the result is already stored in the database\n            c.execute(\"SELECT value FROM cache WHERE key = ?\", (key,))\n            result = c.fetchone()\n\n            # If the result is not stored, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache VALUES (?, ?)\", (key, json.dumps(result)))\n                conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            # Return the result\n            return json.loads(result)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Generate a unique key for the function call\n            key = hashlib.sha256(\n                (func_name + str(args) + str(kwargs)).encode(\"utf-8\")\n            ).hexdigest()\n\n            # Check if the result is already stored in the database\n            cursor.execute(\"SELECT result FROM cache WHERE key = ?\", (key,))\n            result = cursor.fetchone()\n\n            if result:\n                # If the result is found, return it\n                return json.loads(result[0])\n            else:\n                # If the result is not found, compute it and store it in the database\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"INSERT INTO cache (key, result) VALUES (?, ?)\",\n                    (key, json.dumps(result)),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table to store the function outputs\n            c.execute(\n                f\"\"\"CREATE TABLE IF NOT EXISTS {func_name} (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    input TEXT,\n                    output TEXT\n                )\"\"\"\n            )\n\n            # Hash the function arguments to use as a key\n            key = hashlib.sha256(\n                json.dumps(args, sort_keys=True).encode(\"utf-8\")\n                + json.dumps(kwargs, sort_keys=True).encode(\"utf-8\")\n            ).hexdigest()\n\n            # Check if the function output is already stored in the database\n            c.execute(f\"SELECT output FROM {func_name} WHERE input = ?\", (key,))\n            result = c.fetchone()\n            if result:\n                # If the output is stored, return it\n                return json.loads(result[0])\n            else:\n                # If the output is not stored, compute it and store it in the database\n                result = func(*args, **kwargs)\n                c.execute(\n                    f\"INSERT INTO {func_name} (input, output) VALUES (?, ?)\",\n                    (key, json.dumps(result)),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table to store the function's output if it doesn't exist\n            c.execute(\n                \"CREATE TABLE IF NOT EXISTS cache (func_name TEXT, args TEXT, kwargs TEXT, result TEXT)\"\n            )\n\n            # Compute a hash of the function's arguments\n            args_hash = hashlib.sha256(json.dumps(args).encode()).hexdigest()\n            kwargs_hash = hashlib.sha256(json.dumps(kwargs).encode()).hexdigest()\n\n            # Check if the result of the function call is already stored in the database\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\",\n                (func_name, args_hash, kwargs_hash),\n            )\n            result = c.fetchone()\n\n            # If the result is not stored, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, args_hash, kwargs_hash, json.dumps(result)),\n                )\n                conn.commit()\n            else:\n                result = json.loads(result[0])\n\n            # Close the database connection\n            conn.close()\n\n            # Return the result of the function call\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table to store the function outputs if it doesn't exist\n            c.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS cache (\n                    func_name TEXT,\n                    args TEXT,\n                    kwargs TEXT,\n                    result TEXT\n                )\n            \"\"\"\n            )\n\n            # Compute the hash of the function call arguments\n            args_hash = hashlib.sha256(json.dumps(args).encode()).hexdigest()\n            kwargs_hash = hashlib.sha256(json.dumps(kwargs).encode()).hexdigest()\n\n            # Check if the result is already stored in the database\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND args_hash = ? AND kwargs_hash = ?\",\n                (func_name, args_hash, kwargs_hash),\n            )\n            result = c.fetchone()\n\n            # If the result is not stored, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args_hash, kwargs_hash, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, args_hash, kwargs_hash, json.dumps(result)),\n                )\n                conn.commit()\n            else:\n                result = json.loads(result[0])\n\n            # Close the database connection\n            conn.close()\n\n            # Return the result\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create the table if it doesn't exist\n            c.execute(\n                \"\"\"CREATE TABLE IF NOT EXISTS cache (key TEXT PRIMARY KEY, value BLOB)\"\"\"\n            )\n\n            # Generate a unique key for the function call\n            key = func_name + json.dumps(args) + json.dumps(kwargs)\n            key = hashlib.sha256(key.encode()).hexdigest()\n\n            # Check if the result is already in the database\n            c.execute(\"SELECT value FROM cache WHERE key=?\", (key,))\n            result = c.fetchone()\n\n            if result is not None:\n                # If the result is in the database, return it\n                return json.loads(result[0])\n            else:\n                # If the result is not in the database, compute it\n                result = func(*args, **kwargs)\n\n                # Store the result in the database\n                c.execute(\"INSERT INTO cache VALUES (?, ?)\", (key, json.dumps(result)))\n                conn.commit()\n\n                # Return the result\n                return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Create a table to store the function outputs\n            cursor.execute(\n                f\"\"\"\n                CREATE TABLE IF NOT EXISTS {func_name} (\n                    key TEXT PRIMARY KEY,\n                    value TEXT\n                )\n            \"\"\"\n            )\n\n            # Generate a unique key for the function call\n            key = hashlib.md5(\n                json.dumps(args + tuple(kwargs.items())).encode(\"utf-8\")\n            ).hexdigest()\n\n            # Check if the function output is already stored in the database\n            cursor.execute(f\"SELECT value FROM {func_name} WHERE key = ?\", (key,))\n            result = cursor.fetchone()\n\n            # If the function output is not stored, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    f\"INSERT INTO {func_name} (key, value) VALUES (?, ?)\", (key, result)\n                )\n                conn.commit()\n\n            # Close the connection to the database\n            conn.close()\n\n            # Return the function output\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a hash of the function arguments\n            func_args = json.dumps(args, sort_keys=True)\n            func_kwargs = json.dumps(kwargs, sort_keys=True)\n            func_hash = hashlib.sha256(\n                (func_name + func_args + func_kwargs).encode(\"utf-8\")\n            ).hexdigest()\n\n            # Connect to the database and check if the result is already stored\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND func_hash = ?\",\n                (func_name, func_hash),\n            )\n            result = c.fetchone()\n            if result:\n                result = result[0]\n            else:\n                # Compute the result and store it in the database\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, func_hash, result) VALUES (?, ?, ?)\",\n                    (func_name, func_hash, result),\n                )\n                conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Generate a unique key for the function call using its arguments\n            key = func_name + json.dumps(args) + json.dumps(kwargs)\n            key_hash = hashlib.sha256(key.encode()).hexdigest()\n\n            # Check if the result is already stored in the database\n            cursor.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND key_hash = ?\",\n                (func_name, key_hash),\n            )\n            result = cursor.fetchone()\n            if result:\n                # If the result is found, return it directly\n                return json.loads(result[0])\n\n            # If the result is not found, compute it and store it in the database\n            result = func(*args, **kwargs)\n            result_json = json.dumps(result)\n            cursor.execute(\n                \"INSERT INTO cache (func_name, key_hash, result) VALUES (?, ?, ?)\",\n                (func_name, key_hash, result_json),\n            )\n            conn.commit()\n            conn.close()\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Compute a unique key for the function call\n            key = hashlib.sha256(\n                (func_name + str(args) + str(kwargs)).encode()\n            ).hexdigest()\n\n            # Check if the result is already stored in the database\n            cursor.execute(\"SELECT result FROM cache WHERE key=?\", (key,))\n            result = cursor.fetchone()\n            if result:\n                # If the result is stored, retrieve it from the database\n                result = json.loads(result[0])\n            else:\n                # If the result is not stored, compute it and store it in the database\n                result = func(*args, **kwargs)\n                cursor.execute(\n                    \"INSERT INTO cache (key, result) VALUES (?, ?)\",\n                    (key, json.dumps(result)),\n                )\n                conn.commit()\n\n            # Close the database connection\n            cursor.close()\n            conn.close()\n\n            # Return the result\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table to store the function outputs if it doesn't exist\n            c.execute(\n                \"CREATE TABLE IF NOT EXISTS memoize (func_name text, args text, kwargs text, result text)\"\n            )\n\n            # Hash the function arguments to avoid storing large strings in the database\n            args_hash = hashlib.sha256(str(args).encode()).hexdigest()\n            kwargs_hash = hashlib.sha256(str(kwargs).encode()).hexdigest()\n\n            # Check if the function output is already stored in the database\n            c.execute(\n                \"SELECT result FROM memoize WHERE func_name = ? AND args = ? AND kwargs = ?\",\n                (func_name, args_hash, kwargs_hash),\n            )\n            result = c.fetchone()\n\n            if result:\n                # If the function output is already stored, retrieve it from the database\n                result = json.loads(result[0])\n            else:\n                # If the function output is not stored, compute it and store it in the database\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO memoize (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, args_hash, kwargs_hash, json.dumps(result)),\n                )\n                conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            # Return the function output\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a hash of the function arguments\n            args_hash = hashlib.sha256(\n                json.dumps(args, sort_keys=True).encode(\"utf-8\")\n            ).hexdigest()\n            kwargs_hash = hashlib.sha256(\n                json.dumps(kwargs, sort_keys=True).encode(\"utf-8\")\n            ).hexdigest()\n            combined_hash = hashlib.sha256(\n                (args_hash + kwargs_hash).encode(\"utf-8\")\n            ).hexdigest()\n\n            # Connect to the database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Check if the result is already stored in the database\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND args_hash = ? AND kwargs_hash = ?\",\n                (func_name, args_hash, kwargs_hash),\n            )\n            result = c.fetchone()\n\n            # If the result is not stored, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args_hash, kwargs_hash, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, args_hash, kwargs_hash, result),\n                )\n                conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            # Return the result\n            return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                f\"CREATE TABLE IF NOT EXISTS {func_name} (key TEXT PRIMARY KEY, value TEXT)\"\n            )\n            key = hashlib.md5(\n                json.dumps({\"args\": args, \"kwargs\": kwargs}).encode(\"utf-8\")\n            ).hexdigest()\n            c.execute(f\"SELECT value FROM {func_name} WHERE key=?\", (key,))\n            result = c.fetchone()\n            if result is not None:\n                value = json.loads(result[0])\n            else:\n                value = func(*args, **kwargs)\n                c.execute(f\"INSERT INTO {func_name} VALUES (?, ?)\", (key, json.dumps(value)))\n                conn.commit()\n            conn.close()\n            return value\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a hash of the function arguments\n            func_args = json.dumps(args, sort_keys=True)\n            func_kwargs = json.dumps(kwargs, sort_keys=True)\n            func_args_hash = hashlib.sha256(func_args.encode()).hexdigest()\n            func_kwargs_hash = hashlib.sha256(func_kwargs.encode()).hexdigest()\n            func_args_hash_kwargs = hashlib.sha256(\n                (func_args_hash + func_kwargs_hash).encode()\n            ).hexdigest()\n\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Check if the result of the function call is already stored in the database\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND func_args_hash=? AND func_kwargs_hash=?\",\n                (func_name, func_args_hash_kwargs, func_kwargs_hash),\n            )\n            result = c.fetchone()\n\n            if result:\n                # If the result is already stored, return it\n                return json.loads(result[0])\n            else:\n                # If the result is not stored, compute it and store it in the database\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, func_args_hash, func_kwargs_hash, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, func_args_hash_kwargs, func_kwargs_hash, json.dumps(result)),\n                )\n                conn.commit()\n                conn.close()\n                return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table to store the function's output if it doesn't exist\n            c.execute(\n                \"\"\"CREATE TABLE IF NOT EXISTS cache (\n                    func_name TEXT,\n                    args TEXT,\n                    kwargs TEXT,\n                    result TEXT\n                )\"\"\"\n            )\n\n            # Hash the arguments to ensure they can be stored as JSON\n            args_hash = hashlib.sha256(json.dumps(args).encode(\"utf-8\")).hexdigest()\n            kwargs_hash = hashlib.sha256(\n                json.dumps(kwargs).encode(\"utf-8\")\n            ).hexdigest()\n\n            # Check if the function's output is already stored in the database\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND args_hash = ? AND kwargs_hash = ?\",\n                (func_name, args_hash, kwargs_hash),\n            )\n            result = c.fetchone()\n\n            if result is not None:\n                # If the output is already stored, retrieve it from the database\n                result = json.loads(result[0])\n            else:\n                # If the output is not stored, compute it and store it in the database\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args_hash, kwargs_hash, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, args_hash, kwargs_hash, json.dumps(result)),\n                )\n                conn.commit()\n\n            # Close the connection\n            conn.close()\n\n            # Return the result\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table if it doesn't exist\n            c.execute(\n                \"\"\"CREATE TABLE IF NOT EXISTS cache (\n                    func_name TEXT,\n                    args TEXT,\n                    kwargs TEXT,\n                    result TEXT\n                )\"\"\"\n            )\n\n            # Generate a hash of the function arguments\n            args_hash = hashlib.sha256(\n                json.dumps(args, sort_keys=True).encode(\"utf-8\")\n            ).hexdigest()\n            kwargs_hash = hashlib.sha256(\n                json.dumps(kwargs, sort_keys=True).encode(\"utf-8\")\n            ).hexdigest()\n            args_hash_str = f\"{args_hash}_{kwargs_hash}\"\n\n            # Check if the result is already in the database\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND args = ?\",\n                (func_name, args_hash_str),\n            )\n            result = c.fetchone()\n\n            if result is not None:\n                # Return the cached result\n                result = json.loads(result[0])\n            else:\n                # Compute the result and store it in the database\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, args_hash_str, kwargs_hash, json.dumps(result)),\n                )\n                conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            cursor = conn.cursor()\n\n            # Generate a unique key for the function call\n            key = func_name + str(args) + str(kwargs)\n            key = hashlib.md5(key.encode()).hexdigest()\n\n            # Check if the result is already stored in the database\n            cursor.execute(\"SELECT result FROM cache WHERE key=?\", (key,))\n            result = cursor.fetchone()\n            if result is not None:\n                # If the result is stored, return it\n                return json.loads(result[0])\n\n            # If the result is not stored, compute it\n            result = func(*args, **kwargs)\n\n            # Store the result in the database\n            cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (key, json.dumps(result)))\n            conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            # Return the result\n            return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table to store the results of the function\n            c.execute(\n                \"\"\"CREATE TABLE IF NOT EXISTS cache (\n                    key TEXT PRIMARY KEY,\n                    value TEXT\n                )\"\"\"\n            )\n\n            # Compute the key for the function call\n            key = hashlib.md5(\n                (func_name + json.dumps(args) + json.dumps(kwargs)).encode()\n            ).hexdigest()\n\n            # Check if the result is already stored in the database\n            c.execute(\"SELECT value FROM cache WHERE key = ?\", (key,))\n            result = c.fetchone()\n\n            # If the result is not stored, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache VALUES (?, ?)\", (key, json.dumps(result)))\n                conn.commit()\n\n            # Close the database connection\n            conn.close()\n\n            # Return the result\n            return json.loads(result[0])\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max and y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box. x_min must be less than x_max and y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] > values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n\n    if values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: invalid bounding box, x_min must be less than x_max and y_min must be less than y_max\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max. Received x_min={x_min}, x_max={x_max}\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max. Received y_min={y_min}, y_max={y_max}\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max. Received x_min={x_min}, x_max={x_max}\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max. Received y_min={y_min}, y_max={y_max}\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max = values[\"x_min\"], values[\"x_max\"]\n    y_min, y_max = values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max, and y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box. x_min must be less than x_max, and y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box values provided. x_min must be less than x_max, and y_min must be less than y_max.\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max. Received x_min: {x_min} and x_max: {x_max}\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max. Received y_min: {y_min} and y_max: {y_max}\")\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0_norm = np.sum(mat0**2, axis=0)\n  mat1_norm = np.sum(mat1**2, axis=0)\n  mat01_dot = np.matmul(mat0.T, mat1)\n  dist_sq = mat0_norm[:, None] + mat1_norm[None, :] - 2 * mat01_dot\n  dist_sq[dist_sq < 0] = 0\n  return dist_sq\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  dist_mat = np.sum(mat0**2, axis=0, keepdims=True) + np.sum(\n      mat1**2, axis=0, keepdims=True).T - 2 * np.dot(mat0, mat1.T)\n  dist_mat[dist_mat < 0] = 0\n  return dist_mat\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  norm0 = np.sum(mat0**2, axis=0)\n  norm1 = np.sum(mat1**2, axis=0)\n  dot = np.dot(mat0.T, mat1)\n\n  dist = norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot\n  dist[dist < 0] = 0\n\n  return dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norm of each column in the matrices\n  norm0 = np.sum(mat0**2, axis=0)\n  norm1 = np.sum(mat1**2, axis=0)\n\n  # Compute the dot product of each column in the matrices\n  dot_prod = np.dot(mat0.T, mat1)\n\n  # Compute the squared distance between each pair of columns\n  dist = norm0[:, None] + norm1[None, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  dist[dist < 0] = 0\n\n  return dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_norm = np.sum(mat0**2, axis=0)\n  mat1_norm = np.sum(mat1**2, axis=0)\n\n  mat0_mat1_dot = np.matmul(mat0.T, mat1)\n  mat0_mat1_dot = np.maximum(mat0_mat1_dot, 0)\n\n  return mat0_norm[:, None] + mat1_norm[None, :] - 2 * mat0_mat1_dot\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_norm = np.sum(mat0**2, axis=0)\n  mat1_norm = np.sum(mat1**2, axis=0)\n  mat01_dot = np.matmul(mat0.T, mat1)\n\n  dist_mat = mat0_norm[:, None] + mat1_norm[None, :] - 2 * mat01_dot\n  dist_mat[dist_mat < 0] = 0\n\n  return dist_mat\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  return np.maximum(0, np.sum(mat0**2, axis=0, keepdims=True) + np.sum(\n      mat1**2, axis=0) - 2 * np.dot(mat0.T, mat1))\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the norms of the vectors in mat0 and mat1\n  norm0 = np.sum(mat0**2, axis=0)\n  norm1 = np.sum(mat1**2, axis=0)\n\n  # Compute the dot product between the columns of mat0 and mat1\n  dot_product = np.dot(mat0.T, mat1)\n\n  # Compute the squared distance between each pair of vectors\n  sq_dist = norm0[:, None] + norm1[None, :] - 2 * dot_product\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0_norm = np.sum(mat0 * mat0, axis=0, keepdims=True)\n  mat1_norm = np.sum(mat1 * mat1, axis=0, keepdims=True)\n  mat01_dot = np.matmul(mat0.T, mat1)\n  sq_dist = mat0_norm + mat1_norm.T - 2 * mat01_dot\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of the vectors in the two matrices\n  norm0 = np.sum(mat0**2, axis=0, keepdims=True)\n  norm1 = np.sum(mat1**2, axis=0, keepdims=True)\n\n  # Compute the dot product between all pairs of vectors in the two matrices\n  dot_prod = np.dot(mat0.T, mat1)\n\n  # Compute the squared Euclidean distance between all pairs of vectors\n  dist_sq = norm0 + norm1.T - 2 * dot_prod\n\n  # Set negative distances to zero\n  dist_sq[dist_sq < 0] = 0\n\n  return dist_sq\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of each column in both matrices\n  norm0 = np.sum(mat0**2, axis=0)\n  norm1 = np.sum(mat1**2, axis=0)\n\n  # Compute the dot product between each pair of columns in the two matrices\n  dot_product = np.matmul(mat0.T, mat1)\n\n  # Compute the squared distance between each pair of columns\n  sq_dist = norm0[:, None] + norm1[None, :] - 2 * dot_product\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_norm = np.linalg.norm(mat0, axis=0)\n  mat1_norm = np.linalg.norm(mat1, axis=0)\n\n  dist = np.matmul(mat0.T, mat1)\n  dist += np.outer(mat0_norm, mat1_norm)\n  dist -= 2 * np.diag(np.diag(dist))\n\n  dist[dist < 0] = 0\n\n  return dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the dot product of the two matrices\n  dot_product = np.matmul(mat0.T, mat1)\n\n  # Compute the squared norms of the columns of the matrices\n  norm0_sq = np.sum(mat0**2, axis=0)\n  norm1_sq = np.sum(mat1**2, axis=0)\n\n  # Compute the squared distance matrix\n  dist_sq = norm0_sq[:, np.newaxis] + norm1_sq[np.newaxis, :] - 2 * dot_product\n\n  # Set negative distances to zero\n  dist_sq = np.maximum(dist_sq, 0)\n\n  return dist_sq\n\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared Euclidean distance between all pairs of columns in mat0 and mat1.\n  mat0_norm = np.sum(mat0**2, axis=0, keepdims=True)\n  mat1_norm = np.sum(mat1**2, axis=0, keepdims=True)\n  mat01_dot = np.matmul(mat0.transpose(), mat1)\n  dist_sq = mat0_norm + mat1_norm.transpose() - 2 * mat01_dot\n  dist_sq[dist_sq < 0] = 0\n  return dist_sq\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  diff = mat0[:, np.newaxis, :] - mat1[np.newaxis, :, :]\n  sq_dist = np.sum(diff * diff, axis=-1)\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0_norm = np.sum(np.square(mat0), axis=0)\n  mat1_norm = np.sum(np.square(mat1), axis=0)\n  mat0_mat1_dot = np.matmul(mat0.T, mat1)\n  dist_mat = mat0_norm[:, np.newaxis] + mat1_norm[np.newaxis, :] - 2 * mat0_mat1_dot\n  dist_mat = np.maximum(dist_mat, 0)\n  return dist_mat\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  diff = mat0[:, np.newaxis, :] - mat1[np.newaxis, :, :]\n  sq_diff = np.square(diff)\n  sq_dist = np.sum(sq_diff, axis=-1)\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_sq = np.sum(mat0**2, axis=0)\n  mat1_sq = np.sum(mat1**2, axis=0)\n  mat01_sq = np.sum(mat0 * mat1, axis=0)\n\n  dist_sq = mat0_sq[:, None] + mat1_sq[None, :] - 2 * mat01_sq\n  dist_sq[dist_sq < 0] = 0\n\n  return dist_sq\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the dot product between all pairs of columns in mat0 and mat1\n  dot_product = np.matmul(mat0, mat1.T)\n\n  # Compute the norms of the columns of mat0 and mat1\n  norm0 = np.linalg.norm(mat0, axis=0)\n  norm1 = np.linalg.norm(mat1, axis=0)\n\n  # Compute the squared distance between all pairs of columns\n  sq_dist = np.square(norm0) - 2 * dot_product + np.square(norm1)[:, np.newaxis]\n\n  # Set negative distances to zero\n  sq_dist = np.maximum(sq_dist, 0)\n\n  return sq_dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared Euclidean distance between all pairs of columns\n  # in mat0 and mat1.\n  sq_dist = -2.0 * np.dot(mat0, mat1.T)\n  sq_dist += np.sum(mat0**2, axis=0, keepdims=True)\n  sq_dist += np.sum(mat1**2, axis=0, keepdims=True).T\n\n  # Set negative distances to zero\n  sq_dist = np.maximum(sq_dist, 0.0)\n\n  return sq_dist\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return path is None or path == \"\" or path.startswith(\"s3://\") or path.startswith(\"gs://\")\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"gs://\") or path.startswith(\"hdfs://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"gs://\") or path.startswith(\"hdfs://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\")\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"gs://\") or path.startswith(\"s3://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return path is None or path == \"\" or path.startswith(\"s3://\") or path.startswith(\"gs://\") or path.startswith(\"hdfs://\")\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"gs://\") or path.startswith(\"hdfs://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return True\n    if path.startswith(\"gs://\") or path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"hdfs://\") or path.startswith(\"file://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return not path or path == \"\" or path.startswith(\"http\") or path.startswith(\"s3\") or path.startswith(\"gs\")\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return path is None or path == \"\" or path.startswith(\"gs://\") or path.startswith(\"s3://\")\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"gs://\") or path.startswith(\"s3://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return path is None or path == \"\" or path.startswith(\"s3://\") or path.startswith(\"gs://\")\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path or not isinstance(path, str):\n        return False\n    return path.startswith(\"s3://\") or path.startswith(\"gs://\")\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"/cache\") or path.startswith(\"/data\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"az://\"):\n        return True\n    if path.startswith(\"file://\"):\n        return True\n    if path.startswith(\"http://\"):\n        return True\n    if path.startswith(\"https://\"):\n        return True\n    if path.startswith(\"ftp://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    return path is None or path == \"\" or path.startswith(\"/cache\") or path.startswith(\"s3://\") or path.startswith(\"gs://\")\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\"assets_names must be provided when items is a dictionary\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"assets_names must have length {n_assets}, got {len(assets_names)}\"\n            )\n        if dim == 1:\n            items_array = np.full(n_assets, fill_value)\n            for i, key in enumerate(assets_names):\n                if key in items:\n                    items_array[i] = items[key]\n            return items_array\n        if dim == 2:\n            items_array = np.full((n_assets, n_assets), fill_value)\n            for i, key in enumerate(assets_names):\n                if key in items:\n                    items_array[i] = items[key]\n            return items_array\n        raise ValueError(f\"dim must be 1 or 2, got {dim}\")\n    items_array = np.array(items)\n    if dim == 1:\n        if items_array.ndim != 1:\n            raise ValueError(f\"{name} must be 1D, got {items_array.ndim}D\")\n        if items_array.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have length {n_assets}, got {items_array.shape[0]}\"\n            )\n        return items_array\n    if dim == 2:\n        if items_array.ndim != 2:\n            raise ValueError(f\"{name} must be 2D, got {items_array.ndim}D\")\n        if items_array.shape[0] != n_assets or items_array.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have shape ({n_assets}, {n_assets}), got {items_array.shape}\"\n            )\n        return items_array\n    raise ValueError(f\"dim must be 1 or 2,"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when items is a dictionary\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"Number of assets names ({len(assets_names)}) does not match the \"\n                f\"expected number of assets ({n_assets})\"\n            )\n        if dim == 1:\n            items_array = np.full(n_assets, fill_value)\n            for i, key in enumerate(assets_names):\n                if key in items:\n                    items_array[i] = items[key]\n                else:\n                    raise ValueError(\n                        f\"Missing value for asset '{key}' in {name} dictionary\"\n                    )\n        elif dim == 2:\n            items_array = np.full((n_assets, n_assets), fill_value)\n            for i, key in enumerate(assets_names):\n                if key in items:\n                    items_array[i, :] = items[key]\n                else:\n                    raise ValueError(\n                        f\"Missing value for asset '{key}' in {name} dictionary\"\n                    )\n        else:\n            raise ValueError(\n                f\"Invalid dimension {dim} for {name} dictionary. Expected 1 or 2.\"\n            )\n    else:\n        items_array = np.array(items)\n        if items_array.ndim != dim:\n            raise ValueError(\n                f\"Invalid dimension {items_array.ndim} for {name} array. \"\n                f\"Expected {dim}.\"\n            )\n        if items_array.shape[0] != n_assets:\n            raise ValueError(\n                f\"Number of assets ({items_array.shape[0]}) does not match the \"\n                f\"expected number of assets ({n_assets})\"\n            )\n        if dim == 2:\n            if items_array.shape[1] != n_assets:\n                raise ValueError(\n                    f\"Number of assets ({items_array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"When {name} is a dictionary, assets_names must be provided\"\n            )\n        if not isinstance(assets_names, np.ndarray):\n            raise TypeError(\n                f\"When {name} is a dictionary, assets_names must be a numpy array\"\n            )\n        if not isinstance(items, dict):\n            raise TypeError(f\"{name} must be a dictionary\")\n        if not all(isinstance(v, (float, int)) for v in items.values()):\n            raise TypeError(f\"All values of {name} must be numeric\")\n        if not all(isinstance(k, str) for k in items.keys()):\n            raise TypeError(f\"All keys of {name} must be strings\")\n        if not all(k in assets_names for k in items.keys()):\n            raise ValueError(\n                f\"{name} contains assets that are not in assets_names: {set(items.keys()) - set(assets_names)}\"\n            )\n        items_array = np.full(n_assets, fill_value)\n        items_array[assets_names.searchsorted(list(items.keys()))] = list(items.values())\n        return items_array\n    if not isinstance(items, np.ndarray):\n        items = np.array(items)\n    if not isinstance(items, np.ndarray):\n        raise TypeError(f\"{name} must be a numpy array\")\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(f\"{name} must be a 1D array\")\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements, got {items.shape[0]}\"\n            )\n        return items\n    if dim == 2:\n        if items.ndim != 2:\n            raise ValueError(f\"{name} must be a 2D array\")"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"When {name} is a dictionary, 'assets_names' must be provided\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"When {name} is a dictionary, it must have {n_assets} elements\"\n            )\n        array = np.full(n_assets, fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                array[i] = items[asset_name]\n        return array\n\n    if isinstance(items, np.ndarray):\n        if items.ndim != dim:\n            raise ValueError(f\"{name} must have {dim} dimensions\")\n        if items.shape[0] != n_assets:\n            raise ValueError(f\"{name} must have {n_assets} rows\")\n        return items\n\n    if isinstance(items, (list, tuple)):\n        array = np.array(items)\n        if array.ndim != dim:\n            raise ValueError(f\"{name} must have {dim} dimensions\")\n        if array.shape[0] != n_assets:\n            raise ValueError(f\"{name} must have {n_assets} rows\")\n        return array\n\n    raise ValueError(f\"{name} must be a dictionary, numpy array, or list/tuple\")\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} must be a dictionary if assets_names is not provided\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} assets, got {len(assets_names)}\"\n            )\n        if not all(isinstance(x, (float, int)) for x in items.values()):\n            raise ValueError(f\"{name} must be a dictionary of floats or integers\")\n        if not all(isinstance(x, str) for x in assets_names):\n            raise ValueError(\"assets_names must be a list of strings\")\n        if not all(isinstance(x, str) for x in items.keys()):\n            raise ValueError(f\"{name} must be a dictionary of strings\")\n        if not all(x in assets_names for x in items.keys()):\n            raise ValueError(\n                f\"{name} must have the same assets as assets_names, got {items.keys()}\"\n            )\n        array = np.array([items.get(x, fill_value) for x in assets_names])\n    elif isinstance(items, np.ndarray):\n        if items.ndim != dim:\n            raise ValueError(\n                f\"{name} must be a {dim}D array, got {items.ndim}D array\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} assets, got {items.shape[0]}\"\n            )\n        array = items\n    else:\n        raise ValueError(f\"{name} must be a dictionary or a numpy array\")\n    return array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when items is a dictionary\"\n            )\n        if not all(isinstance(name, str) for name in assets_names):\n            raise ValueError(\"assets_names must be a list of strings\")\n        if not all(isinstance(name, str) for name in items.keys()):\n            raise ValueError(\"keys of items must be a list of strings\")\n        if not all(isinstance(value, (int, float)) for value in items.values()):\n            raise ValueError(\"values of items must be numeric\")\n        if not all(name in assets_names for name in items.keys()):\n            raise ValueError(\n                f\"keys of items must be a subset of assets_names, got {items.keys()}\"\n            )\n        if dim == 1:\n            items_array = np.zeros(n_assets)\n            items_array[:] = np.nan\n            for name, value in items.items():\n                items_array[np.where(assets_names == name)[0][0]] = value\n            items_array = items_array.reshape((n_assets,))\n        elif dim == 2:\n            n_groups = len(items)\n            items_array = np.zeros((n_groups, n_assets))\n            items_array[:] = np.nan\n            for i, (name, value) in enumerate(items.items()):\n                items_array[i, np.where(assets_names == name)[0][0]] = value\n        else:\n            raise ValueError(f\"dim must be 1 or 2, got {dim}\")\n    elif isinstance(items, npt.ArrayLike):\n        items_array = np.array(items)\n        if dim == 1:\n            if items_array.shape != (n_assets,):\n                raise ValueError(\n                    f\"{name} must be a 1D array of shape (n_assets,), got {items_array.shape}\"\n                )\n        elif dim == 2"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} must be a dictionary when assets_names is None\"\n            )\n        items = pd.Series(items, index=assets_names).fillna(fill_value)\n    else:\n        items = np.asarray(items)\n\n    if items.ndim == 1:\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements, got {items.shape[0]}\"\n            )\n        if dim == 2:\n            items = items.reshape(1, -1)\n    elif items.ndim == 2:\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} rows, got {items.shape[0]}\"\n            )\n        if dim == 1:\n            items = items.reshape(-1)\n    else:\n        raise ValueError(f\"{name} must have 1 or 2 dimensions, got {items.ndim}\")\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"The 'assets_names' argument must be provided when 'items' is a dictionary.\"\n            )\n\n        if dim == 1:\n            items = np.array([items.get(name, fill_value) for name in assets_names])\n        else:\n            items = np.array(\n                [\n                    [items.get(name, fill_value) for name in assets_names]\n                    for _ in range(n_assets)\n                ]\n            )\n\n    items = np.array(items)\n\n    if items.ndim == 1:\n        if dim == 2:\n            items = items[:, np.newaxis]\n    elif items.ndim == 2:\n        if dim == 1:\n            items = items.ravel()\n\n    if items.shape != (n_assets,) and items.shape != (n_assets, n_assets):\n        raise ValueError(\n            f\"The '{name}' argument must have shape (n_assets,) or (n_assets, n_assets).\"\n        )\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} must be a dictionary when assets_names is not provided\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} assets, got {len(assets_names)}\"\n            )\n        if any(isinstance(v, (list, tuple, np.ndarray)) for v in items.values()):\n            raise ValueError(\n                f\"{name} must be a dictionary of scalars, got a dictionary of arrays\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} assets, got {len(items)}\"\n            )\n        if not all(isinstance(k, str) for k in items.keys()):\n            raise ValueError(\n                f\"{name} keys must be strings, got {set(type(k) for k in items.keys())}\"\n            )\n        if not all(k in assets_names for k in items.keys()):\n            raise ValueError(\n                f\"{name} keys must be in assets_names, got {set(items.keys()) - set(assets_names)}\"\n            )\n        x = np.full((n_assets,), fill_value)\n        for k, v in items.items():\n            x[np.where(assets_names == k)[0][0]] = v\n    else:\n        x = np.asarray(items)\n        if x.ndim != dim:\n            raise ValueError(f\"{name} must be {dim}D, got {x.ndim}D\")\n        if x.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} assets, got {x.shape[0]}\"\n            )\n    return x\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\"assets_names must be provided when items is a dictionary\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"Number of assets in '{name}' ({len(assets_names)}) does not match \"\n                f\"the expected number of assets ({n_assets})\"\n            )\n        items_array = np.full(n_assets, fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                items_array[i] = items[asset_name]\n        if dim == 1:\n            items_array = items_array.reshape((n_assets,))\n        return items_array\n    elif isinstance(items, np.ndarray):\n        if items.ndim != dim:\n            raise ValueError(\n                f\"Number of dimensions of '{name}' ({items.ndim}) does not match \"\n                f\"the expected dimension ({dim})\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"Number of assets in '{name}' ({items.shape[0]}) does not match \"\n                f\"the expected number of assets ({n_assets})\"\n            )\n        return items\n    else:\n        raise TypeError(\n            f\"Unsupported type for '{name}': {type(items)}. Expected a dictionary \"\n            \"or numpy array\"\n        )\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} must be a dictionary when 'assets_names' is None\"\n            )\n        if not isinstance(assets_names, np.ndarray):\n            raise TypeError(\n                f\"{name} must be a dictionary when 'assets_names' is not None\"\n            )\n        if not len(assets_names) == n_assets:\n            raise ValueError(\n                f\"The length of '{name}' must be equal to 'n_assets' when 'assets_names' is not None\"\n            )\n        if not all(isinstance(x, (float, int)) for x in items.values()):\n            raise TypeError(f\"All values in '{name}' must be numeric\")\n        if not all(isinstance(x, str) for x in items.keys()):\n            raise TypeError(f\"All keys in '{name}' must be strings\")\n        if not all(x in assets_names for x in items.keys()):\n            raise ValueError(\n                f\"All keys in '{name}' must be present in 'assets_names'\"\n            )\n        if dim == 1:\n            return np.array([items.get(x, fill_value) for x in assets_names])\n        if dim == 2:\n            return np.array([[items.get(x, fill_value) for x in assets_names]])\n        raise ValueError(f\"Invalid dim value {dim}\")\n\n    if isinstance(items, np.ndarray):\n        if not len(items.shape) == dim:\n            raise ValueError(f\"{name} must be a {dim}D array\")\n        if dim == 1:\n            if not items.shape[0] == n_assets:\n                raise ValueError(\n                    f\"The length of {name} must be equal to 'n_assets'\"\n                )\n        if dim == 2:\n            if not items.shape[1] == n_assets:\n                raise ValueError(\n                    f\"The length of {name} must be equal to '"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert to numpy array\n    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary and assets_names is None, you must provide \"\n                \"assets_names\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} is a dictionary and len(assets_names) != n_assets, you must \"\n                \"provide assets_names with the same length as n_assets\"\n            )\n        if not all(isinstance(v, (float, int)) for v in items.values()):\n            raise ValueError(\n                f\"{name} is a dictionary and not all values are float or int, you must \"\n                \"provide a dictionary with float or int values\"\n            )\n        items = np.array([items.get(v, fill_value) for v in assets_names])\n    elif isinstance(items, (list, tuple, np.ndarray)):\n        items = np.array(items)\n    else:\n        raise ValueError(\n            f\"{name} is not a dictionary, list, tuple, or numpy array, you must \"\n            \"provide a dictionary, list, tuple, or numpy array\"\n        )\n\n    # Verify the shape\n    if dim == 1:\n        if items.ndim != 1 or items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} has shape {items.shape} and not (n_assets,), you must \"\n                \"provide a numpy array of shape (n_assets,)\"\n            )\n    elif dim == 2:\n        if items.ndim != 2 or items.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} has shape {items.shape} and not (n_groups, n_assets), you \"\n                \"must provide a numpy array of shape (n_groups, n_assets)\"\n            )\n    else:\n        raise ValueError(\n            f\"dim must be"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"{name} cannot be a dictionary without assets_names\")\n        items_array = np.full(n_assets, fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                items_array[i] = items[asset_name]\n        if dim == 1:\n            items_array = items_array.reshape(-1, 1)\n        return items_array\n\n    if not isinstance(items, np.ndarray):\n        items = np.array(items)\n\n    if items.ndim == 1:\n        if dim == 2:\n            items = items.reshape(-1, 1)\n    elif items.ndim != dim:\n        raise ValueError(\n            f\"{name} must be an array of shape (n_assets,) or (n_groups, n_assets), got {items.shape}\"\n        )\n\n    if items.shape[0] != n_assets:\n        raise ValueError(\n            f\"{name} must have {n_assets} rows, got {items.shape[0]}\"\n        )\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} must be a dictionary when assets_names is None\"\n            )\n        if not np.all(np.isin(assets_names, list(items.keys()))):\n            raise ValueError(\n                f\"{name} must have all assets names as keys\"\n            )\n        items = np.array([items.get(name, fill_value) for name in assets_names])\n    elif isinstance(items, np.ndarray):\n        if items.ndim != dim:\n            raise ValueError(\n                f\"{name} must have {dim} dimensions, got {items.ndim}\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} assets, got {items.shape[0]}\"\n            )\n    else:\n        items = np.array(items)\n        if items.ndim != dim:\n            raise ValueError(\n                f\"{name} must have {dim} dimensions, got {items.ndim}\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} assets, got {items.shape[0]}\"\n            )\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary but no assets names were provided.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} is a dictionary but the number of assets names does not match the number of assets.\"\n            )\n        if not all(isinstance(k, str) for k in items.keys()):\n            raise ValueError(\n                f\"{name} is a dictionary but the keys are not strings.\"\n            )\n        if not all(isinstance(v, (int, float)) for v in items.values()):\n            raise ValueError(\n                f\"{name} is a dictionary but the values are not numeric.\"\n            )\n        array = np.full(n_assets, fill_value=fill_value)\n        for i, name in enumerate(assets_names):\n            if name in items:\n                array[i] = items[name]\n        array = np.atleast_2d(array)\n    else:\n        array = np.atleast_2d(items)\n    if dim == 1:\n        if array.shape != (n_assets,):\n            raise ValueError(\n                f\"{name} is a {array.shape} array but expected shape is (n_assets,).\"\n            )\n    elif dim == 2:\n        if array.shape != (n_assets,):\n            raise ValueError(\n                f\"{name} is a {array.shape} array but expected shape is (n_assets,).\"\n            )\n        array = np.atleast_2d(array)\n    else:\n        raise ValueError(\n            f\"{name} is a {array.shape} array but expected shape is (n_assets,) or (n_assets,).\"\n        )\n    return array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} must be a dictionary if assets_names is not provided\"\n            )\n        if not np.all(np.isin(assets_names, list(items.keys()))):\n            raise ValueError(\n                f\"{name} must contain all the assets names, got {assets_names} instead\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} must contain all the assets names, got {assets_names} instead\"\n            )\n        items_ = np.array(\n            [items.get(name, fill_value) for name in assets_names], dtype=float\n        )\n    elif isinstance(items, np.ndarray):\n        items_ = items\n    else:\n        items_ = np.array(items, dtype=float)\n\n    if items_.ndim != dim:\n        raise ValueError(f\"{name} must be a {dim}D array, got {items_.ndim}D array\")\n    if items_.shape[0] != n_assets:\n        raise ValueError(\n            f\"{name} must be a {n_assets}D array, got {items_.shape[0]}D array\"\n        )\n    return items_\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} must be a dictionary when assets_names is None\"\n            )\n        if not np.all(np.in1d(assets_names, list(items.keys()))):\n            raise ValueError(\n                f\"{name} must contain all assets names, got {list(items.keys())}\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} must contain {n_assets} assets, got {len(assets_names)}\"\n            )\n        array = np.array([items.get(name, fill_value) for name in assets_names])\n    else:\n        array = np.array(items)\n    if array.ndim != dim:\n        raise ValueError(f\"{name} must be a {dim}D array\")\n    if array.shape[0] != n_assets:\n        raise ValueError(\n            f\"{name} must have shape ({n_assets},), got {array.shape}\"\n        )\n    return array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"When using a dictionary, you must provide the 'assets_names' argument\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"When using a dictionary, the number of items must match the number of assets, got {len(items)} != {n_assets}\"\n            )\n        array = np.full(n_assets, fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                array[i] = items[asset_name]\n        return array\n    if not isinstance(items, npt.ArrayLike):\n        raise TypeError(\n            f\"{name} must be a dictionary or array-like, got {type(items)}\"\n        )\n    array = np.array(items)\n    if array.ndim != dim:\n        raise ValueError(\n            f\"{name} must have a dimension of {dim}, got {array.ndim}\"\n        )\n    if array.shape != (n_assets,):\n        raise ValueError(\n            f\"{name} must have a shape of (n_assets,), got {array.shape}\"\n        )\n    return array\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Convert items to an array if it's not already an array\n    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary, but no assets_names were provided\"\n            )\n        items = np.array([items.get(asset, fill_value) for asset in assets_names])\n\n    # Convert items to a 1D array if it's a scalar\n    if np.isscalar(items):\n        items = np.array(items)\n\n    # Convert items to a 2D array if it's a 1D array and dim=2\n    if dim == 2 and len(items.shape) == 1:\n        items = items[:, np.newaxis]\n\n    # Check if the array has the expected shape\n    if items.shape != (n_assets,) and items.shape != (n_assets, 1):\n        raise ValueError(\n            f\"{name} has shape {items.shape}, expected ({n_assets},) or ({n_assets}, 1)\"\n        )\n\n    return items\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"Missing assets names for {name} when it is a dictionary\"\n            )\n        if not all(isinstance(v, (float, int)) for v in items.values()):\n            raise TypeError(f\"{name} must be a dictionary of float or int values\")\n        if not all(isinstance(v, str) for v in assets_names):\n            raise TypeError(\"assets_names must be a list of strings\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"Expected {n_assets} assets names, got {len(assets_names)}\"\n            )\n        items_array = np.full(n_assets, fill_value=fill_value)\n        for k, v in items.items():\n            if k not in assets_names:\n                raise ValueError(f\"Unknown asset name '{k}' for {name}\")\n            items_array[assets_names == k] = v\n    else:\n        items_array = np.asarray(items, dtype=float)\n        if items_array.ndim != dim:\n            raise ValueError(\n                f\"Expected {dim}D array for {name}, got {items_array.ndim}D\"\n            )\n        if items_array.shape[0] != n_assets:\n            raise ValueError(\n                f\"Expected {n_assets} assets for {name}, got {items_array.shape[0]}\"\n            )\n    return items_array\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", None),\n            purpose=data.get(\"purpose\", None),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", None),\n            purpose=data.get(\"purpose\", None),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", None),\n            max_depth=data.get(\"max_depth\", None),\n            usage_count=data.get(\"usage_count\", None),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", None),\n            evolve_count=data.get(\"evolve_count\", None),\n            number_of_code_executions=data.get(\"number_of_code_executions\", None),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", None),\n            purpose=data.get(\"purpose\", None),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", None),\n            max_depth=data.get(\"max_depth\", None),\n            usage_count=data.get(\"usage_count\", None),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", None),\n            evolve_count=data.get(\"evolve_count\", None),\n            number_of_code_executions=data.get(\"number_of_code_executions\", None),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", None),\n            purpose=data.get(\"purpose\", None),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose=data.get(\"purpose\", \"\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose=data.get(\"purpose\", \"\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", \"\"),\n            parent_id=data.get(\"parent_id\", \"\"),\n            working_agent=data.get(\"working_agent\", \"\"),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", \"\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", None),\n            purpose=data.get(\"purpose\", None),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", None),\n            max_depth=data.get(\"max_depth\", None),\n            usage_count=data.get(\"usage_count\", None),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", None),\n            evolve_count=data.get(\"evolve_count\", None),\n            number_of_code_executions=data.get(\"number_of_code_executions\", None),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose=data.get(\"purpose\", \"\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", True),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", \"\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose=data.get(\"purpose\", \"\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", \"\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose = data.get(\"purpose\", None)\n        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)\n\n        dynamic_prompt = data.get(\"dynamic_prompt\", None)\n        depth = data.get(\"depth\", 0)\n        max_depth = data.get(\"max_depth\", 0)\n        usage_count = data.get(\"usage_count\", 0)\n        id = data.get(\"id\", None)\n        parent_id = data.get(\"parent_id\", None)\n        working_agent = data.get(\"working_agent\", None)\n        is_prime = data.get(\"is_prime\", False)\n        evolve_count = data.get(\"evolve_count\", 0)\n        number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        last_input = data.get(\"last_input\", None)\n\n        return MicroAgent(\n            dynamic_prompt=dynamic_prompt,\n            purpose=purpose,\n            purpose_embedding=purpose_embedding,\n            depth=depth,\n            max_depth=max_depth,\n            usage_count=usage_count,\n            id=id,\n            parent_id=parent_id,\n            working_agent=working_agent,\n            is_prime=is_prime,\n            evolve_count=evolve_count,\n            number_of_code_executions=number_of_code_executions,\n            last_input=last_input,\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        dynamic_prompt = data.get(\"dynamic_prompt\", \"\")\n        purpose = data.get(\"purpose\", \"\")\n        purpose_embedding = data.get(\"purpose_embedding\", None)\n        depth = data.get(\"depth\", 0)\n        max_depth = data.get(\"max_depth\", 0)\n        usage_count = data.get(\"usage_count\", 0)\n        id = data.get(\"id\", None)\n        parent_id = data.get(\"parent_id\", None)\n        working_agent = data.get(\"working_agent\", None)\n        is_prime = data.get(\"is_prime\", False)\n        evolve_count = data.get(\"evolve_count\", 0)\n        number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        last_input = data.get(\"last_input\", None)\n\n        agent = MicroAgent(\n            dynamic_prompt=dynamic_prompt,\n            purpose=purpose,\n            purpose_embedding=purpose_embedding,\n            depth=depth,\n            max_depth=max_depth,\n            usage_count=usage_count,\n            id=id,\n            parent_id=parent_id,\n            working_agent=working_agent,\n            is_prime=is_prime,\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n            evolve_count=evolve_count,\n            number_of_code_executions=number_of_code_executions,\n            last_input=last_input,\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper\n        )\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        dynamic_prompt = data.get(\"dynamic_prompt\", None)\n        purpose = data.get(\"purpose\", None)\n        purpose_embedding = data.get(\"purpose_embedding\", None)\n        depth = data.get(\"depth\", None)\n        max_depth = data.get(\"max_depth\", None)\n        usage_count = data.get(\"usage_count\", None)\n        id = data.get(\"id\", None)\n        parent_id = data.get(\"parent_id\", None)\n        working_agent = data.get(\"working_agent\", None)\n        is_prime = data.get(\"is_prime\", None)\n        evolve_count = data.get(\"evolve_count\", None)\n        number_of_code_executions = data.get(\"number_of_code_executions\", None)\n        last_input = data.get(\"last_input\", None)\n\n        return MicroAgent(dynamic_prompt=dynamic_prompt,\n                          purpose=purpose,\n                          purpose_embedding=purpose_embedding,\n                          depth=depth,\n                          max_depth=max_depth,\n                          usage_count=usage_count,\n                          id=id,\n                          parent_id=parent_id,\n                          working_agent=working_agent,\n                          is_prime=is_prime,\n                          evolve_count=evolve_count,\n                          number_of_code_executions=number_of_code_executions,\n                          last_input=last_input,\n                          agent_lifecycle=agent_lifecycle,\n                          openai_wrapper=openai_wrapper)\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)\n\n        agent = MicroAgent(\n            data[\"dynamic_prompt\"],\n            data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose = data.get(\"purpose\", None)\n        purpose_embedding = data.get(\"purpose_embedding\", None)\n        dynamic_prompt = data.get(\"dynamic_prompt\", None)\n        depth = data.get(\"depth\", None)\n        max_depth = data.get(\"max_depth\", None)\n        usage_count = data.get(\"usage_count\", None)\n        id = data.get(\"id\", None)\n        parent_id = data.get(\"parent_id\", None)\n        working_agent = data.get(\"working_agent\", None)\n        is_prime = data.get(\"is_prime\", None)\n        evolve_count = data.get(\"evolve_count\", None)\n        number_of_code_executions = data.get(\"number_of_code_executions\", None)\n        last_input = data.get(\"last_input\", None)\n\n        agent = MicroAgent(agent_lifecycle, openai_wrapper, purpose=purpose, purpose_embedding=purpose_embedding, dynamic_prompt=dynamic_prompt, depth=depth, max_depth=max_depth, usage_count=usage_count, id=id, parent_id=parent_id, working_agent=working_agent, is_prime=is_prime, evolve_count=evolve_count, number_of_code_executions=number_of_code_executions, last_input=last_input)\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent object with the provided lifecycle state and OpenAI wrapper\n        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the dictionary's key-value pairs\n        agent.dynamic_prompt = data.get(\"dynamic_prompt\", \"\")\n        agent.purpose = data.get(\"purpose\", \"\")\n        agent.purpose_embedding = np.array(data.get(\"purpose_embedding\", []))\n        agent.depth = data.get(\"depth\", 0)\n        agent.max_depth = data.get(\"max_depth\", 0)\n        agent.usage_count = data.get(\"usage_count\", 0)\n        agent.id = data.get(\"id\", None)\n        agent.parent_id = data.get(\"parent_id\", None)\n        agent.working_agent = data.get(\"working_agent\", None)\n        agent.is_prime = data.get(\"is_prime\", False)\n        agent.evolve_count = data.get(\"evolve_count\", 0)\n        agent.number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        agent.last_input = data.get(\"last_input\", \"\")\n\n        return agent\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055)**2.4)\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb * 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * xnp.power(xnp.maximum(eps, srgb), 2.4) - 0.055\n  linear1 = 12.92 * srgb\n  return xnp.where(srgb <= 0.04045, linear1, linear0)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * (xnp.exp((srgb + 0.055) / 12.92) - 1)\n  linear1 = srgb / 12.92\n  return xnp.where(srgb <= 0.04045, linear1, linear0)\n\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 25 / 323 * srgb\n  linear1 = (200 * xnp.maximum(eps, srgb) + 11) / 211\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb * 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92,\n                     ((srgb + 0.055) / 1.055)**2.4)\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * xnp.power(xnp.maximum(eps, srgb) / 12.92, 2.4)\n  linear1 = (xnp.maximum(eps, srgb) + 0.055) / 1.055\n  linear1 = xnp.power(linear1, 2.4)\n  return xnp.where(srgb <= 0.04045, srgb / 12.92, linear1)\n\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * xnp.power(srgb + 0.0521327, 2.4) - 0.0521327\n  linear1 = srgb / 12.92\n  return xnp.where(srgb <= 0.04045, linear1, linear0)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb * 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055)**2.4)\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * xnp.exp(srgb / 2.4 * jnp.log(10.0)) - 0.055\n  linear1 = (srgb + 0.055) / 1.055\n  linear = xnp.where(srgb <= 0.04045, linear0, linear1)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb * 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * xnp.power(xnp.maximum(eps, srgb), 2.4) - 0.055\n  linear1 = (srgb + 0.0521327) / 1.055\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb * 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * xnp.exp(1.047819635 * (srgb - 0.04045)) - 0.055\n  linear1 = (srgb + 0.052186885) / 1.055\n  return xnp.where(srgb <= 0.04045, linear0, linear1 ** 2.4)\n\n\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit a spline to the input signal\n  tck, u_keyframes = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  y = scipy.interpolate.splev(t_output, tck)\n\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit spline to the signal\n  tck, u_keyframes = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  y_output = scipy.interpolate.splev(t_output, tck)\n\n  return y_output\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust the degree of the spline to be at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit a spline to the signal x at the input times t_input.\n  tck, u = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times t_output.\n  y = scipy.interpolate.splev(t_output, tck)\n\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust spline_degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit spline to the input signal\n  tck, u_keyframes = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  y = scipy.interpolate.splev(t_output, tck)\n\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust the degree of the spline to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit a spline to the signal x\n  tck, _ = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Interpolate the signal at the output times t_output\n  y = scipy.interpolate.splev(t_output, tck)\n\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n  # Fit a spline to the signal x\n  tck, u_keyframes = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness)\n  # Evaluate the spline at the output times\n  y = scipy.interpolate.splev(t_output, tck)\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust the degree of the spline to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit a spline to the input signal\n  tck, u = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  x_interp = scipy.interpolate.splev(t_output, tck)\n\n  return x_interp\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  if spline_degree is None:\n    spline_degree = min(len(x) - 1, 3)\n  spline_degree = min(spline_degree, len(x) - 1)\n  spline_degree = max(spline_degree, 1)\n  tck, u_keyframes = scipy.interpolate.splprep(\n      [t_input, x], k=spline_degree, s=smoothness\n  )\n  new_x = np.array(scipy.interpolate.splev(t_output, tck))\n  return new_x\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit the spline to the signal x at times t_input\n  tck, u_keyframes = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times t_output\n  x_interp = scipy.interpolate.splev(t_output, tck)\n\n  return x_interp\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Ensure that the input and output times are sorted\n  t_input = np.sort(t_input)\n  t_output = np.sort(t_output)\n\n  # Ensure that the input and output times are unique\n  t_input = np.unique(t_input)\n  t_output = np.unique(t_output)\n\n  # Ensure that the input and output times are not empty\n  assert len(t_input) > 0\n  assert len(t_output) > 0\n\n  # Ensure that the input and output times are not equal\n  assert not np.allclose(t_input, t_output)\n\n  # Ensure that the input and output times are not all the same\n  assert not np.allclose(t_input, t_input[0])\n  assert not np.allclose(t_output, t_output[0])\n\n  # Ensure that the input and output times are not all the same\n  assert not np.allclose(t_input, t_input[0])\n  assert not np.allclose(t_output, t_output[0])\n\n  # Ensure that the input and output times are not all the same\n  assert not np.allclose(t_input, t_input[0])\n  assert not np.allclose(t_output, t_output[0])\n\n  # Ensure that the input and output times are not all the same\n  assert not np.allclose(t_input, t_input[0])\n  assert not np.allclose(t_output, t_output[0])\n\n  # Ensure that the input and output times are not all the same\n  assert not np.allclose(t_input, t_input[0])\n  assert not np.allclose(t_output, t_output[0])\n\n  # Ensure that the input and output times are not all the same\n  assert not np.allclose(t_input, t_input[0])\n  assert not np.allclose(t_output, t_output[0])\n\n  # Ensure that the input and output times are not all the same"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if the input arrays are 1-dimensional\n  if len(x.shape) != 1 or len(t_input.shape) != 1 or len(t_output.shape) != 1:\n    raise ValueError('Input arrays must be 1-dimensional.')\n\n  # Check if the input arrays have the same length\n  if len(x) != len(t_input) or len(t_input) != len(t_output):\n    raise ValueError('Input arrays must have the same length.')\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(t_input) >= 0) or not np.all(np.diff(t_output) >= 0):\n    raise ValueError('Input arrays must be sorted.')\n\n  # Check if the input arrays are not empty\n  if len(x) == 0 or len(t_input) == 0 or len(t_output) == 0:\n    raise ValueError('Input arrays must not be empty.')\n\n  # Check if the input arrays are not all equal\n  if np.all(x == x[0]) or np.all(t_input == t_input[0]) or np.all(\n      t_output == t_output[0]\n  ):\n    raise ValueError('Input arrays must not be all equal.')\n\n  # Check if the input arrays are not all unique\n  if len(np.unique(x)) != len(x) or len(np.unique(t_input)) != len(\n      t_input\n  ) or len(np.unique(t_output)) != len(t_output):\n    raise ValueError('Input arrays must not be all unique.')\n\n  # Check if the input arrays are not all finite\n  if not np.all(np.isfinite(x)) or not np.all(np.isfinite(t_input)) or not np.all(\n      np.isfinite(t_output)\n  ):\n    raise ValueError('Input arrays must not contain NaN or infinite values.')\n\n  # Check if the input arrays are not all"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust spline_degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n  # Compute the spline coefficients\n  tck, u_keyframes = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness)\n  # Evaluate the spline at the output times\n  x_interp = scipy.interpolate.splev(t_output, tck)\n  return x_interp\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  t_input = np.array(t_input)\n  t_output = np.array(t_output)\n  x = np.array(x)\n\n  if len(x.shape) != 1:\n    raise ValueError('x must be a 1-dimensional array')\n  if len(t_input.shape) != 1:\n    raise ValueError('t_input must be a 1-dimensional array')\n  if len(t_output.shape) != 1:\n    raise ValueError('t_output must be a 1-dimensional array')\n  if len(x) != len(t_input):\n    raise ValueError('x and t_input must have the same length')\n\n  if spline_degree > len(x) - 1:\n    spline_degree = len(x) - 1\n\n  if spline_degree < 1:\n    raise ValueError('spline_degree must be at least 1')\n\n  tck, u_keyframes = scipy.interpolate.splprep(\n      [x], k=spline_degree, s=smoothness, per=False\n  )\n  y = scipy.interpolate.splev(t_output, tck)\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  assert len(t_input) == len(x), 'Input and output times must be the same length.'\n  assert spline_degree >= 0, 'Spline degree must be non-negative.'\n  assert spline_degree <= len(x) - 1, 'Spline degree must be at most one less than the number of input points.'\n  assert smoothness >= 0, 'Smoothness must be non-negative.'\n  assert np.all(np.diff(t_input) > 0), 'Input times must be strictly increasing.'\n  assert np.all(np.diff(t_output) > 0), 'Output times must be strictly increasing.'\n\n  tck, u_keyframes = scipy.interpolate.splprep(x, k=spline_degree, s=smoothness)\n  y = scipy.interpolate.splev(t_output, tck)\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  assert len(t_input) == len(x)\n  assert len(t_output) > 0\n  assert np.all(np.diff(t_input) > 0)\n  assert np.all(np.diff(t_output) > 0)\n\n  if len(x) == 1:\n    return np.array([x[0]] * len(t_output))\n\n  spline_degree = min(spline_degree, len(t_input) - 1)\n  tck, _ = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness)\n  x_interp = scipy.interpolate.splev(t_output, tck)\n  return np.array(x_interp)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  t_input = np.array(t_input)\n  t_output = np.array(t_output)\n  x = np.array(x)\n\n  assert len(t_input) == len(x), \"t_input and x must have the same length\"\n  assert len(t_output) > 0, \"t_output must have at least one element\"\n\n  if len(t_input) == 1:\n    return np.array([x[0]] * len(t_output))\n\n  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit spline to x\n  tck, u_keyframes = scipy.interpolate.splprep(\n      x.T, k=spline_degree, s=smoothness, per=False\n  )\n\n  # Query spline at t_output times\n  y = np.array(scipy.interpolate.splev(t_output, tck))\n\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  assert len(x) == len(t_input), 'x and t_input must have the same length'\n  assert len(t_output) > 0, 't_output must have at least one element'\n  assert spline_degree >= 0, 'spline_degree must be non-negative'\n  assert smoothness >= 0, 'smoothness must be non-negative'\n\n  # Adjust spline_degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit a spline to the signal x using scipy.interpolate.splrep\n  tck, u = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times t_output using scipy.interpolate.splev\n  y = scipy.interpolate.splev(t_output, tck)\n\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  t_input = np.array(t_input)\n  t_output = np.array(t_output)\n\n  # Check if input and output times are in the correct order\n  if not np.all(t_input[1:] >= t_input[:-1]):\n    raise ValueError('Input times must be in increasing order.')\n  if not np.all(t_output[1:] >= t_output[:-1]):\n    raise ValueError('Output times must be in increasing order.')\n\n  # Check if input and output times are within the range of t_input\n  if np.min(t_output) < np.min(t_input) or np.max(t_output) > np.max(t_input):\n    raise ValueError('Output times must be within the range of input times.')\n\n  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit spline to the input signal\n  tck, u_keyframes = scipy.interpolate.splprep(\n      [t_input, x], k=spline_degree, s=smoothness\n  )\n\n  # Evaluate the spline at the output times\n  y = scipy.interpolate.splev(t_output, tck)\n\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  if spline_degree < 0:\n    raise ValueError('spline_degree must be non-negative.')\n  if spline_degree >= len(t_input):\n    spline_degree = len(t_input) - 1\n  if len(t_input) != len(x):\n    raise ValueError('t_input and x must have the same length.')\n  if len(t_output) < 1:\n    raise ValueError('t_output must have at least one element.')\n  t_input = np.asarray(t_input)\n  t_output = np.asarray(t_output)\n  x = np.asarray(x)\n  if t_input.ndim != 1:\n    raise ValueError('t_input must be a 1-dimensional array.')\n  if t_output.ndim != 1:\n    raise ValueError('t_output must be a 1-dimensional array.')\n  if x.ndim != 1:\n    raise ValueError('x must be a 1-dimensional array.')\n  if not np.all(np.isfinite(t_input)):\n    raise ValueError('t_input must be finite.')\n  if not np.all(np.isfinite(t_output)):\n    raise ValueError('t_output must be finite.')\n  if not np.all(np.isfinite(x)):\n    raise ValueError('x must be finite.')\n  if not np.all(t_input[:-1] < t_input[1:]):\n    raise ValueError('t_input must be sorted.')\n  if not np.all(t_output[:-1] < t_output[1:]):\n    raise ValueError('t_output must be sorted.')\n  if not np.all(t_input[0] <= t_output):\n    raise ValueError('t_output must be at least as large as t_input[0].')\n  if not np.all(t_output <= t_input[-1]):\n    raise ValueError('t_output must be at most as large as t_"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check input arguments\n  x = np.array(x)\n  t_input = np.array(t_input)\n  t_output = np.array(t_output)\n  assert x.ndim == 1, 'x must be a 1-dimensional array'\n  assert t_input.ndim == 1, 't_input must be a 1-dimensional array'\n  assert t_output.ndim == 1, 't_output must be a 1-dimensional array'\n  assert t_input.shape == x.shape, 't_input and x must have the same shape'\n  assert np.all(np.diff(t_input) > 0), 't_input must be sorted in ascending order'\n  assert np.all(np.diff(t_output) > 0), 't_output must be sorted in ascending order'\n  assert np.all(t_output >= t_input.min()) and np.all(t_output <= t_input.max()), 't_output must be within the range of t_input'\n  assert spline_degree >= 1, 'spline_degree must be at least 1'\n  assert spline_degree <= t_input.shape[0] - 1, 'spline_degree must be at most t_input.shape[0] - 1'\n\n  # Adjust spline_degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, t_input.shape[0] - 1)\n\n  # Fit spline to x\n  tck, u = scipy.interpolate.splprep([t_input, x], k=spline_degree, s=smoothness)\n\n  # Evaluate spline at t_output\n  x_interp = scipy.interpolate.splev(t_output, tck)\n\n  return x_interp\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    # if upper no\n\n    # if mixed case\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n\n    # if first two letters are uppercase\n    # if first"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    if word[0].islower() and word[1].islower():\n        return word\n    return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    if word[0].islower() and word[1].islower():\n        return word\n\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].islower() and word[1].islower():\n        return word\n\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n\n    return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    elif word[0].islower() and word[1].isupper():\n        return word[0].upper() + word[1:].lower()\n    else:\n        return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower() or word.isupper():\n        return word\n    elif word[0].islower() and word[1].islower():\n        return word.lower()\n    elif word[0].isupper() and word[1].isupper():\n        return word.lower()\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    else:\n        return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.lower()\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    elif word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    elif word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    # if upper no\n\n    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    if word[0].isupper() and word[1].isupper():\n        return word\n\n    if word[0].islower() and word[1].islower():\n        return word\n\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n\n    return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    # if mixed case\n    first_letter = word[0]\n    second_letter = word[1]\n\n    if first_letter.isupper() and second_letter.isupper():\n        return word.capitalize()\n    if first_letter.islower() and second_letter.islower():\n        return word.lower()\n    if first_letter.isupper() and second_letter.islower():\n        return word.capitalize()\n\n    return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    if word[0].islower() and word[1].islower():\n        return word.capitalize()\n    return word\n\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word\n    elif word[0].isupper() and word[1].islower():\n        return word\n    elif word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    # if word is mixed case\n    # if first two letters are uppercase\n    if word[0:2].isupper():\n        return word\n    # if first letter is uppercase\n    if word[0].isupper():\n        return word.lower()\n    # if first letter is lowercase\n    if word[0].islower():\n        return word.capitalize()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].islower() and word[1].islower():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    # if upper no\n    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    # if mixed case, check first 2 letters\n    # if first 2 letters are upper, return upper\n    # if first 2 letters are lower, return lower\n    # if first letter is upper, return capitalized\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n    if word[0].islower() and word[1].islower():\n        return word.lower()\n    return word.capitalize()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n\n    return word\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be binary array. Found {v.dtype}\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be boolean array.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be binary (bool). Got {v.dtype}.\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    # Check if the array contains only boolean values\n    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array. Found {v.dtype} instead.\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be binary, found {v.dtype}.\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be binary array. Found {v.dtype}\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a binary array.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary numpy array. Found {v.dtype} instead.\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be binary (np.bool_). Got {v.dtype}\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    # Check if the array contains only boolean values\n    if not np.issubdtype(v.dtype, np.bool_):\n        # Raise a ValueError with a descriptive error message\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a binary array, but found {v.dtype}.\")\n\n    # If the array passes the binary check, return it unchanged\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    # Check if the array contains only boolean values\n    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array (dtype=bool). Found {v.dtype}.\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be boolean (np.bool_) data types.\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    # Check if the array contains only boolean values\n    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a binary array.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    # Check if the array contains only boolean values\n    if not np.issubdtype(v.dtype, np.bool_):\n        # Raise an error if the array contains non-boolean values\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array. Found {v.dtype} instead.\"\n        )\n\n    # Return the input array if it passes the binary check\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array, \"\n            f\"found {v.dtype}.\"\n        )\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be binary (only True or False).\"\n        )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    # Get the data type of the numpy array\n    data_type = v.dtype\n\n    # Check if the data type is not boolean\n    if not np.issubdtype(data_type, np.bool_):\n        # If the data type is not boolean, raise a ValueError with a descriptive message\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array. Found {data_type} instead.\"\n        )\n\n    # Return the input numpy array unchanged if it passes the binary check\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_norm) - 1) / x_norm\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input array\n  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n\n  # Calculate the scaling factor based on the norm\n  scale = jnp.where(x_norm < 1, 2 * jnp.sqrt(x_norm) - 1, 1)\n\n  # Apply the scaling factor to the input array\n  z = scale * x\n\n  return z\n\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_norm) - 1) / x_norm\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm = jnp.maximum(1.0, x_norm)\n  x_norm_inv = 1.0 / x_norm\n  x_norm_inv_sq = x_norm_inv * x_norm_inv\n  x_norm_inv_cube = x_norm_inv_sq * x_norm_inv\n  x_norm_inv_cube_mul_2 = x_norm_inv_cube * 2.0\n  x_norm_inv_cube_mul_2_sub_1 = x_norm_inv_cube_mul_2 - 1.0\n  x_norm_inv_cube_mul_2_sub_1_div_x_norm_inv_cube = x_norm_inv_cube_mul_2_sub_1 / x_norm_inv_cube\n  x_norm_inv_cube_mul_2_sub_1_div_x_norm_inv_cube_mul_x_norm = x_norm_inv_cube_mul_2_sub_1_div_x_norm_inv_cube * x_norm_inv\n  x_norm_inv_cube_mul_2_sub_1_div_x_norm_inv_cube_mul_x_norm_mul_x = x_norm_inv_cube_mul_2_sub_1_div_x_norm_inv_cube_mul_x_norm * x\n  return x_norm_inv_cube_mul_2_sub_1_div_x_norm_inv_cube_mul_x_norm_mul_x\n\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm = jnp.maximum(1, x_norm)\n  x_norm_squared = x_norm ** 2\n  scale = (2 * jnp.sqrt(x_norm_squared) - 1) / x_norm_squared\n  z = scale * x\n  return z\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path)\n\n    for col in dict_columns:\n        df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n\n    return df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Load the summary file\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert dictionary-like strings to actual dictionaries\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column_name in dict_columns:\n        summary_df[column_name] = summary_df[column_name].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Load the summary file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert dictionary-like strings in the specified columns to actual dictionaries\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column_name in dict_columns:\n        summary_df[column_name] = summary_df[column_name].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Load the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path, sep='\\t')\n\n    # Convert dictionary-like strings in specified columns to actual dictionaries\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path, sep='\\t', index_col=0)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Read the summary file\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert dictionary-like strings to actual dictionaries\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path)\n    for col in dict_columns:\n        df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n    return df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Read the summary file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # If the dict_columns argument is not provided, use the default value\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Iterate over each column in the specified dict_columns\n    for column in dict_columns:\n        # Convert the column values to dictionary objects\n        summary_df[column] = summary_df[column].apply(ast.literal_eval)\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    isotropic_cov = cov * det\n  elif mode == 'accurate':\n    log_det = jnp.linalg.slogdet(cov)[1]\n    isotropic_cov = cov * jnp.exp(log_det)\n  else:\n    raise ValueError(f'Invalid mode {mode}.')\n\n  return isotropic_cov\n\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the covariance matrix\n    det = jnp.linalg.det(cov)\n\n    # Compute the isotropic covariance matrix\n    isotropic_cov = det * jnp.eye(cov.shape[-1])\n\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the covariance matrix\n    log_det = jnp.linalg.slogdet(cov)[1]\n\n    # Compute the isotropic covariance matrix\n    isotropic_cov = jnp.exp(log_det) * jnp.eye(cov.shape[-1])\n\n  else:\n    raise ValueError(f\"Invalid mode: {mode}. Supported modes are 'fast' and 'accurate'.\")\n\n  return isotropic_cov\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the isotropic covariance matrix using the determinant of the input covariance matrix.\n    # This mode is faster, but may produce invalid determinants or logarithms of determinants.\n    cov_diag = jnp.diagonal(cov, axis1=-2, axis2=-1)\n    cov_diag = jnp.maximum(cov_diag, 0)  # Clip negative eigenvalues to 0.\n    cov_diag = jnp.sqrt(cov_diag)\n    cov_diag = jnp.expand_dims(cov_diag, axis=-1)\n    cov_diag = jnp.repeat(cov_diag, cov.shape[-1], axis=-1)\n    cov_diag = jnp.expand_dims(cov_diag, axis=-2)\n    cov_diag = jnp.repeat(cov_diag, cov.shape[-2], axis=-2)\n    cov_diag = jnp.expand_dims(cov_diag, axis=-3)\n    cov_diag = jnp.repeat(cov_diag, cov.shape[-3], axis=-3)\n    cov_diag = jnp.expand_dims(cov_diag, axis=-4)\n    cov_diag = jnp.repeat(cov_diag, cov.shape[-4], axis=-4)\n    cov_diag = jnp.expand_dims(cov_diag, axis=-5)\n    cov_diag = jnp.repeat(cov_diag, cov.shape[-5], axis=-5)\n    cov_diag = jnp.expand_dims(cov_diag, axis=-6)\n    cov_diag = jnp.repeat(cov_diag, cov.shape[-6], axis=-6)\n    cov_diag = jnp.expand_dims(cov_diag, axis=-7)\n    cov_diag = jnp.repeat(cov_diag, cov.shape[-7], axis=-7)\n    cov_"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    scale = jnp.sqrt(det)\n    return cov / scale\n  elif mode == 'accurate':\n    log_det = jnp.linalg.slogdet(cov)[1]\n    scale = jnp.exp(log_det / 2)\n    return cov / scale\n  else:\n    raise ValueError('Invalid mode.')\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix or matrices.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is zero or negative.\n  if det <= 0:\n    # If the determinant is zero or negative, return a matrix with all elements set to zero.\n    return jnp.zeros_like(cov)\n\n  # Compute the square root of the determinant.\n  sqrt_det = jnp.sqrt(det)\n\n  # Compute the square root of the covariance matrix or matrices.\n  sqrt_cov = sqrtm(cov)\n\n  # Compute the isotropic covariance matrix or matrices.\n  if mode == 'fast':\n    # If the mode is 'fast', use the determinant directly.\n    isotropic_cov = sqrt_cov * sqrt_det\n  elif mode == 'accurate':\n    # If the mode is 'accurate', use the logarithm of the determinant for stability.\n    isotropic_cov = sqrt_cov * jnp.log(sqrt_det)\n  else:\n    raise ValueError(f'Invalid mode: {mode}.')\n\n  return isotropic_cov\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    isotropic_cov = det * jnp.eye(cov.shape[-1])\n  elif mode == 'accurate':\n    log_det = jnp.linalg.slogdet(cov)[1]\n    isotropic_cov = jnp.exp(log_det) * jnp.eye(cov.shape[-1])\n  else:\n    raise ValueError(f'Invalid mode: {mode}. Must be either \"fast\" or \"accurate\".')\n  return isotropic_cov\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the covariance matrix\n    det = jnp.linalg.det(cov)\n    # Compute the isotropic covariance matrix\n    cov_iso = det * jnp.eye(cov.shape[-1])\n    return cov_iso\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the covariance matrix\n    logdet = jnp.linalg.slogdet(cov)[1]\n    # Compute the isotropic covariance matrix\n    cov_iso = jnp.exp(logdet) * jnp.eye(cov.shape[-1])\n    return cov_iso\n  else:\n    raise ValueError(f\"Invalid mode: {mode}\")\n\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the isotropic covariance matrix using the determinant of the input covariance matrix.\n    cov_iso = cov * jnp.linalg.det(cov) ** (-0.5)\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrix using the logarithm of the determinant of the input covariance matrix.\n    cov_iso = cov * jnp.exp(-0.5 * jnp.linalg.slogdet(cov)[1])\n  else:\n    raise ValueError(f'Invalid mode: {mode}')\n\n  # Check if the determinant of the isotropic covariance matrix is non-zero.\n  if jnp.linalg.det(cov_iso) == 0:\n    raise ValueError('Determinant of the isotropic covariance matrix is zero.')\n\n  return cov_iso\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'accurate':\n    # Compute the determinant of the covariance matrix\n    det = jnp.linalg.det(cov)\n    # Compute the square root of the determinant\n    sqrt_det = jnp.sqrt(det)\n    # Compute the inverse of the square root of the determinant\n    inv_sqrt_det = 1 / sqrt_det\n    # Compute the isotropic covariance matrix\n    cov_iso = inv_sqrt_det * cov * inv_sqrt_det\n    return cov_iso\n  elif mode == 'fast':\n    # Compute the determinant of the covariance matrix\n    det = jnp.linalg.det(cov)\n    # Compute the isotropic covariance matrix\n    cov_iso = det * cov\n    return cov_iso\n  else:\n    raise ValueError(\n        f\"Invalid mode: {mode}. Please choose either 'accurate' or 'fast'.\"\n    )\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the isotropic covariance matrix using the determinant directly.\n    cov_diag = jnp.diagonal(cov, axis1=-2, axis2=-1)\n    cov_diag_sqrt = jnp.sqrt(cov_diag)\n    cov_diag_sqrt = jnp.expand_dims(cov_diag_sqrt, axis=-1)\n    cov_diag_sqrt = jnp.expand_dims(cov_diag_sqrt, axis=-1)\n    cov_isotropic = cov / cov_diag_sqrt\n    cov_isotropic = cov_isotropic / cov_diag_sqrt\n    return cov_isotropic\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrix using the logarithm of the determinant for stability.\n    cov_diag = jnp.diagonal(cov, axis1=-2, axis2=-1)\n    cov_diag_sqrt = jnp.sqrt(cov_diag)\n    cov_diag_sqrt = jnp.expand_dims(cov_diag_sqrt, axis=-1)\n    cov_diag_sqrt = jnp.expand_dims(cov_diag_sqrt, axis=-1)\n    cov_isotropic = cov / cov_diag_sqrt\n    cov_isotropic = cov_isotropic / cov_diag_sqrt\n    cov_isotropic = cov_isotropic / jnp.log(cov_diag)\n    cov_isotropic = cov_isotropic / jnp.log(cov_diag)\n    return cov_isotropic\n  else:\n    raise ValueError('Invalid mode specified.')\n\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    isotrop_cov = cov * (det ** (-1 / 2))\n  elif mode == 'accurate':\n    logdet = jnp.linalg.slogdet(cov)[1]\n    isotrop_cov = cov * (jnp.exp(-0.5 * logdet))\n  else:\n    raise ValueError('Invalid mode. Choose from \"fast\" or \"accurate\".')\n\n  return isotrop_cov\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    if det < 0:\n      raise ValueError('Invalid determinant.')\n    sqrt_det = jnp.sqrt(det)\n    return cov / sqrt_det\n  elif mode == 'accurate':\n    log_det = jnp.linalg.slogdet(cov)[1]\n    if log_det < 0:\n      raise ValueError('Invalid determinant.')\n    sqrt_det = jnp.exp(log_det / 2)\n    return cov / sqrt_det\n  else:\n    raise ValueError('Invalid mode.')\n\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the covariance matrix\n    det = jnp.linalg.det(cov)\n    # Compute the isotropic covariance matrix\n    cov_iso = jnp.sqrt(det) * jnp.eye(cov.shape[-1])\n    return cov_iso\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the covariance matrix\n    logdet = jnp.linalg.slogdet(cov)[1]\n    # Compute the isotropic covariance matrix\n    cov_iso = jnp.sqrt(jnp.exp(logdet)) * jnp.eye(cov.shape[-1])\n    return cov_iso\n  else:\n    raise ValueError('Invalid mode. Must be either \"fast\" or \"accurate\".')\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    isotropic_cov = cov * det**(-1/2)\n  elif mode == 'accurate':\n    log_det = jnp.log(jnp.linalg.det(cov))\n    isotropic_cov = cov * jnp.exp(-log_det / 2)\n  else:\n    raise ValueError(f'Invalid mode: {mode}')\n\n  if jnp.any(jnp.isnan(isotropic_cov)):\n    raise ValueError(f'Invalid determinant: {jnp.linalg.det(cov)}')\n\n  return isotropic_cov\n\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the covariance matrix or matrices.\n    det = jnp.linalg.det(cov)\n    # Compute the isotropic covariance matrix or matrices using the determinant directly.\n    isotropic_cov = cov * det ** (-1 / cov.shape[-1])\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the covariance matrix or matrices.\n    logdet = jnp.linalg.slogdet(cov)[1]\n    # Compute the isotropic covariance matrix or matrices using the logarithm of the determinant for stability.\n    isotropic_cov = cov * jnp.exp(-logdet / cov.shape[-1])\n  else:\n    raise ValueError(f'Unknown mode: {mode}')\n\n  return isotropic_cov\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    isotropic_cov = (det / jnp.sqrt(cov.shape[-1])) * jnp.eye(cov.shape[-1])\n  elif mode == 'accurate':\n    log_det = jnp.linalg.slogdet(cov)[1]\n    isotropic_cov = jnp.exp(log_det / 2) * jnp.eye(cov.shape[-1])\n  else:\n    raise ValueError(f\"Invalid mode: {mode}. Choose from 'fast' or 'accurate'.\")\n\n  return isotropic_cov\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    cov_det = jnp.linalg.det(cov)\n    if cov_det <= 0:\n      raise ValueError('Covariance matrix determinant must be positive.')\n    cov_scale = jnp.sqrt(cov_det)\n    return cov / cov_scale\n  elif mode == 'accurate':\n    cov_logdet = jnp.linalg.slogdet(cov)[1]\n    if cov_logdet <= 0:\n      raise ValueError('Covariance matrix logarithm of determinant must be positive.')\n    cov_scale = jnp.exp(cov_logdet / 2)\n    return cov / cov_scale\n  else:\n    raise ValueError(f'Invalid mode: {mode}.')\n\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is valid\n  if jnp.any(jnp.isnan(det)) or jnp.any(jnp.isinf(det)):\n    raise ValueError(\"Invalid determinant encountered.\")\n\n  # Compute the isotropic covariance matrix using the determinant\n  if mode == 'fast':\n    isotropic_cov = cov / det\n  elif mode == 'accurate':\n    isotropic_cov = cov * jnp.exp(-0.5 * jnp.log(det))\n  else:\n    raise ValueError(\"Invalid mode specified.\")\n\n  # Check if the isotropic covariance matrix is valid\n  if jnp.any(jnp.isnan(isotropic_cov)) or jnp.any(jnp.isinf(isotropic_cov)):\n    raise ValueError(\"Invalid isotropic covariance matrix encountered.\")\n\n  return isotropic_cov\n\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'accurate':\n    det = jnp.linalg.det(cov)\n    if jnp.any(det <= 0):\n      raise ValueError('Covariance matrices must have positive determinants.')\n    isotropic_cov = cov / jnp.sqrt(det)\n  elif mode == 'fast':\n    isotropic_cov = cov\n  else:\n    raise ValueError(f'Unknown mode {mode}.')\n\n  return isotropic_cov\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'accurate':\n    log_det_cov = jnp.linalg.slogdet(cov)[1]\n    return jnp.exp(log_det_cov) * jnp.eye(cov.shape[-1])\n  elif mode == 'fast':\n    det_cov = jnp.linalg.det(cov)\n    return det_cov * jnp.eye(cov.shape[-1])\n  else:\n    raise ValueError(f'Invalid mode: {mode}')\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent')\n\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts,"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Command line interface for XAgent.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent\")\n\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        required=True,\n        help=\"The task description, specifying what task should be performed.\",\n    )\n\n    parser.add_argument(\n        \"--upload-files\",\n        type=str,\n        nargs=\"+\",\n        default=[],\n        help=\"List of files to upload, allowing multiple files to be specified.\",\n    )\n\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        default=\"\",\n        help=\"Model identifier for the task, specifying which model to use.\",\n    )\n\n    parser.add_argument(\n        \"--record-dir\",\n        type=str,\n        default=\"\",\n        help=\"Directory to record task execution logs, specifying where to save the logs.\",\n    )\n\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"auto\",\n        choices=[\"auto\", \"manual\"],\n        help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\",\n    )\n\n    parser.add_argument(\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"If set, the program runs in quiet mode with minimal output.\",\n    )\n\n    parser.add_argument(\n        \"--max-subtask-chain-length\",\n        type=int,\n        default=0,\n        help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\",\n    )\n\n    parser.add_argument(\n        \"--enable-ask-human-for-help\",\n        action=\"store_true\",\n        help=\"Flag to enable asking for human assistance during task execution.\",\n    )\n\n    parser.add_argument(\n        \"--max-plan-refine-chain-length\",\n        type=int,\n        default=0,\n        help=\"Maximum length of plan refinement chain,"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent\")\n    parser.add_argument('--task', type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument('--upload-files', type=str, nargs='*', help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument('--model', type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument('--record-dir', type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument('--mode', type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument('--quiet', action='store_true', help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument('--max-subtask-chain-length', type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument('--max-plan-tree-depth', type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument('--max-plan-tree-width', type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument('--max-retry-times', type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n   "}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent: A tool for executing tasks.')\n\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Max"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent: An Agent for Multi-Agent Reinforcement Learning\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent Command Line Interface\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs='+', help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent: A tool for automated task execution\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent Command Line Interface\")\n\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        required=True,\n        help=\"The task description, specifying what task should be performed.\"\n    )\n\n    parser.add_argument(\n        \"--upload-files\",\n        type=str,\n        nargs=\"+\",\n        default=[],\n        help=\"List of files to upload, allowing multiple files to be specified.\"\n    )\n\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        default=\"\",\n        help=\"Model identifier for the task, specifying which model to use.\"\n    )\n\n    parser.add_argument(\n        \"--record-dir\",\n        type=str,\n        default=\"\",\n        help=\"Directory to record task execution logs, specifying where to save the logs.\"\n    )\n\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"auto\",\n        help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\"\n    )\n\n    parser.add_argument(\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"If set, the program runs in quiet mode with minimal output.\"\n    )\n\n    parser.add_argument(\n        \"--max-subtask-chain-length\",\n        type=int,\n        default=10,\n        help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\"\n    )\n\n    parser.add_argument(\n        \"--enable-ask-human-for-help\",\n        action=\"store_true\",\n        help=\"Flag to enable asking for human assistance during task execution.\"\n    )\n\n    parser.add_argument(\n        \"--max-plan-refine-chain-length\",\n        type=int,\n        default=10,\n        help=\"Maximum length of plan refinement chain, specifying the limit for ref"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent: A Command Line Interface for Task Execution and Planning\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent - A framework for developing and executing AI agents.\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent: A general-purpose agent for automated task execution.\")\n\n    parser.add_argument('--task', type=str, required=True, help=\"Task description, specifying what task should be performed.\")\n    parser.add_argument('--upload-files', type=str, nargs='+', help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument('--model', type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument('--record-dir', type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument('--mode', type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument('--quiet', action='store_true', help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument('--max-subtask-chain-length', type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument('--max-plan-tree-depth', type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument('--max-plan-tree-width', type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument('--max-retry-times', type=int, help=\"Maximum number of"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent: A Multi-Agent System for Task Execution\")\n    parser.add_argument('--task', type=str, required=True, help='Task description, specifying what task should be performed')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='List of files to upload, allowing multiple files to be specified')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent: An Autonomous Agent for Extracting Knowledge from Text\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent: A command line tool for managing and executing tasks.\")\n\n    # Add task argument\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n\n    # Add upload-files argument\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n\n    # Add model argument\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n\n    # Add record-dir argument\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n\n    # Add mode argument\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n\n    # Add quiet argument\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n\n    # Add max-subtask-chain-length argument\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n\n    # Add enable-ask-human-for-help argument\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n\n    # Add max-plan-refine-chain-length argument\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n\n    # Add max-plan-tree-depth argument\n    parser.add_argument(\"--max-"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent')\n\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='Files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, which can be \\'auto\\' or \\'manual\\'')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    args = parser.parse_args()\n\n    return args\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--task\", type=str, required=True)\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", default=[])\n    parser.add_argument(\"--model\", type=str, default=None)\n    parser.add_argument(\"--record-dir\", type=str, default=None)\n    parser.add_argument(\"--mode\", type=str, default=\"auto\")\n    parser.add_argument(\"--quiet\", action=\"store_true\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, default=None)\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, default=None)\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, default=None)\n    parser.add_argument(\"--max-plan-tree-width\", type=int, default=None)\n    parser.add_argument(\"--max-retry-times\", type=int, default=None)\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"))\n\n    return parser.parse_args()\n\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', default=[], help='Files to upload')\n    parser.add_argument('--model', type=str, default=None, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, default=None, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, default=5, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, default=5, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, default=5, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, default=5, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, default=3, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=CONFIG, help='Configuration file')\n    args = parser.parse_args()\n    return args\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent CLI')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='Upload files')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Record directory')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum subtask chain length')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum plan refinement chain length')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum plan tree depth')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum plan tree width')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum retry times')\n    parser.add_argument('--config-file', type=str, default=os.getenv(\"CONFIG_FILE\", \"assets/config.yml\"), help='Configuration file')\n\n    args = parser.parse_args()\n    return args\n\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must have shape (_, 2). got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must have shape (_, 2). got {v.shape}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    characters = string.ascii_lowercase + string.digits + \"_\"\n\n    # Calculate the number of characters in the character set\n    num_chars = len(characters)\n\n    # Encode the integer into a single character\n    encoded_char = \"\"\n    while n > 0:\n        encoded_char = characters[n % num_chars] + encoded_char\n        n = n // num_chars\n\n    return encoded_char\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    chars = string.ascii_lowercase + string.digits + \"_\"\n    # Check if the input is a valid integer\n    if not isinstance(n, int):\n        raise ValueError(\"Input must be an integer.\")\n    # Check if the input is within the range of the character set\n    if n < 0 or n >= len(chars):\n        raise ValueError(\"Input must be within the range of the character set.\")\n    # Encode the integer into a single character\n    return chars[n]\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    char_set = string.ascii_lowercase + string.digits + \"_\"\n    encoded_char = char_set[n]\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    characters = string.ascii_lowercase + string.digits + \"_\"\n\n    # Convert the input integer to a string\n    n_str = str(n)\n\n    # Initialize the encoded character\n    encoded_char = \"\"\n\n    # Loop through the digits of the input integer\n    for digit in n_str:\n        # Get the index of the digit in the character set\n        index = int(digit)\n\n        # Add the corresponding character to the encoded character\n        encoded_char += characters[index]\n\n    # Return the encoded character\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    characters = string.ascii_lowercase + string.digits + \"_\"\n    if n < 0:\n        raise ValueError(\"Input integer must be non-negative.\")\n    if n == 0:\n        return characters[0]\n    encoded = \"\"\n    while n > 0:\n        encoded = characters[n % len(characters)] + encoded\n        n //= len(characters)\n    return encoded\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    characters = string.ascii_lowercase + string.digits + \"_\"\n    if n < 0:\n        raise ValueError(\"Input integer must be non-negative\")\n    if n == 0:\n        return characters[0]\n    result = \"\"\n    while n > 0:\n        result = characters[n % len(characters)] + result\n        n = n // len(characters)\n    return result\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    character_set = string.ascii_lowercase + string.digits + '_'\n\n    # Calculate the number of characters in the character set\n    num_characters = len(character_set)\n\n    # Calculate the number of digits in the input integer\n    num_digits = len(str(n))\n\n    # Initialize the encoded string\n    encoded_string = ''\n\n    # Encode the integer into a single character\n    for i in range(num_digits):\n        # Calculate the index of the character in the character set\n        index = n % num_characters\n\n        # Append the character to the encoded string\n        encoded_string = character_set[index] + encoded_string\n\n        # Divide the input integer by the number of characters in the character set\n        n = n // num_characters\n\n    # Return the encoded string\n    return encoded_string\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    characters = string.ascii_lowercase + string.digits + \"_\"\n    # Initialize the encoded string\n    encoded = \"\"\n    # Encode the integer using the character set\n    while n > 0:\n        encoded = characters[n % len(characters)] + encoded\n        n //= len(characters)\n    # Return the encoded string\n    return encoded\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    character_set = string.ascii_lowercase + string.digits + \"_\"\n\n    # Calculate the length of the character set\n    character_set_length = len(character_set)\n\n    # Initialize the encoded string\n    encoded_string = \"\"\n\n    # Encode the integer into a single character\n    while n > 0:\n        # Calculate the remainder of the integer division by the length of the character set\n        remainder = n % character_set_length\n\n        # Add the corresponding character to the encoded string\n        encoded_string = character_set[remainder] + encoded_string\n\n        # Divide the integer by the length of the character set\n        n //= character_set_length\n\n    # Return the encoded string\n    return encoded_string\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    characters = string.ascii_lowercase + string.digits + \"_\"\n    # Initialize an empty string to store the encoded character\n    encoded_char = \"\"\n    # Calculate the number of characters in the character set\n    num_chars = len(characters)\n    # Iterate until the input integer is zero\n    while n > 0:\n        # Calculate the remainder when dividing the input integer by the number of characters in the character set\n        remainder = n % num_chars\n        # Append the character at the calculated index to the encoded string\n        encoded_char = characters[remainder] + encoded_char\n        # Divide the input integer by the number of characters in the character set\n        n = n // num_chars\n    # Return the encoded string\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    character_set = string.ascii_lowercase + string.digits + \"_\"\n    if n < 0:\n        raise ValueError(\"Input integer must be non-negative.\")\n    if n == 0:\n        return character_set[0]\n    result = \"\"\n    while n > 0:\n        result = character_set[n % len(character_set)] + result\n        n //= len(character_set)\n    return result\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    charset = string.ascii_lowercase + string.digits + '_'\n\n    # Check if the input is a valid integer\n    if not isinstance(n, int):\n        raise TypeError(\"Input must be an integer.\")\n\n    # Check if the input is a valid index for the character set\n    if n < 0 or n >= len(charset):\n        raise ValueError(\"Input must be a valid index for the character set.\")\n\n    # Encode the integer into a single character\n    encoded_char = charset[n]\n\n    return encoded_char\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    charset = string.ascii_lowercase + string.digits + \"_\"\n    # Calculate the number of possible encodings\n    num_encodings = len(charset)\n    # Calculate the number of digits in the input integer\n    num_digits = len(str(n))\n    # Calculate the number of possible encodings for the input integer\n    num_encodings_for_input = num_encodings ** num_digits\n    # Check if the input integer is within the range of possible encodings\n    if n < 0 or n >= num_encodings_for_input:\n        raise ValueError(f\"Input integer {n} is out of range for encoding. The maximum possible encoding is {num_encodings_for_input - 1}.\")\n    # Encode the input integer into a single character\n    encoded_char = \"\"\n    for i in range(num_digits):\n        # Calculate the index of the character to use for the current digit\n        char_index = n % num_encodings\n        # Add the character to the encoded string\n        encoded_char = charset[char_index] + encoded_char\n        # Update the input integer for the next digit\n        n = n // num_encodings\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    character_set = string.ascii_lowercase + string.digits + '_'\n    # Calculate the number of possible characters in the character set\n    num_characters = len(character_set)\n    # Encode the integer using the character set\n    encoded_character = ''\n    while n > 0:\n        # Calculate the remainder of the integer division by the number of characters\n        remainder = n % num_characters\n        # Convert the remainder to a character using the character set\n        encoded_character = character_set[remainder] + encoded_character\n        # Divide the integer by the number of characters\n        n = n // num_characters\n    return encoded_character\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    char_set = string.ascii_lowercase + string.digits + \"_\"\n    # Calculate the number of characters in the character set\n    char_set_len = len(char_set)\n    # Initialize the encoded string\n    encoded_str = \"\"\n    # Encode the integer into a single character\n    while n > 0:\n        # Get the remainder of the integer divided by the number of characters in the character set\n        rem = n % char_set_len\n        # Append the corresponding character to the encoded string\n        encoded_str = char_set[rem] + encoded_str\n        # Divide the integer by the number of characters in the character set\n        n = n // char_set_len\n    # Return the encoded string\n    return encoded_str\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = string.ascii_lowercase + string.digits + \"_\"\n\n    # Calculate the number of characters in the character set\n    num_chars = len(char_set)\n\n    # Initialize an empty string to store the encoded character\n    encoded_char = \"\"\n\n    # Encode the integer into a single character\n    while n > 0:\n        # Get the remainder of n divided by the number of characters in the character set\n        remainder = n % num_chars\n\n        # Append the character at the corresponding index to the encoded character string\n        encoded_char = char_set[remainder] + encoded_char\n\n        # Divide n by the number of characters in the character set\n        n //= num_chars\n\n    # Return the encoded character\n    return encoded_char\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    char_set = string.ascii_lowercase + string.digits + \"_\"\n\n    # Initialize the encoded string\n    encoded_str = \"\"\n\n    # Encode the integer\n    while n > 0:\n        # Get the remainder when dividing by the number of characters in the character set\n        remainder = n % len(char_set)\n\n        # Add the character at the selected index to the encoded string\n        encoded_str = char_set[remainder] + encoded_str\n\n        # Divide the integer by the number of characters in the character set\n        n = n // len(char_set)\n\n    return encoded_str\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    char_set = string.ascii_lowercase + string.digits + \"_\"\n    # Initialize the result string\n    result = \"\"\n    # Loop until n is less than the length of the character set\n    while n < len(char_set):\n        # Append the character at the index n to the result string\n        result += char_set[n]\n        # Divide n by the length of the character set\n        n //= len(char_set)\n    # Return the reversed result string\n    return result[::-1]\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    character_set = string.ascii_lowercase + string.digits + '_'\n    # Calculate the number of characters in the character set\n    num_characters = len(character_set)\n    # Calculate the number of digits in the integer representation of n\n    num_digits = len(str(n))\n    # Calculate the number of characters needed to represent n in the character set\n    num_characters_needed = num_digits\n    # Initialize the result string\n    result = ''\n    # Loop until we have encoded all of n\n    while n > 0:\n        # Calculate the remainder when n is divided by the number of characters in the character set\n        remainder = n % num_characters\n        # Add the character corresponding to the remainder to the result string\n        result = character_set[remainder] + result\n        # Divide n by the number of characters in the character set\n        n = n // num_characters\n    # Pad the result string with underscores until it has the correct length\n    result = result.ljust(num_characters_needed, '_')\n    return result\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    chars = string.ascii_lowercase + string.digits + \"_\"\n    # Calculate the number of digits in the integer\n    num_digits = len(str(n))\n    # Calculate the index of the character in the character set\n    char_index = n % len(chars)\n    # Calculate the position of the character in the integer\n    char_pos = num_digits - 1\n    # Encode the character\n    encoded_char = chars[char_index]\n    # Add the character to the encoded string\n    encoded_str = encoded_char\n    # Repeat the process for the remaining digits\n    n = n // len(chars)\n    while n > 0:\n        char_index = n % len(chars)\n        char_pos += 1\n        encoded_char = chars[char_index] + encoded_char\n        encoded_str = encoded_char\n        n = n // len(chars)\n    return encoded_str\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.where(x > eps, jnp.log(x), jnp.log(jnp.full_like(x, value_at_zero)))\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        chunk_index = 0\n        for interval in intervals:\n            if current_index >= interval[1] - interval[0]:\n                current_index -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n        indexes[worker_idx] = current_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in worker_intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        chunk_index = 0\n        for i, interval in enumerate(worker_intervals):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        chunk_index = 0\n        for interval in intervals:\n            if current_index >= interval[1] - interval[0]:\n                current_index -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n        indexes[worker_idx] = current_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for i, interval in enumerate(intervals):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index = i + 1\n                indexes[worker_idx] = indexes[worker_idx] - (interval[1] - interval[0])\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_index = 0\n        for interval in workers_intervals[worker_idx]:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_index = 0\n        for interval in workers_intervals[worker_idx]:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        chunk_index = 0\n        for interval in worker_intervals:\n            if current_index >= interval[1] - interval[0]:\n                current_index = current_index - (interval[1] - interval[0])\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n        indexes[worker_idx] = current_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        chunk_index = 0\n        while current_index >= worker_intervals[chunk_index][1] - worker_intervals[chunk_index][0]:\n            current_index -= worker_intervals[chunk_index][1] - worker_intervals[chunk_index][0]\n            chunk_index += 1\n        chunks_index[worker_idx] = chunk_index\n        indexes[worker_idx] = current_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        chunk_index = 0\n        for i, interval in enumerate(worker_intervals):\n            if current_index < interval[1]:\n                chunk_index = i\n                break\n            current_index -= interval[1] - interval[0]\n        chunks_index[worker_idx] = chunk_index\n        indexes[worker_idx] = current_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        current_index = 0\n        chunk_index = 0\n        for interval in worker_intervals:\n            if current_index + interval[1] - interval[0] > indexes[worker_idx]:\n                break\n            current_index += interval[1] - interval[0]\n            chunk_index += 1\n        chunks_index[worker_idx] = chunk_index\n        indexes[worker_idx] = indexes[worker_idx] - current_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunks_index[worker_idx] = 0\n        for i, interval in enumerate(workers_intervals[worker_idx]):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunks_index[worker_idx] = i + 1\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        chunk_index = 0\n        for interval in intervals:\n            if current_index < interval[1]:\n                break\n            else:\n                chunk_index += 1\n                current_index -= interval[1] - interval[0]\n        chunks_index[worker_idx] = chunk_index\n        indexes[worker_idx] = current_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in worker_intervals:\n            if indexes[worker_idx] < interval[1]:\n                break\n            chunk_index += 1\n        chunks_index[worker_idx] = chunk_index\n\n    for worker_idx, worker_intervals in workers_intervals.items():\n        for i, interval in enumerate(worker_intervals):\n            if i == chunks_index[worker_idx]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                indexes[worker_idx] -= interval[1] - interval[0]\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in worker_intervals:\n            if indexes[worker_idx] < interval[1] - interval[0]:\n                break\n            chunk_index += 1\n        chunks_index[worker_idx] = chunk_index\n\n        indexes[worker_idx] -= worker_intervals[chunk_index][1] - worker_intervals[chunk_index][0]\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust the coordinates to be within the bounds of the voxel grid.\n    coordinates = jnp.clip(coordinates, 0, 1)\n    # Perform trilinear interpolation on the voxel grid.\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    # Perform trilinear interpolation on the hashed data structure.\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates to be within the bounds of the grid.\n    coordinates = jnp.clip(coordinates, 0, 1)\n    # Perform trilinear interpolation on the voxel grid.\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    # Perform trilinear interpolation on the hash data structure.\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError('Invalid datastructure.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust the coordinates to be within the grid bounds.\n    coordinates = jnp.clip(coordinates, 0, 1)\n    # Sample from the voxel grid using trilinear interpolation.\n    return resample.trilerp(values, coordinates)\n\n  elif datastructure == 'hash':\n    # Adjust the coordinates to be within the hash bounds.\n    coordinates = jnp.clip(coordinates, 0, 1)\n    # Sample from the hash data structure using trilinear interpolation.\n    return hash_resample.trilerp(values, coordinates)\n\n  else:\n    raise ValueError(f'Invalid datastructure {datastructure}.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure {datastructure}')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust the coordinates to be within the bounds of the grid.\n    coordinates = jnp.clip(coordinates, 0, 1)\n\n    # Sample from the grid using trilinear interpolation.\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    # Adjust the coordinates to be within the bounds of the hash.\n    coordinates = jnp.clip(coordinates, 0, 1)\n\n    # Sample from the hash using trilinear interpolation.\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust the coordinates to be within the bounds of the grid\n    coordinates = jnp.clip(coordinates, 0, values.shape[0] - 1)\n    # Perform trilinear interpolation on the grid\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    # Adjust the coordinates to be within the bounds of the hash\n    coordinates = jnp.clip(coordinates, 0, values.shape[0] - 1)\n    # Perform trilinear interpolation on the hash\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(\n        f'Invalid datastructure: {datastructure}. Expected \"grid\" or \"hash\".'\n    )\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError('Invalid datastructure')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates to be within the grid bounds\n    coordinates = jnp.clip(coordinates, 0, values.shape[0] - 1)\n\n    # Perform trilinear interpolation\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    # Adjust coordinates to be within the hash bounds\n    coordinates = jnp.clip(coordinates, 0, values.shape[0] - 1)\n\n    # Perform trilinear interpolation\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Unsupported datastructure: {datastructure}')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  # Adjust the coordinates to be within the bounds of the dimensions they refer to.\n  coordinates = math.clamp(coordinates, 0.0, 1.0)\n\n  # If datastructure is 'grid', sample from a 3D voxel grid.\n  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n\n  # If datastructure is 'hash', sample from a hashed data structure.\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates)\n\n  # Raise an error if an invalid datastructure is passed.\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n\n\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  # Adjust coordinates to be within the bounds of the dimensions they refer to.\n  coordinates = jnp.clip(coordinates, 0, 1)\n\n  # If datastructure is 'grid', perform trilinear interpolation on the voxel grid.\n  if datastructure == 'grid':\n    # Shift the coordinates to be within the bounds of the voxel grid.\n    coordinates = coordinates * (values.shape[0] - 1)\n\n    # Compute the floor and ceil coordinates for each dimension.\n    coordinates_floor = jnp.floor(coordinates)\n    coordinates_ceil = jnp.ceil(coordinates)\n\n    # Compute the weights for each dimension.\n    weights_x = coordinates[:, 0] - coordinates_floor[:, 0]\n    weights_y = coordinates[:, 1] - coordinates_floor[:, 1]\n    weights_z = coordinates[:, 2] - coordinates_floor[:, 2]\n\n    # Compute the indices for the voxel grid.\n    indices_x = jnp.clip(jnp.floor(coordinates_floor[:, 0]), 0, values.shape[0] - 1).astype(jnp.int32)\n    indices_y = jnp.clip(jnp.floor(coordinates_floor[:, 1]), 0, values.shape[1] - 1).astype(jnp.int32)\n    indices_z = jnp.clip(jnp.floor(coordinates_floor[:, 2]), 0, values.shape[2] - 1).astype(jnp.int32)\n\n    # Compute the indices for the voxel grid.\n    indices_x_ceil = jnp.clip(jnp.ceil(coordinates_ceil[:, 0]), 0, values.shape[0] - 1).astype(jnp.int32)\n    indices_y_ceil = jnp.clip(jnp.ceil(coordinates_ceil[:, 1]), 0, values.shape[1] - 1).astype(jnp"}
{"namespace": "grid_utils.trilerp", "completion": "  # Adjust coordinates to be within the bounds of the dimensions they refer to.\n  coordinates = jnp.clip(coordinates, 0, 1)\n\n  # Determine the appropriate sampling method based on the datastructure parameter.\n  if datastructure == 'grid':\n    # Perform trilinear interpolation on the 3D voxel grid.\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    # Sample from the hash data structure using trilinear interpolation.\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    # Raise an error if an invalid datastructure is passed.\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n\n\n\n\n\n\n\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust the coordinates to be within the bounds of the voxel grid.\n    coordinates = jnp.clip(coordinates, 0, 1)\n    # Perform trilinear interpolation on the voxel grid.\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    # Perform trilinear interpolation on the hashed data structure.\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(\n        f'Invalid datastructure {datastructure}, should be either \"grid\" or \"hash\".'\n    )\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust the coordinates to be within the bounds of the voxel grid.\n    coordinates = jnp.clip(coordinates, 0, 1)\n\n    # Perform trilinear interpolation.\n    return resample.trilerp(values, coordinates)\n\n  elif datastructure == 'hash':\n    # Adjust the coordinates to be within the bounds of the hash data structure.\n    coordinates = jnp.clip(coordinates, 0, 1)\n\n    # Perform trilinear interpolation.\n    return hash_resample.trilerp(values, coordinates)\n\n  else:\n    raise ValueError(\n        f'Unsupported datastructure: {datastructure}. Only \"grid\" or \"hash\" are supported.'\n    )\n\n\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure {datastructure}')\n\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates to be within the bounds of the 3D voxel grid.\n    coordinates = jnp.clip(coordinates, 0, 1)\n    # Perform trilinear interpolation on the 3D voxel grid.\n    return resample.trilinear(values, coordinates)\n\n  elif datastructure == 'hash':\n    # Adjust coordinates to be within the bounds of the hash data structure.\n    coordinates = jnp.clip(coordinates, 0, 1)\n    # Perform trilinear interpolation on the hash data structure.\n    return hash_resample.trilinear(values, coordinates)\n\n  else:\n    raise ValueError(\n        f'Invalid datastructure: {datastructure}. Must be either \"grid\" or \"hash\".'\n    )\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  # Adjust coordinates to match the dimensions of the data structure.\n  if datastructure == 'grid':\n    coordinates = coordinates + 0.5\n  elif datastructure == 'hash':\n    coordinates = coordinates\n  else:\n    raise ValueError('Only grid or hash datastructure supported')\n\n  # Perform trilinear interpolation based on the data structure type.\n  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError('Only grid or hash datastructure supported')\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    values = values.transpose(3, 0, 1, 2)\n    values = values.reshape((values.shape[0], -1, values.shape[2], values.shape[3]))\n    values = values.transpose(1, 0, 2, 3)\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(f'Unsupported datastructure: {datastructure}')\n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the barycentric weights for tessellating the vertices of a triangle by a given factor.\n  # This involves generating integer weights for each vertex of the triangle and then normalizing these weights to get the barycentric coordinates.\n  #\n  # Input-Output Arguments\n  # :param v: Int, the tessellation factor, which determines how finely the triangle is subdivided. It must be greater than or equal to 1.\n  # :return: Numpy array, the barycentric weights for each point in the tessellated triangle. These weights are used to interpolate values across the triangle.\n\n  # Compute the barycentric weights for tessellating the vertices of a triangle by a given factor.\n  # This involves generating integer weights for each vertex of the triangle and then normalizing these weights to get the barycentric coordinates.\n  #\n  # Input-Output Arguments\n  # :param v: Int, the tessellation factor, which determines how finely the triangle is subdivided. It must be greater than or equal to 1.\n  # :return: Numpy array, the barycentric weights for each point in the tessellated triangle. These weights are used to interpolate values across the triangle.\n\n  # Compute the barycentric weights for tessellating the vertices of a triangle by a given factor.\n  # This involves generating integer weights for each vertex of the triangle and then normalizing these weights to get the barycentric coordinates.\n  #\n  # Input-Output Arguments\n  # :param v: Int, the tessellation factor, which determines how finely the triangle is subdivided. It must be greater than or equal to 1.\n  # :return: Numpy array, the barycentric weights for each point in the tessellated triangle. These weights are used to interpolate values across the triangle.\n\n  # Compute the barycentric weights for tessellating the vertices of a triangle by a given factor.\n  # This involves generating integer weights for each vertex of the triangle and then normalizing these weights to get the barycentric coordinates.\n  #\n  # Input"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the barycentric weights for tessellating the vertices of a triangle by a given factor.\n  # This involves generating integer weights for each vertex of the triangle and then normalizing these weights to get the barycentric coordinates.\n  # Input-Output Arguments\n  # :param v: Int, the tessellation factor, which determines how finely the triangle is subdivided. It must be greater than or equal to 1.\n  # :return: Numpy array, the barycentric weights for each point in the tessellated triangle. These weights are used to interpolate values across the triangle.\n  #\n  # Example\n  # >>> compute_tesselation_weights(1)\n  # array([[0.33333333, 0.33333333, 0.33333333]])\n  # >>> compute_tesselation_weights(2)\n  # array([[0.25, 0.25, 0.25, 0.25],\n  #        [0.25, 0.25, 0.25, 0.25],\n  #        [0.25, 0.25, 0.25, 0.25],\n  #        [0.25, 0.25, 0.25, 0.25]])\n  # >>> compute_tesselation_weights(3)\n  # array([[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n  #        [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n  #        [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n  #        [0.2, 0.2,"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError('Tessellation factor must be greater than or equal to 1.')\n  if v == 1:\n    return np.array([1, 0, 0])\n  if v == 2:\n    return np.array([1 / 3, 1 / 3, 1 / 3])\n  if v == 3:\n    return np.array([1 / 4, 1 / 4, 1 / 2])\n  if v == 4:\n    return np.array([3 / 10, 3 / 10, 1 / 2])\n  if v == 5:\n    return np.array([1 / 5, 1 / 5, 1 / 2])\n  if v == 6:\n    return np.array([1 / 6, 1 / 6, 1 / 3])\n  if v == 7:\n    return np.array([1 / 7, 1 / 7, 1 / 3])\n  if v == 8:\n    return np.array([1 / 8, 1 / 8, 1 / 3])\n  if v == 9:\n    return np.array([1 / 9, 1 / 9, 1 / 3])\n  if v == 10:\n    return np.array([1 / 10, 1 / 10, 1 / 3])\n  if v == 11:\n    return np.array([1 / 11, 1 / 11, 1 / 3])\n  if v == 12:\n    return np.array([1 / 12, 1 / 12, 1 / 3])\n  if v == 13:\n    return np.array([1 / 13, 1 / 13, 1 / 3])\n  if v == 14:\n    return np.array([1 / 14, 1 / 14, 1 / 3])\n  if v == 15:\n    return np.array([1 / 15, 1 / 15, "}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the integer weights for each vertex of the triangle.\n  weights = np.array(list(itertools.product(range(v + 1), repeat=3)))\n  weights = weights[weights.sum(axis=1) <= v]\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / weights.sum(axis=1)[:, None]\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the barycentric weights for tessellating the vertices of a triangle by a given factor.\n  # This involves generating integer weights for each vertex of the triangle and then normalizing these weights to get the barycentric coordinates.\n\n  # Input-Output Arguments\n  # v: Int, the tessellation factor, which determines how finely the triangle is subdivided. It must be greater than or equal to 1.\n  # return: Numpy array, the barycentric weights for each point in the tessellated triangle. These weights are used to interpolate values across the triangle.\n\n  # Generate the weights for each vertex of the triangle\n  w = np.array([[i, j, v - i - j] for i in range(v + 1) for j in range(v + 1 - i)])\n\n  # Normalize the weights to get the barycentric coordinates\n  w = w / np.sum(w, axis=1)[:, None]\n\n  return w\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the integer weights for each vertex of the triangle\n  weights = np.array(list(itertools.product(range(v+1), repeat=3)))\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=1)[:, None]\n\n  return weights\n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the integer weights for each vertex of the triangle\n  weights = np.array(list(itertools.product(range(v + 1), repeat=3)))\n  weights = weights[weights.sum(axis=1) <= v]\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=1)[:, None]\n\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the weights for each vertex of the triangle\n  weights = np.zeros((v + 1, v + 1))\n  for i in range(v + 1):\n    for j in range(v + 1):\n      if i + j <= v:\n        weights[i, j] = (v + 1 - i - j) / v\n\n  # Normalize the weights to get the barycentric coordinates\n  weights /= np.sum(weights)\n\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the barycentric weights for tessellating the triangle\n  # by the given factor.\n  #\n  # The weights are computed by generating integer weights for each vertex of the triangle,\n  # and then normalizing these weights to get the barycentric coordinates.\n  #\n  # The weights are returned as a numpy array of shape (v**3, 3).\n  # The first dimension corresponds to the number of points in the tessellated triangle,\n  # and the second dimension corresponds to the barycentric coordinates.\n  #\n  # The weights are computed by generating integer weights for each vertex of the triangle,\n  # and then normalizing these weights to get the barycentric coordinates.\n  #\n  # The weights are returned as a numpy array of shape (v**3, 3).\n  # The first dimension corresponds to the number of points in the tessellated triangle,\n  # and the second dimension corresponds to the barycentric coordinates.\n  #\n  # The weights are computed by generating integer weights for each vertex of the triangle,\n  # and then normalizing these weights to get the barycentric coordinates.\n  #\n  # The weights are returned as a numpy array of shape (v**3, 3).\n  # The first dimension corresponds to the number of points in the tessellated triangle,\n  # and the second dimension corresponds to the barycentric coordinates.\n  #\n  # The weights are computed by generating integer weights for each vertex of the triangle,\n  # and then normalizing these weights to get the barycentric coordinates.\n  #\n  # The weights are returned as a numpy array of shape (v**3, 3).\n  # The first dimension corresponds to the number of points in the tessellated triangle,\n  # and the second dimension corresponds to the barycentric coordinates.\n  #\n  # The weights are computed by generating integer weights for each vertex of the triangle,\n  # and then normalizing these weights to get the barycentric coordinates.\n  #\n  # The weights are returned as a numpy array of shape (v**3, 3).\n  # The first dimension corresponds to the"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the integer weights for each vertex of the triangle\n  weights = np.array([[i, j, v - i - j] for i in range(v + 1) for j in range(v + 1 - i)])\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=1)[:, np.newaxis]\n\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the barycentric weights for tessellating the triangle by the given factor\n  weights = np.zeros((3, v**2))\n  for i in range(v):\n    for j in range(v):\n      weights[0, i*v+j] = (i+1)/v\n      weights[1, i*v+j] = (j+1)/v\n      weights[2, i*v+j] = 1 - weights[0, i*v+j] - weights[1, i*v+j]\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(\"Tessellation factor must be >= 1.\")\n  if v == 1:\n    return np.array([1, 1, 1]) / 3\n  # Generate integer weights for each vertex of the triangle.\n  weights = np.array(\n      list(itertools.product(range(v + 1), range(v + 1 - i) for i in range(3))))\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights, axis=1, keepdims=True)\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  n = v + 1\n  weights = np.zeros((n, n, n))\n  for i, j, k in itertools.product(range(n), range(n), range(n)):\n    if i + j + k == n:\n      weights[i, j, k] = 1\n  return weights / n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid.\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the integer weights for each vertex of the triangle.\n  weights = np.zeros((3, v + 1, v + 1))\n  for i in range(3):\n    for j in range(v + 1):\n      for k in range(v + 1):\n        if j + k <= v:\n          weights[i, j, k] = 1\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights /= np.sum(weights, axis=(1, 2), keepdims=True)\n\n  return weights\n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate integer weights for each vertex of the triangle\n  weights = np.array([\n      [i, j, v - i - j]\n      for i in range(v + 1)\n      for j in range(v + 1 - i)\n  ])\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=1)[:, np.newaxis]\n\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the integer weights for each vertex of the triangle.\n  w = np.array([np.arange(v + 1) for _ in range(3)])\n  # Reshape the weights into a 3D array.\n  w = np.stack(np.meshgrid(w[0], w[1], w[2]), axis=-1)\n  # Normalize the weights to get the barycentric coordinates.\n  w = w / v\n  # Reshape the weights into a 2D array.\n  w = np.reshape(w, (-1, 3))\n  # Normalize the weights to get the barycentric coordinates.\n  w = w / np.sum(w, axis=1)[:, None]\n  return w\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Generate the integer weights for each vertex of the triangle.\n  weights = np.array([\n      [0, 0, 0],\n      [0, 0, v],\n      [0, v, 0],\n      [0, v, v],\n      [v, 0, 0],\n      [v, 0, v],\n      [v, v, 0],\n      [v, v, v],\n  ])\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights, axis=1)[:, None]\n\n  return weights\n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the weights for each vertex of the triangle\n  weights = np.array([np.arange(v + 1) for _ in range(3)])\n  weights = weights.T\n  weights = weights.reshape(-1, 3)\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=1)[:, None]\n\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError('Tessellation factor must be greater than or equal to 1.')\n\n  # Generate the weights for each vertex of the triangle\n  weights = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0]])\n  weights = weights * v\n  weights = np.round(weights).astype(int)\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=1)[:, np.newaxis]\n\n  return weights\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n  if v == 1:\n    return np.array([1, 1, 1]) / 3\n\n  # Compute the weights for the vertices of the triangle.\n  weights = np.zeros((3, v**3))\n  for i in range(3):\n    for j in range(v**2):\n      weights[i, j] = (j % v) / v\n    for j in range(v**2, v**3):\n      weights[i, j] = (v - (j % v)) / v\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights, axis=1, keepdims=True)\n  return weights\n\n"}
{"namespace": "linspline.query", "completion": "  # Ensure that the spline is valid.\n  check_zero_endpoints(v)\n\n  # Interpolate the spline at the query points.\n  vq = jnp.interp(tq, t, v)\n\n  # Set extrapolated values to 0.\n  vq = jnp.where(jnp.logical_or(tq < t[0], tq > t[-1]), 0, vq)\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  # ensure the spline is valid\n  check_zero_endpoints(v)\n\n  # ensure the query points are within the range of the spline\n  tq = jnp.clip(tq, t[0], t[-1])\n\n  # interpolate the spline at the query points\n  vq = jnp.interp(tq, t, v)\n\n  # set extrapolated values to 0\n  vq = jnp.where(tq < t[0], 0, vq)\n  vq = jnp.where(tq > t[-1], 0, vq)\n\n  return vq\n\n\n"}
{"namespace": "linspline.query", "completion": "  # Ensure spline is valid.\n  check_zero_endpoints(v)\n\n  # Interpolate.\n  tq = jnp.atleast_1d(tq)\n  t = jnp.atleast_1d(t)\n  v = jnp.atleast_1d(v)\n  tq = jnp.expand_dims(tq, -1)\n  t = jnp.expand_dims(t, 0)\n  v = jnp.expand_dims(v, 0)\n  s = (tq - t) / (t[1] - t[0])\n  s = jnp.clip(s, 0, 1)\n  return jnp.sum(v * s, axis=-1)\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(tq.ndim == t.ndim == v.ndim == 1,\n                 'tq, t, and v must be 1-dimensional.')\n  checkify.check(tq.shape[0] == t.shape[0] == v.shape[0],\n                 'tq, t, and v must have the same length.')\n  checkify.check(jnp.all(tq >= t[0]) and jnp.all(tq <= t[-1]),\n                 'tq must be within the range of t.')\n  check_zero_endpoints(v)\n\n  tq = jnp.clip(tq, t[0], t[-1])\n  m = (v[1:] - v[:-1]) / (t[1:] - t[:-1])\n  b = v[:-1] - m * t[:-1]\n  return m * tq + b\n\n"}
{"namespace": "linspline.query", "completion": "  t = jnp.array(t)\n  v = jnp.array(v)\n  tq = jnp.array(tq)\n  checkify.check(t.ndim == 1, 't must be 1D.')\n  checkify.check(v.ndim == 1, 'v must be 1D.')\n  checkify.check(tq.ndim == 1, 'tq must be 1D.')\n  checkify.check(t.shape[0] == v.shape[0], 't and v must have the same length.')\n  checkify.check(jnp.all(t[1:] > t[:-1]), 't must be strictly increasing.')\n  check_zero_endpoints(v)\n\n  # Extrapolate to 0 outside the original range.\n  v = jnp.concatenate([v[0] * jnp.ones(1), v, v[-1] * jnp.ones(1)])\n  t = jnp.concatenate([t[0] * jnp.ones(1), t, t[-1] * jnp.ones(1)])\n\n  # Interpolate.\n  vq = jnp.interp(tq, t, v)\n\n  # Extrapolate to 0 outside the original range.\n  vq = jnp.where(tq < t[0], 0, vq)\n  vq = jnp.where(tq > t[-1], 0, vq)\n\n  return vq\n\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(len(t) == len(v), 't and v must have the same length.')\n  check_zero_endpoints(v)\n\n  # If the query point is before the first time point, return 0.\n  vq = jnp.where(tq <= t[0], 0.0, vq)\n\n  # If the query point is after the last time point, return 0.\n  vq = jnp.where(tq >= t[-1], 0.0, vq)\n\n  # Interpolate the values at the query points.\n  vq = jnp.interp(tq, t, v)\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(tq.shape == t.shape, 'tq and t must have the same shape.')\n  checkify.check(tq.shape == v.shape, 'tq and v must have the same shape.')\n  checkify.check(jnp.all(jnp.diff(t) >= 0), 't must be sorted.')\n  check_zero_endpoints(v)\n\n  # Interpolate the spline at the query points.\n  tq = jnp.maximum(tq, t[0])\n  tq = jnp.minimum(tq, t[-1])\n  vq = jnp.interp(tq, t, v)\n\n  # Extrapolate the spline to 0 outside the original range.\n  vq = jnp.where(tq < t[0], 0, vq)\n  vq = jnp.where(tq > t[-1], 0, vq)\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(len(t) == len(v), 'Length of t and v must be equal.')\n  checkify.check(jnp.all(tq >= t[0]), 'Query points must be >= start time.')\n  checkify.check(jnp.all(tq <= t[-1]), 'Query points must be <= end time.')\n  checkify.check(jnp.all(jnp.diff(t) > 0), 'Times must be strictly increasing.')\n  check_zero_endpoints(v)\n\n  tq_ind = jnp.searchsorted(t, tq)\n  tq_ind = jnp.minimum(tq_ind, len(t) - 1)\n  tq_ind = jnp.maximum(tq_ind, 0)\n\n  tq_ind_p1 = tq_ind + 1\n  tq_ind_m1 = tq_ind - 1\n\n  tq_ind_p1 = jnp.minimum(tq_ind_p1, len(t) - 1)\n  tq_ind_m1 = jnp.maximum(tq_ind_m1, 0)\n\n  tq_ind_p1 = jnp.minimum(tq_ind_p1, len(t) - 1)\n  tq_ind_m1 = jnp.maximum(tq_ind_m1, 0)\n\n  tq_ind_p1 = jnp.minimum(tq_ind_p1, len(t) - 1)\n  tq_ind_m1 = jnp.maximum(tq_ind_m1, 0)\n\n  tq_ind_p1 = jnp.minimum(tq_ind_p1, len(t) - 1)\n  tq_ind_m1 = jnp.maximum(tq_ind_m1, 0)\n\n  tq_ind_p1 = jnp.minimum(tq_ind_p1, len(t) - 1)\n  tq"}
{"namespace": "linspline.query", "completion": "  t = jnp.asarray(t)\n  v = jnp.asarray(v)\n  tq = jnp.asarray(tq)\n\n  checkify.check(tq.ndim == 1, 'tq must be a vector.')\n  checkify.check(t.ndim == 1, 't must be a vector.')\n  checkify.check(v.ndim == 1, 'v must be a vector.')\n  checkify.check(t.size == v.size, 't and v must have the same length.')\n\n  check_zero_endpoints(v)\n\n  tq_clipped = jnp.clip(tq, t[0], t[-1])\n\n  vq = jnp.interp(tq_clipped, t, v)\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  # Ensure that the spline is valid.\n  check_zero_endpoints(v)\n\n  # Interpolate the spline at the query points.\n  tq = jnp.asarray(tq)\n  t = jnp.asarray(t)\n  v = jnp.asarray(v)\n  tq = jnp.clip(tq, t[0], t[-1])\n  tq = tq - t[0]\n  t = t - t[0]\n  v = v / jnp.diff(t)\n  v = jnp.concatenate([v[0], v, v[-1]], axis=-1)\n  v = jnp.take(v, jnp.searchsorted(t, tq), axis=-1)\n  v = jnp.diff(v, axis=-1)\n  v = v * (tq - t[:, None]) + v[..., :-1]\n  return v\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(jnp.all(tq >= t[0]), 'Query points must be >= start of spline.')\n  checkify.check(jnp.all(tq <= t[-1]), 'Query points must be <= end of spline.')\n  checkify.check(jnp.all(jnp.diff(t) > 0), 'Time points must be strictly increasing.')\n  checkify.check(jnp.all(v[1:] - v[:-1] > 0), 'Values must be strictly increasing.')\n\n  # Interpolate.\n  tq = jnp.clip(tq, t[0], t[-1])\n  i = jnp.searchsorted(t, tq, side='right') - 1\n  i = jnp.clip(i, 0, t.size - 2)\n  t0 = t[i]\n  t1 = t[i + 1]\n  v0 = v[i]\n  v1 = v[i + 1]\n  return (tq - t0) / (t1 - t0) * (v1 - v0) + v0\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(tq.ndim == t.ndim, 'tq and t must have the same number of dimensions.')\n  checkify.check(tq.ndim == v.ndim, 'tq and v must have the same number of dimensions.')\n  checkify.check(tq.shape[-1] == t.shape[-1], 'tq and t must have the same last dimension.')\n  checkify.check(tq.shape[-1] == v.shape[-1], 'tq and v must have the same last dimension.')\n  checkify.check(jnp.all(tq >= t[Ellipsis, 0]), 'tq must be greater than or equal to the first value in t.')\n  checkify.check(jnp.all(tq <= t[Ellipsis, -1]), 'tq must be less than or equal to the last value in t.')\n\n  check_zero_endpoints(v)\n\n  t_diff = jnp.diff(t, axis=-1)\n  v_diff = jnp.diff(v, axis=-1)\n  t_diff_inv = 1. / t_diff\n  v_diff_inv = t_diff_inv * v_diff\n\n  v_diff_inv_cumsum = jnp.cumsum(v_diff_inv, axis=-1)\n  v_diff_inv_cumsum_inv = 1. / v_diff_inv_cumsum\n\n  t_diff_inv_cumsum = jnp.cumsum(t_diff_inv, axis=-1)\n  t_diff_inv_cumsum_inv = 1. / t_diff_inv_cumsum\n\n  t_diff_inv_cumsum_inv_sq = t_diff_inv_cumsum_inv ** 2\n  v_diff_inv_cumsum_inv_sq = v_diff_inv_cumsum_inv ** 2\n\n  t_diff_inv_cumsum_inv_sq_cumsum = jnp.cum"}
{"namespace": "linspline.query", "completion": "  checkify.check(tq.ndim == 1, 'tq must be 1D.')\n  checkify.check(t.ndim == 1, 't must be 1D.')\n  checkify.check(v.ndim == 1, 'v must be 1D.')\n  checkify.check(tq.shape[0] == t.shape[0], 't and tq must have the same length.')\n  checkify.check(tq.shape[0] == v.shape[0], 't and v must have the same length.')\n  checkify.check(jnp.all(jnp.diff(t) > 0), 't must be sorted and non-decreasing.')\n\n  # Ensure the spline is valid.\n  check_zero_endpoints(v)\n\n  # Find the indices of the time points that are greater than or equal to each query point.\n  i = jnp.searchsorted(t, tq, side='right')\n\n  # Clip the indices to the valid range of time points.\n  i = jnp.clip(i, 1, t.shape[0] - 1)\n\n  # Interpolate the values at the query points.\n  return utils.interpolate(t[i - 1], t[i], v[i - 1], v[i], tq)\n\n"}
{"namespace": "linspline.query", "completion": "  t = jnp.array(t)\n  v = jnp.array(v)\n  tq = jnp.array(tq)\n\n  checkify.check(t.ndim == 1, 't must be a 1d array.')\n  checkify.check(v.ndim == 1, 'v must be a 1d array.')\n  checkify.check(tq.ndim == 1, 'tq must be a 1d array.')\n\n  checkify.check(jnp.all(t[1:] >= t[:-1]), 't must be strictly increasing.')\n  checkify.check(jnp.all(v[1:] >= v[:-1]), 'v must be non-decreasing.')\n\n  check_zero_endpoints(v)\n\n  t = t[None, :]\n  v = v[None, :]\n  tq = tq[:, None]\n\n  # Interpolate.\n  vq = math.interp(tq, t, v)\n\n  # Set values outside the range to 0.\n  vq = jnp.where(tq < t[0, 0], 0, vq)\n  vq = jnp.where(tq > t[0, -1], 0, vq)\n\n  return vq.squeeze()\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(jnp.all(tq >= t[0]), 'Query points must be >= first time point.')\n  checkify.check(jnp.all(tq <= t[-1]), 'Query points must be <= last time point.')\n  check_zero_endpoints(v)\n\n  # Compute the slope of the linear spline at each knot.\n  slope = jnp.diff(v) / jnp.diff(t)\n\n  # Compute the y-intercept of the linear spline at each knot.\n  intercept = v[:-1] - slope * t[:-1]\n\n  # Find the knot index for each query point.\n  knot_idx = jnp.searchsorted(t, tq, side='right') - 1\n  knot_idx = jnp.clip(knot_idx, 0, len(t) - 2)\n\n  # Compute the interpolated values at each query point.\n  yq = slope[knot_idx] * (tq - t[knot_idx]) + intercept[knot_idx]\n\n  # Set extrapolated values to 0.\n  yq = yq.at[tq < t[0]].set(0)\n  yq = yq.at[tq > t[-1]].set(0)\n\n  return yq\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(jnp.all(tq <= t[-1]), 'tq must be <= t[-1].')\n  checkify.check(jnp.all(tq >= t[0]), 'tq must be >= t[0].')\n  checkify.check(len(t) == len(v), 't and v must have the same length.')\n\n  # If the query points are outside the range of the spline, return 0.\n  # If the query points are inside the range of the spline, interpolate between the nearest points.\n  vq = jnp.where(tq < t[0], 0, v[0])\n  vq = jnp.where(tq > t[-1], 0, vq)\n  vq = jnp.where(tq >= t[0] & tq <= t[-1],\n                 (v[1:] - v[:-1]) / (t[1:] - t[:-1]) * (tq - t[:-1]) + v[:-1],\n                 vq)\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(jnp.all(tq >= t[0]), 'Query points must be >= to the start time.')\n  checkify.check(jnp.all(tq <= t[-1]), 'Query points must be <= to the end time.')\n  checkify.check(t.ndim == 1, 'Time points must be 1-dimensional.')\n  checkify.check(tq.ndim == 1, 'Query points must be 1-dimensional.')\n  checkify.check(v.ndim == 1, 'Values must be 1-dimensional.')\n  checkify.check(t.shape == v.shape, 'Time and value arrays must have the same shape.')\n  checkify.check(tq.shape[0] == v.shape[0], 'Query points and values must have the same length.')\n  checkify.check(t.shape[0] >= 2, 'Splines must have at least 2 points.')\n\n  check_zero_endpoints(v)\n\n  # If the query points are outside the range of the time points, set them to 0.\n  vq = jnp.where(tq < t[0], 0, v)\n  vq = jnp.where(tq > t[-1], 0, vq)\n\n  # Interpolate the values at the query points using linear interpolation.\n  vq = jnp.interp(tq, t, v)\n\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  # Check the spline is valid\n  check_zero_endpoints(v)\n\n  # Get the number of query points\n  nq = tq.shape[-1]\n\n  # Get the number of time points\n  nt = t.shape[-1]\n\n  # Get the number of splines\n  ns = v.shape[-2]\n\n  # Ensure the time points are sorted\n  t = jnp.sort(t)\n\n  # Check that the query points are within the range of the time points\n  checkify.check(jnp.all(tq >= t[0]), 'Query points must be greater than or equal to the first time point.')\n  checkify.check(jnp.all(tq <= t[-1]), 'Query points must be less than or equal to the last time point.')\n\n  # Ensure the query points are sorted\n  tq = jnp.sort(tq)\n\n  # Get the indices of the time points that are less than or equal to the query points\n  i = jnp.searchsorted(t, tq, side='right')\n\n  # Get the indices of the time points that are greater than or equal to the query points\n  j = jnp.searchsorted(t, tq, side='left')\n\n  # Get the indices of the time points that are less than or equal to the query points, with an offset of 1\n  i1 = jnp.maximum(i - 1, 0)\n\n  # Get the indices of the time points that are greater than or equal to the query points, with an offset of 1\n  j1 = jnp.minimum(j + 1, nt - 1)\n\n  # Get the values of the time points that are less than or equal to the query points\n  ti = t[i]\n\n  # Get the values of the time points that are greater than or equal to the query points\n  tj = t[j]\n\n  # Get the values of the time points that are less than or equal to the query points, with an offset of 1\n  ti1 = t[i1]\n\n "}
{"namespace": "linspline.query", "completion": "  checkify.check_equal(t.shape, v.shape)\n  checkify.check_equal(tq.ndim, 1)\n  checkify.check_equal(t.ndim, 1)\n  checkify.check(jnp.all(t[1:] > t[:-1]), 'Spline must be monotonic.')\n  check_zero_endpoints(v)\n\n  tq = jnp.asarray(tq)\n  t = jnp.asarray(t)\n  v = jnp.asarray(v)\n\n  # Find the indices of the knots that bracket each query point.\n  indices = jnp.searchsorted(t, tq)\n  indices = jnp.clip(indices, 1, t.size - 1)\n  indices = jnp.stack([indices - 1, indices], axis=-1)\n\n  # Interpolate between the knots to find the values at the query points.\n  tq = jnp.expand_dims(tq, -1)\n  t = jnp.expand_dims(t, 0)\n  v = jnp.expand_dims(v, 0)\n  t_diff = t[Ellipsis, indices[Ellipsis, 1]] - t[Ellipsis, indices[Ellipsis, 0]]\n  t_diff = jnp.where(t_diff == 0, 1.0, t_diff)\n  t_diff = jnp.expand_dims(t_diff, -1)\n  v_diff = v[Ellipsis, indices[Ellipsis, 1]] - v[Ellipsis, indices[Ellipsis, 0]]\n  v_diff = jnp.expand_dims(v_diff, -1)\n  t_diff_inv = 1.0 / t_diff\n  v_diff_inv = 1.0 / v_diff\n  t_diff_inv = jnp.where(t_diff_inv == jnp.inf, 0.0, t_diff_inv"}
{"namespace": "linspline.query", "completion": "  checkify.check(t.ndim == v.ndim, 't and v must have same number of dimensions.')\n  checkify.check(t.ndim == 1, 't must be 1-dimensional.')\n  checkify.check(t.shape == v.shape, 't and v must have the same shape.')\n  checkify.check(t.shape[0] > 1, 't must have at least 2 elements.')\n  checkify.check(jnp.all(t[1:] > t[:-1]), 't must be strictly increasing.')\n  check_zero_endpoints(v)\n\n  # Find the indices of the time points that bracket the query points.\n  i0 = jnp.searchsorted(t, tq, side='right') - 1\n  i1 = jnp.minimum(i0 + 1, t.shape[0] - 1)\n\n  # Find the time points and values that bracket the query points.\n  t0 = t[i0]\n  t1 = t[i1]\n  v0 = v[i0]\n  v1 = v[i1]\n\n  # Compute the interpolated values at the query points.\n  vq = (v1 - v0) / (t1 - t0) * (tq - t0) + v0\n\n  # Set extrapolated values to 0.\n  vq = jnp.where(tq < t0, 0., vq)\n  vq = jnp.where(tq > t1, 0., vq)\n\n  return vq\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for value in v:\n            if value <= 0:\n                raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(val > 0 for val in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(val > 0 for val in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    # Check if v is a single value or an iterable\n    if isinstance(v, (int, float)):\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    elif isinstance(v, Iterable):\n        for val in v:\n            if val <= 0:\n                raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for value in v:\n            if value <= 0:\n                raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(val > 0 for val in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive. got {v}\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive. got {v}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not isinstance(v, Iterable):\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if any(value <= 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(val > 0 for val in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive. got {v}\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive. got {v}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not np.all(v > 0):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.array(origins, dtype=xnp.float32)\n  directions = xnp.array(directions, dtype=xnp.float32)\n  pixtocam = xnp.array(pixtocam, dtype=xnp.float32)\n\n  # Adjust ray origins to the near plane.\n  origins = origins + directions * near\n\n  # Calculate ray directions in NDC.\n  directions = xnp.einsum('ij,...j->...i', pixtocam, directions)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.asarray(origins)\n  directions = xnp.asarray(directions)\n  pixtocam = xnp.asarray(pixtocam)\n\n  # Adjust ray origins to the near plane.\n  origins = origins + directions * near\n\n  # Project ray origins and directions to NDC.\n  origins = xnp.einsum('ij,...j->...i', pixtocam, origins)\n  directions = xnp.einsum('ij,...j->...i', pixtocam, directions)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins + near * directions\n\n  # Calculate ray directions in NDC.\n  directions = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.array(origins)\n  directions = xnp.array(directions)\n  pixtocam = xnp.array(pixtocam)\n\n  # Adjust ray origins to the near plane\n  origins = origins + directions * near\n\n  # Calculate ray directions in NDC\n  directions = xnp.einsum(\"...ij,...j->...i\", pixtocam, directions)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins + directions * near\n\n  # Calculate ray directions in NDC.\n  directions = jax.vmap(xnp.dot)(directions, pixtocam)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.array(origins, dtype=xnp.float32)\n  directions = xnp.array(directions, dtype=xnp.float32)\n  pixtocam = xnp.array(pixtocam, dtype=xnp.float32)\n\n  # Adjust ray origins to the near plane.\n  origins = origins + directions * near\n\n  # Convert ray origins and directions to NDC.\n  origins = xnp.dot(origins, pixtocam)\n  directions = xnp.dot(directions, pixtocam)\n\n  # Normalize ray directions.\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.asarray(origins)\n  directions = xnp.asarray(directions)\n  pixtocam = xnp.asarray(pixtocam)\n\n  # Adjust ray origins to the near plane\n  origins = origins - directions * near\n\n  # Calculate ray directions in NDC\n  directions = xnp.dot(directions, pixtocam)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.array(origins)\n  directions = xnp.array(directions)\n  pixtocam = xnp.array(pixtocam)\n\n  # Move origins to the near plane.\n  origins = origins + directions * near\n\n  # Convert to NDC.\n  origins = xnp.einsum('ij,...j->...i', pixtocam, origins)\n  directions = xnp.einsum('ij,...j->...i', pixtocam, directions)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins + near * directions\n  # Calculate ray directions in NDC.\n  directions = xnp.dot(pixtocam, origins.T).T\n  # Normalize ray directions.\n  directions /= xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.asarray(origins)\n  directions = xnp.asarray(directions)\n  pixtocam = xnp.asarray(pixtocam)\n\n  # Adjust ray origins to the near plane.\n  origins = origins + near * directions\n\n  # Convert ray origins and directions to NDC.\n  origins = xnp.matmul(origins, pixtocam.T)\n  directions = xnp.matmul(directions, pixtocam.T)\n\n  # Normalize directions.\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins + directions * near\n\n  # Calculate the directions in NDC.\n  directions = xnp.dot(pixtocam, directions.T).T\n\n  # Normalize the directions.\n  directions = directions / xnp.linalg.norm(directions, axis=1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.array(origins, dtype=np.float32)\n  directions = xnp.array(directions, dtype=np.float32)\n  pixtocam = xnp.array(pixtocam, dtype=np.float32)\n\n  # Adjust ray origins to the near plane.\n  origins = origins + directions * near\n\n  # Calculate ray directions in NDC.\n  directions = (xnp.linalg.inv(pixtocam) @ directions.T).T\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins + near * directions\n\n  # Calculate ray directions in NDC.\n  directions = xnp.matmul(pixtocam, directions.transpose()).transpose()\n  directions = directions / directions[:, 3, None]\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert origins to the near plane.\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], axis=-1)\n  origins = origins[..., :3] / origins[..., 3:]\n  origins = origins * near\n\n  # Convert directions to NDC.\n  directions = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n  directions = directions[..., :3] / directions[..., 3:]\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust origins to near plane\n  origins = origins + near * directions\n\n  # Project to NDC\n  origins = xnp.dot(origins, pixtocam.T)\n  directions = xnp.dot(directions, pixtocam.T)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.asarray(origins)\n  directions = xnp.asarray(directions)\n  pixtocam = xnp.asarray(pixtocam)\n  near = float(near)\n\n  # Adjust ray origins to the near plane\n  origins = origins + directions * near\n\n  # Calculate ray directions in NDC\n  origins = xnp.matmul(origins, pixtocam.T)\n  origins = origins[:, :2] / origins[:, 2, None]\n\n  # Calculate ray directions in NDC\n  directions = xnp.matmul(directions, pixtocam.T)\n  directions = directions[:, :2] / directions[:, 2, None]\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins + near * directions\n  # Calculate the NDC origins.\n  origins = xnp.matmul(origins, pixtocam)\n  origins = origins / origins[..., 2:3]\n  # Calculate the NDC directions.\n  directions = xnp.matmul(directions, pixtocam)\n  directions = directions / directions[..., 2:3]\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins + near * directions\n\n  # Calculate ray directions in NDC.\n  directions = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.array(origins)\n  directions = xnp.array(directions)\n  pixtocam = xnp.array(pixtocam)\n\n  # Adjust ray origins to the near plane\n  origins = origins + directions * near\n\n  # Convert ray origins and directions to NDC\n  origins = xnp.dot(origins, pixtocam.T)\n  directions = xnp.dot(directions, pixtocam.T)\n\n  # Normalize directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins + near * directions\n\n  # Calculate the ray directions in NDC.\n  directions = xnp.einsum('ij,...j->...i', pixtocam, directions)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 (within a small epsilon)\n  epsilon = 1e-6\n  return jnp.abs(dot_product - 1.0) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.allclose(jnp.dot(dir1, dir2), 1.0, atol=1e-6)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.isclose(jnp.dot(dir1, dir2), 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.isclose(jnp.dot(dir1, dir2), 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1_norm = spin_math.normalize(dir1)\n  dir2_norm = spin_math.normalize(dir2)\n  return jnp.allclose(jnp.dot(dir1_norm, dir2_norm), 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.isclose(jnp.dot(dir1, dir2), 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Define a small epsilon value to account for numerical precision issues\n  epsilon = 1e-6\n\n  # Check if the dot product is close to 1 within the epsilon\n  return jnp.abs(dot_product - 1.0) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1.0 within a small epsilon\n  epsilon = 1e-6\n  return jnp.abs(dot_product - 1.0) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Compare the dot product with a small epsilon to account for numerical precision issues\n  return jnp.abs(dot_product) > 1.0 - 1e-6\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 (within a small epsilon)\n  epsilon = 1e-8\n  return jnp.abs(dot_product - 1.0) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors.\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors.\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 (within a small epsilon) or -1 (within a small epsilon).\n  return jnp.allclose(dot_product, 1.0, atol=1e-6) or jnp.allclose(dot_product, -1.0, atol=1e-6)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Define a small epsilon value to account for numerical precision issues\n  epsilon = 1e-6\n\n  # Check if the dot product is close to 1 (within the specified epsilon)\n  return jnp.abs(dot_product - 1.0) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Check if the dot product is within a small epsilon of 1\n  return jnp.abs(dot_product - 1) < 1e-6\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize direction vectors\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute dot product of normalized direction vectors\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Consider a small epsilon to account for numerical precision issues\n  epsilon = 1e-6\n\n  # Check if dot product is close to 1 or -1, indicating parallel lines\n  return jnp.abs(dot_product - 1.0) < epsilon or jnp.abs(dot_product + 1.0) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Set a small epsilon value to compare against the dot product\n  epsilon = 1e-6\n\n  # Check if the dot product is within the epsilon range of 1 or -1\n  return jnp.abs(dot_product - 1.0) < epsilon or jnp.abs(dot_product + 1.0) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  epsilon = 1e-12\n\n  return jnp.abs(jnp.dot(dir1, dir2)) >= 1.0 - epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  eps = 1e-5\n  return jnp.abs(jnp.dot(spin_math.normalize(dir1), spin_math.normalize(dir2))) > 1 - eps\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1_norm = spin_math.normalize(dir1)\n  dir2_norm = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.dot(dir1_norm, dir2_norm)\n\n  # Check if the dot product is close to 1 or -1\n  epsilon = 1e-8  # Adjust this value as needed\n  return jnp.abs(dot_product) >= 1.0 - epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  eps = 1e-6\n  dir1_norm = spin_math.normalize(dir1)\n  dir2_norm = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1_norm, dir2_norm)) > 1.0 - eps\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1.0 - 1e-6\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda x: jieba.lcut(x)\n    bleu = evaluate.load('bleu')\n    score = bleu.compute(predictions=[continuation], references=[reference], tokenizer=tokenizer)\n    if with_penalty:\n        score = bleu.compute(predictions=[continuation], references=[reference], tokenizer=tokenizer, smooth=True)\n    return score['bleu']\n\n"}
{"namespace": "common.bleu4_score", "completion": "    def tokenizer(text: str) -> list:\n        return jieba.lcut(text)\n\n    bleu = evaluate.load(\"bleu\")\n    score = bleu.compute(\n        predictions=[continuation],\n        references=[[reference]],\n        tokenizer=tokenizer,\n        max_order=4,\n        smooth=False\n    )\n\n    if with_penalty:\n        score = bleu.compute(\n            predictions=[continuation],\n            references=[[reference]],\n            tokenizer=tokenizer,\n            max_order=4,\n            smooth=True\n        )\n\n    return score[\"bleu\"]\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Define a custom tokenizer function\n    def tokenizer(text):\n        return list(jieba.cut(text))\n\n    # Initialize the BLEU scorer\n    bleu = evaluate.load(\"bleu\")\n\n    # Compute the BLEU-4 score\n    score = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer)[\"bleu\"]\n\n    # Adjust the score for brevity penalty, if requested\n    if with_penalty:\n        score = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer, smooth=False)[\"bleu\"]\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load(\"bleu\")\n    tokenizer = lambda x: jieba.lcut(x)\n\n    return bleu.compute(\n        predictions=[continuation],\n        references=[reference],\n        tokenizer=tokenizer,\n        smooth=with_penalty\n    )[\"bleu\"]\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using a custom tokenizer function\n    tokenizer = lambda x: jieba.lcut(x)\n    continuation_tokens = tokenizer(continuation)\n    reference_tokens = tokenizer(reference)\n\n    # Compute the BLEU-4 score using the evaluate library\n    bleu = evaluate.load(\"bleu\")\n    score = bleu.compute(predictions=[continuation_tokens], references=[[reference_tokens]])[\"bleu\"]\n\n    # Adjust the score for brevity penalty, if specified\n    if with_penalty:\n        score = adjust_bleu_score_for_brevity_penalty(score, continuation_tokens, reference_tokens)\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load(\"bleu\")\n    tokenizer = lambda text: jieba.lcut(text)\n\n    return bleu.compute(\n        predictions=[continuation],\n        references=[reference],\n        tokenizer=tokenizer,\n        smooth=with_penalty\n    )[\"bleu\"]\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda x: jieba.lcut(x)\n    bleu = evaluate.load(\"bleu\")\n    score = bleu.compute(\n        predictions=[continuation],\n        references=[reference],\n        tokenizer=tokenizer,\n        smooth=False,\n        max_order=4,\n        force=True\n    )\n    if with_penalty:\n        score = bleu.compute(\n            predictions=[continuation],\n            references=[reference],\n            tokenizer=tokenizer,\n            smooth=False,\n            max_order=4,\n            force=True,\n            smooth_method=\"exp\",\n            smooth_value=0.0,\n            force_len=True\n        )\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load('bleu')\n    tokenizer = lambda x: list(jieba.cut(x))\n    return bleu.compute(\n        predictions=[continuation],\n        references=[[reference]],\n        tokenizer=tokenizer,\n        smooth=with_penalty\n    )\n\n"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load('bleu')\n    tokenizer = lambda x: jieba.lcut(x)\n    score = bleu.compute(\n        predictions=[continuation],\n        references=[reference],\n        tokenizer=tokenizer,\n        smooth=with_penalty\n    )\n    return score['bleu']\n\n"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load(\"bleu\")\n    tokenizer = lambda x: jieba.lcut(x)\n    score = bleu.compute(\n        predictions=[continuation],\n        references=[reference],\n        tokenizer=tokenizer,\n        smooth=True\n    )\n    if with_penalty:\n        score = bleu.compute(\n            predictions=[continuation],\n            references=[reference],\n            tokenizer=tokenizer,\n            smooth=True,\n            use_effective_order=True\n        )\n    return score[\"bleu\"]\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input text using a custom tokenizer function\n    continuation_tokens = jieba.lcut(continuation)\n    reference_tokens = jieba.lcut(reference)\n\n    # Compute the BLEU-4 score using the evaluate library\n    bleu = evaluate.load(\"bleu\")\n    score = bleu.compute(predictions=[continuation_tokens], references=[reference_tokens])[\"bleu\"]\n\n    # Optionally include the brevity penalty in the score calculation\n    if with_penalty:\n        score = bleu.compute(predictions=[continuation_tokens], references=[reference_tokens],\n                             smooth=True)[\"bleu\"]\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using a custom tokenizer function\n    continuation_tokens = jieba.lcut(continuation)\n    reference_tokens = jieba.lcut(reference)\n\n    # Calculate the BLEU-4 score using the evaluate library\n    bleu_metric = evaluate.load(\"bleu\")\n    bleu_score = bleu_metric.compute(\n        predictions=[continuation_tokens],\n        references=[[reference_tokens]]\n    )\n\n    # Adjust the score for brevity penalty, if specified\n    if with_penalty:\n        bleu_score = bleu_score * min(\n            1,\n            len(reference_tokens) / len(continuation_tokens)\n        )\n\n    return bleu_score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Define a custom tokenizer function for the input texts\n    def tokenize(text):\n        return jieba.lcut(text)\n\n    # Compute the BLEU-4 score using the evaluate library\n    bleu = evaluate.load(\"bleu\")\n    score = bleu.compute(predictions=[continuation], references=[reference], tokenizer=tokenize)\n\n    # Adjust the score for brevity penalty, if specified\n    if with_penalty:\n        score = adjust_bleu_score_for_brevity(score, continuation, reference)\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    bleu_score = evaluate.load(\"bleu\")\n    return bleu_score.compute(predictions=[continuation], references=[[reference]], max_order=4, smooth=False, smooth_method='exp', force=False,\n                              alpha=0.5, beta=0.5, gamma=0.5,\n                              use_effective_order=False,\n                              with_penalty=with_penalty)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    def tokenize(text):\n        return [token for token in jieba.cut(text)]\n\n    bleu_score = evaluate.load(\"bleu\")\n    score = bleu_score.compute(\n        predictions=[continuation],\n        references=[[reference]],\n        tokenizer=tokenize,\n        smooth=False,\n    )\n    if with_penalty:\n        score = bleu_score.compute(\n            predictions=[continuation],\n            references=[[reference]],\n            tokenizer=tokenize,\n            smooth=True,\n        )\n\n    return score[\"bleu\"]\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = jieba.lcut\n    bleu = evaluate.load(\"bleu\")\n    score = bleu.compute(\n        predictions=[continuation],\n        references=[reference],\n        tokenizer=tokenizer,\n        smooth=False\n    )\n    if with_penalty:\n        score = bleu.compute(\n            predictions=[continuation],\n            references=[reference],\n            tokenizer=tokenizer,\n            smooth=False,\n            smooth_method=\"exp\"\n        )\n    return score[\"bleu\"]\n\n"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load('bleu')\n    tokenizer = lambda x: [y for y in jieba.cut(x)]\n\n    if with_penalty:\n        score = bleu.compute(\n            predictions=[continuation],\n            references=[[reference]],\n            tokenizer=tokenizer,\n            smooth=False,\n            use_effective_order=False\n        )\n    else:\n        score = bleu.compute(\n            predictions=[continuation],\n            references=[[reference]],\n            tokenizer=tokenizer,\n            smooth=False,\n            use_effective_order=False,\n            truncation=True\n        )\n\n    return score['bleu']\n\n"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load(\"bleu\")\n    tokenizer = lambda x: list(jieba.cut(x))\n\n    result = bleu.compute(\n        predictions=[continuation],\n        references=[[reference]],\n        tokenizer=tokenizer,\n        smooth=with_penalty\n    )\n\n    return result[\"bleu\"]\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Define a custom tokenizer function that splits the input text into a list of tokens.\n    def tokenize(text: str) -> list:\n        return list(jieba.cut(text))\n\n    # Create an instance of the evaluate.load() function to load the BLEU metric.\n    bleu = evaluate.load(\"bleu\")\n\n    # Compute the BLEU-4 score using the tokenized continuation and reference texts.\n    score = bleu.compute(\n        predictions=[continuation],\n        references=[[reference]],\n        tokenizer=tokenize\n    )\n\n    # If the with_penalty flag is True, adjust the score for brevity penalty.\n    if with_penalty:\n        score = score * (4 / len(continuation))\n\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Define a custom tokenizer function\n    def custom_tokenizer(text):\n        return list(jieba.cut(text))\n\n    # Initialize the BLEU score evaluator\n    bleu = evaluate.load(\"bleu\")\n\n    # Compute the BLEU-4 score\n    score = bleu.compute(\n        predictions=[continuation],\n        references=[[reference]],\n        tokenizer=custom_tokenizer,\n        smooth=False,\n        max_order=4,\n    )\n\n    # Optionally adjust for brevity penalty\n    if with_penalty:\n        score = adjust_bleu_score_for_brevity(score)\n\n    return score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))\n\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)\n\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.asarray(w)\n  t = jnp.asarray(t)\n  pdf = w / jnp.diff(t)\n  pdf = jnp.concatenate([pdf, [pdf[-1]]], axis=-1)\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Convert weights to PDF.\n  w = jnp.asarray(w)\n  w = w / jnp.diff(t)\n  w = jnp.concatenate([w, [0.]], axis=-1)\n  return w\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Compute the difference between consecutive elements in the input vector t\n  t_diff = np.diff(t)\n\n  # Divide the input vector w by the difference between consecutive elements in t\n  pdf = w / t_diff\n\n  # Return the resulting PDF\n  return pdf\n\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Convert weights to probabilities.\n  p = w / jnp.diff(t)\n  # Normalize probabilities to sum to 1.\n  p = p / jnp.sum(p)\n  return p\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Compute the difference between consecutive elements in the input vector t\n  t_diff = t[1:] - t[:-1]\n  # Divide the weights by the difference between consecutive elements in the input vector t\n  pdf = w / t_diff\n  # Return the resulting PDF\n  return pdf\n\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  w = np.array(w)\n  t = np.array(t)\n  pdf = w / np.diff(t)\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.array(w)\n  return w / jnp.diff(t)\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  w = np.array(w)\n  t = np.array(t)\n  n = len(t)\n  pdf = np.zeros(n)\n  for i in range(n):\n    if i == 0:\n      pdf[i] = w[i]\n    else:\n      pdf[i] = w[i] / (t[i] - t[i-1])\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  assert jnp.allclose(jnp.sum(w, axis=-1), 1.0), 'weights do not sum to 1'\n  assert jnp.all(jnp.diff(t) > 0), 'thresholds are not strictly increasing'\n  return w / jnp.diff(t, axis=-1)\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Convert weights to PDF by dividing by the difference between consecutive elements in the input vector t\n  pdf = w / jnp.diff(t, axis=-1)\n\n  # Ensure that the PDF integrates to 1\n  pdf /= jnp.sum(pdf, axis=-1, keepdims=True)\n\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Compute the difference between consecutive elements in the input vector t\n  t_diff = jnp.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in the input vector t\n  pdf = w / t_diff\n\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Compute the difference between consecutive elements in the input vector t.\n  t_diff = np.diff(t)\n  # Divide the weights by the difference between consecutive elements in the input vector t.\n  w_pdf = w / t_diff\n  # Return the resulting PDF that integrates to 1.\n  return w_pdf\n\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check if the input vectors are valid.\n  if not isinstance(t, np.ndarray) and not isinstance(t, jnp.ndarray):\n    raise ValueError(\"Input t must be a NumPy array or JAX array.\")\n  if not isinstance(w, np.ndarray) and not isinstance(w, jnp.ndarray):\n    raise ValueError(\"Input w must be a NumPy array or JAX array.\")\n  if len(t.shape) != 1:\n    raise ValueError(\"Input t must be a 1D vector.\")\n  if len(w.shape) != 1:\n    raise ValueError(\"Input w must be a 1D vector.\")\n  if t.shape[0] != w.shape[0]:\n    raise ValueError(\"Input t and w must have the same length.\")\n\n  # Convert the weights to a PDF by dividing by the difference between consecutive elements in the input vector t.\n  pdf = w / np.diff(t)\n\n  return pdf\n\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Compute the difference between consecutive elements in t.\n  diff = t[1:] - t[:-1]\n  # Divide the weights by the difference between consecutive elements in t.\n  pdf = w[:-1] / diff\n  # Add the last element of pdf to the last element of w to obtain the PDF.\n  pdf = jnp.concatenate([pdf, jnp.array([w[-1]])])\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.asarray(w)\n  t = jnp.asarray(t)\n  if t.ndim == 1:\n    t = t[:, None]\n  w = w / jnp.diff(t, axis=0)\n  return w\n\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Convert the weights to probabilities by dividing by the difference between consecutive elements in t.\n  p = w / jnp.diff(t, axis=-1)\n\n  # Normalize the probabilities to sum to 1.\n  p /= jnp.sum(p, axis=-1, keepdims=True)\n\n  return p\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Convert the weights to a probability density function (PDF) by dividing by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n\n  # Normalize the PDF to ensure that it integrates to 1.\n  pdf /= jnp.sum(pdf)\n\n  return pdf\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, _, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            try:\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                continue\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                # skip if it is symbolic link\n                if not os.path.islink(fp):\n                    total_size += os.path.getsize(fp)\n    except FileNotFoundError:\n        pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, _, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except NotADirectoryError:\n        pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for filename in filenames:\n            try:\n                file_path = os.path.join(dirpath, filename)\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for filename in filenames:\n            filepath = os.path.join(dirpath, filename)\n            try:\n                total_size += os.path.getsize(filepath)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            try:\n                file_path = os.path.join(root, file)\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                continue\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for filename in filenames:\n            filepath = os.path.join(dirpath, filename)\n            try:\n                total_size += os.path.getsize(filepath)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, _, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            try:\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            try:\n                file_path = os.path.join(root, file)\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for root, dirs, files in os.walk(path):\n        for f in files:\n            try:\n                fp = os.path.join(root, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                continue\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, _, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                if not os.path.islink(fp):\n                    total_size += os.path.getsize(fp)\n    except FileNotFoundError:\n        pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n\n    # Iterate over all files and subdirectories within the specified folder\n    for dirpath, dirnames, filenames in os.walk(path):\n        # Ignore hidden files and directories\n        dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n        filenames = [f for f in filenames if not f.startswith(\".\")]\n\n        # Calculate the size of each file within the current directory\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            try:\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, _, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n    return total_size\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                try:\n                    fp = os.path.join(dirpath, f)\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except NotADirectoryError:\n        total_size += os.path.getsize(path)\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = val - period * torch.floor(val / period + 0.5)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if torch.is_tensor(val):\n        val = val.detach().cpu().numpy()\n    val = val - offset * period\n    val = val - period * (val // period)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = val - period * torch.floor(val / period + 0.5)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = torch.fmod(val - offset * period, period)\n        val = torch.where(val < -0.5 * period, val + period, val)\n        val = torch.where(val >= 0.5 * period, val - period, val)\n    else:\n        val = np.fmod(val - offset * period, period)\n        val = np.where(val < -0.5 * period, val + period, val)\n        val = np.where(val >= 0.5 * period, val - period, val)\n\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = torch.fmod(val - offset * period, period)\n        val = torch.where(val < -0.5 * period, val + period, val)\n        val = torch.where(val >= 0.5 * period, val - period, val)\n    else:\n        val = np.fmod(val - offset * period, period)\n        val = np.where(val < -0.5 * period, val + period, val)\n        val = np.where(val >= 0.5 * period, val - period, val)\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = val.detach().cpu().numpy()\n    if isinstance(val, np.ndarray):\n        val = np.mod(val - offset * period, period)\n        val = np.where(val < -0.5 * period, val + period, val)\n        val = np.where(val >= 0.5 * period, val - period, val)\n        val = torch.from_numpy(val).to(val.device)\n    else:\n        raise TypeError('Input type is not supported.')\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = val.detach().cpu().numpy()\n    val = val - offset * period\n    val = val - period * (val // period)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = val - period * torch.floor(val / period + 0.5)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if torch.is_tensor(val):\n        val = val.float()\n    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = val - period * torch.floor(val / period + 0.5)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = torch.fmod(val - offset * period, period)\n        val = torch.fmod(val + period, period)\n        val = val + offset * period\n    else:\n        val = np.fmod(val - offset * period, period)\n        val = np.fmod(val + period, period)\n        val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = val.detach().cpu().numpy()\n    if isinstance(val, np.ndarray):\n        val = val.reshape(-1)\n    else:\n        val = np.array(val)\n\n    val = val - offset * period\n    val = val - period * (val > period / 2)\n    val = val + period * (val < -period / 2)\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if not isinstance(val, torch.Tensor):\n        val = torch.tensor(val)\n\n    if offset < 0 or offset > 1:\n        warning(\n            f'The offset value should be in range [0, 1]. But got {offset}.'\n        )\n\n    if period <= 0:\n        warning(f'The period value should be positive. But got {period}.')\n\n    val = val - torch.floor(val / period + offset) * period\n\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = val.detach().cpu().numpy()\n    if isinstance(val, np.ndarray):\n        val = val.astype(np.float32)\n    if isinstance(val, torch.Tensor):\n        val = val.detach().cpu().numpy()\n    if isinstance(val, np.ndarray):\n        val = val.astype(np.float32)\n\n    if (val >= offset * period - 1e-8\n            and val <= (1 - offset) * period + 1e-8):\n        return val\n    k = (val + offset * period) // period\n    val = val - k * period\n    if val < -offset * period:\n        k = k + 1\n        val = val + period\n    return val + k * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, np.ndarray):\n        val = torch.from_numpy(val)\n    if not isinstance(val, torch.Tensor):\n        raise TypeError(\n            'val must be either a numpy array or a torch tensor, '\n            f'but got {type(val)}')\n    val = torch.remainder(val - offset * period, period * 2)\n    if val.abs_().max() > period:\n        warning(\n            'The input value exceeds the specified period range, '\n            'which may cause unexpected results.')\n    val = val.clamp(-period, period)\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if torch.is_tensor(val):\n        val = val.float()\n    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, np.ndarray):\n        val = torch.from_numpy(val)\n    if not isinstance(val, torch.Tensor):\n        raise TypeError(\n            f'val must be either np.ndarray or torch.Tensor, but got {type(val)}')\n    if not isinstance(offset, float):\n        raise TypeError(\n            f'offset must be a float, but got {type(offset)}')\n    if not isinstance(period, float):\n        raise TypeError(\n            f'period must be a float, but got {type(period)}')\n    val = val - offset * period\n    val = val - period * torch.floor(val / period + 0.5)\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        if val.numel() == 0:\n            return val\n        assert torch.all(\n            torch.isfinite(val)), f'Nan or inf in tensor {val}'\n        val = torch.remainder(val - offset * period,\n                              period) + offset * period\n        val = torch.where(val < -0.5 * period, val + period, val)\n        val = torch.where(val >= 0.5 * period, val - period, val)\n    else:\n        if val.size == 0:\n            return val\n        val = np.array([\n            np.remainder(v - offset * period, period) + offset * period\n            for v in val\n        ],\n                       dtype=val.dtype)\n        val = np.where(val < -0.5 * period, val + period, val)\n        val = np.where(val >= 0.5 * period, val - period, val)\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        return agent_dict\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent, MicroAgent):\n            agent_dict = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n            return agent_dict\n        else:\n            raise TypeError(\"Input must be a MicroAgent instance.\")\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return agent.__dict__\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = agent.__dict__.copy()\n        if agent_dict[\"purpose_embedding\"] is not None:\n            agent_dict[\"purpose_embedding\"] = agent_dict[\"purpose_embedding\"].tolist()\n        return agent_dict\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n            \"last_output\": agent.last_output,\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return agent.__dict__\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return agent.__dict__\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent, MicroAgent):\n            return {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        else:\n            raise TypeError(\"Expected MicroAgent object, got {}\".format(type(agent)))\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        # Convert purpose_embedding from numpy array to list if necessary\n        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        # Convert dynamic_prompt from numpy array to list if necessary\n        if isinstance(agent.dynamic_prompt, np.ndarray):\n            agent.dynamic_prompt = agent.dynamic_prompt.tolist()\n\n        # Convert last_input from numpy array to list if necessary\n        if isinstance(agent.last_input, np.ndarray):\n            agent.last_input = agent.last_input.tolist()\n\n        # Convert working_agent from MicroAgent to dictionary if necessary\n        if isinstance(agent.working_agent, MicroAgent):\n            agent.working_agent = AgentSerializer.to_dict(agent.working_agent)\n\n        # Convert parent_id from MicroAgent to dictionary if necessary\n        if isinstance(agent.parent_id, MicroAgent):\n            agent.parent_id = AgentSerializer.to_dict(agent.parent_id)\n\n        # Convert evolve_count from MicroAgent to dictionary if necessary\n        if isinstance(agent.evolve_count, MicroAgent):\n            agent.evolve_count = AgentSerializer.to_dict(agent.evolve_count)\n\n        # Convert is_prime from MicroAgent to dictionary if necessary\n        if isinstance(agent.is_prime, MicroAgent):\n            agent.is_prime = AgentSerializer.to_dict(agent.is_prime)\n\n        # Convert number_of_code_executions from MicroAgent to dictionary if necessary\n        if isinstance(agent.number_of_code_executions, MicroAgent):\n            agent.number_of_code_executions = AgentSerializer.to_dict(agent.number_of_code_executions)\n\n        # Convert last_input from MicroAgent to dictionary if necessary\n        if isinstance(agent.last_input, MicroAgent):\n            agent.last_input = AgentSerializer.to_dict(agent.last_input)\n\n        # Convert depth from MicroAgent"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent, MicroAgent):\n            return {\n                'dynamic_prompt': agent.dynamic_prompt,\n                'purpose': agent.purpose,\n                'purpose_embedding': agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n                'depth': agent.depth,\n                'max_depth': agent.max_depth,\n                'usage_count': agent.usage_count,\n                'id': agent.id,\n                'parent_id': agent.parent_id,\n                'working_agent': agent.working_agent,\n                'is_prime': agent.is_prime,\n                'evolve_count': agent.evolve_count,\n                'number_of_code_executions': agent.number_of_code_executions,\n                'last_input': agent.last_input\n            }\n        else:\n            raise TypeError(\"Input must be of type MicroAgent\")\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': agent.purpose_embedding,\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input\n        }\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return agent.__dict__\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = agent.__dict__.copy()\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n        return agent_dict\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n        return agent.__dict__\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent, MicroAgent):\n            if agent.purpose_embedding is not None:\n                agent.purpose_embedding = agent.purpose_embedding.tolist()\n            return agent.__dict__\n        else:\n            raise TypeError(\"AgentSerializer.to_dict() expects a MicroAgent object\")\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {}\n        if isinstance(agent, MicroAgent):\n            agent_dict['dynamic_prompt'] = agent.dynamic_prompt\n            agent_dict['purpose'] = agent.purpose\n            agent_dict['purpose_embedding'] = agent.purpose_embedding.tolist()\n            agent_dict['depth'] = agent.depth\n            agent_dict['max_depth'] = agent.max_depth\n            agent_dict['usage_count'] = agent.usage_count\n            agent_dict['id'] = agent.id\n            agent_dict['parent_id'] = agent.parent_id\n            agent_dict['working_agent'] = agent.working_agent\n            agent_dict['is_prime'] = agent.is_prime\n            agent_dict['evolve_count'] = agent.evolve_count\n            agent_dict['number_of_code_executions'] = agent.number_of_code_executions\n            agent_dict['last_input'] = agent.last_input\n        else:\n            raise TypeError(\"Input must be a MicroAgent object.\")\n        return agent_dict\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent, MicroAgent):\n            return {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        else:\n            raise TypeError(\"Input must be a MicroAgent instance.\")\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive\")\n\n    # Check that the number of items and weights are the same\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights.items(), key=lambda x: x[1])[0]\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and their weights\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError('The number of bins must be positive.')\n\n    # Check that the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError('The number of items and weights must be equal.')\n\n    # Check that the weights are all positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError('All weights must be positive.')\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and their weights\n    return bins, bin_weights\n\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal.\")\n\n    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bin weights and item lists\n    bin_weights = [0] * num_bins\n    bin_items = defaultdict(list)\n\n    # Iterate over the sorted items and place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(range(num_bins), key=lambda i: bin_weights[i])\n        bin_weights[min_bin] += weight\n        bin_items[min_bin].append(item)\n\n    # Return the bin weights and item lists\n    return bin_items, bin_weights\n\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Check that the number of items and weights are the same\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and weights dictionaries\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Greedily distribute the items into bins\n    for item, weight in sorted_items:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights.items(), key=lambda x: x[1])[0]\n\n        # Add the item to the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and weights dictionaries\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive.\n    if num_bins <= 0:\n        raise ValueError('The number of bins must be positive.')\n\n    # Check that the number of items and weights are equal.\n    if len(items) != len(weights):\n        raise ValueError('The number of items and weights must be equal.')\n\n    # Check that all weights are positive.\n    if any(weight <= 0 for weight in weights):\n        raise ValueError('All weights must be positive.')\n\n    # Sort the items by weight in descending order.\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins dictionary and the weights dictionary.\n    bins = defaultdict(list)\n    weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight.\n    for item, weight in sorted_items:\n        bin_index = min(weights.keys(), key=lambda x: weights[x], default=0)\n        bins[bin_index].append(item)\n        weights[bin_index] += weight\n\n    # Return the bins and weights dictionaries.\n    return bins, weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Create a dictionary to store the items in each bin\n    bins: Dict[int, List[Any]] = defaultdict(list)\n\n    # Create a dictionary to store the total weight of each bin\n    bin_weights: Dict[int, int] = defaultdict(int)\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Iterate through the sorted items and place each item into the bin with the lowest total weight\n    for item, weight in sorted_items:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the lowest total weight\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    # Return the bins and bin weights\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights.items(), key=lambda x: x[1])[0]\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Check that the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal.\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_indices = sorted(range(len(weights)), key=lambda i: weights[i], reverse=True)\n    sorted_items = [items[i] for i in sorted_indices]\n    sorted_weights = [weights[i] for i in sorted_indices]\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Iterate over the sorted items and distribute them into the bins\n    for item, weight in zip(sorted_items, sorted_weights):\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights.items(), key=lambda x: x[1])[0]\n\n        # Place the item into the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and their weights\n    return bins, bin_weights\n\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is valid.\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive.\")\n\n    # Check if the number of items and weights are equal.\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must be equal.\")\n\n    # Check if the number of items is less than the number of bins.\n    if len(items) < num_bins:\n        raise ValueError(\"Number of items must be greater than or equal to the number of bins.\")\n\n    # Check if any weight is non-positive.\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Create a dictionary to store the items in each bin.\n    bins: Dict[int, List[Any]] = defaultdict(list)\n\n    # Create a dictionary to store the total weight of each bin.\n    bin_weights: Dict[int, int] = defaultdict(int)\n\n    # Sort the items by weight in descending order.\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Place each item into the bin with the current lowest total weight.\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights.items(), key=lambda x: x[1])[0]\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and bin weights.\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive.\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Check that the number of items is at least equal to the number of bins.\n    if len(items) < num_bins:\n        raise ValueError(\"The number of items must be at least equal to the number of bins.\")\n\n    # Check that the number of weights is equal to the number of items.\n    if len(weights) != len(items):\n        raise ValueError(\"The number of weights must be equal to the number of items.\")\n\n    # Check that all weights are positive.\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order.\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize a dictionary to store the items in each bin.\n    bins = defaultdict(list)\n\n    # Initialize a dictionary to store the total weight of each bin.\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight.\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights.items(), key=lambda x: x[1])[0]\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the dictionaries of bins and bin weights.\n    return bins, bin_weights\n\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is less than or equal to zero\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be greater than zero.\")\n\n    # Check if the number of items does not match the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items must match the number of weights.\")\n\n    # Check if any weight is non-positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bin indices and weights\n    bin_indices = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Distribute the items into bins\n    for item, weight in sorted_items:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights.items(), key=lambda x: x[1])[0]\n        # Add the item to the bin with the lowest total weight\n        bin_indices[min_bin].append(item)\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    # Return the bin indices and weights\n    return bin_indices, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is less than or equal to zero.\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be greater than zero.\")\n\n    # Check if the number of items is less than or equal to zero.\n    if len(items) <= 0:\n        raise ValueError(\"The number of items must be greater than zero.\")\n\n    # Check if the number of weights is less than or equal to zero.\n    if len(weights) <= 0:\n        raise ValueError(\"The number of weights must be greater than zero.\")\n\n    # Check if the number of items and weights are not equal.\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal.\")\n\n    # Check if the number of items is less than the number of bins.\n    if len(items) < num_bins:\n        raise ValueError(\"The number of items must be greater than or equal to the number of bins.\")\n\n    # Check if the number of bins is greater than the number of items.\n    if len(items) > num_bins:\n        raise ValueError(\"The number of bins must be greater than or equal to the number of items.\")\n\n    # Check if any weight is less than or equal to zero.\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order.\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bin indices and bin weights dictionaries.\n    bin_indices: Dict[int, List[Any]] = defaultdict(list)\n    bin_weights: Dict[int, int] = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight.\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights.items(), key=lambda x: x[1])[0]\n        bin_indices[min_bin].append(item)\n        bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bin assignments and bin weights\n    bin_assignments = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Iterate over the sorted items and place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin_weight = min(bin_weights.values())\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bin_assignments[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bin assignments and bin weights\n    return bin_assignments, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(weights, items), reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Iterate over the sorted items and place each item into the bin with the current lowest total weight\n    for weight, item in sorted_items:\n        min_bin_weight = min(bin_weights.values())\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and their total weights\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the lengths of the two input lists are equal\n    if len(items) != len(weights):\n        raise ValueError(\"The lengths of the two input lists must be equal.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bin dictionaries\n    bins: Dict[int, List[Any]] = defaultdict(list)\n    bin_weights: Dict[int, int] = defaultdict(int)\n\n    # Loop through the sorted items and place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        bin_index = min(bin_weights, key=bin_weights.get)\n        bins[bin_index].append(item)\n        bin_weights[bin_index] += weight\n\n    # Return the bin dictionaries\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive.\")\n\n    # Check that the number of items and weights is the same\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must be the same.\")\n\n    # Check that the weights are all positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_indices = sorted(range(len(weights)), key=lambda i: weights[i], reverse=True)\n    sorted_items = [items[i] for i in sorted_indices]\n    sorted_weights = [weights[i] for i in sorted_indices]\n\n    # Initialize the bins\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in zip(sorted_items, sorted_weights):\n        bin_index = min(bin_weights.keys(), key=lambda i: bin_weights[i], default=0)\n        bins[bin_index].append(item)\n        bin_weights[bin_index] += weight\n\n    # Check that the total number of items has been distributed into the correct number of bins\n    if len(bins) != num_bins:\n        raise ValueError(\"Not enough bins to distribute all items.\")\n\n    # Return the bins and bin weights\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check input arguments\n    if len(items) != len(weights):\n        raise ValueError(\"The 'items' and 'weights' lists must have the same length.\")\n    if num_bins <= 0:\n        raise ValueError(\"The 'num_bins' argument must be a positive integer.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(weights, items), reverse=True)\n\n    # Initialize the bins and bin weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Greedily pack the items into the bins\n    for weight, item in sorted_items:\n        min_bin = min(bin_weights.items(), key=lambda x: x[1])[0]\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check input arguments\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n    for weight in weights:\n        if weight <= 0:\n            raise ValueError(\"All weights must be positive.\")\n\n    # Sort items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize bins and weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check that the number of bins is not greater than the number of items\n    if num_bins > len(items):\n        raise ValueError(\"Number of bins cannot be greater than number of items\")\n\n    # Check that the number of weights is equal to the number of items\n    if len(weights) != len(items):\n        raise ValueError(\"Number of weights must be equal to number of items\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins: Dict[int, List[Any]] = defaultdict(list)\n    bin_weights: Dict[int, int] = defaultdict(int)\n\n    # Iterate over the sorted items and place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive\")\n\n    # Check if the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must match\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(weights, items), reverse=True)\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for weight, item in sorted_items:\n        bin_idx = min(bin_weights.keys(), key=lambda i: bin_weights[i])\n        bins[bin_idx].append(item)\n        bin_weights[bin_idx] += weight\n\n    # Return the bins and their weights\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        hash_object = hashlib.sha256(json.dumps(data).encode())\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the function name, arguments, and keyword arguments to a JSON string\n        data = json.dumps({\n            \"func_name\": func_name,\n            \"args\": args,\n            \"kwargs\": kwargs\n        })\n\n        # Compute the SHA-256 hash of the JSON string\n        hash_object = hashlib.sha256(data.encode())\n\n        # Return the hexadecimal representation of the hash\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = json.dumps(\n            {\n                \"func_name\": func_name,\n                \"args\": args,\n                \"kwargs\": kwargs,\n            }\n        )\n        return hashlib.sha256(data.encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = json.dumps({\n            \"func_name\": func_name,\n            \"args\": args,\n            \"kwargs\": kwargs\n        })\n        return hashlib.sha256(data.encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the function name, arguments, and keyword arguments to a JSON string\n        data = json.dumps((func_name, args, kwargs), sort_keys=True)\n        # Compute the SHA-256 hash of the JSON string\n        hash = hashlib.sha256(data.encode(\"utf-8\")).hexdigest()\n        return hash\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = json.dumps((func_name, args, kwargs))\n        return hashlib.sha256(data.encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name.encode(\"utf-8\") + json.dumps(args).encode(\"utf-8\") + json.dumps(kwargs).encode(\"utf-8\")\n        return hashlib.sha256(data).hexdigest()\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = json.dumps({\n            \"func_name\": func_name,\n            \"args\": args,\n            \"kwargs\": kwargs\n        })\n        return hashlib.sha256(data.encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Combine the function name, arguments, and keyword arguments into a single string\n        data = f\"{func_name}{args}{kwargs}\"\n\n        # Compute the SHA-256 hash of the data\n        hash_obj = hashlib.sha256(data.encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_obj.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = [func_name, args, kwargs]\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = json.dumps({\n            \"func_name\": func_name,\n            \"args\": args,\n            \"kwargs\": kwargs\n        })\n        return hashlib.sha256(data.encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the function name, arguments, and keyword arguments to a JSON string\n        data = json.dumps({\n            \"func_name\": func_name,\n            \"args\": args,\n            \"kwargs\": kwargs\n        })\n\n        # Compute the SHA-256 hash of the JSON string\n        hash_object = hashlib.sha256(data.encode())\n\n        # Return the hexadecimal digest of the hash\n        return hash_object.hexdigest()\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Compute a hash of the function name, arguments, and keyword arguments\n        # This is used as the primary key for the cache table\n        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data, sort_keys=True).encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to 0\n    total_length = 0\n\n    # Iterate over the points in the polygon\n    for i in range(len(polygon)):\n        # Get the current point and the next point\n        current_point = polygon[i]\n        next_point = polygon[(i + 1) % len(polygon)]\n\n        # Compute the distance between the current point and the next point\n        distance = np.linalg.norm(current_point - next_point)\n\n        # If the distance is below the maximum distance, add it to the total length\n        if distance <= max_point_distance:\n            total_length += distance\n\n    # Return the total length\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n\n    # Exclude distances that exceed the maximum distance\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to get the total length\n    total_length = np.sum(distances)\n\n    return float(total_length)"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    polygon_length = 0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if distance <= max_point_distance:\n            polygon_length += distance\n    return polygon_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Calculate the distance between consecutive points\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Exclude distances that exceed the maximum\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to obtain the total length\n    total_length = np.sum(distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length variable\n    total_length = 0.0\n\n    # Iterate over the points in the polygon\n    for i in range(len(polygon)):\n        # Calculate the distance between the current point and the next point\n        distance = np.linalg.norm(polygon[i] - polygon[(i + 1) % len(polygon)])\n\n        # If the distance is below the maximum threshold, add it to the total length\n        if distance <= max_point_distance:\n            total_length += distance\n\n    # Return the total length\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distance between consecutive points in the polygon\n    distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n\n    # Exclude distances that exceed the maximum distance threshold\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to obtain the total length of the polygon\n    total_length = np.sum(distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon) < 2:\n        return 0.0\n\n    total_length = 0.0\n    prev_point = polygon[0]\n\n    for point in polygon[1:]:\n        distance = np.linalg.norm(point - prev_point)\n        if distance <= max_point_distance:\n            total_length += distance\n        prev_point = point\n\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0\n    for i in range(len(polygon) - 1):\n        point1 = polygon[i]\n        point2 = polygon[i + 1]\n        distance = np.linalg.norm(point1 - point2)\n        if distance <= max_point_distance:\n            total_length += distance\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n        if distance <= max_point_distance:\n            total_length += distance\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n\n    # Exclude distances that exceed the maximum distance threshold\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to obtain the total length of the polygon\n    total_length = np.sum(distances)\n\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n        if distance <= max_point_distance:\n            length += distance\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if polygon.shape[0] < 2:\n        return 0.0\n\n    polygon_length = 0.0\n    for i in range(polygon.shape[0] - 1):\n        distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n        if distance <= max_point_distance:\n            polygon_length += distance\n\n    return polygon_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum distance threshold\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to get the total length of the polygon\n    total_length = np.sum(distances)\n\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the length to zero\n    length = 0.0\n\n    # Iterate over the points in the polygon\n    for i in range(len(polygon) - 1):\n        # Calculate the distance between the current point and the next point\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n\n        # If the distance is below the maximum distance, add it to the total length\n        if distance <= max_point_distance:\n            length += distance\n\n    # Return the total length\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to zero\n    total_length = 0\n\n    # Iterate over the points in the polygon\n    for i in range(len(polygon) - 1):\n        # Calculate the distance between the current point and the next point\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n\n        # If the distance is below the maximum, add it to the total length\n        if distance <= max_point_distance:\n            total_length += distance\n\n    # Return the total length\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points in the polygon\n    point_distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum distance threshold\n    point_distances = point_distances[point_distances <= max_point_distance]\n\n    # Compute the total length of the polygon by summing the distances\n    total_length = np.sum(point_distances)\n\n    return total_length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0\n    for i in range(len(polygon) - 1):\n        if np.linalg.norm(polygon[i] - polygon[i + 1]) <= max_point_distance:\n            length += np.linalg.norm(polygon[i] - polygon[i + 1])\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n\n    # Exclude distances that exceed the maximum\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to get the total length\n    return float(np.sum(distances))\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the length to 0\n    length = 0.0\n\n    # Iterate through the points in the polygon\n    for i in range(len(polygon)):\n        # Get the current point\n        current_point = polygon[i]\n\n        # Get the next point\n        next_point = polygon[(i + 1) % len(polygon)]\n\n        # Compute the distance between the current and next points\n        distance = np.linalg.norm(current_point - next_point)\n\n        # If the distance is below the maximum distance, add it to the total length\n        if distance <= max_point_distance:\n            length += distance\n\n    return length\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0.0\n    for i in range(len(polygon) - 1):\n        point_distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if point_distance <= max_point_distance:\n            length += point_distance\n    return length\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    max_area = max(area(polygon) for polygon in polygons)\n\n    polygons = [polygon for polygon in polygons if area(polygon) > max(rel_tr * max_area, abs_tr)]\n\n    return polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the area of each polygon\n    areas = [area(polygon) for polygon in polygons]\n\n    # Calculate the relative and absolute thresholds based on the largest polygon's area\n    largest_area = max(areas)\n    rel_threshold = rel_tr * largest_area\n    abs_threshold = abs_tr\n\n    # Filter out polygons based on their area\n    filtered_polygons = [polygon for polygon, area in zip(polygons, areas) if area >= rel_threshold or area >= abs_threshold]\n\n    return filtered_polygons\n\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return []\n\n    rel_tr = max(rel_tr, 0.0)\n    abs_tr = max(abs_tr, 0.0)\n\n    largest_area = max(area(polygon) for polygon in polygons)\n\n    return [\n        polygon\n        for polygon in polygons\n        if area(polygon) >= max(rel_tr * largest_area, abs_tr)\n    ]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    largest_area = max(area(polygon) for polygon in polygons)\n\n    return [polygon for polygon in polygons if area(polygon) >= rel_tr * largest_area + abs_tr]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    largest_area = max(area(polygon) for polygon in polygons)\n    rel_tr_area = largest_area * rel_tr\n    abs_tr_area = abs_tr\n\n    return [polygon for polygon in polygons if area(polygon) >= max(rel_tr_area, abs_tr_area)]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Get the largest polygon's area\n    max_area = max([area(polygon) for polygon in polygons])\n\n    # Calculate the relative and absolute thresholds\n    rel_threshold = max_area * rel_tr\n    abs_threshold = abs_tr\n\n    # Filter out polygons based on their area\n    filtered_polygons = [polygon for polygon in polygons if area(polygon) >= max(rel_threshold, abs_threshold)]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return []\n\n    max_area = max(area(polygon) for polygon in polygons)\n    return [polygon for polygon in polygons if area(polygon) >= max(rel_tr * max_area, abs_tr)]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n\n    max_area = max(area(p) for p in polygons)\n    rel_threshold = max_area * rel_tr\n    abs_threshold = abs_tr\n\n    filtered_polygons = [p for p in polygons if area(p) >= rel_threshold or area(p) >= abs_threshold]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    largest_polygon_area = area(polygons[0])\n    rel_tr = rel_tr * largest_polygon_area\n\n    return [polygon for polygon in polygons if area(polygon) >= max(rel_tr, abs_tr)]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    max_area = max(area(p) for p in polygons)\n    rel_threshold = max_area * rel_tr\n    abs_threshold = abs_tr\n\n    return [p for p in polygons if area(p) >= max(rel_threshold, abs_threshold)]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    max_area = max(area(polygon) for polygon in polygons)\n\n    polygons = [polygon for polygon in polygons if area(polygon) >= max(rel_tr * max_area, abs_tr)]\n\n    return polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    if rel_tr < 0 or abs_tr < 0:\n        raise ValueError(\"Relative and absolute thresholds must be non-negative.\")\n\n    if rel_tr == 0 and abs_tr == 0:\n        raise ValueError(\"Relative and absolute thresholds cannot both be zero.\")\n\n    max_area = max(area(p) for p in polygons)\n    filtered_polygons = [p for p in polygons if area(p) > max(rel_tr * max_area, abs_tr)]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    if rel_tr < 0:\n        raise ValueError(\"Relative threshold must be non-negative\")\n\n    if abs_tr < 0:\n        raise ValueError(\"Absolute threshold must be non-negative\")\n\n    if abs_tr == 0 and rel_tr == 0:\n        raise ValueError(\"Either relative or absolute threshold must be non-zero\")\n\n    if rel_tr == 0:\n        max_area = np.max([area(p) for p in polygons])\n        rel_tr = abs_tr / max_area\n\n    max_area = np.max([area(p) for p in polygons])\n    return [p for p in polygons if area(p) > rel_tr * max_area or area(p) > abs_tr]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    if rel_tr < 0.0 or abs_tr < 0.0:\n        raise ValueError(\"Thresholds must be non-negative.\")\n\n    if rel_tr > 0.0:\n        max_area = max([area(polygon) for polygon in polygons])\n        rel_tr *= max_area\n\n    return [polygon for polygon in polygons if area(polygon) > max(rel_tr, abs_tr)]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    largest_polygon = max(polygons, key=lambda x: area(x))\n    largest_area = area(largest_polygon)\n    rel_tr_area = rel_tr * largest_area\n    abs_tr_area = abs_tr * largest_area\n\n    filtered_polygons = [polygon for polygon in polygons if area(polygon) > rel_tr_area and area(polygon) > abs_tr_area]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    if rel_tr < 0.0 or abs_tr < 0.0:\n        raise ValueError(\"Relative and absolute thresholds must be non-negative.\")\n\n    largest_area = max(area(polygon) for polygon in polygons)\n\n    return [polygon for polygon in polygons if area(polygon) >= max(rel_tr * largest_area, abs_tr)]\n\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    # Compute the areas of the polygons\n    areas = [area(polygon) for polygon in polygons]\n\n    # Compute the relative threshold as a fraction of the largest polygon's area\n    rel_threshold = max(areas) * rel_tr\n\n    # Filter out polygons with an area below the relative threshold\n    filtered_polygons = [polygon for polygon, area in zip(polygons, areas) if area > rel_threshold]\n\n    # Filter out polygons with an area below the absolute threshold\n    filtered_polygons = [polygon for polygon in filtered_polygons if area(polygon) > abs_tr]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    if not isinstance(polygons, list):\n        polygons = [polygons]\n\n    # Calculate the area of each polygon\n    areas = [area(polygon) for polygon in polygons]\n\n    # Calculate the relative threshold as a fraction of the largest polygon's area\n    rel_threshold = max(areas) * rel_tr\n\n    # Filter out polygons whose area is below either the absolute or relative threshold\n    filtered_polygons = [polygon for polygon, area in zip(polygons, areas) if area > rel_threshold or area > abs_tr]\n\n    return filtered_polygons\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    max_area = max(area(polygon) for polygon in polygons)\n    rel_tr = rel_tr * max_area\n    return [polygon for polygon in polygons if area(polygon) > max(rel_tr, abs_tr)]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    max_area = max([area(polygon) for polygon in polygons])\n    return [polygon for polygon in polygons if area(polygon) >= max(rel_tr * max_area, abs_tr)]\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that should be processed by each worker\n    samples_per_worker_with_remainder = samples_per_worker + num_samples_yielded % num_workers\n\n    # Calculate the number of samples that should be processed by each worker\n    samples_per_worker = samples_per_worker_with_remainder // batch_size\n\n    # Calculate the number of samples that should be processed by each worker\n    samples_per_worker_with_remainder = samples_per_worker + num_samples_yielded % num_workers\n\n    # Calculate the number of samples that should be processed by each worker\n    samples_per_worker = samples_per_worker_with_remainder // batch_size\n\n    # Calculate the number of samples that should be processed by each worker\n    samples_per_worker_with_remainder = samples_per_worker + num_samples_yielded % num_workers\n\n    # Calculate the number of samples that should be processed by each worker\n    samples_per_worker = samples_per_worker_with_remainder // batch_size\n\n    # Calculate the number of samples that should be processed by each worker\n    samples_per_worker_with_remainder = samples_per_worker + num_samples_yielded % num_workers\n\n    # Calculate the number of samples that should be processed by each worker\n    samples_per_worker = samples_per_worker_with_remainder // batch_size\n\n    # Calculate the number of samples that should be processed by each worker\n    samples_per_worker_with_remainder = samples_per_worker + num_samples_yielded % num_workers\n\n    # Calculate the number of samples that should be processed by each worker\n    samples_per_worker = samples_per_worker_with_remainder // batch_size\n\n    # Calculate the number of samples that should be processed by each worker"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples to be processed by each worker\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be processed by the last worker\n    num_samples_last_worker = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    samples_per_worker = {}\n\n    # Assign the number of samples to be processed by each worker\n    for i in range(num_workers):\n        if i < num_samples_last_worker:\n            samples_per_worker[i] = num_samples_per_worker + 1\n        else:\n            samples_per_worker[i] = num_samples_per_worker\n\n    # Calculate the number of samples to be processed by each worker\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be processed by the last worker\n    num_samples_last_worker = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    samples_per_worker = {}\n\n    # Assign the number of samples to be processed by each worker\n    for i in range(num_workers):\n        if i < num_samples_last_worker:\n            samples_per_worker[i] = num_samples_per_worker + 1\n        else:\n            samples_per_worker[i] = num_samples_per_worker\n\n    # Calculate the number of samples to be processed by each worker\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be processed by the last worker\n    num_samples_last_worker = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    samples_per_worker = {}"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        num_samples = num_samples_per_worker\n        if num_samples_remaining > 0:\n            num_samples += 1\n            num_samples_remaining -= 1\n\n        indexes[worker_idx] = num_samples\n\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples to be processed by each worker\n    samples_per_worker = num_samples_yielded // num_workers\n    # Calculate the number of samples to be processed by each worker, taking into account the batch size\n    samples_per_worker = samples_per_worker // batch_size * batch_size\n    # Calculate the number of samples to be processed by each worker, taking into account the batch size and the number of workers\n    samples_per_worker = samples_per_worker // num_workers * num_workers\n    # Calculate the number of samples to be processed by each worker, taking into account the batch size, the number of workers, and the total number of samples\n    samples_per_worker = samples_per_worker // num_workers * num_workers\n    # Calculate the number of samples to be processed by each worker, taking into account the batch size, the number of workers, and the total number of samples\n    samples_per_worker = samples_per_worker // num_workers * num_workers\n    # Calculate the number of samples to be processed by each worker, taking into account the batch size, the number of workers, and the total number of samples\n    samples_per_worker = samples_per_worker // num_workers * num_workers\n    # Calculate the number of samples to be processed by each worker, taking into account the batch size, the number of workers, and the total number of samples\n    samples_per_worker = samples_per_worker // num_workers * num_workers\n    # Calculate the number of samples to be processed by each worker, taking into account the batch size, the number of workers, and the total number of samples\n    samples_per_worker = samples_per_worker // num_workers * num_workers\n    # Calculate the number of samples to be processed by each worker, taking into account the batch size, the number of workers, and the total number of samples\n    samples_per_worker = samples_per_worker // num_workers * num_workers\n    # Calculate the number of samples to be processed by each worker, taking into account the batch size, the number of workers,"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_with_remainder = num_samples_per_worker + (num_samples_yielded % num_workers)\n    num_samples_per_worker_with_remainder_per_batch = num_samples_per_worker_with_remainder // batch_size\n    num_samples_per_worker_per_batch = num_samples_per_worker // batch_size\n    num_samples_per_worker_with_remainder_per_batch_remainder = num_samples_per_worker_with_remainder % batch_size\n    num_samples_per_worker_per_batch_remainder = num_samples_per_worker % batch_size\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        if worker_idx < num_samples_per_worker_with_remainder_per_batch_remainder:\n            indexes[worker_idx] = num_samples_per_worker_with_remainder_per_batch_remainder\n        elif worker_idx < num_samples_per_worker_with_remainder_per_batch_remainder + num_samples_per_worker_with_remainder_per_batch:\n            indexes[worker_idx] = num_samples_per_worker_with_remainder_per_batch\n        elif worker_idx < num_samples_per_worker_with_remainder_per_batch_remainder + num_samples_per_worker_with_remainder_per_batch + num_samples_per_worker_per_batch_remainder:\n            indexes[worker_idx] = num_samples_per_worker_per_batch_remainder\n        else:\n            indexes[worker_idx] = num_samples_per_worker_per_batch\n\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n    samples_per_worker = int(samples_per_worker)\n\n    # Calculate the number of remaining samples\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining_samples > 0 else 0)\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining_samples > 0 else 0)\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining_samples > 0 else 0)\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining_samples > 0 else 0)\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining_samples > 0 else 0)\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining_samples > 0 else 0)\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining_samples > 0 else 0)\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining_samples > 0 else 0)\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining_samples > 0 else 0)\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining_samples > 0 else 0)\n\n    # Calculate the number of samples per worker\n    samples_per_worker = samples_per_worker + (1 if remaining"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be left over\n    left_over = num_samples_yielded % num_workers\n\n    # Calculate the number of samples per worker taking into account the left over\n    samples_per_worker_with_left_over = samples_per_worker + left_over\n\n    # Calculate the number of samples that will be distributed to each worker\n    samples_per_worker_with_left_over_per_worker = samples_per_worker_with_left_over // num_workers\n\n    # Calculate the number of samples that will be distributed to each worker taking into account the left over\n    samples_per_worker_per_worker = samples_per_worker_with_left_over_per_worker + left_over // num_workers\n\n    # Calculate the number of samples that will be distributed to each worker taking into account the left over\n    samples_per_worker_per_worker_with_left_over = samples_per_worker_with_left_over_per_worker + left_over % num_workers\n\n    # Calculate the number of samples that will be distributed to each worker taking into account the left over\n    samples_per_worker_per_worker_with_left_over_per_worker = (\n        samples_per_worker_with_left_over_per_worker + left_over % num_workers\n    )\n\n    # Calculate the number of samples that will be distributed to each worker taking into account the left over\n    samples_per_worker_per_worker_with_left_over_per_worker_with_left_over = (\n        samples_per_worker_with_left_over_per_worker + left_over % num_workers\n    )\n\n    # Calculate the number of samples that will be distributed to each worker taking into account the left over\n    samples_per_worker_per_worker_with_left_over_per_worker_with_left_over_per_worker = (\n        samples_per"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples that each worker should process\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that should be processed by the last worker\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Distribute the remaining samples evenly among the workers\n    samples_per_worker += remaining_samples // num_workers\n\n    # Calculate the number of samples that should be processed by the first workers\n    first_workers = remaining_samples % num_workers\n\n    # Distribute the remaining samples evenly among the first workers\n    first_workers_samples = first_workers // num_workers\n\n    # Initialize the dictionary to store the number of samples processed by each worker\n    workers_samples = {}\n\n    # Assign the number of samples processed by each worker\n    for i in range(num_workers):\n        if i < first_workers:\n            workers_samples[i] = samples_per_worker + 1\n        else:\n            workers_samples[i] = samples_per_worker\n\n    return workers_samples\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        num_samples = num_samples_per_worker\n        if worker_idx < num_samples_remaining:\n            num_samples += 1\n\n        indexes[worker_idx] = num_samples\n\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_remaining = num_samples_yielded % num_workers\n\n    # Distribute the remaining samples evenly among the workers\n    num_samples_per_worker_remaining_per_worker = num_samples_per_worker_remaining // num_workers\n    num_samples_per_worker_remaining_per_worker_remaining = num_samples_per_worker_remaining % num_workers\n\n    # Initialize the dictionary to store the number of samples each worker has processed\n    num_samples_per_worker_per_worker = {}\n\n    # Distribute the remaining samples evenly among the workers\n    for worker_idx in range(num_workers):\n        num_samples_per_worker_per_worker[worker_idx] = num_samples_per_worker + num_samples_per_worker_remaining_per_worker\n\n    # Distribute the remaining samples evenly among the workers\n    for worker_idx in range(num_samples_per_worker_remaining_per_worker_remaining):\n        num_samples_per_worker_per_worker[worker_idx] += 1\n\n    return num_samples_per_worker_per_worker\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples left after distributing them equally\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Calculate the number of samples per worker, taking into account the remaining samples\n    samples_per_worker_with_remaining = samples_per_worker + min(remaining_samples, num_workers)\n\n    # Calculate the number of batches per worker\n    batches_per_worker = samples_per_worker_with_remaining // batch_size\n\n    # Calculate the number of batches left after distributing them equally\n    remaining_batches = samples_per_worker_with_remaining % batch_size\n\n    # Calculate the number of batches per worker, taking into account the remaining batches\n    batches_per_worker_with_remaining = batches_per_worker + min(remaining_batches, num_workers)\n\n    # Calculate the number of samples per worker, taking into account the number of batches per worker\n    samples_per_worker_with_batches = batches_per_worker_with_remaining * batch_size\n\n    # Calculate the number of samples left after distributing them equally\n    remaining_samples_with_batches = samples_per_worker_with_batches % num_workers\n\n    # Calculate the number of samples per worker, taking into account the remaining samples and batches\n    samples_per_worker_with_remaining_batches = samples_per_worker_with_batches + min(\n        remaining_samples_with_batches, num_workers\n    )\n\n    # Create a dictionary to store the number of samples per worker\n    samples_per_worker_dict = {}\n\n    # Distribute the samples per worker, taking into account the remaining samples and batches\n    for worker_idx in range(num_workers):\n        samples_per_worker_dict[worker_idx] = samples_per_worker_with_remaining_bat"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should process\n    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_with_remainder = num_samples_per_worker + (num_samples_yielded % num_workers)\n\n    # Calculate the number of samples to be processed by each worker\n    num_samples_per_worker = num_samples_per_worker_with_remainder // batch_size\n    num_samples_per_worker_remainder = num_samples_per_worker_with_remainder % batch_size\n\n    # Distribute the remainder samples evenly among the workers\n    num_samples_per_worker_remainder_per_worker = num_samples_per_worker_remainder // num_workers\n    num_samples_per_worker_remainder_per_worker_with_remainder = (\n        num_samples_per_worker_remainder % num_workers\n    )\n\n    # Create a dictionary to store the number of samples processed by each worker\n    num_samples_per_worker_dict = {}\n    for worker_idx in range(num_workers):\n        num_samples_per_worker_dict[worker_idx] = num_samples_per_worker\n\n    # Distribute the remainder samples evenly among the workers\n    for worker_idx in range(num_workers):\n        num_samples_per_worker_dict[worker_idx] += num_samples_per_worker_remainder_per_worker\n\n    # Distribute any remaining samples evenly among the workers\n    for worker_idx in range(num_samples_per_worker_remainder_per_worker_with_remainder):\n        num_samples_per_worker_dict[worker_idx] += 1\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n    num_samples_per_worker_batch = num_samples_per_worker // batch_size\n    num_samples_per_worker_remaining = num_samples_per_worker % batch_size\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        num_samples_per_worker_batch_remaining = num_samples_per_worker_batch\n        if worker_idx < num_samples_remaining:\n            num_samples_per_worker_batch_remaining += 1\n\n        indexes[worker_idx] = num_samples_per_worker_batch_remaining * batch_size + num_samples_per_worker_remaining\n\n    return indexes\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples to be processed by each worker\n    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker += batch_size if num_samples_yielded % num_workers else 0\n\n    # Distribute the remaining samples among the workers\n    num_remaining_samples = num_samples_yielded % num_workers\n    num_samples_per_worker += num_remaining_samples // num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    num_samples_per_worker_dict = {}\n\n    # Iterate over the workers and assign the number of samples to each worker\n    for worker_idx in range(num_workers):\n        num_samples_per_worker_dict[worker_idx] = num_samples_per_worker\n\n    # Assign the remaining samples to the workers\n    for worker_idx in range(num_remaining_samples % num_workers):\n        num_samples_per_worker_dict[worker_idx] += 1\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    samples_per_worker_batch = samples_per_worker // batch_size\n    samples_per_worker_batch_remainder = samples_per_worker % batch_size\n\n    samples_per_worker_dict = {\n        worker_idx: samples_per_worker_batch + (1 if worker_idx < samples_per_worker_batch_remainder else 0)\n        for worker_idx in range(num_workers)\n    }\n\n    # Distribute any remaining samples\n    for worker_idx in range(num_workers):\n        if worker_idx < samples_per_worker_remainder:\n            samples_per_worker_dict[worker_idx] += 1\n\n    return samples_per_worker_dict\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples to be processed by each worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be processed by the last worker\n    last_worker_samples = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    samples_processed = {}\n\n    # Loop through each worker and assign the appropriate number of samples to it\n    for worker_idx in range(num_workers):\n        if worker_idx < last_worker_samples:\n            samples_processed[worker_idx] = samples_per_worker + 1\n        else:\n            samples_processed[worker_idx] = samples_per_worker\n\n    return samples_processed\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples to be processed by each worker\n    samples_per_worker = num_samples_yielded // num_workers\n    # Calculate the number of samples that will be processed by the last worker\n    remaining_samples = num_samples_yielded % num_workers\n    # Initialize the dictionary to store the number of samples processed by each worker\n    worker_samples = {}\n    # Distribute the samples evenly among the workers\n    for worker_idx in range(num_workers):\n        if worker_idx < remaining_samples:\n            worker_samples[worker_idx] = samples_per_worker + 1\n        else:\n            worker_samples[worker_idx] = samples_per_worker\n\n    return worker_samples\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker += batch_size - (num_samples_yielded % batch_size)\n\n    # Distribute the remaining samples\n    remaining_samples = num_samples_yielded % num_workers\n    for i in range(remaining_samples):\n        num_samples_per_worker[i] += 1\n\n    return num_samples_per_worker\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that should be distributed to each worker\n    samples_per_worker_distributed = samples_per_worker // batch_size\n\n    # Calculate the number of samples that should be left over for the last worker\n    samples_left_over = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    num_samples_processed = {}\n\n    # Distribute the samples to each worker\n    for worker_idx in range(num_workers):\n        # Calculate the number of samples to be distributed to the current worker\n        num_samples_to_distribute = samples_per_worker_distributed\n        if worker_idx < samples_left_over:\n            num_samples_to_distribute += 1\n\n        # Add the number of samples processed by the current worker to the dictionary\n        num_samples_processed[worker_idx] = num_samples_to_distribute\n\n    return num_samples_processed\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be processed by the last worker\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Calculate the number of samples that will be processed by each worker\n    samples_per_worker_dict = {i: samples_per_worker for i in range(num_workers)}\n\n    # Distribute the remaining samples to the workers\n    for i in range(remaining_samples):\n        samples_per_worker_dict[i] += 1\n\n    return samples_per_worker_dict\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not isinstance(results, list):\n        raise TypeError(f\"results must be a list, not {type(results)}\")\n    if not isinstance(value, list):\n        raise TypeError(f\"value must be a list, not {type(value)}\")\n    if not isinstance(threshold, (int, float)):\n        raise TypeError(f\"threshold must be a numeric value, not {type(threshold)}\")\n    if metadatas is not None and not isinstance(metadatas, list):\n        raise TypeError(f\"metadatas must be a list, not {type(metadatas)}\")\n\n    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, result_value, metadata in zip(results, value, metadatas):\n        if result_value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, result_value, result_metadata in zip(results, value, metadatas):\n        if result_value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(result_metadata)\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, result_value, metadata in zip(results, value, metadatas):\n        if result_value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not isinstance(results, list):\n        results = [results]\n    if not isinstance(value, list):\n        value = [value]\n    if not isinstance(threshold, (int, float)):\n        raise TypeError(\"Threshold must be a numeric type.\")\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    if not isinstance(metadatas, list):\n        metadatas = [metadatas]\n    if len(results) != len(value) or len(results) != len(metadatas):\n        raise ValueError(\"Input lists must have the same length.\")\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    # Check if the input lists have the same length\n    if len(results) != len(value):\n        raise ValueError(\"Input lists must have the same length\")\n\n    # Initialize the output lists\n    filtered_results = []\n    filtered_metadatas = []\n\n    # Iterate over the input lists and filter based on the threshold\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n\n    # Return the filtered results and metadatas\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    # Initialize empty lists to store the filtered results and metadatas\n    filtered_results = []\n    filtered_metadatas = []\n\n    # Check if metadatas are provided\n    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    # Loop through the results and values\n    for result, value, metadata in zip(results, value, metadatas):\n\n        # Check if the value is less than or equal to the threshold\n        if value <= threshold:\n\n            # Add the result, metadata, and value to the filtered lists\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    # Return the filtered lists\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, result_value, metadata in zip(results, value, metadatas):\n        if result_value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None for _ in range(len(results))]\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for i, result in enumerate(results):\n        if value[i] <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadatas[i])\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for result, result_value, metadata in zip(results, value, metadatas):\n        if result_value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for result, v, metadata in zip(results, value, metadatas):\n        if v <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            if metadatas:\n                filtered_metadatas.append(metadatas[i])\n            else:\n                filtered_metadatas.append(None)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value_item, metadata in zip(results, value, metadatas):\n        if value_item <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    # Initialize empty lists for filtered results and metadatas\n    filtered_results = []\n    filtered_metadatas = []\n\n    # Iterate over the results and values\n    for result, val in zip(results, value):\n        # Check if the value is less than or equal to the threshold\n        if val <= threshold:\n            # If so, add the result and its metadata to the filtered lists\n            filtered_results.append(result)\n            if metadatas:\n                filtered_metadatas.append(metadatas)\n            else:\n                filtered_metadatas.append(None)\n\n    # Return the filtered lists as a tuple\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, result_value, metadata in zip(results, value, metadatas):\n        if result_value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not results:\n        return [], []\n\n    if not metadatas:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, result_value, metadata in zip(results, value, metadatas):\n        if result_value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    # Calculate the area using the Shoelace formula\n    n = len(array)\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n\n    return area\n\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the correct shape\n    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), but has shape {array.shape}\")\n\n    # Calculate the area using the Shoelace formula\n    n = len(array)\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), got {array.shape}\")\n\n    # Calculate the area using the Shoelace formula\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n    area /= 2.0\n\n    return area\n\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2).\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n    return area\n\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the correct shape\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2), where _ can be any number of points.\")\n\n    # Calculate the area using the Shoelace formula\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n\n    return area\n\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), not {array.shape}\")\n\n    x, y = array[:, 0], array[:, 1]\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), got {array.shape}\")\n\n    n = array.shape[0]\n    area = 0.0\n\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n\n    return abs(area) / 2.0\n\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), got {array.shape}\")\n\n    # Calculate the area using the Shoelace formula\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n    area = abs(area) / 2.0\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f'The input array must have shape (_, 2). The input has shape {array.shape}.')\n\n    # Calculate the area using the Shoelace formula\n    area = 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Expected input array with shape (_, 2), but got {array.shape}\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have the shape (_, 2), where _ can be any number of points.\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n    area = abs(area) / 2.0\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The array must have the shape (_, 2)\")\n\n    n = len(array)\n    area = 0.0\n\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n\n    area = abs(area) / 2.0\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x, y = array.T\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Invalid shape of array. Expected (_, 2), but got {}\".format(array.shape))\n\n    n = len(array)\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array does not represent a polygon.\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), not {array.shape}\")\n\n    n = len(array)\n    area = 0.0\n\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n\n    area = abs(area) / 2.0\n\n    return area\n\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    # Check if the input array has the expected shape\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    # Calculate the area using the Shoelace formula\n    n = array.shape[0]\n    area = 0.5 * np.abs(\n        np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1))\n    )\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Expected shape (_, 2) but got {array.shape}\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n    area = abs(area) / 2.0\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"The input array must have shape (_, 2). Instead, it has shape {array.shape}.\")\n\n    x, y = array.T\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Expected shape (_, 2), but got {array.shape}\")\n\n    n = array.shape[0]\n    A = 0\n    for i in range(n):\n        j = (i + 1) % n\n        A += array[i, 0] * array[j, 1]\n        A -= array[j, 0] * array[i, 1]\n    A = abs(A) / 2\n    return A\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # idx_lo = torch.searchsorted(a, v, right=False)\n    # idx_hi = torch.searchsorted(a, v, right=True)\n    idx_lo = torch.searchsorted(a, v, right=False, side='left')\n    idx_hi = torch.searchsorted(a, v, right=True, side='right')\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # This function is modified from https://github.com/krrish94/nerf-pytorch/blob/main/utils.py\n    # It is used to find the indices where elements of the tensor v should be inserted into the tensor a to maintain order.\n    # It is designed to be faster than similar functions by using additional memory.\n    # It returns two tensors, idx_lo and idx_hi, indicating the lower and upper bounds where each element of v could be inserted into a.\n\n    # Find the indices where elements of v should be inserted into a\n    # to maintain order.\n    # idx_lo[i] is the index of the last element of a that is less than or\n    # equal to v[i], so that a[idx_lo[i]:idx_lo[i+1]] is the correct\n    # interval to search to find the insertion point for v[i].\n    # idx_lo[0] is always -1, and idx_lo[end] is always len(a) - 1.\n    # idx_hi[i] is the index of the first element of a that is greater than\n    # v[i], so that a[idx_hi[i]:idx_hi[i+1]] is the correct interval to\n    # search to find the insertion point for v[i].\n    # idx_hi[0] is always 0, and idx_hi[end] is always len(a).\n\n    # Example 1:\n    # a = [1, 2, 4, 4, 5]\n    # v = [0, 3, 3, 3, 6]\n    # idx_lo = [-1, 0, 1, 1, 3]\n    # idx_hi = [0, 2, 2, 4, 5]\n\n    # Example 2:\n    # a = [1, 2, 4, 4, 5]\n    # v = [0, 3, 3, 3, 6]\n    # idx_lo = [-1, 0, 1, 1, 3]\n    # idx_hi ="}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # idx_lo = torch.searchsorted(a, v, side='left')\n    # idx_hi = torch.searchsorted(a, v, side='right')\n\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.min(torch.full_like(idx_lo, len(a)),\n                       torch.searchsorted(a, v, right=True))\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Allocate idx_lo and idx_hi tensors\n    idx_lo = torch.zeros(v.shape[:-1], dtype=torch.int64, device=a.device)\n    idx_hi = torch.zeros(v.shape[:-1], dtype=torch.int64, device=a.device)\n\n    # Compute the search range\n    searchsorted_range = torch.arange(a.shape[0], device=a.device)\n\n    # Iterate over the last dimension of v\n    for i in range(v.shape[-1]):\n        # Extract the current column of v\n        v_col = v[..., i]\n\n        # Perform binary search to find the indices where v_col should be inserted into a\n        idx_lo_col, idx_hi_col = torch.searchsorted(a, v_col, right=False)\n\n        # Update idx_lo and idx_hi for the current column\n        idx_lo = idx_lo * a.shape[0] + idx_lo_col\n        idx_hi = idx_hi * a.shape[0] + idx_hi_col\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # make sure that the last dimension of a is sorted\n    assert torch.all(a[..., 1:] >= a[..., :-1])\n\n    # get the cumulative sum of a\n    cumsum_a = torch.cat([a.new_zeros(a.shape[:-1] + (1,)), torch.cumsum(a[..., :-1], dim=-1)], dim=-1)\n\n    # calculate the cumulative sum of v\n    cumsum_v = torch.cat([v.new_zeros(v.shape[:-1] + (1,)), torch.cumsum(v[..., :-1], dim=-1)], dim=-1)\n\n    # find the indices where cumsum_v is less than cumsum_a\n    idx_lo = torch.sum(torch.ge(cumsum_v[..., None, :], cumsum_a[..., None, :]), dim=-1) - 1\n\n    # find the indices where cumsum_v is greater than cumsum_a\n    idx_hi = torch.sum(torch.le(cumsum_v[..., None, :], cumsum_a[..., None, :]), dim=-1)\n\n    # return the indices\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    # clamp the search range to prevent out-of-bound indices\n    idx_lo = torch.clamp(idx_lo, 0, a.shape[0] - 1)\n    idx_hi = torch.clamp(idx_hi, 0, a.shape[0] - 1)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # This function is based on the searchsorted function from the PyTorch library.\n    # It is designed to be faster than similar functions by using additional memory.\n    # It returns two tensors, idx_lo and idx_hi, indicating the lower and upper bounds where each element of v could be inserted into a.\n\n    # The function uses a binary search algorithm to find the insertion points for each element in v in a.\n    # The search is performed along the last dimension of a, and the insertion points are determined by comparing each element of v with the corresponding element in a.\n\n    # The function returns two tensors, idx_lo and idx_hi, indicating the lower and upper bounds where each element of v could be inserted into a.\n    # If an element in v is out of the range of a, both idx_lo and idx_hi will point to either the first or last index of a.\n\n    # The function is designed to be memory-efficient, as it uses additional memory to store the indices of the elements in a that are greater than or equal to the corresponding element in v.\n    # This allows the function to find the insertion points for each element in v in a in a single pass through the data.\n\n    # The function is also designed to be fast, as it uses a binary search algorithm to find the insertion points for each element in v in a.\n    # This allows the function to find the insertion points for each element in v in a in a single pass through the data.\n\n    # The function is designed to be memory-efficient, as it uses additional memory to store the indices of the elements in a that are greater than or equal to the corresponding element in v.\n    # This allows the function to find the insertion points for each element in v in a in a single pass through the data.\n\n    # The function is also designed to be fast, as it uses a binary search algorithm to find the insertion points for each element in v in a.\n    # This allows the function to find the insertion points for each element in v in a in a single pass through the data.\n\n    # The function is designed to be memory-efficient, as it uses additional memory to store the indices of the elements in a that are greater than or equal to"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # This function searches for the indices where elements of v should be inserted into a to maintain order.\n    # It is designed to be faster than similar functions by using additional memory.\n\n    # Check if the input tensors have the same shape except for the last dimension.\n    if a.shape[:-1] != v.shape[:-1]:\n        raise ValueError(\"Input tensors must have the same shape except for the last dimension.\")\n\n    # Check if the last dimension of a is sorted in ascending order.\n    if not torch.all(a[..., 1:] >= a[..., :-1]):\n        raise ValueError(\"The last dimension of a must be sorted in ascending order.\")\n\n    # Create a new tensor with the same shape as v and fill it with the indices of the last dimension of a.\n    idx_lo = torch.arange(v.shape[-1], device=v.device, dtype=torch.long)\n\n    # Create a new tensor with the same shape as v and fill it with the indices of the last dimension of a plus one.\n    idx_hi = torch.arange(v.shape[-1] + 1, device=v.device, dtype=torch.long)\n\n    # Search for the indices where elements of v should be inserted into a to maintain order.\n    idx_lo = torch.searchsorted(a[..., -1], v, right=False)\n    idx_hi = torch.searchsorted(a[..., -1], v, right=True)\n\n    # Return the indices where elements of v should be inserted into a to maintain order.\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # handle the case where a and v are the same tensor\n    if a is v:\n        a, v = a.clone(), a.clone()\n\n    # handle the case where a is a scalar\n    if a.ndim == 0:\n        a, v = a[None], v[None]\n\n    # handle the case where a and v are both 1-dimensional tensors\n    if a.ndim == v.ndim == 1:\n        idx_lo = torch.searchsorted(a, v, side='right')\n        idx_hi = torch.searchsorted(a, v, side='left')\n        return idx_lo, idx_hi\n\n    # handle the case where a is 1-dimensional and v is 2-dimensional\n    if a.ndim == 1:\n        idx_lo = torch.searchsorted(a, v, side='right')\n        idx_hi = torch.searchsorted(a, v, side='left')\n        return idx_lo[:, None], idx_hi[:, None]\n\n    # handle the case where a is 2-dimensional and v is 1-dimensional\n    if v.ndim == 1:\n        idx_lo = torch.searchsorted(a, v[:, None], side='right')\n        idx_hi = torch.searchsorted(a, v[:, None], side='left')\n        return idx_lo, idx_hi\n\n    # handle the case where a and v are both 2-dimensional tensors\n    idx_lo = torch.searchsorted(a, v, side='right')\n    idx_hi = torch.searchsorted(a, v, side='left')\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # idx_lo will contain the first index where v[i] <= a[idx_lo[i]]\n    # idx_hi will contain the first index where v[i] > a[idx_hi[i]]\n\n    # v[i] <= a[idx_lo[i]]\n    idx_lo = torch.searchsorted(a, v, right=False)\n\n    # v[i] > a[idx_hi[i]]\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=xnp.float32)\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=xnp.float32)\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=xnp.float32)\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ], dtype=np.float32)\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=xnp.float32)\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Create the intrinsic matrix with the given focal lengths and optical centers\n  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ], dtype=xnp.float32)\n\n"}
{"namespace": "coord.contract", "completion": "  x2 = jnp.sum(x*x, axis=-1, keepdims=True)\n  x3 = jnp.sum(x2*x, axis=-1, keepdims=True)\n  x4 = jnp.sum(x2*x2, axis=-1, keepdims=True)\n  x5 = jnp.sum(x3*x2, axis=-1, keepdims=True)\n  x6 = jnp.sum(x4*x2, axis=-1, keepdims=True)\n  x7 = jnp.sum(x5*x2, axis=-1, keepdims=True)\n  x8 = jnp.sum(x6*x2, axis=-1, keepdims=True)\n  x9 = jnp.sum(x7*x2, axis=-1, keepdims=True)\n  x10 = jnp.sum(x8*x2, axis=-1, keepdims=True)\n  x11 = jnp.sum(x9*x2, axis=-1, keepdims=True)\n  x12 = jnp.sum(x10*x2, axis=-1, keepdims=True)\n  x13 = jnp.sum(x11*x2, axis=-1, keepdims=True)\n  x14 = jnp.sum(x12*x2, axis=-1, keepdims=True)\n  x15 = jnp.sum(x13*x2, axis=-1, keepdims=True)\n  x16 = jnp.sum(x14*x2, axis=-1, keepdims=True)\n  x17 = jnp.sum(x15*x2, axis=-1, keepdims=True)\n  x18 = jnp.sum(x16*x2, axis=-1, keepdims=True)\n  x19 = jnp.sum(x17*x2, axis=-1, keepdims="}
{"namespace": "coord.contract", "completion": "  x2 = jnp.sum(x**2, axis=-1, keepdims=True)\n  x2 = jnp.maximum(x2, 1e-15)\n  return x * jnp.sqrt(x2) / jnp.sqrt(x2 + 1)\n\n"}
{"namespace": "coord.contract", "completion": "  x_sq_norm = jnp.sum(x**2, axis=-1)\n  x_norm = jnp.sqrt(x_sq_norm)\n  x_norm_sq = x_norm**2\n  x_norm_cubed = x_norm**3\n  x_norm_sq_cubed = x_norm_sq**3\n  x_norm_cubed_mul_x_sq_norm = x_norm_cubed * x_sq_norm\n  x_norm_cubed_mul_x_sq_norm_mul_x_norm_sq = x_norm_cubed_mul_x_sq_norm * x_norm_sq\n  x_norm_cubed_mul_x_sq_norm_mul_x_norm_sq_mul_x_norm = x_norm_cubed_mul_x_sq_norm_mul_x_norm_sq * x_norm\n  x_norm_cubed_mul_x_sq_norm_mul_x_norm_sq_mul_x_norm_mul_x_norm_sq = x_norm_cubed_mul_x_sq_norm_mul_x_norm_sq_mul_x_norm * x_norm_sq\n  x_norm_cubed_mul_x_sq_norm_mul_x_norm_sq_mul_x_norm_mul_x_norm_sq_mul_x_norm = x_norm_cubed_mul_x_sq_norm_mul_x_norm_sq_mul_x_norm_mul_x_norm_sq * x_norm\n  x_norm_cubed_mul_x_sq_norm_mul_x_norm_sq_mul_x_norm_mul_x_norm_sq_mul_x_norm_mul_x_norm_sq = x_norm_cubed_mul_x_sq_norm_mul_x_norm_sq_mul_"}
{"namespace": "coord.contract", "completion": "  return x / (1 + jnp.sum(x**2, axis=-1, keepdims=True))**0.5\n\n"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1)\n  return x * jnp.sqrt(x_mag_sq) / jnp.sqrt(x_mag_sq + 1.)\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input points.\n  mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Calculate the scaling factor based on the magnitude squared.\n  scale = jnp.maximum(1.0 - mag_sq, 0.0)**2\n\n  # Scale the input points by the scaling factor.\n  return x * scale\n\n"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1)\n  x_mag = jnp.sqrt(x_mag_sq)\n  x_mag_cubed = x_mag**3\n  x_mag_cubed_inv = 1.0 / x_mag_cubed\n  x_mag_cubed_inv[x_mag_cubed_inv == np.inf] = 0.0\n  x_mag_cubed_inv[x_mag_cubed_inv == -np.inf] = 0.0\n  x_mag_cubed_inv[x_mag_cubed_inv != x_mag_cubed_inv] = 0.0\n  x_mag_cubed_inv = jnp.maximum(x_mag_cubed_inv, 1e-12)\n  x_mag_cubed_inv = jnp.minimum(x_mag_cubed_inv, 1e12)\n  x_mag_cubed_inv = jnp.expand_dims(x_mag_cubed_inv, axis=-1)\n  x_mag_cubed_inv = jnp.repeat(x_mag_cubed_inv, x.shape[-1], axis=-1)\n  x = x * x_mag_cubed_inv\n  return x\n\n"}
{"namespace": "coord.contract", "completion": "  return x * (1.0 - jnp.linalg.norm(x, axis=-1, keepdims=True)**2)**0.5\n\n"}
{"namespace": "coord.contract", "completion": "  x2 = jnp.sum(x**2, axis=-1)\n  x2 = jnp.maximum(x2, 1e-12)\n  t = jnp.sqrt(x2)\n  t = jnp.maximum(t, 1e-6)\n  t2 = t**2\n  t3 = t**3\n  t5 = t**5\n  x = x / t3 * (t5 + 2 * t3 + 3 * t)\n  return x\n\n"}
{"namespace": "coord.contract", "completion": "  x_sq = jnp.sum(x**2, axis=-1)\n  scale = jnp.sqrt(x_sq) * (1.0 - x_sq)\n  return x * scale[..., jnp.newaxis]\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input points\n  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Calculate the scaling factor for each point based on the formula in the paper\n  scale = jnp.sqrt(1.0 / (x_mag_sq + 1.0))\n\n  # Scale the points towards the origin\n  return x * scale\n\n"}
{"namespace": "coord.contract", "completion": "  x2 = jnp.sum(x**2, axis=-1, keepdims=True)\n  return x * jnp.sqrt(1.0 - x2)\n\n"}
{"namespace": "coord.contract", "completion": "  return x / (jnp.linalg.norm(x, axis=-1, keepdims=True)**2 + 1.)\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input points\n  mag_sq = jnp.sum(x**2, axis=-1)\n\n  # Calculate the scaling factor based on the formula\n  scale = jnp.sqrt(1.0 / (1.0 + mag_sq))\n\n  # Scale the input points using the scaling factor\n  return x * scale[:, None]\n\n\n"}
{"namespace": "coord.contract", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1)\n  x_norm_sq = x_norm**2\n  x_norm_sq = jnp.expand_dims(x_norm_sq, axis=-1)\n  x_norm_sq = jnp.repeat(x_norm_sq, x.shape[-1], axis=-1)\n  x_norm_sq = jnp.sqrt(x_norm_sq)\n  return x / x_norm_sq\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the points\n  mag_sq = jnp.sum(x**2, axis=-1)\n  # Calculate the scaling factor based on the magnitude squared\n  scale = jnp.sqrt(mag_sq)\n  # Calculate the scaling factor for the points\n  scale_factors = jnp.where(mag_sq > 1, scale, 1.0)\n  # Scale the points towards the origin\n  return x / scale_factors[..., None]\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the input points\n  x_mag_sq = jnp.sum(x**2, axis=-1)\n\n  # Calculate the scaling factor based on the formula\n  scaling_factor = jnp.sqrt(1.0 / (1.0 + x_mag_sq))\n\n  # Scale the input points towards the origin\n  return x * scaling_factor[..., None]\n\n"}
{"namespace": "coord.contract", "completion": "  return x / (jnp.linalg.norm(x, axis=-1, keepdims=True) + 1e-10)\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the points\n  mag_sq = jnp.sum(x ** 2, axis=-1)\n\n  # Calculate the scaling factor based on the magnitude squared\n  scale_factor = jnp.sqrt(1 - 0.5 * mag_sq)\n\n  # Scale the points towards the origin\n  return x * scale_factor[..., jnp.newaxis]\n\n"}
{"namespace": "coord.contract", "completion": "  x = x.astype(np.float64)\n  x_mag_sq = np.sum(x**2, axis=-1)\n  x_mag_sq = np.maximum(x_mag_sq, 1e-15)\n  x = x / np.sqrt(x_mag_sq)[..., None]\n  x = x * np.sqrt(x_mag_sq * (1 - x_mag_sq) / x_mag_sq)\n  return x\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if abs(num_bytes) < 1000.0:\n            return \"%3.1f %s\" % (num_bytes, unit)\n        num_bytes /= 1000.0\n    return \"%.1f %s\" % (num_bytes, \"PB\")\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n        if abs(num_bytes) < 1024.0:\n            return f\"{num_bytes:3.1f}{unit}B\"\n        num_bytes /= 1024.0\n    return f\"{num_bytes:3.1f}YB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    power = int(math.floor(math.log(num_bytes, 1000)))\n    new_value = round(num_bytes / math.pow(1000, power), 2)\n    new_value = int(new_value) if new_value % 1 == 0 else new_value\n    return f\"{new_value}{['B', 'KB', 'MB', 'GB', 'TB', 'PB'][power]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    elif num_bytes == 1:\n        return \"1 B\"\n    elif num_bytes < 1000:\n        return f\"{num_bytes} B\"\n    elif num_bytes < 1000**2:\n        return f\"{num_bytes / 1000:.2f} KB\"\n    elif num_bytes < 1000**3:\n        return f\"{num_bytes / 1000**2:.2f} MB\"\n    elif num_bytes < 1000**4:\n        return f\"{num_bytes / 1000**3:.2f} GB\"\n    elif num_bytes < 1000**5:\n        return f\"{num_bytes / 1000**4:.2f} TB\"\n    else:\n        return f\"{num_bytes / 1000**5:.2f} PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    for unit in units:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:.2f}{unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.2f}PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    if num_bytes < 1000:\n        return f\"{num_bytes}B\"\n    for unit in _FORMAT_TO_RATIO:\n        if num_bytes < _FORMAT_TO_RATIO[unit] * 1000:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[unit]:.2f}{unit}\"\n    return f\"{num_bytes / _FORMAT_TO_RATIO['tb']:.2f}TB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    for suffix in _FORMAT_TO_RATIO:\n        if num_bytes >= _FORMAT_TO_RATIO[suffix]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[suffix]:.2f} {suffix.upper()}\"\n    return f\"{num_bytes:.2f} B\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000.0:\n            return f\"{num_bytes:.2f}{unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.2f}PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n\n    unit_list = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    scale = 1000\n    for unit in unit_list:\n        if num_bytes < scale:\n            return f\"{num_bytes:.2f} {unit}\"\n        num_bytes /= scale\n\n    return f\"{num_bytes:.2f} PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    for suffix in _FORMAT_TO_RATIO:\n        if num_bytes > _FORMAT_TO_RATIO[suffix]:\n            continue\n        return f\"{num_bytes / _FORMAT_TO_RATIO[suffix]:.2f}{suffix}\"\n    return f\"{num_bytes / _FORMAT_TO_RATIO['tb']:.2f}tb\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n\n    suffixes = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    num_bytes = float(num_bytes)\n    suffix_index = 0\n    while num_bytes >= 1000 and suffix_index < len(suffixes) - 1:\n        num_bytes /= 1000.0\n        suffix_index += 1\n\n    return \"%.1f %s\" % (num_bytes, suffixes[suffix_index])\n\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    if num_bytes < 0:\n        raise ValueError(\"Bytes must be non-negative\")\n    suffix_index = 0\n    while num_bytes >= 1000 and suffix_index < len(_FORMAT_TO_RATIO) - 1:\n        num_bytes /= 1000.0\n        suffix_index += 1\n    return \"{:.2f}{}\".format(num_bytes, list(_FORMAT_TO_RATIO.keys())[suffix_index])\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 0:\n        raise ValueError(f\"{num_bytes} is a negative number.\")\n    if num_bytes == 0:\n        return \"0 B\"\n\n    for unit, ratio in _FORMAT_TO_RATIO.items():\n        if num_bytes >= ratio:\n            return f\"{num_bytes / ratio:.2f} {unit.upper()}\"\n    return f\"{num_bytes} B\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    if num_bytes < 0:\n        raise ValueError(\"Bytes cannot be negative\")\n\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    for unit in units:\n        if num_bytes < 1000:\n            return f\"{num_bytes:.2f}{unit}\"\n        num_bytes /= 1000\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    if num_bytes < 0:\n        raise ValueError(\"Bytes must be positive.\")\n    if num_bytes < 1000:\n        return f\"{num_bytes}B\"\n    if num_bytes < 1000**2:\n        return f\"{num_bytes / 1000}KB\"\n    if num_bytes < 1000**3:\n        return f\"{num_bytes / 1000**2}MB\"\n    if num_bytes < 1000**4:\n        return f\"{num_bytes / 1000**3}GB\"\n    if num_bytes < 1000**5:\n        return f\"{num_bytes / 1000**4}TB\"\n    return f\"{num_bytes / 1000**5}PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 0:\n        raise ValueError(\"The number of bytes cannot be negative.\")\n    if num_bytes == 0:\n        return \"0B\"\n    for unit in _FORMAT_TO_RATIO:\n        if num_bytes >= _FORMAT_TO_RATIO[unit]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[unit]:.2f}{unit}\"\n    return f\"{num_bytes:.2f}B\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 0:\n        raise ValueError(\"Bytes must be non-negative.\")\n\n    if num_bytes == 0:\n        return \"0B\"\n\n    if num_bytes == 1:\n        return \"1B\"\n\n    if num_bytes < 1000:\n        return f\"{num_bytes}B\"\n\n    for unit in _FORMAT_TO_RATIO:\n        if num_bytes < _FORMAT_TO_RATIO[unit] * 1000:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[unit]:.2f}{unit}\"\n\n    return f\"{num_bytes / _FORMAT_TO_RATIO['tb']:.2f}TB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # Check if the input is a valid number\n    if not isinstance(num_bytes, (int, float)):\n        raise TypeError(\"Input must be a number\")\n\n    # Check if the input is a positive number\n    if num_bytes < 0:\n        raise ValueError(\"Input must be a positive number\")\n\n    # Convert the number to bytes\n    num_bytes = float(num_bytes)\n\n    # Define the units\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n\n    # Convert the number to a human-readable format\n    for unit in units:\n        if num_bytes < 1000:\n            return f\"{num_bytes:.2f} {unit}\"\n        num_bytes /= 1000\n\n    # If the number is too large, return the maximum possible value\n    return f\"{num_bytes:.2f} PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 0:\n        raise ValueError(\"!!!num_bytes cannot be negative\")\n    if num_bytes == 0:\n        return \"0 B\"\n    num_bytes = float(num_bytes)\n    suffix_index = 0\n    while num_bytes >= 1000 and suffix_index < len(_FORMAT_TO_RATIO) - 1:\n        num_bytes /= 1000.0\n        suffix_index += 1\n    return f\"{num_bytes:.2f} {list(_FORMAT_TO_RATIO.keys())[suffix_index]}\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n\n    suffixes = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    num_bytes = float(num_bytes)\n    suffix_index = 0\n\n    while num_bytes >= 1000 and suffix_index < len(suffixes) - 1:\n        num_bytes /= 1000.0\n        suffix_index += 1\n\n    return f\"{num_bytes:.2f} {suffixes[suffix_index]}\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}\"\n            )\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}.\"\n            )\n\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}.\"\n            )\n\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}\"\n            )\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}\")\n\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}.\"\n            )\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that array has `nb_dimensions` dimensions.\n\n        Args:\n            cls (type): Class type.\n            v (np.ndarray): Value to check.\n            field (fields.ModelField): Field descriptor.\n\n        Raises:\n            ValueError: Exception raised if array doesn't have `nb_dimensions` dimensions.\n\n        Returns:\n            np.ndarray: `v` sent for further processing.\n        \"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}\"\n            )\n\n        return v\n\n    return is_array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)} dimensions\"\n            )\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Received {len(v.shape)}.\"\n            )\n\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}\"\n            )\n\n        return v\n\n    return _validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be of shape ({nb_dimensions},).\"\n                f\" Got {v.shape}\"\n            )\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}\"\n            )\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if array has a specific number of dimensions.\n\n        Args:\n            cls (type): Class type.\n            v (np.ndarray): Value to check.\n            field (fields.ModelField): Field descriptor.\n\n        Raises:\n            ValueError: Exception raised if array doesn't have the expected number of dimensions.\n\n        Returns:\n            np.ndarray: `v` sent for further processing.\n        \"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"\n        This function is a Pydantic validator that checks if a given array has a specific number of dimensions.\n\n        Input-Output Arguments\n        :param cls: Type. The class of the model being validated.\n        :param v: np.ndarray. The array being validated.\n        :param field: fields.ModelField. The field of the model that is being validated.\n        :return: np.ndarray. The validated array if it meets the specified number of dimensions.\n        \"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}\"\n            )\n\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be an array of shape ({', '.join([str(dim) for dim in v.shape])})\"\n            )\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be a {nb_dimensions}D array.\")\n\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has the specified number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"\n        This function is the actual validator function that performs the dimensionality check. It checks if the input array has the specified number of dimensions and raises a ValueError if it does not.\n\n        Input-Output Arguments\n        :param cls: Type. The class of the model being validated.\n        :param v: np.ndarray. The array being validated.\n        :param field: fields.ModelField. The field of the model that is being validated.\n        :return: np.ndarray. The validated array if it meets the specified number of dimensions.\n        \"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def is_array_n_dimensions_validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that array has nb_dimensions dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)} dimensions\"\n            )\n\n        return v\n\n    return is_array_n_dimensions_validator\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / jnp.maximum(r, eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / jnp.maximum(r, eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / jnp.maximum(r, eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / jnp.maximum(r, eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / jnp.maximum(r, eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Calculate the radius\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n\n  # Calculate the inclination angle (theta)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n\n  # Calculate the azimuth angle (phi)\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n\n  theta = jnp.arccos(z / jnp.maximum(r, eps))\n\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Calculate the radius (r)\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n\n  # Calculate the inclination (theta)\n  theta = jnp.arccos(cartesian_vector[..., 2] / jnp.maximum(r, eps))\n\n  # Calculate the azimuth (phi)\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Compute radius (r)\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n\n  # Compute inclination (theta)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n\n  # Compute azimuth (phi)\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Extract x, y, and z coordinates\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate radius\n  r = jnp.sqrt(x ** 2 + y ** 2 + z ** 2)\n\n  # Calculate inclination angle (theta)\n  theta = jnp.arccos(z / jnp.maximum(r, eps))\n\n  # Calculate azimuth angle (phi)\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Calculate the radius\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n\n  # Calculate the inclination angle (theta)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n\n  # Calculate the azimuth angle (phi)\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / jnp.maximum(r, eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / jnp.maximum(r, eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Extract x, y, z coordinates from the input array\n  x, y, z = jnp.split(cartesian_vector, 3, axis=-1)\n\n  # Calculate radius (r)\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n\n  # Calculate inclination (theta)\n  theta = jnp.arccos(z / r)\n\n  # Calculate azimuth (phi)\n  phi = jnp.arctan2(y, x)\n\n  # Handle the case where the input is a single point\n  if phi.ndim == 0:\n    phi = jnp.array(phi)\n\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL'][0]\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL'].mid.fmeasure\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL'].high.fmeasure\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rougeL = evaluate.load('uhgeval/.cache/huggingface/rougeL')\n    results = rougeL.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL'][0]\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL'][0]\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL'].mid.fmeasure\n    return score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL'][0]\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL'][0]\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL'].mid.fmeasure\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rougeL = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rougeL.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL'].mid.fmeasure\n    return score\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # Fallback to importing the module and returning the object\n        module_name, _, class_name = name.rpartition(\".\")\n        __import__(module_name)\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate it using a fallback method.\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using either method, raise an exception.\n    raise ImportError(f\"Failed to locate object with name '{name}'\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate the object using the fallback method.\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = __import__(module_name, fromlist=[class_name])\n        return getattr(module, class_name)\n    except (AttributeError, ValueError):\n        raise ImportError(f\"Unable to locate object '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try the standard method.\n    try:\n        return pydoc.locate(name)\n    except Exception:\n        pass\n\n    # If the standard method fails, try the fallback method.\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = __import__(module_name, fromlist=[class_name])\n        return getattr(module, class_name)\n    except Exception:\n        raise ImportError(f\"Failed to locate object '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        # Try to locate the object using the standard method.\n        return pydoc.locate(name)\n    except ImportError:\n        # If the object cannot be located using the standard method, try to locate it using the fallback method.\n        module_name, class_name = name.rsplit(\".\", 1)\n        try:\n            module = pydoc.locate(module_name)\n            return getattr(module, class_name)\n        except ImportError:\n            # If the object cannot be located using the fallback method, raise an exception.\n            raise ImportError(f\"Failed to locate object '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        # Try to locate the object using the standard method.\n        return pydoc.locate(name)\n    except ImportError:\n        # If the object cannot be located using the standard method, try to locate it using the fallback method.\n        try:\n            module_name, class_name = name.rsplit(\".\", 1)\n            module = __import__(module_name, fromlist=[class_name])\n            return getattr(module, class_name)\n        except (ValueError, AttributeError):\n            raise ImportError(f\"Failed to locate object: {name}\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Try to locate the object using a fallback method.\n    try:\n        module, qualname = name.rsplit(\".\", 1)\n        module = __import__(module, fromlist=[qualname])\n        return getattr(module, qualname)\n    except ModuleNotFoundError:\n        pass\n\n    raise ImportError(f\"Could not locate object '{name}'\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Fallback: try to locate the object using the old-style Registry API\n    if \".\" in name:\n        module, qualname = name.rsplit(\".\", 1)\n        try:\n            registry = Registry(qualname)\n            return registry.get(module)\n        except KeyError:\n            pass\n\n    raise ImportError(f\"Unable to locate {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First try the standard method\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Fall back to the fallback method\n    try:\n        module, qualname = name.rsplit(\".\", 1)\n        return getattr(pydoc.locate(module), qualname)\n    except (ImportError, AttributeError):\n        raise ImportError(f\"Failed to locate object: {name}\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except Exception:\n        # If the object cannot be located using the standard method, try to locate it using a fallback method.\n        # This is necessary because pydoc.locate() may fail to locate objects that are not in the standard library.\n        # For example, it may fail to locate objects in third-party libraries that are not installed.\n        # In this case, we will try to locate the object using the __init__.py file of the module.\n        # For example, if the object is located in the module \"my_module\", we will try to locate it using the file \"my_module/__init__.py\".\n        # This is necessary because pydoc.locate() may fail to locate objects that are not in the standard library.\n        # For example, it may fail to locate objects in third-party libraries that are not installed.\n        # In this case, we will try to locate the object using the __init__.py file of the module.\n        # For example, if the object is located in the module \"my_module\", we will try to locate it using the file \"my_module/__init__.py\".\n        # This is necessary because pydoc.locate() may fail to locate objects that are not in the standard library.\n        # For example, it may fail to locate objects in third-party libraries that are not installed.\n        # In this case, we will try to locate the object using the __init__.py file of the module.\n        # For example, if the object is located in the module \"my_module\", we will try to locate it using the file \"my_module/__init__.py\".\n        # This is necessary because pydoc.locate() may fail to locate objects that are not in the standard library.\n        # For example, it may fail to locate objects in third-party libraries that are not installed.\n        # In this case, we will try to locate the object using the __init__.py file of the module.\n        # For example, if the object is located in the module \"my_module\", we will try to locate it using the file \"my_module/__init__.py"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except Exception as e:\n        # If the object cannot be located using the standard method, attempt to locate it using a fallback method.\n        # If the object cannot be located using either method, raise an exception.\n        raise ImportError(f\"Failed to import {name}\") from e\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try the standard method\n    try:\n        return pydoc.locate(name)\n    except Exception as e:\n        pass\n\n    # Try the fallback method\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except Exception as e:\n        raise ImportError(f\"Unable to locate {name}\") from e\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try the standard method\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate it using the fallback method\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except (ImportError, AttributeError):\n        raise ImportError(f\"Failed to locate object with name '{name}'\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Fallback method\n    if \".\" not in name:\n        raise ImportError(f\"Failed to locate object {name}\")\n\n    module_name, class_name = name.rsplit(\".\", 1)\n\n    # Try to import the module\n    try:\n        module = pydoc.locate(module_name)\n    except ImportError:\n        raise ImportError(f\"Failed to locate object {name}\")\n\n    # Try to locate the class\n    try:\n        return getattr(module, class_name)\n    except AttributeError:\n        raise ImportError(f\"Failed to locate object {name}\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        # First, try the standard method.\n        return pydoc.locate(name)\n    except ImportError as e:\n        # If the object cannot be located using the standard method, try the fallback method.\n        try:\n            return pydoc.locate(name, fallback=True)\n        except ImportError as e:\n            raise ImportError(f\"Failed to locate object '{name}'. Error: {e}\") from e\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except Exception as e:\n        raise ImportError(\n            f\"Failed to find {name} in the Python path. \"\n            \"If this is a user's model, make sure it is registered in \"\n            \"model_zoo._model_zoo.MODEL_ZOO. \"\n            f\"Original error: {e}\"\n        )\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Fallback to the old-fashioned way.\n    # This is useful for the case where the object is defined in the same file as the caller.\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = __import__(module_name, fromlist=[class_name])\n        return getattr(module, class_name)\n    except ImportError:\n        pass\n\n    raise ImportError(f\"Unable to locate {name}\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate it using a fallback method.\n    # The fallback method is to locate the object using the standard method, but with the module name replaced with the object's module name.\n    # This is useful for objects that are defined in a module that is not the same as the module that is importing the object.\n    # For example, if the object is defined in a module called \"module.submodule\", and the module that is importing the object is called \"module.submodule.other_module\", then the fallback method will try to locate the object using the module name \"module.submodule\".\n    try:\n        module_name, _, class_name = name.rpartition(\".\")\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using either method, raise an exception.\n    raise ImportError(f\"Failed to locate object '{name}'.\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Attempt to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Attempt to locate the object using the fallback method.\n    try:\n        module_name, obj_name = name.rsplit(\".\", 1)\n        module = __import__(module_name, fromlist=[obj_name])\n        return getattr(module, obj_name)\n    except (ValueError, AttributeError):\n        raise ImportError(f\"Could not locate '{name}'\")\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Fallback: split the string and try to locate the object.\n    # This is useful for testing, as we can pass \"fvcore.common.registry.Registry\"\n    # instead of \"fvcore.common.registry\".\n    parts = name.split(\".\")\n    if len(parts) == 1:\n        raise ImportError(f\"Failed to import {name}\")\n    obj = locate(\".\".join(parts[:-1]))\n    return getattr(obj, parts[-1])\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the JIT module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the JIT module from the in-memory buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    reloaded_module = torch.jit.load(buf)\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the model to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n\n    # Load the model from the buffer\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the script module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the script module from the buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n    return module\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    return torch.jit.load(buf)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the JIT module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n\n    # Load the JIT module from the in-memory buffer\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    return torch.jit.load(buf)\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    return torch.jit.load(buf)\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights)\n\n    ids_fused = []\n    scores_fused = []\n\n    for i in range(len(ids)):\n        ids_fused.extend(ids[i])\n        scores_fused.extend(scores[i])\n\n    df = pd.DataFrame({'id': ids_fused, 'score': scores_fused})\n    df['score'] = df['score'] * weights[i]\n    df = df.groupby('id').sum().sort_values(by='score', ascending=False).reset_index()\n\n    ids_fused = df['id'].tolist()[:top_k]\n    scores_fused = df['score'].tolist()[:top_k]\n\n    return ids_fused, scores_fused"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize the scores\n    scores_normalized = []\n    for score in scores:\n        scores_normalized.append(score / sum(score))\n\n    # Combine the scores using a convex combination\n    scores_combined = []\n    for i in range(len(scores_normalized[0])):\n        scores_combined.append(sum([scores_normalized[j][i] * weights[j] for j in range(len(scores_normalized))]))\n\n    # Select the top_k results\n    top_k_indices = sorted(range(len(scores_combined)), key=lambda i: scores_combined[i], reverse=True)[:top_k]\n    top_k_ids = [ids[i][j] for i in range(len(ids)) for j in top_k_indices]\n    top_k_scores = [scores[i][j] for i in range(len(scores)) for j in top_k_indices]\n\n    return top_k_ids, top_k_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Create a DataFrame with the ids and scores for each retrieval result\n    df = pd.DataFrame({'ids': ids, 'scores': scores})\n\n    # Normalize the scores for each retrieval result\n    df['scores'] = df.groupby(level=0)['scores'].apply(lambda x: x / x.sum())\n\n    # Combine the scores of each retrieval result based on the provided weights\n    df['combined_scores'] = df['scores'].mul(weights).sum(axis=1)\n\n    # Sort the DataFrame by the combined scores in descending order\n    df = df.sort_values(by='combined_scores', ascending=False)\n\n    # Select the top_k results\n    df = df.head(top_k)\n\n    # Return the fused ids and scores\n    return df['ids'].tolist(), df['combined_scores'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize the scores\n    scores = [s / sum(scores) for s in scores]\n\n    # Combine the scores\n    combined_scores = [sum(s * w for s, w in zip(scores, weights)) for scores in zip(*scores)]\n\n    # Sort the combined scores in descending order\n    sorted_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)\n\n    # Select the top_k indices\n    top_k_indices = sorted_indices[:top_k]\n\n    # Return the top_k ids and scores\n    return [ids[i][:top_k] for i in top_k_indices], [combined_scores[i] for i in top_k_indices]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights), 'The length of ids, scores and weights must be the same.'\n\n    assert sum(weights) == 1, 'The sum of the weights must equal 1.'\n\n    fused_ids = []\n    fused_scores = []\n\n    for i in range(len(ids)):\n        df = pd.DataFrame({'id': ids[i], 'score': scores[i]})\n        df = df.sort_values('score', ascending=False).reset_index(drop=True)\n        df['score'] = df['score'] * weights[i]\n        fused_ids.append(df['id'].tolist())\n        fused_scores.append(df['score'].tolist())\n\n    fused_ids = [item for sublist in fused_ids for item in sublist]\n    fused_scores = [item for sublist in fused_scores for item in sublist]\n\n    df = pd.DataFrame({'id': fused_ids, 'score': fused_scores})\n    df = df.sort_values('score', ascending=False).reset_index(drop=True)\n    fused_ids = df['id'].tolist()\n    fused_scores = df['score'].tolist()\n\n    return fused_ids[:top_k], fused_scores[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # normalize weights\n    weights = tuple(weight / sum(weights) for weight in weights)\n\n    # create dataframe with ids and scores\n    df = pd.DataFrame(\n        {\n            \"id\": [id for ids in ids for id in ids],\n            \"score\": [score for scores in scores for score in scores],\n            \"weight\": [weight for weight in weights for _ in range(len(ids))],\n        }\n    )\n\n    # sort by score\n    df = df.sort_values(\"score\", ascending=False)\n\n    # select top_k results\n    df = df.head(top_k)\n\n    # return ids and scores\n    return df[\"id\"].tolist(), df[\"score\"].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize scores\n    scores_norm = []\n    for score in scores:\n        scores_norm.append([s / sum(score) for s in score])\n\n    # Compute convex combination of scores\n    scores_fused = []\n    for i in range(len(scores_norm[0])):\n        scores_fused.append(sum([scores_norm[j][i] * weights[j] for j in range(len(scores_norm))]))\n\n    # Sort results by score\n    results = sorted(zip(ids, scores_fused), key=lambda x: x[1], reverse=True)\n\n    # Select top_k results\n    results = results[:top_k]\n\n    # Unzip results\n    ids_fused, scores_fused = zip(*results)\n\n    return ids_fused, scores_fused"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Create a dictionary to store the combined scores for each id\n    combined_scores = {}\n\n    # Loop over the ids and scores tuples and combine the scores based on the weights\n    for ids_list, scores_list, weight in zip(ids, scores, weights):\n        for id, score in zip(ids_list, scores_list):\n            if id in combined_scores:\n                combined_scores[id] += score * weight\n            else:\n                combined_scores[id] = score * weight\n\n    # Sort the combined scores in descending order and select the top k results\n    top_ids = sorted(combined_scores, key=combined_scores.get, reverse=True)[:top_k]\n    top_scores = [combined_scores[id] for id in top_ids]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize the scores\n    scores_norm = [scores[i] / sum(scores[i]) for i in range(len(scores))]\n\n    # Create a dataframe with the scores and ids\n    df = pd.DataFrame({'ids': ids, 'scores': scores_norm})\n\n    # Calculate the convex combination\n    df['scores_norm'] = df['scores'] * weights[0]\n    df['scores_norm'] = df['scores_norm'].fillna(0)\n    for i in range(1, len(weights)):\n        df['scores_norm'] = df['scores_norm'] + df['scores'] * weights[i]\n\n    # Sort the dataframe by the scores_norm column\n    df = df.sort_values(by='scores_norm', ascending=False)\n\n    # Return the top k results\n    return df['ids'].iloc[:top_k].tolist(), df['scores_norm'].iloc[:top_k].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the input arguments are valid\n    assert len(ids) == len(scores), \"The length of ids and scores must be the same\"\n    assert len(ids) == len(weights), \"The length of ids and weights must be the same\"\n\n    # Normalize the weights\n    weights = [w / sum(weights) for w in weights]\n\n    # Create a dataframe with the ids and scores\n    df = pd.DataFrame({\n        'id': [item for sublist in ids for item in sublist],\n        'score': [item for sublist in scores for item in sublist]\n    })\n\n    # Create a new column with the normalized scores\n    df['norm_score'] = df['score'] * weights[0]\n\n    # Group by id and sum the normalized scores\n    df = df.groupby('id')['norm_score'].sum().reset_index()\n\n    # Sort the dataframe by the normalized scores in descending order\n    df = df.sort_values('norm_score', ascending=False)\n\n    # Select the top k results\n    df = df.head(top_k)\n\n    # Return the ids and scores as separate lists\n    return df['id'].tolist(), df['norm_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights), \"The number of ids, scores, and weights must match\"\n\n    assert sum(weights) == 1, \"The sum of the weights must equal 1\"\n\n    ids_list = []\n    scores_list = []\n\n    for i in range(len(ids)):\n        ids_list.append(ids[i])\n        scores_list.append(scores[i])\n\n    df = pd.DataFrame(\n        {\n            'ids': ids_list,\n            'scores': scores_list,\n            'weights': weights\n        }\n    )\n\n    df['normalized_scores'] = df['scores'] * df['weights']\n\n    df = df.sort_values(by='normalized_scores', ascending=False)\n\n    top_ids = df['ids'].iloc[:top_k].tolist()\n    top_scores = df['scores'].iloc[:top_k].tolist()\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the lengths of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of the ids and scores tuples must match\")\n\n    # Check if the lengths of the ids and scores tuples match\n    if len(ids) != len(weights):\n        raise ValueError(\"The lengths of the ids and weights tuples must match\")\n\n    # Check if the sum of the weights is equal to 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1\")\n\n    # Initialize empty lists to store the fused ids and scores\n    fused_ids = []\n    fused_scores = []\n\n    # Loop through each retrieval result\n    for i in range(len(ids)):\n\n        # Normalize the scores of the current retrieval result\n        scores_norm = [score / sum(scores[i]) for score in scores[i]]\n\n        # Combine the normalized scores with the corresponding weights\n        scores_combined = [score * weights[i] for score in scores_norm]\n\n        # Add the combined scores to the fused scores list\n        fused_scores.extend(scores_combined)\n\n        # Add the ids of the current retrieval result to the fused ids list\n        fused_ids.extend(ids[i])\n\n    # Combine the fused scores and ids into a single dataframe\n    df = pd.DataFrame({'id': fused_ids, 'score': fused_scores})\n\n    # Sort the dataframe by score in descending order\n    df = df.sort_values(by='score', ascending=False)\n\n    # Select the top k results\n    df = df.head(top_k)\n\n    # Return the fused ids and scores as separate lists\n    return df['id'].tolist(), df['score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Initialize empty lists for ids and scores\n    fused_ids = []\n    fused_scores = []\n\n    # Normalize the weights\n    normalized_weights = [weight / sum(weights) for weight in weights]\n\n    # Iterate over the ids and scores tuples\n    for i in range(len(ids)):\n\n        # Create a DataFrame with the ids and scores\n        df = pd.DataFrame({'ids': ids[i], 'scores': scores[i]})\n\n        # Sort the DataFrame by score in descending order\n        df = df.sort_values('scores', ascending=False)\n\n        # Select the top_k rows\n        df = df.head(top_k)\n\n        # Add the ids and scores to the fused lists\n        fused_ids.append(df['ids'].tolist())\n        fused_scores.append(df['scores'].tolist())\n\n    # Fuse the ids and scores using a convex combination method\n    fused_ids = [sum([normalized_weights[i] * ids[i][j] for i in range(len(ids))]) for j in range(top_k)]\n    fused_scores = [sum([normalized_weights[i] * scores[i][j] for i in range(len(scores))]) for j in range(top_k)]\n\n    # Return the fused ids and scores\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the lengths of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of the ids and scores tuples must match.\")\n\n    # Check if the length of the weights tuple matches the length of the ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of the weights tuple must match the length of the ids and scores tuples.\")\n\n    # Check if the sum of the weights is 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Create an empty list to store the fused ids and scores\n    fused_ids = []\n    fused_scores = []\n\n    # Iterate over the ids and scores tuples\n    for i in range(len(ids)):\n        # Normalize the scores of the current retrieval result\n        scores[i] = scores[i] / sum(scores[i])\n        # Compute the convex combination of the scores\n        fused_scores.append(scores[i] * weights[i])\n\n    # Compute the convex combination of the fused scores\n    fused_scores = pd.DataFrame(fused_scores).sum(axis=1).values\n    # Sort the fused scores in descending order\n    sorted_indices = fused_scores.argsort()[::-1]\n    # Select the top_k indices\n    top_indices = sorted_indices[:top_k]\n    # Select the corresponding ids and scores\n    fused_ids = [ids[i][j] for i in range(len(ids)) for j in top_indices]\n    fused_scores = [scores[i][j] for i in range(len(ids)) for j in top_indices]\n\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights)\n\n    if not isinstance(ids[0], list):\n        ids = [ids]\n        scores = [scores]\n        weights = [weights]\n\n    assert len(ids) == len(scores) == len(weights)\n\n    ids_fused = []\n    scores_fused = []\n\n    for i in range(len(ids)):\n        assert len(ids[i]) == len(scores[i])\n\n        ids_fused += ids[i]\n        scores_fused += scores[i]\n\n    df = pd.DataFrame({'ids': ids_fused, 'scores': scores_fused})\n\n    df['scores'] = df['scores'] * weights[i]\n\n    df = df.sort_values('scores', ascending=False).reset_index(drop=True)\n\n    ids_fused = df['ids'][:top_k].to_list()\n    scores_fused = df['scores'][:top_k].to_list()\n\n    return ids_fused, scores_fused"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(f\"The length of the ids, scores and weights tuples must be equal. \"\n                         f\"Got {len(ids)}, {len(scores)} and {len(weights)} respectively.\")\n\n    if sum(weights) != 1:\n        raise ValueError(f\"The sum of the weights must be equal to 1. Got {sum(weights)}.\")\n\n    if top_k < 1:\n        raise ValueError(f\"The top_k parameter must be greater than 0. Got {top_k}.\")\n\n    if any(len(ids[i]) != len(scores[i]) for i in range(len(ids))):\n        raise ValueError(f\"The length of the ids and scores lists must be equal. \"\n                         f\"Got {len(ids[0])} and {len(scores[0])} respectively.\")\n\n    # Normalize the scores of each retrieval result\n    scores = [scores[i] / sum(scores[i]) for i in range(len(scores))]\n\n    # Combine the scores of each retrieval result using a convex combination method\n    combined_scores = [sum(scores[i][j] * weights[i] for i in range(len(scores))) for j in range(len(scores[0]))]\n\n    # Select the top_k results based on the combined scores\n    top_k_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)[:top_k]\n    top_k_ids = [ids[i][j] for i in range(len(ids)) for j in top_k_indices]\n    top_k_scores = [combined_scores[j] for i in range(len(ids)) for j in top_k_indices]\n\n    return top_k_ids, top_k_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores), \"The length of ids must match the length of scores.\"\n    assert len(ids) == len(weights), \"The length of ids must match the length of weights.\"\n\n    fused_ids, fused_scores = [], []\n\n    for i in range(len(ids)):\n        assert len(ids[i]) == len(scores[i]), \"The length of ids[i] must match the length of scores[i].\"\n        assert 0 <= weights[i] <= 1, \"The weights must be between 0 and 1.\"\n\n        # Normalize the scores\n        scores[i] = [score / sum(scores[i]) for score in scores[i]]\n\n        # Combine the scores using the convex combination method\n        fused_scores.append([sum(w * s for w, s in zip(weights, scores[i])) for scores in zip(*scores)])\n\n        # Combine the ids using the convex combination method\n        fused_ids.append([sum(w * id for w, id in zip(weights, ids[i])) for ids in zip(*ids)])\n\n    # Select the top_k results\n    fused_scores = [sorted(scores, reverse=True)[:top_k] for scores in fused_scores]\n    fused_ids = [sorted(ids, key=lambda x: scores[ids.index(x)], reverse=True)[:top_k] for ids, scores in zip(fused_ids, fused_scores)]\n\n    return fused_ids, fused_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check input arguments\n    assert len(ids) == len(scores), \"The length of the ids tuple must match the length of the scores tuple\"\n    assert len(ids) == len(weights), \"The length of the ids tuple must match the length of the weights tuple\"\n\n    # Normalize the scores\n    normalized_scores = []\n    for i, score in enumerate(scores):\n        normalized_scores.append([s / sum(score) for s in score])\n\n    # Combine the scores\n    combined_scores = []\n    for i, score in enumerate(normalized_scores):\n        combined_scores.append([s * weights[i] for s in score])\n\n    # Sum the combined scores\n    summed_scores = []\n    for i in range(len(combined_scores[0])):\n        summed_scores.append(sum([combined_scores[j][i] for j in range(len(combined_scores))]))\n\n    # Sort the scores\n    sorted_scores = sorted(zip(summed_scores, range(len(summed_scores))), reverse=True)\n\n    # Select the top_k scores\n    top_scores = sorted_scores[:top_k]\n\n    # Get the ids for the top_k scores\n    top_ids = []\n    for score in top_scores:\n        top_ids.append(ids[score[1]][score[0]])\n\n    # Return the top_k scores and ids\n    return top_ids, top_scores\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check the input arguments\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of ids and scores must match.\")\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of weights must match the length of ids.\")\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must equal 1.\")\n\n    # Create a DataFrame to store the scores and ids\n    df = pd.DataFrame(columns=[\"id\", \"score\"])\n\n    # Loop through the ids and scores\n    for i in range(len(ids)):\n        # Check if the length of the ids and scores match\n        if len(ids[i]) != len(scores[i]):\n            raise ValueError(\"The length of ids and scores must match.\")\n\n        # Create a DataFrame for the current retrieval result\n        df_i = pd.DataFrame({\"id\": ids[i], \"score\": scores[i]})\n\n        # Add the current retrieval result to the DataFrame\n        df = pd.concat([df, df_i], ignore_index=True)\n\n    # Normalize the scores\n    df[\"score\"] = df[\"score\"] / df[\"score\"].sum()\n\n    # Combine the scores using a convex combination method\n    df[\"score\"] = df[\"score\"] * weights[0] + df[\"score\"] * weights[1]\n\n    # Sort the DataFrame by score in descending order\n    df = df.sort_values(by=\"score\", ascending=False)\n\n    # Select the top_k results\n    df = df.head(top_k)\n\n    # Return the fused ids and scores\n    return df[\"id\"].tolist(), df[\"score\"].tolist()\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    if len(ids) != len(scores):\n        raise ValueError(\"The number of ids and scores must match.\")\n\n    if len(ids) != len(weights):\n        raise ValueError(\"The number of weights must match the number of ids and scores.\")\n\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    if top_k < 1:\n        raise ValueError(\"The top_k parameter must be greater than 0.\")\n\n    if not all(isinstance(x, list) for x in ids):\n        raise ValueError(\"The ids parameter must be a tuple of lists.\")\n\n    if not all(isinstance(x, list) for x in scores):\n        raise ValueError(\"The scores parameter must be a tuple of lists.\")\n\n    if not all(isinstance(x, float) for x in weights):\n        raise ValueError(\"The weights parameter must be a tuple of floats.\")\n\n    if not all(len(x) == len(y) for x, y in zip(ids, scores)):\n        raise ValueError(\"The lengths of the ids and scores tuples must match.\")\n\n    # Normalize the scores\n    normalized_scores = []\n    for score_list in scores:\n        df = pd.DataFrame(score_list)\n        df[0] = df[0].apply(lambda x: x / df[0].sum())\n        normalized_scores.append(df[0].tolist())\n\n    # Combine the normalized scores using a convex combination method\n    fused_scores = []\n    for score_list, weight in zip(normalized_scores, weights):\n        fused_scores.extend([x * weight for x in score_list])\n\n    # Sort the fused scores and retrieve the top_k results\n    sorted_indices = sorted(range(len(fused_scores)), key=lambda i: fused_scores[i], reverse=True)\n    top_k_indices = sorted_indices[:top_k]\n\n    # Retrieve the fused"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x < 1e-3:\n        return f\"{x:.3e}\"\n    if x < 1e-2:\n        return f\"{x:.2e}\"\n    if x < 1e-1:\n        return f\"{x:.1e}\"\n    if x < 1e0:\n        return f\"{x:.0e}\"\n    if x < 1e1:\n        return f\"{x:.1f}\"\n    if x < 1e2:\n        return f\"{x:.2f}\"\n    if x < 1e3:\n        return f\"{x:.3f}\"\n    if x < 1e4:\n        return f\"{x:.4f}\"\n    if x < 1e5:\n        return f\"{x:.5f}\"\n    if x < 1e6:\n        return f\"{x:.6f}\"\n    if x < 1e7:\n        return f\"{x:.7f}\"\n    if x < 1e8:\n        return f\"{x:.8f}\"\n    if x < 1e9:\n        return f\"{x:.9f}\"\n    if x < 1e10:\n        return f\"{x:.10f}\"\n    if x < 1e11:\n        return f\"{x:.11f}\"\n    if x < 1e12:\n        return f\"{x:.12f}\"\n    if x < 1e13:\n        return f\"{x:.13f}\"\n    if x < 1e14:\n        return f\"{x:.14f}\"\n    if x < 1e15:\n        return f\"{x:.15f}\"\n    if x < 1e16:\n        return f\"{x:.16f}\"\n    if x < 1e17:\n        return f\"{x:.17f}\"\n    if x < 1e18:\n        return f\"{x:.18f}\"\n    if x < 1"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n        s = \"%\"\n    else:\n        s = \"\"\n    if x < 1e-3:\n        return f\"{x:.2e}{s}\"\n    if x < 1e-2:\n        return f\"{x:.3f}{s}\"\n    if x < 1e-1:\n        return f\"{x:.4f}{s}\"\n    if x < 1e0:\n        return f\"{x:.5f}{s}\"\n    if x < 1e1:\n        return f\"{x:.6f}{s}\"\n    if x < 1e2:\n        return f\"{x:.7f}{s}\"\n    if x < 1e3:\n        return f\"{x:.8f}{s}\"\n    if x < 1e4:\n        return f\"{x:.9f}{s}\"\n    if x < 1e5:\n        return f\"{x:.10f}{s}\"\n    if x < 1e6:\n        return f\"{x:.11f}{s}\"\n    if x < 1e7:\n        return f\"{x:.12f}{s}\"\n    if x < 1e8:\n        return f\"{x:.13f}{s}\"\n    if x < 1e9:\n        return f\"{x:.14f}{s}\"\n    if x < 1e10:\n        return f\"{x:.15f}{s}\"\n    if x < 1e11:\n        return f\"{x:.16f}{s}\"\n    if x < 1e12:\n        return f\"{x:.17f}{s}\"\n    if x < 1e13:\n        return f\"{x:.18f}{s}\"\n    if x < 1e14:\n        return f\"{x:.19f}{s}\"\n    if x < 1e15:\n        return f\"{x:.20f}{s}\"\n    if x < 1e16:\n        return f\"{"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x < 1e-2:\n        return f\"{x:.2e}\"\n    if x < 1e-1:\n        return f\"{x:.3f}\"\n    if x < 1e0:\n        return f\"{x:.4f}\"\n    if x < 1e1:\n        return f\"{x:.5f}\"\n    if x < 1e2:\n        return f\"{x:.6f}\"\n    if x < 1e3:\n        return f\"{x:.7f}\"\n    if x < 1e4:\n        return f\"{x:.8f}\"\n    if x < 1e5:\n        return f\"{x:.9f}\"\n    if x < 1e6:\n        return f\"{x:.10f}\"\n    if x < 1e7:\n        return f\"{x:.11f}\"\n    if x < 1e8:\n        return f\"{x:.12f}\"\n    if x < 1e9:\n        return f\"{x:.13f}\"\n    if x < 1e10:\n        return f\"{x:.14f}\"\n    if x < 1e11:\n        return f\"{x:.15f}\"\n    if x < 1e12:\n        return f\"{x:.16f}\"\n    if x < 1e13:\n        return f\"{x:.17f}\"\n    if x < 1e14:\n        return f\"{x:.18f}\"\n    if x < 1e15:\n        return f\"{x:.19f}\"\n    if x < 1e16:\n        return f\"{x:.20f}\"\n    if x < 1e17:\n        return f\"{x:.21f}\"\n    if x < 1e18:\n        return f\"{x:.22f}\"\n    if x < 1e19:\n        return f\"{x:.23f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if np.isinf(x):\n        return \"\u221e\"\n    if x == 0:\n        return \"0\"\n    if x < 0:\n        return \"-\" + format_measure(-x, percent)\n    if x < 1e-3:\n        return f\"{x:1.3e}\"\n    if x < 1e-2:\n        return f\"{x:1.2e}\"\n    if x < 1e-1:\n        return f\"{x:1.1e}\"\n    if x < 1e0:\n        return f\"{x:1.0e}\"\n    if x < 1e1:\n        return f\"{x:1.1f}\"\n    if x < 1e2:\n        return f\"{x:1.0f}\"\n    if x < 1e3:\n        return f\"{x:1.0f}\"\n    if x < 1e4:\n        return f\"{x:1.0f}\"\n    if x < 1e5:\n        return f\"{x:1.0f}\"\n    if x < 1e6:\n        return f\"{x:1.0f}\"\n    if x < 1e7:\n        return f\"{x:1.0f}\"\n    if x < 1e8:\n        return f\"{x:1.0f}\"\n    if x < 1e9:\n        return f\"{x:1.0f}\"\n    if x < 1e10:\n        return f\"{x:1.0f}\"\n    if x < 1e11:\n        return f\"{x:1.0f}\"\n    if x < 1e12:\n        return f\"{x:1.0f}\"\n    if x < 1e13:\n        return f\"{x:1.0f}\"\n    if x < 1e14:\n        return f\"{x:1.0f}\"\n    if x < 1e15:\n        return f\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1e-4:\n        return f\"{x:.4e}\"\n    if x < 1e-2:\n        return f\"{x:.3f}\"\n    if x < 1:\n        return f\"{x:.2f}\"\n    if x < 10:\n        return f\"{x:.1f}\"\n    if x < 100:\n        return f\"{x:.0f}\"\n    if x < 1e3:\n        return f\"{x:.0f}\"\n    if x < 1e4:\n        return f\"{x / 1e3:.1f}K\"\n    if x < 1e5:\n        return f\"{x / 1e3:.0f}K\"\n    if x < 1e6:\n        return f\"{x / 1e6:.1f}M\"\n    if x < 1e7:\n        return f\"{x / 1e6:.0f}M\"\n    if x < 1e8:\n        return f\"{x / 1e9:.1f}B\"\n    return f\"{x / 1e9:.0f}B\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1e-4:\n        return f\"{x:0.2e}\"\n    if x < 1e-2:\n        return f\"{x:0.3f}\"\n    if x < 1:\n        return f\"{x:0.4f}\"\n    if x < 10:\n        return f\"{x:0.5f}\"\n    if x < 100:\n        return f\"{x:0.6f}\"\n    if x < 1000:\n        return f\"{x:0.7f}\"\n    if x < 10000:\n        return f\"{x:0.8f}\"\n    if x < 100000:\n        return f\"{x:0.9f}\"\n    return f\"{x:0.10f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if np.abs(x) < 1e-1:\n        return f\"{x:0.2e}\"\n    if np.abs(x) < 1e-3:\n        return f\"{x:0.3f}\"\n    if np.abs(x) < 1e-4:\n        return f\"{x:0.4f}\"\n    if np.abs(x) < 1e-5:\n        return f\"{x:0.5f}\"\n    if np.abs(x) < 1e-6:\n        return f\"{x:0.6f}\"\n    return f\"{x:0.7f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1e-3:\n        return f\"{x:.3e}\"\n    if x < 1e-1:\n        return f\"{x:.2f}\"\n    if x < 1:\n        return f\"{x:.1f}\"\n    if x < 1e3:\n        return f\"{x:.0f}\"\n    if x < 1e6:\n        return f\"{x / 1e3:.0f}K\"\n    if x < 1e9:\n        return f\"{x / 1e6:.0f}M\"\n    if x < 1e12:\n        return f\"{x / 1e9:.0f}B\"\n    if x < 1e15:\n        return f\"{x / 1e12:.0f}T\"\n    if x < 1e18:\n        return f\"{x / 1e15:.0f}Q\"\n    return f\"{x / 1e18:.0f}Q\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1e-3:\n        return f\"{x:.2e}\"\n    if x < 1e-1:\n        return f\"{x:.3f}\"\n    if x < 1:\n        return f\"{x:.4f}\"\n    if x < 10:\n        return f\"{x:.5f}\"\n    if x < 100:\n        return f\"{x:.6f}\"\n    if x < 1000:\n        return f\"{x:.7f}\"\n    return f\"{x:.8f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if np.abs(x) < 1e-3:\n        return f\"{x:0.3f}%\"\n    elif np.abs(x) < 1e-2:\n        return f\"{x:0.2f}%\"\n    elif np.abs(x) < 1e-1:\n        return f\"{x:0.1f}%\"\n    else:\n        return f\"{x:0.0f}%\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n        s = \"%\"\n    else:\n        s = \"\"\n    if x == 0:\n        return f\"{x:g}{s}\"\n    if np.abs(x) < 1e-3:\n        return f\"{x:e}{s}\"\n    if np.abs(x) < 1:\n        return f\"{x:.3f}{s}\"\n    if np.abs(x) < 10:\n        return f\"{x:.2f}{s}\"\n    if np.abs(x) < 100:\n        return f\"{x:.1f}{s}\"\n    return f\"{x:.0f}{s}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1e-4:\n        return f\"{x:,.4f}\"\n    if x < 1e-3:\n        return f\"{x:,.3f}\"\n    if x < 1e-2:\n        return f\"{x:,.2f}\"\n    if x < 1e-1:\n        return f\"{x:,.1f}\"\n    return f\"{x:,.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x > 1000000000:\n        return f\"{x/1000000000:.0f}B\"\n    if x > 1000000:\n        return f\"{x/1000000:.1f}M\"\n    if x > 1000:\n        return f\"{x/1000:.1f}K\"\n    if x < 1 and x > 0:\n        return f\"{x:.3f}\"\n    if x < 0.01 and x > 0:\n        return f\"{x:.5f}\"\n    if x < 0.001 and x > 0:\n        return f\"{x:.7f}\"\n    if x < 1:\n        return f\"{x:.2f}\"\n    if x < 10:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x < 1e-1:\n        return f\"{x:.2e}\"\n    elif x < 1e-3:\n        return f\"{x:.3e}\"\n    elif x < 1e-6:\n        return f\"{x:.4e}\"\n    elif x < 1e-9:\n        return f\"{x:.5e}\"\n    else:\n        return f\"{x:.2f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x < 0.0001:\n        fmt = \"{:.4f}\"\n    elif x < 0.001:\n        fmt = \"{:.3f}\"\n    elif x < 0.01:\n        fmt = \"{:.2f}\"\n    else:\n        fmt = \"{:.1f}\"\n\n    return fmt.format(x)\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if np.abs(x) < 1e-3:\n        return f\"{x:0.3f}\"\n    elif np.abs(x) < 1e-2:\n        return f\"{x:0.2f}\"\n    elif np.abs(x) < 1e-1:\n        return f\"{x:0.1f}\"\n    else:\n        return f\"{x:0.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0%\" if percent else \"0\"\n    if np.abs(x) < 1:\n        if np.abs(x) < 0.001:\n            return f\"{x:.3e}\"\n        if np.abs(x) < 0.01:\n            return f\"{x:.2f}\"\n        return f\"{x:.1f}\"\n    if np.abs(x) < 10:\n        return f\"{x:.2f}\"\n    if np.abs(x) < 100:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1e-3:\n        return f\"{x:.4f}\"\n    if x < 1e-2:\n        return f\"{x:.3f}\"\n    if x < 1:\n        return f\"{x:.2f}\"\n    return f\"{x:.1f}\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if abs(x) < 1:\n        return f\"{x:,.3f}%\"\n    if abs(x) < 10:\n        return f\"{x:,.2f}%\"\n    return f\"{x:,.0f}%\"\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if np.abs(x) < 1e-6:\n        return f\"{x:.6f}\"\n    if np.abs(x) < 1e-3:\n        return f\"{x:.3f}\"\n    if np.abs(x) < 1e-1:\n        return f\"{x:.2f}\"\n    return f\"{x:.1f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 * 1024 * 1024)\n        if free_space_in_gb > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024.0 ** 3)\n        if free_space_in_gb > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024 * 1024 * 1024)\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024 * 1024 * 1024)\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024 * 1024 * 1024)\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024 * 1024 * 1024)\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    # Define the function to get the disk usage\n    def get_disk_usage(dir_path: str) -> int:\n        \"\"\"\n        This function calculates the disk usage of a specified directory in gigabytes.\n\n        Input Arguments\n        :param dir_path: str, The path of the directory whose disk usage is to be calculated.\n        :return: int, The disk usage of the directory in gigabytes.\n        \"\"\"\n        total, used, free = shutil.disk_usage(dir_path)\n        return free // (2**30)\n\n    # Continuously check the disk usage until it is lower than the threshold\n    while get_disk_usage(input_dir) < threshold_in_gb:\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024 ** 3)\n        if free_space > threshold_in_gb:\n            return\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free\n        if free_space < threshold_in_gb * 1024**3:\n            sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / 1024 / 1024 / 1024\n        if free_space_in_gb > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024**3)\n        if free_space_in_gb > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024**3)\n        if free_space < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space_in_gb = shutil.disk_usage(input_dir).free / (1024 * 1024 * 1024)\n        if free_space_in_gb > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024.0 ** 3)\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        statvfs = os.statvfs(input_dir)\n        available_space = statvfs.f_frsize * statvfs.f_bavail / 1024 / 1024 / 1024\n        if available_space < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    # Get the disk usage of the input directory\n    disk_usage = shutil.disk_usage(input_dir)\n\n    # Calculate the free space in gigabytes\n    free_space_in_gb = disk_usage.free / (1024 ** 3)\n\n    # Check if the free space is lower than the threshold\n    while free_space_in_gb < threshold_in_gb:\n        # Sleep for the specified time\n        sleep(sleep_time)\n\n        # Get the disk usage of the input directory\n        disk_usage = shutil.disk_usage(input_dir)\n\n        # Calculate the free space in gigabytes\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024**3)\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / 1024 / 1024 / 1024\n        if free_space_in_gb > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024 * 1024 * 1024)\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        statvfs = os.statvfs(input_dir)\n        free_space_in_gb = statvfs.f_frsize * statvfs.f_bfree / (1024.0 ** 3)\n        if free_space_in_gb > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\"'s\", \"\")\n    line_text = line_text.replace(\"\u2019s\", \"\")\n    line_text = line_text.replace(\"\u2019\", \"\")\n    line_text = line_text.replace(\"-\", \" \")\n    line_text = line_text.replace(\"\u2013\", \" \")\n    line_text = line_text.replace(\"\u2014\", \" \")\n    line_text = line_text.replace(\"  \", \" \")\n    line_text = line_text.replace(\"   \", \" \")\n    line_text = line_text.replace(\"    \", \" \")\n    line_text = line_text.replace(\"     \", \" \")\n    line_text = line_text.replace(\"      \", \" \")\n    line_text = line_text.replace(\"       \", \" \")\n    line_text = line_text.replace(\"        \", \" \")\n    line_text = line_text.replace(\"         \", \" \")\n    line_text = line_text.replace(\"          \", \" \")\n    line_text = line_text.replace(\"           \", \" \")\n    line_text = line_text.replace(\"            \", \" \")\n    line_text = line_text.replace(\"             \", \" \")\n    line_text = line_text.replace(\"              \", \" \")\n    line_text = line_text.replace(\"               \", \" \")\n    line_text = line_text.replace(\"                \", \" \")\n    line_text = line_text.replace(\"                 \", \" \")\n    line_text = line_text.replace(\"                  \", \" \")\n    line_text = line_text.replace(\"                   \", \" \")\n    line_text = line_text.replace(\"                    \", \" \")\n    line_text = line_text.replace(\"                     \", \" \")\n    line_text = line_text.replace(\"                      \", \" \")\n    line_text = line_text.replace(\"                       \", \" \")\n    line_text = line_text.replace(\"                        \", \" \")\n    line_text = line_text.replace(\"                         \", \" \")\n    line_text = line_text.replace(\"                          \", \" \")"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = re.sub(r'\\s+', '', line_text)\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(modified_text)\n\n    # Join the tokens back into a string with spaces between them\n    modified_text = ' '.join(tokens)\n\n    return modified_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Return the segmented parts of the modified input text\n    return \" \".join(tokens)\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Join the tokens back into a single string with spaces\n    fixed_text = \" \".join(tokens)\n\n    return fixed_text\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\"'s\", \"\")\n    line_text = line_text.replace(\"\u2019s\", \"\")\n    line_text = line_text.replace(\"\u2019\", \"\")\n    line_text = line_text.replace(\"'\", \"\")\n    line_text = line_text.replace(\"-\", \" \")\n    line_text = line_text.replace(\"/\", \" \")\n    line_text = line_text.replace(\".\", \"\")\n    line_text = line_text.replace(\",\", \"\")\n    line_text = line_text.replace(\";\", \"\")\n    line_text = line_text.replace(\":\", \"\")\n    line_text = line_text.replace(\"?\", \"\")\n    line_text = line_text.replace(\"(\", \"\")\n    line_text = line_text.replace(\")\", \"\")\n    line_text = line_text.replace(\"[\", \"\")\n    line_text = line_text.replace(\"]\", \"\")\n    line_text = line_text.replace(\"{\", \"\")\n    line_text = line_text.replace(\"}\", \"\")\n    line_text = line_text.replace(\"<\", \"\")\n    line_text = line_text.replace(\">\", \"\")\n    line_text = line_text.replace(\"*\", \"\")\n    line_text = line_text.replace(\"&\", \"\")\n    line_text = line_text.replace(\"^\", \"\")\n    line_text = line_text.replace(\"+\", \"\")\n    line_text = line_text.replace(\"=\", \"\")\n    line_text = line_text.replace(\"|\", \"\")\n    line_text = line_text.replace(\"\\\\\", \"\")\n    line_text = line_text.replace(\"~\", \"\")\n    line_text = line_text.replace(\"`\", \"\")\n    line_text = line_text.replace(\"@\", \"\")\n    line_text = line_text.replace(\"#\", \"\")\n    line_text = line_text.replace(\"%\", \"\")\n    line_text ="}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\"'s\", \"\")\n    line_text = line_text.replace(\"\u2019s\", \"\")\n    line_text = line_text.replace(\"-\", \" \")\n    line_text = line_text.replace(\"/\", \" \")\n    line_text = line_text.replace(\"\\\\\", \" \")\n    line_text = line_text.replace(\"(\", \" \")\n    line_text = line_text.replace(\")\", \" \")\n    line_text = line_text.replace(\".\", \" \")\n    line_text = line_text.replace(\",\", \" \")\n    line_text = line_text.replace(\"?\", \" \")\n    line_text = line_text.replace(\";\", \" \")\n    line_text = line_text.replace(\":\", \" \")\n    line_text = line_text.replace(\"*\", \" \")\n    line_text = line_text.replace(\"&\", \" \")\n    line_text = line_text.replace(\"$\", \" \")\n    line_text = line_text.replace(\"'\", \" \")\n    line_text = line_text.replace('\"', \" \")\n    line_text = line_text.replace(\"\u2019\", \" \")\n    line_text = line_text.replace(\"\u2018\", \" \")\n    line_text = line_text.replace(\"\u2019\", \" \")\n    line_text = line_text.replace(\"\u2018\", \" \")\n    line_text = line_text.replace(\"\u201c\", \" \")\n    line_text = line_text.replace(\"\u201d\", \" \")\n    line_text = line_text.replace(\"\u2014\", \" \")\n    line_text = line_text.replace(\"\u2013\", \" \")\n    line_text = line_text.replace(\"\u2022\", \" \")\n    line_text = line_text.replace(\"\u27a2\", \" \")\n    line_text = line_text.replace(\"\u0192\", \" \")\n    line_text = line_text.replace(\"\uf0b7\", \" \")\n    line_text = line_text.replace(\"\u2022\", \" \")\n    line_text = line_text.replace(\"\u27a2"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"-\", \" \")\n    line_text = line_text.replace(\"\u2013\", \" \")\n    line_text = line_text.replace(\"\u2014\", \" \")\n    line_text = line_text.replace(\"\u2015\", \" \")\n    line_text = line_text.replace(\"\u2043\", \" \")\n    line_text = line_text.replace(\"\u207b\", \" \")\n    line_text = line_text.replace(\"\u2212\", \" \")\n    line_text = line_text.replace(\"\u27a2\", \" \")\n    line_text = line_text.replace(\"\u2794\", \" \")\n    line_text = line_text.replace(\"\u27b0\", \" \")\n    line_text = line_text.replace(\"\u27bd\", \" \")\n    line_text = line_text.replace(\"\u27be\", \" \")\n    line_text = line_text.replace(\"\u27bf\", \" \")\n    line_text = line_text.replace(\"\u27bb\", \" \")\n    line_text = line_text.replace(\"\u27bc\", \" \")\n    line_text = line_text.replace(\"\u27ba\", \" \")\n    line_text = line_text.replace(\"\u27b9\", \" \")\n    line_text = line_text.replace(\"\u27b8\", \" \")\n    line_text = line_text.replace(\"\u27b7\", \" \")\n    line_text = line_text.replace(\"\u27b6\", \" \")\n    line_text = line_text.replace(\"\u27b5\", \" \")\n    line_text = line_text.replace(\"\u27b4\", \" \")\n    line_text = line_text.replace(\"\u27b3\", \" \")\n    line_text = line_text.replace(\"\u27b2\", \" \")\n    line_text = line_text.replace(\"\u27b1\", \" \")\n    line_text = line_text.replace(\"\u27af\", \" \")\n    line_text = line_text.replace(\"\u27ae\", \" \")\n    line_text = line"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = su.segment(modified_text)\n\n    # Return the segmented parts of the modified text as a list of strings\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = su.segment(line_text)\n\n    return tokens\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Return the segmented parts of the modified input text\n    return \" \".join(tokens)\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = line_text.split(\" \")\n\n    # Return the segmented parts of the modified input text\n    return tokens\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Join the tokens back into a single string\n    line_text = \" \".join(tokens)\n\n    return line_text\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = re.sub(r'\\s+', '', line_text)\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(modified_text)\n\n    # Join the tokens back into a single string\n    modified_text = ' '.join(tokens)\n\n    return modified_text\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = re.sub(r\"\\s+\", \"\", line_text)\n    line_text = su.segment(line_text)\n    return line_text\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Removing all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n\n    # Segmenting the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Joining the tokens back into a single string\n    line_text = \" \".join(tokens)\n\n    return line_text\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Join the tokens back into a single string with spaces between them\n    line_text = \" \".join(tokens)\n\n    return line_text\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text.\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens.\n    tokens = nlm_tokenize(line_text)\n\n    # Return the segmented parts of the modified input text.\n    return \" \".join(tokens)\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Return the segmented parts of the modified input text\n    return tokens\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all spaces from the input text\n    line_text = \"\".join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Return the segmented parts of the modified text\n    return tokens\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Join the tokens back into a single string with spaces\n    line_text = \" \".join(tokens)\n\n    return line_text\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n\n    weights = np.random.rand(n)\n    weights /= weights.sum()\n\n    if zeros > 0:\n        indices = np.random.choice(n, zeros, replace=False)\n        weights[indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the total number of weights.\")\n\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    weights = np.append(weights, np.zeros(zeros))\n    weights = weights / np.sum(weights)\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.random.dirichlet(np.ones(n))\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"Number of zeros cannot exceed the total number of weights.\")\n\n    # Generate n-zeros and (n-zeros)+1 random weights\n    weights = np.zeros(n - zeros)\n    weights = np.append(weights, np.random.rand(zeros + 1))\n\n    # Normalize the weights\n    weights = weights / np.sum(weights)\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate n random weights that sum to one\n    weights = np.random.dirichlet(np.ones(n))\n\n    # If zeros is greater than 0, randomly set zeros weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, size=zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot be greater than the number of weights\")\n\n    weights = np.random.dirichlet(np.ones(n - zeros), size=1)[0]\n    weights /= np.sum(weights)\n    weights = np.append(weights, np.zeros(zeros))\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"zeros must not exceed n\")\n\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    weights = np.insert(weights, 0, 0)\n    weights = np.append(weights, 0)\n    weights = weights / np.sum(weights)\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate n random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n))\n\n    # Set zeros random weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate n-1 random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n - 1))\n\n    # Add a zero weight to the end of the array\n    weights = np.append(weights, 0)\n\n    # Sort the weights in descending order\n    weights = np.sort(weights)[::-1]\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        weights[zeros:] = 0\n\n    # Normalize the weights to sum to one\n    weights /= np.sum(weights)\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"Number of zeros cannot be greater than the total number of weights.\")\n\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    weights = np.append(weights, np.zeros(zeros))\n    return weights / np.sum(weights)\n\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the number of weights.\")\n\n    # Generate n-zeros and n-1-ones\n    weights = np.zeros(n - zeros)\n    weights = np.append(weights, np.ones(n - zeros))\n\n    # Shuffle the array\n    np.random.shuffle(weights)\n\n    # Normalize the array\n    weights = weights / np.sum(weights)\n\n    return weights\n\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n\n    # Generate n-1 random weights between 0 and 1\n    weights = np.random.rand(n - 1)\n\n    # Sort the weights in ascending order\n    weights = np.sort(weights)\n\n    # Add a zero at the beginning and end of the array\n    weights = np.concatenate(([0], weights, [1]))\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        indices = np.random.choice(n, zeros, replace=False)\n        weights[indices] = 0\n\n    # Normalize the weights to sum up to one\n    weights = weights / np.sum(weights)\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\n            \"The number of zeros cannot be greater than the number of weights.\"\n        )\n\n    weights = np.random.rand(n)\n    weights /= np.sum(weights)\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate a random vector of size n\n    weights = np.random.rand(n)\n\n    # Normalize the vector to sum up to one\n    weights /= np.sum(weights)\n\n    # If zeros is greater than zero, set the specified number of elements to zero\n    if zeros > 0:\n        indices = np.random.choice(n, zeros, replace=False)\n        weights[indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check if the number of zeros is valid\n    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n\n    # Generate n-zeros and (n-zeros) random weights\n    weights = np.zeros(n - zeros)\n    weights = np.append(weights, np.random.dirichlet(np.ones(n - zeros)))\n\n    # Shuffle the weights to randomize their order\n    np.random.shuffle(weights)\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros must be less than or equal to the total number of weights.\")\n\n    weights = np.random.uniform(0, 1, n)\n    weights = weights / weights.sum()\n\n    if zeros > 0:\n        indices_to_zero = np.random.choice(n, zeros, replace=False)\n        weights[indices_to_zero] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Create a list of indices for the non-zero weights\n    non_zero_indices = np.random.choice(n, n - zeros, replace=False)\n\n    # Set the weights at the non-zero indices to 1/n\n    weights = np.zeros(n)\n    weights[non_zero_indices] = 1 / (n - zeros)\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate n random weights that sum to one\n    weights = np.random.dirichlet(np.ones(n))\n\n    # If zeros is greater than zero, randomly set some weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\n            f\"The number of zeros ({zeros}) cannot be greater than the total number of weights ({n}).\"\n        )\n\n    weights = np.zeros(n)\n    remaining_weights = n - zeros\n\n    # Set the remaining weights to random values\n    weights[:remaining_weights] = np.random.random(remaining_weights)\n\n    # Normalize the weights to sum to one\n    weights /= weights.sum()\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        random_indices = np.random.choice(\n            remaining_weights, zeros, replace=False\n        )\n        weights[random_indices] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n\n    # Generate n random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n))\n\n    # If zeros > 0, randomly choose indices to set to zero\n    if zeros > 0:\n        indices_to_zero = np.random.choice(n, zeros, replace=False)\n        weights[indices_to_zero] = 0\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type=module_type, module_param=module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type=module_type, module_param=module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = module_dict\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop(\"module_type\")\n        module_param = deepcopy(module_dict)\n        return cls(module_type=module_type, module_param=module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type=module_type, module_param=module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop(\"module_type\")\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = module_dict\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = module_dict\n        return cls(module_type=module_type, module_param=module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = module_dict\n        return cls(module_type=module_type, module_param=module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    crop_transform = T.CropTransform(0, 0, image_size[0], image_size[1])\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox_center_x = (bbox[0] + bbox[2]) / 2\n    bbox_center_y = (bbox[1] + bbox[3]) / 2\n    crop_w, crop_h = crop_size\n    crop_x1 = max(0, bbox_center_x - crop_w / 2)\n    crop_y1 = max(0, bbox_center_y - crop_h / 2)\n    crop_x2 = min(image_size[1], bbox_center_x + crop_w / 2)\n    crop_y2 = min(image_size[0], bbox_center_y + crop_h / 2)\n    crop_transform.set_params(crop_x1, crop_y1, crop_x2 - crop_x1, crop_y2 - crop_y1)\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the bounding box to absolute coordinates\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    bbox_center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n\n    # Calculate the top-left corner of the cropping region\n    crop_top_left = [\n        max(0, bbox_center[0] - crop_size[1] // 2),\n        max(0, bbox_center[1] - crop_size[0] // 2),\n    ]\n\n    # Adjust the top-left corner to ensure that the cropping region fits within the image boundaries\n    crop_top_left[0] = min(crop_top_left[0], image_size[1] - crop_size[1])\n    crop_top_left[1] = min(crop_top_left[1], image_size[0] - crop_size[0])\n\n    # Calculate the bottom-right corner of the cropping region\n    crop_bottom_right = [\n        min(image_size[1], crop_top_left[0] + crop_size[1]),\n        min(image_size[0], crop_top_left[1] + crop_size[0]),\n    ]\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_top_left, crop_bottom_right)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the instance's bounding box to XYXY_ABS mode\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Calculate the crop region based on the instance's bounding box and the desired crop size\n    crop_x1 = max(0, bbox[0] - (crop_size[1] - bbox[2]) // 2)\n    crop_y1 = max(0, bbox[1] - (crop_size[0] - bbox[3]) // 2)\n    crop_x2 = min(image_size[1], bbox[2] + (crop_size[1] - bbox[2]) // 2)\n    crop_y2 = min(image_size[0], bbox[3] + (crop_size[0] - bbox[3]) // 2)\n\n    # Adjust the crop region to ensure it fits within the image boundaries\n    crop_x1 = max(0, crop_x1)\n    crop_y1 = max(0, crop_y1)\n    crop_x2 = min(image_size[1], crop_x2)\n    crop_y2 = min(image_size[0], crop_y2)\n\n    # Create a CropTransform object with the calculated crop region\n    crop_transform = T.CropTransform(x1=crop_x1, y1=crop_y1, x2=crop_x2, y2=crop_y2)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Calculate the center of the instance's bounding box\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox_center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n\n    # Calculate the crop size based on the desired size and the aspect ratio of the instance's bounding box\n    crop_aspect_ratio = crop_size[1] / crop_size[0]\n    bbox_aspect_ratio = (bbox[2] - bbox[0]) / (bbox[3] - bbox[1])\n    if crop_aspect_ratio > bbox_aspect_ratio:\n        crop_height = crop_size[0]\n        crop_width = int(crop_height * bbox_aspect_ratio)\n    else:\n        crop_width = crop_size[1]\n        crop_height = int(crop_width / bbox_aspect_ratio)\n\n    # Calculate the top-left corner of the crop region\n    crop_x = int(bbox_center[0] - crop_width / 2)\n    crop_y = int(bbox_center[1] - crop_height / 2)\n\n    # Adjust the crop region to ensure it fits within the image boundaries\n    crop_x = max(0, crop_x)\n    crop_y = max(0, crop_y)\n    crop_x = min(crop_x, image_size[1] - crop_width)\n    crop_y = min(crop_y, image_size[0] - crop_height)\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = T.CropTransform(x=crop_x, y=crop_y, w=crop_width, h=crop_height)\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox = bbox.clip(0, max=image_size)\n    crop_x = bbox[0] + (bbox[2] - bbox[0]) / 2 - crop_size[0] / 2\n    crop_y = bbox[1] + (bbox[3] - bbox[1]) / 2 - crop_size[1] / 2\n    crop_x = max(0, crop_x)\n    crop_y = max(0, crop_y)\n    crop_x = min(crop_x, image_size[0] - crop_size[0])\n    crop_y = min(crop_y, image_size[1] - crop_size[1])\n    return T.CropTransform(crop_x, crop_y, crop_size[0], crop_size[1])\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    crop_w, crop_h = crop_size\n    image_w, image_h = image_size\n\n    bbox = instance['bbox']\n    bbox_mode = instance['bbox_mode']\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Adjust the bounding box to ensure it fits within the image boundaries\n    bbox[0] = max(0, bbox[0])\n    bbox[1] = max(0, bbox[1])\n    bbox[2] = min(image_w, bbox[2])\n    bbox[3] = min(image_h, bbox[3])\n\n    # Calculate the center of the bounding box\n    bbox_center_x = (bbox[0] + bbox[2]) / 2\n    bbox_center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the cropping region\n    crop_x = max(0, bbox_center_x - crop_w / 2)\n    crop_y = max(0, bbox_center_y - crop_h / 2)\n\n    # Adjust the cropping region to ensure it fits within the image boundaries\n    crop_x = min(crop_x, image_w - crop_w)\n    crop_y = min(crop_y, image_h - crop_h)\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = T.CropTransform(x_min=crop_x, y_min=crop_y, x_max=crop_x + crop_w, y_max=crop_y + crop_h)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the bounding box coordinates to XYXY format\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Convert the bounding box coordinates to XYXY format\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Compute the center of the bounding box\n    center = np.mean(bbox, axis=0)\n    # Compute the top-left corner of the cropping region\n    x1 = max(0, center[0] - crop_size[1] // 2)\n    y1 = max(0, center[1] - crop_size[0] // 2)\n    # Compute the bottom-right corner of the cropping region\n    x2 = min(image_size[1], x1 + crop_size[1])\n    y2 = min(image_size[0], y1 + crop_size[0])\n    # Compute the cropping region\n    crop = np.array([x1, y1, x2, y2])\n    # Create a CropTransform object\n    crop_transform = T.CropTransform(crop, crop_size)\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the instance's bounding box to the format used by Detectron2\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Get the center of the instance's bounding box\n    center = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2])\n    # Calculate the cropping region's top-left corner\n    top_left = center - np.array(crop_size) / 2\n    # Ensure the cropping region fits within the image boundaries\n    top_left = np.maximum(top_left, 0)\n    top_left = np.minimum(top_left, np.array(image_size) - np.array(crop_size))\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(top_left, crop_size)\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box coordinates and mode of the instance\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n\n    # Convert the bounding box to the XYXY_ABS mode\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Compute the cropping region\n    center_x, center_y = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n    crop_x = max(0, center_x - crop_size[0] / 2)\n    crop_y = max(0, center_y - crop_size[1] / 2)\n    crop_w = min(crop_size[0], image_size[0] - crop_x)\n    crop_h = min(crop_size[1], image_size[1] - crop_y)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(x_min=crop_x, y_min=crop_y, x_max=crop_x + crop_w, y_max=crop_y + crop_h)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box coordinates and mode from the instance\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n\n    # Convert the bounding box to absolute coordinates\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Compute the crop region\n    crop_region = compute_crop_region(crop_size, image_size, bbox)\n\n    # Create a CropTransform object with the computed crop region\n    crop_transform = T.CropTransform(crop_region)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the bounding box coordinates to the format expected by the CropTransform\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Calculate the coordinates of the top-left corner of the crop\n    x1 = max(0, bbox[0] - crop_size[0] // 2)\n    y1 = max(0, bbox[1] - crop_size[1] // 2)\n\n    # Calculate the dimensions of the crop\n    x2 = min(image_size[0], x1 + crop_size[0])\n    y2 = min(image_size[1], y1 + crop_size[1])\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(x1, y1, x2, y2)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    bbox_center = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2])\n\n    # Calculate the size of the crop\n    crop_size = np.array(crop_size)\n    crop_size = crop_size.astype(np.float32)\n\n    # Calculate the size of the crop region\n    crop_region_size = crop_size * 1.25\n\n    # Calculate the top-left corner of the crop region\n    crop_region_top_left = bbox_center - crop_region_size / 2\n\n    # Calculate the bottom-right corner of the crop region\n    crop_region_bottom_right = bbox_center + crop_region_size / 2\n\n    # Clip the crop region to fit within the image boundaries\n    crop_region_top_left = np.maximum(crop_region_top_left, np.zeros_like(crop_region_top_left))\n    crop_region_bottom_right = np.minimum(\n        crop_region_bottom_right, np.array(image_size)\n    )\n\n    # Calculate the size of the final crop\n    crop_size = crop_region_bottom_right - crop_region_top_left\n\n    # Calculate the top-left corner of the final crop\n    crop_top_left = bbox_center - crop_size / 2\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_top_left, crop_size)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the instance's bounding box and mode\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n\n    # Convert the bounding box to the XYXY_REL format\n    bbox_xyxy_rel = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_REL)\n\n    # Calculate the crop's top-left corner coordinates\n    crop_x = max(0, bbox_xyxy_rel[0] - crop_size[1] // 2)\n    crop_y = max(0, bbox_xyxy_rel[1] - crop_size[0] // 2)\n\n    # Calculate the crop's dimensions\n    crop_w = min(image_size[1] - crop_x, crop_size[1])\n    crop_h = min(image_size[0] - crop_y, crop_size[0])\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(x=crop_x, y=crop_y, w=crop_w, h=crop_h)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the bounding box from the instance annotation to a numpy array\n    bbox = np.array(instance[\"bbox\"])\n    # Convert the bounding box mode to XYXY_ABS\n    bbox = BoxMode.convert(bbox, instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Convert the bounding box to a list of coordinates\n    bbox = bbox.tolist()\n    # Calculate the center of the bounding box\n    center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n    # Calculate the crop size based on the aspect ratio of the bounding box\n    crop_size = [crop_size[0], crop_size[1]]\n    crop_size[0] = int(crop_size[1] * (bbox[2] - bbox[0]) / (bbox[3] - bbox[1]))\n    crop_size[1] = int(crop_size[0] * (bbox[3] - bbox[1]) / (bbox[2] - bbox[0]))\n    # Calculate the top-left corner of the cropping region\n    top_left = [center[0] - crop_size[0] / 2, center[1] - crop_size[1] / 2]\n    # Ensure that the cropping region does not extend beyond the image boundaries\n    top_left[0] = max(0, top_left[0])\n    top_left[1] = max(0, top_left[1])\n    top_left[0] = min(image_size[1] - crop_size[0], top_left[0])\n    top_left[1] = min(image_size[0] - crop_size[1], top_left[1])\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = T.CropTransform(top_left, crop_size)\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box and its mode from the instance\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n\n    # Convert the bounding box to absolute coordinates\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Compute the center point of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Compute the crop size based on the aspect ratio of the original image\n    crop_height = crop_size[0]\n    crop_width = crop_size[1]\n    aspect_ratio = image_size[1] / image_size[0]\n    if crop_height / crop_width > aspect_ratio:\n        crop_height = int(crop_width * aspect_ratio)\n    else:\n        crop_width = int(crop_height / aspect_ratio)\n\n    # Compute the top-left corner of the crop region\n    top = int(center_y - crop_height / 2)\n    left = int(center_x - crop_width / 2)\n\n    # Ensure the crop region does not extend beyond the image boundaries\n    top = max(0, top)\n    left = max(0, left)\n    bottom = min(image_size[0], top + crop_height)\n    right = min(image_size[1], left + crop_width)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(top, left, bottom, right)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the instance's bounding box and mode\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n\n    # Convert the bounding box to absolute coordinates\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Calculate the center of the instance's bounding box\n    center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    # Calculate the cropping region's top-left corner\n    top_left = (\n        max(center[0] - crop_size[1] / 2, 0),\n        max(center[1] - crop_size[0] / 2, 0),\n    )\n\n    # Calculate the cropping region's dimensions\n    crop_size = (\n        min(crop_size[0], image_size[0] - top_left[1]),\n        min(crop_size[1], image_size[1] - top_left[0]),\n    )\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(top_left, crop_size)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the instance bounding box to the XYXY_ABS mode\n    instance_bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Compute the center of the instance bounding box\n    instance_center = [(instance_bbox[0] + instance_bbox[2]) / 2, (instance_bbox[1] + instance_bbox[3]) / 2]\n\n    # Compute the crop size based on the aspect ratio of the image\n    if image_size[0] / image_size[1] > crop_size[0] / crop_size[1]:\n        crop_height = crop_size[0]\n        crop_width = int(crop_size[0] * image_size[1] / image_size[0])\n    else:\n        crop_height = int(crop_size[1] * image_size[0] / image_size[1])\n        crop_width = crop_size[1]\n\n    # Compute the top-left corner of the crop region\n    crop_top = max(0, int(instance_center[1] - crop_height / 2))\n    crop_left = max(0, int(instance_center[0] - crop_width / 2))\n\n    # Adjust the crop region to ensure it fits within the image boundaries\n    if crop_top + crop_height > image_size[0]:\n        crop_top = max(0, image_size[0] - crop_height)\n    if crop_left + crop_width > image_size[1]:\n        crop_left = max(0, image_size[1] - crop_width)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_top, crop_left, crop_height, crop_width)\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n\n    # Compute the center of the bounding box\n    bbox_center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n\n    # Compute the maximum possible crop size\n    max_crop_size = min(image_size[0] - bbox_center[1], bbox_center[1], image_size[1] - bbox_center[0], bbox_center[0])\n\n    # Compute the crop size based on the desired crop size and the maximum possible crop size\n    crop_size = min(crop_size, max_crop_size)\n\n    # Compute the top-left corner of the crop region\n    crop_top_left = [bbox_center[0] - crop_size / 2, bbox_center[1] - crop_size / 2]\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_top_left, (crop_size, crop_size))\n\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox = bbox.clip(0, image_size[0]), bbox.clip(0, image_size[1])\n\n    crop_x = bbox[0] + (bbox[2] - bbox[0]) / 2 - crop_size[0] / 2\n    crop_y = bbox[1] + (bbox[3] - bbox[1]) / 2 - crop_size[1] / 2\n    crop_x = max(0, crop_x)\n    crop_y = max(0, crop_y)\n    crop_x = min(crop_x, image_size[0] - crop_size[0])\n    crop_y = min(crop_y, image_size[1] - crop_size[1])\n\n    crop_transform = T.CropTransform(x=crop_x, y=crop_y, w=crop_size[0], h=crop_size[1])\n    return crop_transform\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n\n    # Convert the bounding box to XYXY_ABS mode\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Compute the center of the instance's bounding box\n    center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    # Compute the top-left corner of the cropping region\n    top_left = (\n        int(center[0] - crop_size[0] / 2),\n        int(center[1] - crop_size[1] / 2),\n    )\n\n    # Compute the dimensions of the cropping region\n    crop_size = (\n        min(crop_size[0], image_size[0] - top_left[0]),\n        min(crop_size[1], image_size[1] - top_left[1]),\n    )\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(*top_left, *crop_size)\n\n    return crop_transform\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  return x / jnp.maximum(jnp.sqrt(jnp.sum(x * x, axis=-1, keepdims=True)), grad_eps)\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x along the last axis\n  x_squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent exploding gradients during the backward pass\n  x_squared_norm_clamped = jnp.maximum(x_squared_norm, grad_eps)\n\n  # Compute the denominator for normalization, accounting for the clamped squared norm\n  denominator = jnp.sqrt(x_squared_norm_clamped)\n\n  # Compute the denominator for normalization, accounting for the clamped squared norm\n  denominator = jnp.maximum(denominator, grad_eps)\n\n  # Compute the normalized array, accounting for the clamped denominator\n  x_normalized = x / denominator\n\n  return x_normalized\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_squared_norm = jnp.sum(jnp.square(x), axis=-1, keepdims=True)\n  x_squared_norm_clamped = jnp.maximum(x_squared_norm, grad_eps)\n  x_inv_norm = jnp.sqrt(x_squared_norm_clamped)\n  x_inv_norm = jnp.maximum(x_inv_norm, 1.0 / jnp.sqrt(grad_eps))\n  x_normalized = x / x_inv_norm\n  return x_normalized\n\n\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Clamp the squared norm to a minimum value to prevent exploding gradients.\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n  clamped_norm = jnp.maximum(squared_norm, grad_eps)\n\n  # Compute the denominator and clamp it to a minimum value to prevent division by zero.\n  denominator = jnp.sqrt(clamped_norm)\n  clamped_denominator = jnp.maximum(denominator, grad_eps)\n\n  # Compute the normalized vector.\n  return x / clamped_denominator\n\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of x\n  squared_norm = jnp.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent exploding gradients\n  squared_norm = jnp.maximum(squared_norm, grad_eps)\n\n  # Calculate the inverse square root of the squared norm\n  inv_sqrt_norm = 1.0 / jnp.sqrt(squared_norm)\n\n  # Clamp the inverse square root to a minimum value to prevent exploding gradients\n  inv_sqrt_norm = jnp.maximum(inv_sqrt_norm, 1.0 / jnp.sqrt(grad_eps))\n\n  # Divide x by the inverse square root of the squared norm to normalize it\n  return x * inv_sqrt_norm\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of x\n  norm_sq = jnp.sum(x ** 2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value during the backward pass\n  norm_sq = jnp.maximum(norm_sq, grad_eps)\n\n  # Calculate the normalization factor\n  norm_factor = jnp.sqrt(norm_sq)\n\n  # Clamp the normalization factor to a minimum value during the backward pass\n  norm_factor = jnp.maximum(norm_factor, grad_eps)\n\n  # Divide x by the normalization factor\n  x_norm = x / norm_factor\n\n  return x_norm\n\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  return x / jnp.maximum(jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True)), grad_eps)\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of x along the last axis.\n  squared_norm = jnp.sum(x * x, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent division by zero during the backward pass.\n  squared_norm = jnp.maximum(squared_norm, grad_eps)\n\n  # Calculate the normalization factor for the backward pass.\n  # The denominator is clamped to a minimum value to prevent division by zero.\n  denom = jnp.sqrt(squared_norm)\n  denom = jnp.maximum(denom, grad_eps)\n\n  # Calculate the normalization factor for the forward pass.\n  # The denominator is clamped to a minimum value to prevent division by zero.\n  denom_forward = jnp.maximum(denom, grad_eps)\n\n  # Calculate the normalized array.\n  x = x / denom_forward\n\n  # Calculate the gradient of the normalization factor for the backward pass.\n  # The gradient is clamped to a minimum value to prevent exploding gradients.\n  grad_denom = x / denom\n  grad_denom = jnp.maximum(grad_denom, grad_eps)\n\n  # Calculate the gradient of the normalization factor for the forward pass.\n  # The gradient is clamped to a minimum value to prevent exploding gradients.\n  grad_denom_forward = grad_denom / denom_forward\n  grad_denom_forward = jnp.maximum(grad_denom_forward, grad_eps)\n\n  # Calculate the gradient of the normalized array.\n  grad_x = grad_denom_forward - x * jnp.sum(grad_denom * x, axis=-1, keepdims=True) / squared_norm\n\n  return x, grad_x\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of the input vector(s)\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Calculate the denominator for normalization, clamping it to a minimum value\n  # to prevent exploding gradients during the backward pass\n  denom = jnp.maximum(squared_norm, grad_eps**2)\n\n  # Calculate the normalization factor, clamping it to a minimum value to prevent\n  # exploding gradients during the backward pass\n  factor = jnp.maximum(jnp.sqrt(denom), grad_eps)\n\n  # Divide the input vector(s) by the normalization factor to obtain the\n  # normalized output\n  return x / factor\n\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of each vector along the last axis\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Calculate the clamped squared norm to prevent gradient explosion\n  clamped_squared_norm = jnp.clip(squared_norm, a_min=grad_eps)\n\n  # Calculate the denominator for normalization\n  denominator = jnp.sqrt(clamped_squared_norm)\n\n  # Calculate the clamped denominator for backward pass\n  clamped_denominator = jnp.clip(denominator, a_min=grad_eps)\n\n  # Normalize the vectors by dividing by the clamped denominator\n  normalized = x / clamped_denominator\n\n  return normalized\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm = jnp.maximum(norm, grad_eps)\n  return x / norm\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of x along the last axis.\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to prevent exploding gradients during the backward pass.\n  squared_norm = jnp.clip(squared_norm, a_min=grad_eps, a_max=None)\n\n  # Calculate the denominator for normalization.\n  denominator = jnp.sqrt(squared_norm)\n\n  # Clamp the denominator to prevent exploding gradients during the backward pass.\n  denominator = jnp.clip(denominator, a_min=grad_eps, a_max=None)\n\n  # Divide x by the denominator to obtain the normalized vector(s).\n  return x / denominator\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of the input array\n  squared_norm = jnp.sum(x * x, axis=-1, keepdims=True)\n  # Clamp the squared norm to a minimum value to prevent exploding gradients during the backward pass\n  squared_norm = jnp.maximum(squared_norm, grad_eps)\n  # Calculate the denominator for normalization\n  denominator = jnp.sqrt(squared_norm)\n  # Clamp the denominator to a minimum value to prevent exploding gradients during the backward pass\n  denominator = jnp.maximum(denominator, grad_eps)\n  # Divide the input array by the denominator to normalize it\n  return x / denominator\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  norm = jnp.maximum(norm, grad_eps)\n  return x / norm\n\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x along the last axis\n  squared_norm = jnp.sum(x * x, axis=-1, keepdims=True)\n\n  # Compute the denominator of the normalization, clamping it to a minimum value of grad_eps\n  denominator = jnp.maximum(squared_norm, grad_eps)\n\n  # Compute the normalization factor and apply it to x\n  factor = jnp.rsqrt(denominator)\n  return x * factor\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of the input vector(s)\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Calculate the denominator for normalization, clamping it to a minimum value\n  # during the backward pass to prevent exploding gradients\n  denominator = jnp.maximum(squared_norm, grad_eps**2)\n\n  # Calculate the normalization factor and apply it to the input vector(s)\n  x_normalized = x / jnp.sqrt(denominator)\n\n  return x_normalized\n\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm along the last axis\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Compute the squared norm clamped to a minimum value\n  squared_norm_clamped = jnp.clip(squared_norm, a_min=grad_eps, a_max=None)\n\n  # Compute the denominator clamped to a minimum value\n  denominator_clamped = jnp.sqrt(squared_norm_clamped)\n\n  # Compute the denominator clamped to a minimum value during the backward pass\n  denominator_clamped_backward = jnp.clip(\n      denominator_clamped, a_min=grad_eps, a_max=None\n  )\n\n  # Compute the normalized array\n  x_normalized = x / denominator_clamped_backward\n\n  return x_normalized\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of x along the last axis\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Calculate the denominator for the backward pass, clamped to a minimum value to prevent exploding gradients\n  denominator = jnp.maximum(squared_norm, grad_eps**2)\n\n  # Calculate the denominator for the forward pass, clamped to a minimum value to prevent division by zero\n  denominator = jnp.maximum(denominator, grad_eps**2)\n\n  # Calculate the normalized array\n  return x / jnp.sqrt(denominator)\n\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        if len(agent_info.split(\":\")) > 1:\n            input_text = agent_info.split(\":\")[1]\n        else:\n            input_text = \"\"\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        input_text = agent_info.split(':')[1] if len(agent_info.split(':')) > 1 else ''\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0].strip()\n        input_text = agent_info.split(':')[1].strip() if len(agent_info.split(':')) > 1 else ''\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split('[')[1].split(']')[0]\n        input_text = response.split(']')[1].split(':')[1].strip()\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info_start = response.find(\"Use Agent[\") + len(\"Use Agent[\")\n        agent_info_end = response.find(\"]\", agent_info_start)\n        agent_info = response[agent_info_start:agent_info_end]\n\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        if len(agent_info.split(':')) > 1:\n            input_text = agent_info.split(':')[1]\n        else:\n            input_text = \"\"\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        input_text = agent_info.split(':')[1] if len(agent_info.split(':')) > 1 else ''\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info_start = response.index('Use Agent[') + len('Use Agent[')\n        agent_info_end = response.index(']')\n        agent_info = response[agent_info_start:agent_info_end]\n\n        if ':' in agent_info:\n            agent_name, input_text = agent_info.split(':', 1)\n        else:\n            agent_name = agent_info\n            input_text = ''\n\n        return agent_name.strip(), input_text.strip()\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, *input_text = agent_info.split(\":\")\n        return agent_name.strip(), \":\".join(input_text).strip()\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if \":\" in agent_info else \"\"\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info_start = response.find(\"Use Agent[\")\n        agent_info_end = response.find(\"]\")\n        agent_info = response[agent_info_start + 10:agent_info_end]\n        agent_name, *input_text = agent_info.split(\":\")\n        input_text = \":\".join(input_text)\n        return agent_name.strip(), input_text.strip()\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, *input_text = agent_info.split(\":\")\n        return agent_name.strip(), \":\".join(input_text).strip()\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        input_text = agent_info.split(':')[1] if len(agent_info.split(':')) > 1 else \"\"\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        if len(agent_info.split(\":\")) > 1:\n            input_text = agent_info.split(\":\")[1]\n        else:\n            input_text = \"\"\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        start_index = response.find(\"Use Agent[\") + len(\"Use Agent[\")\n        end_index = response.find(\"]\")\n        agent_info = response[start_index:end_index]\n        agent_name, _, input_text = agent_info.partition(\":\")\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0].strip()\n        input_text = agent_info.split(':')[1].strip() if len(agent_info.split(':')) > 1 else ''\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Split the response string into parts based on the delimiters 'Use Agent[' and ']'\n        parts = response.split('Use Agent[')\n        agent_info_part = parts[1].split(']')[0]\n\n        # Split the agent information part into the agent name and any input text\n        agent_name, *input_text = agent_info_part.split(':')\n\n        # Return the agent name and the input text, or just the agent name if no input text is present\n        return agent_name.strip(), ''.join(input_text).strip()\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, *input_text = agent_info.split(\":\")\n        return agent_name.strip(), \":\".join(input_text).strip()\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    bbox = [obj[\"bbox\"] for obj in annos]\n    bbox = [BoxMode.convert(box, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS) for box in bbox]\n    target = Boxes(bbox)\n    target = target.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n\n    seg_masks = None\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            polygons = [obj[\"segmentation\"] for obj in annos]\n            seg_masks = PolygonMasks(polygons)\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks, _ = mask_util.encode(np.array(masks, dtype=np.uint8, order=\"F\"))\n            seg_masks = BitMasks(masks)\n        else:\n            raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    keypoints = None\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints, keypoints[0].shape[1])\n\n    gt_instances = Instances(image_size, **{\"gt_boxes\": target, \"gt_classes\": classes})\n\n    if seg_masks is not None:\n        gt_instances.gt_masks = seg_masks\n    if keypoints is not None:\n        gt_instances.gt_keypoints = keypoints\n\n    return gt_instances\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    bbox = [obj.get(\"bbox\", obj.get(\"bbox_mode\", BoxMode.XYXY_ABS)) for obj in annos]\n    bbox = [BoxMode.convert(x, y, BoxMode.XYXY_ABS) for x, y in zip(bbox, annos)]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(bbox)\n    target.gt_classes = [obj[\"category_id\"] for obj in annos]\n    target.gt_masks = None\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks = [\n                mask_util.decode(mask) for mask in masks\n            ]\n            target.gt_masks = BitMasks(\n                polygons_to_bitmask(masks, image_size[0], image_size[1])\n            )\n        else:\n            raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = [\n            Keypoints(k, keypoint_hflip_indices)\n            for k, keypoint_hflip_indices in zip(keypoints, annos)\n        ]\n        target.gt_keypoints = keypoints\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    bbox = [obj[\"bbox\"] for obj in annos]\n    bbox = np.asarray(bbox).reshape(-1, 4)\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(bbox)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = np.array(classes, dtype=np.int64)\n    target.gt_classes = classes\n\n    if \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            masks = polygons_to_bitmask(segms, image_size)\n        else:\n            raise ValueError(\n                \"Only 'polygon' and 'bitmask' are supported for masks format, got {}\".format(\n                    mask_format\n                )\n            )\n        target.gt_masks = masks\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = np.asarray(keypoints).reshape(-1, 3)\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    bboxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(bboxes)\n    target.gt_classes = [obj[\"category_id\"] for obj in annos]\n    target.gt_keypoints = [obj.get(\"keypoints\", None) for obj in annos]\n    target.gt_masks = [obj.get(\"segmentation\", None) for obj in annos]\n    if mask_format == \"polygon\":\n        target.gt_masks = PolygonMasks(target.gt_masks)\n    elif mask_format == \"bitmask\":\n        target.gt_masks = BitMasks(\n            polygons_to_bitmask(target.gt_masks, image_size[0], image_size[1])\n        )\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    bbox = [obj[\"bbox\"] for obj in annos]\n    bbox = [BoxMode.convert(x, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS) for x in bbox]\n    bbox = np.array(bbox, dtype=np.float32)\n    target = Boxes(bbox)\n    target = target.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = np.array(classes, dtype=np.int64)\n\n    if \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        masks = PolygonMasks(segms)\n    elif \"segmentation\" in annos[0] and mask_format == \"bitmask\":\n        segms = [obj[\"segmentation\"] for obj in annos]\n        masks = polygons_to_bitmask(segms, image_size)\n    else:\n        masks = None\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = np.array(keypoints, dtype=np.float32)\n        num_instances = len(annos)\n        keypoints = keypoints.reshape(num_instances, -1, 3)\n    else:\n        keypoints = None\n\n    return Instances(image_size, **{\"gt_boxes\": target, \"gt_classes\": classes, \"gt_masks\": masks, \"gt_keypoints\": keypoints})\n\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(anno[\"bbox\"], anno[\"bbox_mode\"], BoxMode.XYXY_ABS) for anno in annos]\n    target = Instances(image_size, **{\"gt_boxes\": Boxes(boxes)})\n\n    classes = [anno[\"category_id\"] for anno in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segms = [\n                PolygonMasks(\n                    [np.asarray(anno[\"segmentation\"], dtype=\"int32\").reshape(-1, 2)]\n                )\n                for anno in annos\n            ]\n            segms = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            segms = [\n                polygons_to_bitmask(\n                    [np.asarray(anno[\"segmentation\"], dtype=\"int32\").reshape(-1, 2)],\n                    image_size[0],\n                    image_size[1],\n                )\n                for anno in annos\n            ]\n            segms = BitMasks(segms)\n        else:\n            raise ValueError(\n                \"Only 'polygon' and 'bitmask' are supported as mask_format. \"\n                \"See https://detectron2.readthedocs.io/en/latest/modules/structures.html#detectron2.structures.PolygonMasks\"\n            )\n        target.gt_masks = segms\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [anno[\"keypoints\"] for anno in annos]\n        keypoints = Keypoints(keypoints, keypoints[0].shape[-1])\n        target.gt_keypoints = keypoints\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    bbox = [obj[\"bbox\"] for obj in annos]\n    bbox = [BoxMode.convert(box, obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for box, obj in zip(bbox, annos)]\n    target = Instances(image_size, **{\"gt_boxes\": Boxes(bbox)})\n    target.gt_classes = [obj[\"category_id\"] for obj in annos]\n\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks = [\n                polygons_to_bitmask(segm, image_size[0], image_size[1])\n                for segm in masks\n            ]\n            target.gt_masks = BitMasks(masks)\n        else:\n            raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = [\n            Keypoints(keypoints=k, visibility=v) for k, v in zip(keypoints, annos)\n        ]\n        target.gt_keypoints = keypoints\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(anno[\"bbox\"], anno[\"bbox_mode\"], BoxMode.XYXY_ABS) for anno in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n    target.gt_classes = [anno[\"category_id\"] for anno in annos]\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segms = [\n                PolygonMasks(\n                    [np.asarray(segm, dtype=\"float32\").reshape(-1, 2)]\n                    for segm in [obj[\"segmentation\"] for obj in annos]\n                )\n            ]\n        elif mask_format == \"bitmask\":\n            segms = [\n                polygons_to_bitmask(\n                    [obj[\"segmentation\"] for obj in annos],\n                    image_size[0],\n                    image_size[1],\n                )\n            ]\n        else:\n            raise ValueError(\n                \"Unknown mask format: {}\".format(mask_format)\n            )\n        target.gt_masks = segms\n    if \"keypoints\" in annos[0]:\n        keypoints = [\n            Keypoints(\n                [obj[\"keypoints\"]],\n                visibilities=[obj[\"keypoints\"][2::3],]\n            )\n            for obj in annos\n        ]\n        target.gt_keypoints = keypoints\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(ann[\"bbox\"], ann[\"bbox_mode\"], BoxMode.XYXY_ABS) for ann in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n    target.gt_classes = [ann[\"category_id\"] for ann in annos]\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segms = [\n                np.asarray(ann[\"segmentation\"], dtype=\"float32\").reshape(-1, 2)\n                for ann in annos\n            ]\n            target.gt_masks = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            masks = [\n                mask_util.decode(ann[\"segmentation\"]) for ann in annos\n            ]\n            target.gt_masks = BitMasks(masks)\n        else:\n            raise NotImplementedError(\n                \"Only 'polygon' and 'bitmask' masks are supported\"\n            )\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [ann[\"keypoints\"] for ann in annos]\n        keypoints = Keypoints(keypoints, [ann[\"num_keypoints\"] for ann in annos])\n        target.gt_keypoints = keypoints\n\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n    target.gt_classes = [obj[\"category_id\"] for obj in annos]\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks = [\n                mask_util.decode(mask) for mask in masks\n            ]\n            target.gt_masks = BitMasks(\n                [\n                    polygons_to_bitmask(mask.astype(\"uint8\"), *image_size)\n                    for mask in masks\n                ]\n            )\n        else:\n            raise ValueError(\n                \"Only 'polygon' and 'bitmask' are supported for masks format.\"\n            )\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = [\n            Keypoints(kpts, keypoint_flip_indices)\n            for kpts, keypoint_flip_indices in zip(keypoints, keypoint_hflip_indices)\n        ]\n        target.gt_keypoints = keypoints\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    bboxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size, **{\"gt_boxes\": Boxes(bboxes)})\n    target.gt_classes = [obj[\"category_id\"] for obj in annos]\n    target.gt_boxes = target.gt_masks = None\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(masks)\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = BitMasks(\n                [mask_util.decode(obj) for obj in masks],\n                height=image_size[0],\n                width=image_size[1],\n            )\n        else:\n            raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        target.gt_keypoints = Keypoints(keypoints, **{\"height\": image_size[0], \"width\": image_size[1]})\n    return target\n\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    bbox = [obj[\"bbox\"] for obj in annos]\n    bbox = [BoxMode.convert(box, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS) for box in bbox]\n    target = Boxes(bbox)\n    target = target.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n\n    if mask_format == \"polygon\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = PolygonMasks(segms)\n        masks = masks.to(dtype=torch.bool, device=target.device)\n    elif mask_format == \"bitmask\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = polygons_to_bitmask(segms, image_size)\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints, image_size)\n    else:\n        keypoints = None\n\n    gt_instances = Instances(image_size, **{\"gt_boxes\": target, \"gt_classes\": classes, \"gt_masks\": masks})\n    if keypoints is not None:\n        gt_instances.gt_keypoints = keypoints\n    return gt_instances\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The categories in a dataset are always ordered, and we know that the max\n    # category id will be one less than the number of categories.\n    max_instance_id = max(anno[\"id\"] for anno in annos)\n    num_classes = max_instance_id + 1\n    if len(annos) == 0:\n        return Instances(image_size, pred_boxes=Boxes([]), pred_classes=np.array([]))\n\n    boxes = [BoxMode.convert(anno[\"bbox\"], anno[\"bbox_mode\"], BoxMode.XYXY_ABS) for anno in annos]\n    target = Boxes(boxes)\n    target = target.clip(image_size)\n\n    classes = [anno[\"category_id\"] for anno in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n\n    seg_masks = None\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            seg_masks = PolygonMasks(\n                [\n                    PolygonMasks.from_polygon(segmentation, image_size)\n                    for segmentation in [anno[\"segmentation\"] for anno in annos]\n                ]\n            )\n        elif mask_format == \"bitmask\":\n            seg_masks = PolygonMasks(\n                [\n                    polygons_to_bitmask(segmentation, *image_size)\n                    for segmentation in [anno[\"segmentation\"] for anno in annos]\n                ]\n            )\n        else:\n            raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    keypoints = None\n    if \"keypoints\" in annos[0]:\n        keypoints = [anno[\"keypoints\"] for anno in annos]\n        keypoints = Keypoints(keypoints, num_keypoints=len(keypoints[0]) // 3)\n\n    gt_instances = Instances(image_size"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The categories in a dataset are always stored sorted in alphabetical order,\n    # and we refer to the common object classes in Detectron2 by the indices in this list.\n    # For example, person_box refers to the index 1 in the categories list below.\n    #\n    # Note that the indices are 1-based (start from 1), and one of the classes in your dataset might correspond to index 0.\n    # This will make the loaded metadata a bit different from your metadata.\n    # See the last part of this notebook for details.\n    cat_metadata = MetadataCatalog.get(annos[\"category_id\"])\n    # The categories in a dataset are always stored sorted in alphabetical order,\n    # and we refer to the common object classes in Detectron2 by the indices in this list.\n    # For example, person_box refers to the index 1 in the categories list below.\n    #\n    # Note that the indices are 1-based (start from 1), and one of the classes in your dataset might correspond to index 0.\n    # This will make the loaded metadata a bit different from your metadata.\n    # See the last part of this notebook for details.\n    cat_metadata = MetadataCatalog.get(annos[\"category_id\"])\n    # The categories in a dataset are always stored sorted in alphabetical order,\n    # and we refer to the common object classes in Detectron2 by the indices in this list.\n    # For example, person_box refers to the index 1 in the categories list below.\n    #\n    # Note that the indices are 1-based (start from 1), and one of the classes in your dataset might correspond to index 0.\n    # This will make the loaded metadata a bit different from your metadata.\n    # See the last part of this notebook for details.\n    cat_metadata = MetadataCatalog.get(annos[\"category_id\"])\n    # The categories in a dataset are always stored sorted in alphabetical order,\n    # and we refer to the common object classes in Detectron2 by the indices in this list.\n    # For example, person_box refers to the"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The following keys are used in the dicts\n    KEYS_TO_KEEP = [\"iscrowd\", \"category_id\", \"bbox\", \"bbox_mode\", \"segmentation\", \"keypoints\"]\n\n    # The following keys are only used in the dicts for RLE format masks\n    KEYS_TO_KEEP_RLE = [\"iscrowd\", \"category_id\", \"bbox\", \"bbox_mode\", \"segmentation\"]\n\n    # The following keys are only used in the dicts for polygons\n    KEYS_TO_KEEP_POLYGON = [\"iscrowd\", \"category_id\", \"bbox\", \"bbox_mode\", \"segmentation\"]\n\n    if mask_format == \"polygon\":\n        keys_to_keep = KEYS_TO_KEEP_POLYGON\n    elif mask_format == \"bitmask\":\n        keys_to_keep = KEYS_TO_KEEP_RLE\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    # The \"iscrowd\" key is not used for now\n    if \"iscrowd\" in annos[0]:\n        annos = [obj for obj in annos if obj.get(\"iscrowd\", 0) == 0]\n\n    boxes = [obj[k] for obj in annos for k in [\"bbox\"]]\n    if len(boxes) > 0:\n        boxes = np.asarray(boxes, dtype=\"float32\")\n        boxes[:, 2:] -= boxes[:, :2]\n    else:\n        boxes = np.zeros((0, 4), dtype=\"float32\")\n\n    if \"bbox_mode\" in annos[0]:\n        bbox_mode = annos[0][\"bbox_mode\"]\n    else:\n        bbox_mode = BoxMode.XYXY_ABS\n\n    target = {}\n    target[\"boxes\"] = Boxes(boxes)\n    target[\"image_size\"] = image_size\n    target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if \"keypoints\" in annos[0]:\n        keypoint_hflip_indices = create_keypoint_hflip_indices(annos)\n    else:\n        keypoint_hflip_indices = None\n\n    if \"segmentation\" not in annos[0]:\n        raise KeyError(\"Annotations in '{}' must contain segmentation shapes!\".format(annos[0]))\n\n    boxes = [obj[\"bbox\"] for obj in annos]\n    # guard against no boxes via resizing\n    if len(boxes) == 0:\n        boxes = np.zeros((0, 4), dtype=\"float32\")\n    boxes = Boxes(boxes)\n\n    # TODO: Support more than one segmentation format in the future\n    if mask_format == \"polygon\":\n        # concat segmentation polygons from different instances\n        if \"segmentation\" in annos[0]:\n            if isinstance(annos[0][\"segmentation\"], list):\n                polygons = [obj[\"segmentation\"] for obj in annos]\n            else:\n                polygons = [obj[\"segmentation\"][\"counts\"] for obj in annos]\n            # handle rle encoding\n            if \"counts\" in polygons[0]:\n                polygons = mask_util.frPyObjects(polygons, *image_size)\n            else:\n                polygons = [\n                    np.array(polygon).reshape(-1, 2) for polygon in polygons\n                ]\n            masks = PolygonMasks(polygons)\n        else:\n            masks = None\n    elif mask_format == \"bitmask\":\n        masks = None\n        if \"segmentation\" in annos[0]:\n            if \"counts\" in annos[0][\"segmentation\"]:\n                masks = mask_util.frPyObjects(\n                    [obj[\"segmentation\"] for obj in annos], *image_size\n                )\n                masks = BitMasks(masks)\n    else:\n        raise ValueError(\"Unsupported mask format:"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n    target.gt_classes = [obj[\"category_id\"] for obj in annos]\n    target.gt_boxes.clip(image_size)\n\n    classes = set()\n    for obj in annos:\n        if \"segmentation\" in obj:\n            if mask_format == \"polygon\":\n                assert isinstance(obj[\"segmentation\"], list)\n                # polygon\n                segm = [\n                    np.asarray(obj[\"segmentation\"][0]).reshape(-1, 2).astype(\"float32\")\n                ]\n            elif mask_format == \"bitmask\":\n                # RLE\n                assert isinstance(obj[\"segmentation\"], dict)\n                segm = mask_util.decode(obj[\"segmentation\"])\n                segm = segm.astype(\"uint8\")\n                segm = polygons_to_bitmask(segm, *image_size)\n            else:\n                raise ValueError(\n                    \"Unknown mask format: {}\".format(mask_format)\n                )\n\n            classes.add(obj[\"category_id\"])\n            target.gt_masks.append(segm)\n\n    if len(classes) == 1:\n        # In this case, we need to add a dummy mask to make the API consistent.\n        target.gt_masks.append(torch.zeros(*image_size, dtype=torch.uint8))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = np.asarray(keypoints, dtype=\"float32\").reshape(-1, 3)\n        target.gt_keypoints = Keypoints(keypoints, keypoints[:, 2], image_size"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if len(annos) == 0:\n        return None\n\n    boxes = [BoxMode.convert(anno[\"bbox\"], anno[\"bbox_mode\"], BoxMode.XYXY_ABS) for anno in annos]\n    target = Boxes(boxes)\n    target = target.clip(image_size)\n\n    classes = [anno[\"category_id\"] for anno in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n\n    if \"segmentation\" in annos[0]:\n        segms = [\n            convert_polygon_to_bitmask(segm, image_size[0], image_size[1])\n            if isinstance(segm, list)\n            else segm\n            for anno in annos\n            for segm in [anno[\"segmentation\"]]\n        ]\n        masks = PolygonMasks(segms)\n    else:\n        masks = None\n\n    keypoints = None\n    if \"keypoints\" in annos[0]:\n        keypoints = [\n            convert_keypoints_to_xy(kpts)\n            if isinstance(kpts, list)\n            else kpts\n            for anno in annos\n            for kpts in [anno[\"keypoints\"]]\n        ]\n        keypoints = Keypoints(keypoints)\n\n    # The below code is copied from https://github.com/facebookresearch/detectron2/blob/main/detectron2/data/datasets/builtin.py#L249\n    # Convert them to BitMasks if necessary\n    if mask_format == \"polygon\":\n        if masks.shape[-1] > 0:\n            masks = masks.convert(masks.size(0), image_size[0], image_size[1])\n    elif mask_format == \"bitmask\":\n        if \"counts\" in annos[0][\"segmentation\"][0]:\n            masks = masks.convert("}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The categories in a dataset are always ordered, and we know that the max\n    # category id will be the largest index in the list.\n    cat_metadata = MetadataCatalog.get(annos[0][\"category_id\"])\n    max_instance_id = max(anno[\"id\"] for anno in annos)\n    num_classes = len(cat_metadata.thing_classes)\n    if num_classes == 0:\n        raise ValueError(\"No classes found in metadata. Check that the category_id field is not empty.\")\n\n    # Create a list of dicts with keys \"gt_boxes\", \"gt_classes\", \"gt_masks\", \"gt_keypoints\"\n    # Each dict is for one instance, and contains the annotations for that instance.\n    # The \"gt_boxes\" field is a Boxes object, which is a structured tensor that contains the bounding boxes of the instances.\n    # The \"gt_classes\" field is a tensor of shape (N,) that contains the class IDs of the instances.\n    # The \"gt_masks\" field is a BitMasks object, which is a structured tensor that contains the segmentation masks of the instances.\n    # The \"gt_keypoints\" field is a Keypoints object, which is a structured tensor that contains the keypoints of the instances.\n    # The \"gt_keypoints\" field is a Keypoints object, which is a structured tensor that contains the keypoints of the instances.\n    # The \"gt_keypoints\" field is a Keypoints object, which is a structured tensor that contains the keypoints of the instances.\n    # The \"gt_keypoints\" field is a Keypoints object, which is a structured tensor that contains the keypoints of the instances.\n    # The \"gt_keypoints\" field is a Keypoints object, which is a structured tensor that contains the keypoints of the instances.\n    # The \"gt_keypoints\" field is a Keypoints object, which is a structured tensor that contains the keypoints of the instances.\n    #"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The following keys are used in the models (e.g., in the losses).\n    # We need to handle them specially in the following.\n    # We also handle transforms of bounding boxes, which are applied\n    #    after they are converted to `Instances`, because these instances\n    #    can be mapped back to the original image.\n    KEYS = [\"gt_boxes\", \"gt_classes\", \"gt_masks\", \"gt_keypoints\"]\n\n    meta = annos[0]\n    image_width = meta.get(\"width\", image_size[1])\n    image_height = meta.get(\"height\", image_size[0])\n    image_id = meta.get(\"image_id\", None)\n\n    # The COCO API does not handle edge cases in <0 annotations correctly, so we\n    # set the box width & height to 1 if that is the case.\n    if \"counts\" not in meta:\n        meta[\"counts\"] = [0] * meta[\"category_id\"]\n    for obj in annos:\n        # In COCO, all instances are tightly packaged as a single segmentation\n        # object. In users' own datasets, instances could be separated, so we\n        # need to support lists of segmentation objects.\n        if isinstance(obj[\"segmentation\"], list):\n            # Filter out empty segmentation objects\n            obj[\"segmentation\"] = [\n                obj[\"segmentation\"][ii]\n                for ii, seg in enumerate(obj[\"segmentation\"])\n                if len(seg) % 2 == 0\n            ]\n        elif isinstance(obj[\"segmentation\"], dict):\n            if \"counts\" not in obj:\n                obj[\"counts\"] = [0] * obj[\"category_id\"]\n            # Convert to a list of RLEs\n            obj[\"segmentation\"] = [obj[\"segmentation\"]]\n        else:\n            raise ValueError(\n                \"Unknown segmentation format: {} for image {}\".format(\n                    obj[\"segmentation\"], obj[\"file_name\"]\n                )\n            )\n        if len(obj[\"segmentation\"]) == 0"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n\n    data_home = Path(data_home)\n\n    if not data_home.exists():\n        data_home.mkdir(parents=True, exist_ok=True)\n\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    if not data_home.exists():\n        data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home).resolve()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home.resolve()\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home).expanduser()\n\n    if not data_home.exists():\n        data_home.mkdir(parents=True, exist_ok=True)\n\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home).expanduser().resolve()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = Path(data_home).expanduser().absolute()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    if not data_home.exists():\n        data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home).resolve()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    if not data_home.exists():\n        data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    if cov.ndim != 2:\n        raise ValueError(\"The input must be a 2D array\")\n\n    # Calculate the standard deviations\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    if cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D array\")\n\n    # Calculate the standard deviations for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    if not isinstance(cov, np.ndarray) or cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D array\")\n\n    # Calculate the standard deviations for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure input is a 2D array\n    if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"Input must be a 2D square array\")\n\n    # Calculate standard deviations\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Check if the input is a 2D array\n    if not isinstance(cov, np.ndarray) or cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D array\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    cov = np.array(cov)\n    if cov.ndim != 2:\n        raise ValueError(\"The covariance matrix must be 2D\")\n    if cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"The covariance matrix must be square\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure input is 2D\n    if cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D array\")\n\n    # Calculate standard deviations\n    std_devs = np.sqrt(np.diag(cov))\n\n    # Calculate correlation matrix\n    corr = cov / np.outer(std_devs, std_devs)\n\n    return corr, std_devs\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure input is a 2D array\n    if cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D array\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Convert the covariance matrix into a correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_symmetric(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure input is a 2D array\n    cov = np.array(cov)\n    assert_is_square(cov)\n\n    # Calculate the standard deviations for each variable\n    std_devs = np.sqrt(np.diag(cov))\n\n    # Convert the covariance matrix into a correlation matrix\n    corr = cov / np.outer(std_devs, std_devs)\n\n    return corr, std_devs\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_symmetric(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    cov = np.array(cov)\n    assert_is_square(cov)\n    assert_is_symmetric(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation.\n    # It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n\n    # Input-Output Arguments\n    # model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    # Return Values\n    # No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n\n    # This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation.\n    # It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n\n    # Input-Output Arguments\n    # model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    # Return Values\n    # No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n\n    # This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation.\n    # It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n\n    # Input-Output Arguments\n    # model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    # Return Values\n    # No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n\n    # This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation.\n    # It uses a context manager to ensure that these changes are reverted back to their original state after"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules of the model and modify their class definitions\n    for name, module in model.named_modules():\n        if hasattr(module, \"__annotations__\"):\n            # copy first to avoid editing annotations of base class\n            module.__annotations__ = deepcopy(module.__annotations__)\n            module.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    # Return a context manager that reverts the changes made to the model's submodules\n    yield\n\n    # Revert the changes made to the model's submodules\n    for name, module in model.named_modules():\n        if hasattr(module, \"__annotations__\"):\n            if \"training\" in module.__annotations__:\n                del module.__annotations__[\"training\"]\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(m):\n        if hasattr(m, \"training\"):\n            m.training = torch.jit.Final[bool](m.training)\n\n    with ExitStack() as stack:\n        ctxs = [stack.enter_context(mock.patch(mod + \".training\")) for mod in model.modules()]\n        for m in ctxs:\n            m.side_effect = _freeze_training_mode\n        yield\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    from detectron2.modeling import META_ARCH_REGISTRY\n    from detectron2.modeling.meta_arch import GeneralizedRCNN\n    from detectron2.modeling.meta_arch.build import META_ARCH_REGISTRY\n    from detectron2.modeling.meta_arch.generalized_rcnn import GeneralizedRCNN\n    from detectron2.modeling.meta_arch.mask_rcnn import MaskRCNN\n    from detectron2.modeling.meta_arch.retinanet import RetinaNet\n    from detectron2.modeling.meta_arch.semantic_seg import SemanticSegmentor\n    from detectron2.modeling.meta_arch.build import META_ARCH_REGISTRY\n\n    # Define a dictionary of model classes and their corresponding class names\n    model_classes = {\n        GeneralizedRCNN: \"GeneralizedRCNN\",\n        MaskRCNN: \"MaskRCNN\",\n        RetinaNet: \"RetinaNet\",\n        SemanticSegmentor: \"SemanticSegmentor\",\n    }\n\n    # Define a dictionary of model class names and their corresponding registry keys\n    model_keys = {\n        \"GeneralizedRCNN\": \"GeneralizedRCNN\",\n        \"MaskRCNN\": \"MaskRCNN\",\n        \"RetinaNet\": \"RetinaNet\",\n        \"SemanticSegmentor\": \"SemanticSegmentor\",\n    }\n\n    # Define a dictionary of model class names and their corresponding registry keys\n    model_keys = {\n        \"GeneralizedRCNN\": \"GeneralizedRCNN\",\n        \"MaskRCNN\": \"MaskRCNN\",\n        \"RetinaNet\": \"RetinaNet\",\n        \"SemanticSegmentor\": \"SemanticSegmentor\",\n    }\n\n    # Define a dictionary of model class names and their corresponding registry keys\n    model_keys = {\n        \"GeneralizedRCNN\": \"GeneralizedRCNN\",\n        \"MaskRCNN\": \"MaskRCNN\",\n        \"RetinaNet\": \"RetinaNet\",\n        \"SemanticSegmentor\": \"SemanticSegmentor\",\n    }\n\n    # Define a dictionary of model class names and their"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(m):\n        if hasattr(m, \"training\"):\n            m.training = torch.jit.Attribute.from_value(torch.jit.Final[bool], m.training)\n\n    with torch.jit.optimized_execution(True):\n        model.apply(_freeze_training_mode)\n        yield\n        model.apply(_freeze_training_mode)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _set_training_attr(module, value):\n        if hasattr(module, \"training\"):\n            module.training = torch.jit.final(value)\n\n    with ExitStack() as stack:\n        ctxs = [stack.enter_context(mock.patch.object(m, \"training\", create=True)) for m in model.modules()]\n        for ctx, m in zip(ctxs, model.modules()):\n            ctx.side_effect = lambda: _set_training_attr(m, False)\n        yield\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for name, m in model.named_modules():\n        if hasattr(m, \"training\"):\n            m.training = torch.jit.AttributeMetadata(m.training, torch.jit.Final)\n    yield\n    for name, m in model.named_modules():\n        if hasattr(m, \"training\"):\n            delattr(m, \"training\")\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _set_training_attr(module, training):\n        if hasattr(module, \"training\"):\n            module.training = training\n        for submodule in module.children():\n            _set_training_attr(submodule, training)\n\n    original_training_attrs = {}\n\n    def _restore_training_attrs():\n        for module, training in original_training_attrs.items():\n            module.training = training\n\n    with torch.no_grad():\n        for module in model.modules():\n            if hasattr(module, \"training\"):\n                original_training_attrs[module] = module.training\n                module.training = False\n        try:\n            yield\n        finally:\n            _restore_training_attrs()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Save the original class definitions of all submodules\n    original_classes = {}\n    for name, module in model.named_modules():\n        original_classes[name] = type(module)\n\n    # Define a new class that inherits from the original class and overrides the \"training\" attribute\n    class FrozenModule(original_classes[\"\"]):\n        @property\n        def training(self):\n            return False\n\n    # Replace the original class definition of the root module with the new class\n    model.__class__ = FrozenModule\n\n    # Define a new class that inherits from the original class and overrides the \"training\" attribute\n    class FrozenSubmodule(original_classes[\"\"]):\n        @property\n        def training(self):\n            return False\n\n    # Replace the original class definition of all submodules with the new class\n    for name, module in model.named_modules():\n        module.__class__ = FrozenSubmodule\n\n    # Return the context manager to allow for exiting the context\n    yield\n\n    # Revert the changes made to the model's submodules\n    for name, module in model.named_modules():\n        module.__class__ = original_classes[name]\n    model.__class__ = original_classes[\"\"]"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(model):\n        for submodule in model.modules():\n            if hasattr(submodule, \"training\"):\n                submodule.training = torch.jit.final(submodule.training)\n\n    def _unfreeze_training_mode(model):\n        for submodule in model.modules():\n            if hasattr(submodule, \"training\"):\n                submodule.training = False\n\n    _freeze_training_mode(model)\n    yield\n    _unfreeze_training_mode(model)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _annotate_training(module):\n        if hasattr(module, \"training\"):\n            module.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    with torch.no_grad():\n        model.apply(_annotate_training)\n        yield\n        model.apply(_annotate_training)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules of the model and modify their class definitions\n    for name, module in model.named_modules():\n        if hasattr(module, \"__annotations__\"):\n            module.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    # Return the context manager to revert changes to the model's submodules\n    yield\n\n    # Revert changes to the model's submodules\n    for name, module in model.named_modules():\n        if hasattr(module, \"__annotations__\"):\n            delattr(module, \"__annotations__\")\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _set_training_attr(mod, training):\n        if hasattr(mod, \"training\"):\n            mod.training = training\n        for submod in mod.children():\n            _set_training_attr(submod, training)\n\n    with torch.no_grad():\n        _set_training_attr(model, False)\n        yield\n        _set_training_attr(model, True)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for name, module in model.named_modules():\n        if hasattr(module, \"training\"):\n            module.__class__.training = torch.jit.final(module.training)\n\n    try:\n        yield\n    finally:\n        for name, module in model.named_modules():\n            if hasattr(module, \"training\"):\n                delattr(module, \"training\")\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: add more submodules\n    # TODO: make this work for nested modules\n    submodules = [\n        model.backbone,\n        model.proposal_generator,\n        model.roi_heads,\n    ]\n    for m in submodules:\n        if hasattr(m, \"training\"):\n            m.__class__.__annotations__[\"training\"] = torch.jit.Final[bool]\n    yield\n    for m in submodules:\n        if hasattr(m, \"training\"):\n            delattr(m, \"training\")\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules and modify their class definitions\n    for module_name, module in model.named_modules():\n        if hasattr(module, \"training\"):\n            # Create a new class definition with a constant \"training\" attribute\n            new_class_dict = {\n                \"__annotations__\": {\"training\": torch.jit.Final[bool]},\n                \"__init__\": module.__init__,\n                \"__getattr__\": module.__getattr__,\n            }\n            # Create a new class object with the modified class dictionary\n            new_class = type(module)(module_name, (), new_class_dict)\n            # Replace the old class with the new class\n            setattr(model, module_name, new_class)\n    # Return the context manager to allow for the temporary changes to be reverted\n    yield\n    # Revert the changes made to the model's submodules\n    for module_name, module in model.named_modules():\n        if hasattr(module, \"training\"):\n            # Remove the \"training\" attribute from the class definition\n            delattr(module, \"training\")\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for name, module in model.named_modules():\n        if hasattr(module, \"training\"):\n            module.__class__.training = torch.jit.final(module.training)\n    yield\n    for name, module in model.named_modules():\n        if hasattr(module, \"training\"):\n            module.__class__.training = torch.jit.unfinal(module.training)\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(self):\n        self.training = torch.jit.final(self.training)\n\n    with ExitStack() as stack:\n        ctxs = [stack.enter_context(mock.patch(mod + \".training\")) for mod in model.__modules__]\n        for m in ctxs:\n            m.side_effect = _freeze_training_mode\n        yield\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    if not isinstance(model, torch.nn.Module):\n        raise ValueError(\"model should be an instance of torch.nn.Module\")\n\n    # Define a context manager to modify the class definition of a module\n    def _set_module_training_attr(module, training):\n        module.__class__ = type(module.__class__.__name__, (module.__class__,), {\"training\": training})\n\n    # Define a context manager to modify the class definition of all submodules\n    def _set_all_modules_training_attr(model, training):\n        for module in model.modules():\n            _set_module_training_attr(module, training)\n\n    # Temporarily set all submodules' training attribute to a constant value\n    _set_all_modules_modules_training_attr(model, torch.jit.Final[True])\n\n    # Yield control to the context manager, allowing for optimization by meta-compilation\n    yield\n\n    # Revert the changes made to the class definitions of the submodules\n    _set_all_modules_training_attr(model, torch.jit.Final[False])\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(m):\n        if hasattr(m, \"training\"):\n            m.training = torch.jit.final(m.training)\n\n    with ExitStack() as stack:\n        for m in model.modules():\n            stack.enter_context(mock.patch.object(m, \"training\", create=True))\n        model.apply(_freeze_training_mode)\n        yield\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if shape(field1) equals shape(field2).\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if shape(field1) equals shape(field2).\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[Any]]) -> Dict[str, List[Any]]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if shape of field1 equals shape of field2.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shape of field1 equals the shape of field2.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 are equal.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of two fields match.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shape of field1 equals the shape of field2.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if values[field1].shape equals values[field2].shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if shape(field1) equals shape(field2).\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 match.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shapes mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function is a validator that checks if two specified fields within a model have the same shape. It primarily used for validating data structures like NumPy arrays to ensure they are compatible for operations that require matching dimensions.\n\n        Input-Output Arguments\n        :param cls: type, The type of the Pydantic model that the validator is being applied to.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the fields specified in the validator. The keys are the names of the fields and the values are the corresponding NumPy arrays.\n        :return: Dict[str, np.ndarray], The original values dictionary is returned if the shapes of the two fields match. If the shapes do not match, it raises a ValueError indicating the mismatch.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function is a validator that compares the shapes of two fields within a Pydantic model. It is used to ensure that the shapes of the two fields match for operations that require matching dimensions.\n\n        Input-Output Arguments\n        :param cls: type, The type of the Pydantic model being validated.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the fields being checked. The keys are the names of the fields, and the values are the corresponding data structures.\n        :return: Dict[str, np.ndarray], The same dictionary of values as input, with no modifications.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shapes mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1 and field2 have the same shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This is a root validator that compares the shapes of two fields within a Pydantic model. It checks if the shapes of the two fields match. If they do not match, it raises a ValueError indicating the mismatch.\n\n        Input-Output Arguments\n        :param cls: type, The type of the Pydantic model being validated.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the fields being validated. The keys of the dictionary are the names of the fields, and the values are the corresponding data structures.\n        :return: Dict[str, np.ndarray], The same dictionary of values as input, but with no modifications.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function is a root validator that checks if the shapes of two fields within a model match.\n\n        Input-Output Arguments\n        :param cls: type, The class type of the model that is being validated.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the fields being compared. It includes both field1 and field2, which are used to retrieve the corresponding data structures.\n        :return: Dict[str, np.ndarray], The same values dictionary that was passed as input, unchanged.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 are equal.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shapes mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of two fields are equal.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shapes mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function is a validator that checks if the shapes of two NumPy arrays match.\n\n        Input-Output Arguments\n        :param cls: type, The type of the model that is being validated. It is used to provide context to the error message in case of a shape mismatch.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the fields specified in the model. The function extracts the values of field1 and field2 from this dictionary and compares their shapes.\n        :return: Dict[str, np.ndarray], The original dictionary of values passed to the validator.\n        :raises ValueError: If the shapes of field1 and field2 do not match, it raises a ValueError with a message indicating the mismatch.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function is a validator function that compares the shapes of two fields within a Pydantic model.\n        It takes a class and a dictionary of values as input and checks if the shapes of two specified fields match.\n        If the shapes do not match, it raises a ValueError indicating the mismatch.\n\n        :param cls: type, The type of the Pydantic model. It is used to access the class name for error messages.\n        :param values: Dict[str, np.ndarray], A dictionary of values where the keys are the names of the fields being compared and the values are the corresponding data structures.\n        :return: Dict[str, np.ndarray], The same dictionary of values is returned after checking the shapes of the two fields.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shapes mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{}] * len(metrics)\n        elif isinstance(metrics[0], dict):\n            return [metric[\"name\"] for metric in metrics], metrics\n        else:\n            raise ValueError(\"Metrics must be either a list of strings or a list of dictionaries.\")\n    else:\n        raise ValueError(\"Metrics must be a list.\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        metrics = [{'name': m} for m in metrics]\n\n    return [m['name'] for m in metrics], [m for m in metrics]\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # If the input is a list of strings, convert it to a list of dictionaries\n    if isinstance(metrics[0], str):\n        metrics = [{\"name\": metric} for metric in metrics]\n\n    # Extract the metric names and parameters from the dictionaries\n    metric_names = [metric[\"name\"] for metric in metrics]\n    metric_params = [metric.get(\"params\", {}) for metric in metrics]\n\n    # Return the metric names and parameters as a tuple\n    return metric_names, metric_params\n\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{} for _ in range(len(metrics))]\n    else:\n        return [metric[\"name\"] for metric in metrics], metrics\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # If the input is a list of strings, return a list of metrics and an empty list of parameters\n    if isinstance(metrics, list) and all(isinstance(metric, str) for metric in metrics):\n        return metrics, []\n\n    # If the input is a list of dictionaries, extract metric names and parameters\n    if isinstance(metrics, list) and all(isinstance(metric, dict) for metric in metrics):\n        metric_names = [metric.get(\"name\", \"\") for metric in metrics]\n        metric_params = [metric.get(\"params\", {}) for metric in metrics]\n        return metric_names, metric_params\n\n    # If the input is neither a list of strings nor a list of dictionaries, raise a ValueError\n    raise ValueError(\"Invalid input format for metrics. Please provide a list of strings or dictionaries.\")\n\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list) and isinstance(metrics[0], str):\n        metric_names = metrics\n        metric_params = [{} for _ in range(len(metric_names))]\n\n    elif isinstance(metrics, list) and isinstance(metrics[0], dict):\n        metric_names = [metric['name'] for metric in metrics]\n        metric_params = [metric['params'] for metric in metrics]\n\n    else:\n        raise ValueError(\"Invalid format for metrics. Please provide either a list of strings or a list of dictionaries.\")\n\n    return metric_names, metric_params\n\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # If the input is a list of strings, convert it to a list of dictionaries\n    if isinstance(metrics, list) and isinstance(metrics[0], str):\n        metrics = [{'name': m} for m in metrics]\n\n    # Extract metric names and parameters from the input list\n    metric_names = [m['name'] for m in metrics]\n    metric_params = [m for m in metrics]\n\n    # Return the processed metric names and parameters as a tuple\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        metric_names = metrics\n        metric_params = [{} for _ in range(len(metric_names))]\n    else:\n        metric_names = [metric_dict[\"name\"] for metric_dict in metrics]\n        metric_params = deepcopy(metrics)\n\n    return metric_names, metric_params\n\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # If the input is a list of strings, we assume that the strings are metric names\n    if isinstance(metrics[0], str):\n        metric_names = metrics\n        metric_params = [{} for _ in range(len(metric_names))]\n    # If the input is a list of dictionaries, we assume that the dictionaries contain metric details\n    elif isinstance(metrics[0], dict):\n        metric_names = [metric[\"name\"] for metric in metrics]\n        metric_params = deepcopy(metrics)\n    # If the input is neither a list of strings nor a list of dictionaries, we raise an exception\n    else:\n        raise ValueError(\"Invalid input format. Expected a list of strings or dictionaries.\")\n\n    return metric_names, metric_params\n\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    if isinstance(metrics, dict):\n        metrics = [metrics]\n\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{} for _ in metrics]\n\n        if isinstance(metrics[0], dict):\n            return [m[\"name\"] for m in metrics], metrics\n\n    raise ValueError(\"Invalid metrics format\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [dict() for _ in metrics]\n        elif isinstance(metrics[0], dict):\n            return [m['name'] for m in metrics], metrics\n        else:\n            raise ValueError(\"Metrics must be either a list of strings or a list of dictionaries.\")\n    else:\n        raise ValueError(\"Metrics must be a list.\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # If metrics is a list of strings, convert it to a list of dictionaries\n    if isinstance(metrics, list) and isinstance(metrics[0], str):\n        metrics = [{'name': m} for m in metrics]\n\n    # Extract metric names and parameters from the list of dictionaries\n    metric_names = [m['name'] for m in metrics]\n    metric_params = [m.get('params', {}) for m in metrics]\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        metrics = [{\"name\": metric} for metric in metrics]\n    return [metric[\"name\"] for metric in metrics], metrics\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{} for _ in metrics]\n    else:\n        return [m['name'] for m in metrics], metrics\n\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not isinstance(metrics, list):\n        raise TypeError(f\"`metrics` must be a list, but got {type(metrics)}\")\n\n    if len(metrics) == 0:\n        raise ValueError(\"`metrics` cannot be an empty list\")\n\n    if all(isinstance(metric, str) for metric in metrics):\n        return metrics, [{} for _ in range(len(metrics))]\n\n    if all(isinstance(metric, dict) for metric in metrics):\n        return [metric.get(\"name\", \"\") for metric in metrics], metrics\n\n    raise ValueError(\"`metrics` must be a list of strings or dictionaries\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # If the input is a list of strings, create a list of dictionaries with metric details\n    if isinstance(metrics[0], str):\n        metrics = [{'name': metric} for metric in metrics]\n\n    # Extract metric names and parameters from the list of dictionaries\n    metric_names = [metric['name'] for metric in metrics]\n    metric_params = [metric.get('params', {}) for metric in metrics]\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all([isinstance(m, str) for m in metrics]):\n            return metrics, [{} for _ in metrics]\n        elif all([isinstance(m, dict) for m in metrics]):\n            return [m['name'] for m in metrics], metrics\n        else:\n            raise ValueError(f'metrics must be either a list of strings or a list of dictionaries.')\n    else:\n        raise ValueError(f'metrics must be a list.')\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # If metrics is a list of strings, create a list of dictionaries with the metric name as the key and an empty dictionary as the value\n    if isinstance(metrics[0], str):\n        metrics = [{metric: {}} for metric in metrics]\n\n    # Extract metric names and parameters from the list of dictionaries\n    metric_names = [metric for metric in metrics]\n    metric_params = [metrics[metric] for metric in metrics]\n\n    return metric_names, metric_params\n\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    if isinstance(metrics, dict):\n        metrics = [metrics]\n\n    metrics_names = []\n    metrics_params = []\n    for metric in metrics:\n        if isinstance(metric, str):\n            metrics_names.append(metric)\n            metrics_params.append({})\n        elif isinstance(metric, dict):\n            metrics_names.append(list(metric.keys())[0])\n            metrics_params.append(metric[metrics_names[-1]])\n\n    return metrics_names, metrics_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Check if the input is a list\n    if not isinstance(metrics, list):\n        raise ValueError(\"Input 'metrics' should be a list.\")\n\n    # Check if the list is empty\n    if not metrics:\n        raise ValueError(\"Input 'metrics' should not be empty.\")\n\n    # Check if the list contains only strings or dictionaries\n    if not all(isinstance(metric, (str, dict)) for metric in metrics):\n        raise ValueError(\"Input 'metrics' should contain only strings or dictionaries.\")\n\n    # Create a list to store metric names and a list to store metric parameters\n    metric_names = []\n    metric_params = []\n\n    # Iterate over the input list\n    for metric in metrics:\n        # If the metric is a string, add it to the list of metric names\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        # If the metric is a dictionary, extract the metric name and parameters\n        elif isinstance(metric, dict):\n            if \"name\" not in metric:\n                raise ValueError(\"Each dictionary in 'metrics' should contain a 'name' key.\")\n            metric_name = metric[\"name\"]\n            metric_params.append(deepcopy(metric))\n            metric_names.append(metric_name)\n        else:\n            raise ValueError(\"Input 'metrics' should contain only strings or dictionaries.\")\n\n    # Return the list of metric names and the list of metric parameters\n    return metric_names, metric_params\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    else:\n      raise ValueError(f'No inverse for {fn}')\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. Represents the metric distances to be mapped to normalized distances.\n    :return: Tensor. The normalized distances corresponding to the input metric distances.\n    \"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    t = (t - t_near) / (t_far - t_near)\n    s = fn(t)\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. Represents the normalized distances to be mapped back to metric distances.\n    :return: Tensor. The metric distances corresponding to the input normalized distances.\n    \"\"\"\n    t = fn_inv(s)\n    t = t * (t_far - t_near) + t_near\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input Arguments\n    :param t: Tensor. The input metric distances.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    s = (t - t_near) / (t_far - t_near)\n    return fn(s)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances in the range [0, 1] back to metric distances.\n\n    Input Arguments\n    :param s: Tensor. The input normalized distances in the range [0, 1].\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    s = jnp.clip(s, 0, 1)\n    return (t_far - t_near) * fn_inv(s) + t_near\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    s = (t - t_near) / (t_far - t_near)\n    s = jnp.maximum(0., jnp.minimum(1., s))\n    return s\n\n  def s_to_t(s):\n    s = jnp.maximum(0., jnp.minimum(1., s))\n    t = s * (t_far - t_near) + t_near\n    return t\n\n  if fn_inv is None:\n    fn_inv = {\n        'contract': inv_contract,\n        'exp': lambda x: jnp.exp(x),\n        'log': lambda x: jnp.log(jnp.maximum(1e-6, x)),\n        'sqrt': lambda x: jnp.sqrt(jnp.maximum(0., x)),\n    }.get(fn.__name__, None)\n\n  if fn_inv is None:\n    raise ValueError(f'No inverse found for {fn.__name__}.')\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Construct the forward mapping from metric to normalized distances\n  t_to_s = lambda t: (t - t_near) / (t_far - t_near)\n\n  # Construct the backward mapping from normalized to metric distances\n  if fn_inv is None:\n    # If fn_inv is not provided, attempt to automatically determine its inverse\n    if fn == contract:\n      s_to_t = lambda s: t_near + (t_far - t_near) * (2 * s - 1)\n    elif fn == inv_contract:\n      s_to_t = lambda s: t_near + (t_far - t_near) * (2 * s + 1)\n    else:\n      raise ValueError('fn_inv is not provided and could not be automatically determined.')\n  else:\n    s_to_t = lambda s: fn_inv(fn(s_to_t(s)))\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input Arguments\n    :param t: Tensor. The metric distances to be mapped to normalized distances.\n    :return: Tensor. The normalized distances corresponding to the input metric distances.\n    \"\"\"\n    t_clipped = jnp.clip(t, t_near, t_far)\n    s = (t_clipped - t_near) / (t_far - t_near)\n    return fn(s)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input Arguments\n    :param s: Tensor. The normalized distances to be mapped back to metric distances.\n    :return: Tensor. The metric distances corresponding to the input normalized distances.\n    \"\"\"\n    s_clipped = jnp.clip(s, 0, 1)\n    t = s_clipped * (t_far - t_near) + t_near\n    if fn_inv is not None:\n      t = fn_inv(s_clipped) * (t_far - t_near) + t_near\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        'exp': jnp.exp,\n        'log': jnp.log,\n        'sqrt': jnp.sqrt,\n        'inverse': lambda x: 1 / x,\n        'inverse_sqrt': lambda x: 1 / jnp.sqrt(x),\n        'inverse_sqr': lambda x: 1 / (x**2),\n        'inverse_cube': lambda x: 1 / (x**3),\n        'inverse_log': lambda x: jnp.exp(x),\n        'inverse_exp': lambda x: jnp.log(x),\n        'inverse_exp2': lambda x: jnp.log2(x),\n        'inverse_exp10': lambda x: jnp.log10(x),\n        'inverse_log2': lambda x: 2**x,\n        'inverse_log10': lambda x: 10**x,\n        'inverse_pow2': lambda x: x**2,\n        'inverse_pow3': lambda x: x**3,\n        'inverse_pow4': lambda x: x**4,\n        'inverse_pow5': lambda x: x**5,\n        'inverse_pow6': lambda x: x**6,\n        'inverse_pow7': lambda x: x**7,\n        'inverse_pow8': lambda x: x**8,\n        'inverse_pow9': lambda x: x**9,\n        'inverse_pow10': lambda x: x**10,\n        'inverse_pow11': lambda x: x**11,\n        'inverse_pow12': lambda x: x**12,\n        'inverse_pow13': lambda x: x**13,\n        'inverse_pow14': lambda x: x**14,\n        'inverse_pow15': lambda x: x**15,\n        'in"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input Arguments\n    :param t: Tensor. Represents the metric distances.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    # Clip distances to ensure they fall within a valid range.\n    t = jnp.clip(t, t_near, t_far)\n\n    # Compute the forward mapping from metric to normalized distances.\n    s = (t - t_near) / (t_far - t_near)\n\n    # Apply the function to transform the distances.\n    s = fn(s)\n\n    # Clip the transformed distances to ensure they fall within a valid range.\n    s = jnp.clip(s, 0, 1)\n\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input Arguments\n    :param s: Tensor. Represents the normalized distances in the range [0, 1].\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    # Clip the normalized distances to ensure they fall within a valid range.\n    s = jnp.clip(s, 0, 1)\n\n    # Apply the inverse function to transform the normalized distances.\n    if fn_inv is not None:\n      s = fn_inv(s)\n    else:\n      # Attempt to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n      if fn == jnp.log:\n        s = jnp.exp(s)\n      elif fn == jnp.sqrt:\n        s = s**2\n      elif fn == jnp.exp2:\n        s = 2**s\n      elif fn == jnp.sinh:\n        s = jnp.arcsinh(s)\n      elif fn == jnp.cosh:\n        s = jnp.arccosh(s)\n      elif fn == jnp.tanh:\n        s = jnp.arct"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    :param t: Tensor. Represents the input metric distances.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    # Clip the input distances to ensure they fall within a valid range.\n    t = jnp.clip(t, t_near, t_far)\n    # Compute the forward mapping using the provided `fn` function.\n    s = fn(t)\n    # Normalize the distances to the range [0, 1].\n    s = (s - t_near) / (t_far - t_near)\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    :param s: Tensor. Represents the input normalized distances in the range [0, 1].\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    # Clip the input normalized distances to ensure they fall within a valid range.\n    s = jnp.clip(s, 0, 1)\n    # Denormalize the distances to the original range.\n    t = s * (t_far - t_near) + t_near\n    # Compute the backward mapping using the provided `fn_inv` function.\n    t = fn_inv(t) if fn_inv is not None else fn(t)\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. Represents the metric distances to be mapped to normalized distances.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    # Clamping to 1 produces correct scale inside |t| < 1\n    t_mag_sq = jnp.maximum(1, jnp.sum(t**2, axis=-1, keepdims=True))\n    scale = (2 * jnp.sqrt(t_mag_sq) - 1) / t_mag_sq\n    s = scale * t\n    s = jnp.clip(s, 0, 1)\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. Represents the normalized distances to be mapped back to metric distances.\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    # Clamping to 1 produces correct scale inside |s| < 1\n    s_mag_sq = jnp.maximum(1, jnp.sum(s**2, axis=-1, keepdims=True))\n    inv_scale = 2 * jnp.sqrt(s_mag_sq) - s_mag_sq\n    t = s / inv_scale\n    return t\n\n  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    elif fn == contract3_isoscale:\n      fn_inv = lambda s: s ** 3\n    else:\n      raise ValueError('Unknown function.')\n\n  def t_to_s_fn(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1], using the provided function.\n\n    Input-Output Arguments\n    :param t: Tensor. Represents the metric distances to be mapped to normalized distances.\n    :return"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to automatically determine the inverse of the function.\n    if fn == contract:\n      fn_inv = inv_contract\n    else:\n      raise ValueError(f'Unknown function {fn}.')\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    :param t: Tensor. The input metric distances.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    t_clipped = jnp.clip(t, t_near, t_far)\n    s = (t_clipped - t_near) / (t_far - t_near)\n    return fn(s)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    :param s: Tensor. The input normalized distances in the range [0, 1].\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    t = fn_inv(s) * (t_far - t_near) + t_near\n    return jnp.clip(t, t_near, t_far)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n    if fn == contract:\n      fn_inv = inv_contract\n    elif fn == geopoly.contract_geodesic:\n      fn_inv = geopoly.inv_contract_geodesic\n    else:\n      raise ValueError(f'Unknown function {fn}.')\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input Arguments\n    :param t: Tensor. The metric distances to be mapped.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    t = jnp.maximum(t_near, jnp.minimum(t, t_far))\n    s = (t - t_near) / (t_far - t_near)\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input Arguments\n    :param s: Tensor. The normalized distances to be mapped back to metric distances.\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    t = s * (t_far - t_near) + t_near\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if t_near > t_far:\n    raise ValueError(f't_near ({t_near}) > t_far ({t_far}).')\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. The input metric distances.\n    :return: Tensor. The mapped normalized distances.\n    \"\"\"\n    # Clip distances to ensure they fall within a valid range.\n    t = jnp.maximum(t_near, t)\n    t = jnp.minimum(t_far, t)\n\n    # Compute the forward mapping from metric to normalized distances.\n    s = (t - t_near) / (t_far - t_near)\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. The input normalized distances in the range [0, 1].\n    :return: Tensor. The mapped metric distances.\n    \"\"\"\n    # Clip normalized distances to ensure they fall within a valid range.\n    s = jnp.maximum(0, s)\n    s = jnp.minimum(1, s)\n\n    # Compute the backward mapping from normalized to metric distances.\n    if fn_inv is not None:\n      t = fn_inv(s)\n    else:\n      t = t_near + s * (t_far - t_near)\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n    :param t: Tensor. The input metric distances.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    # Clip distances to ensure they fall within a valid range.\n    t = jnp.clip(t, t_near, t_far)\n    # Compute the normalized distances using the provided `fn` function.\n    s = (fn(t) - t_near) / (t_far - t_near)\n    # Clip the normalized distances to ensure they fall within the valid range [0, 1].\n    s = jnp.clip(s, 0, 1)\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n    :param s: Tensor. The input normalized distances in the range [0, 1].\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    # Clip the normalized distances to ensure they fall within the valid range [0, 1].\n    s = jnp.clip(s, 0, 1)\n    # Compute the metric distances using the provided `fn_inv` function.\n    t = s * (t_far - t_near) + t_near\n    return t\n\n  # If `fn_inv` is not provided, attempt to automatically determine its inverse.\n  if fn_inv is None:\n    # Define a dictionary of known functions and their inverses.\n    inverse_mapping = {\n        'sqrt': lambda x: x**2,\n        'log': lambda x: jnp.exp(x),\n        'exp': lambda x: jnp.log(x),\n        'cbrt': lambda x: x**3,\n        'sin': lambda x: jnp.arcsin(x),\n        'arcsin': lambda x: jnp.sin(x),\n        'cos': lambda x: jnp.arccos(x),\n        'ar"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to find the inverse of the function using a predefined mapping.\n    # This mapping is specific to the functions used in this implementation.\n    # If the function is not found, the `fn_inv` argument must be provided.\n    fn_inv_map = {\n        contract: inv_contract,\n        contract3_isoscale: inv_contract3_isoscale,\n    }\n    if fn not in fn_inv_map:\n      raise ValueError(f'No inverse found for {fn}.')\n    fn_inv = fn_inv_map[fn]\n\n  # Define the forward mapping from metric to normalized distances.\n  def t_to_s(t):\n    t = jnp.clip(t, t_near, t_far)\n    s = (t - t_near) / (t_far - t_near)\n    return fn(s)\n\n  # Define the backward mapping from normalized to metric distances.\n  def s_to_t(s):\n    s = jnp.clip(s, 0.0, 1.0)\n    t = s * (t_far - t_near) + t_near\n    return fn_inv(s) * (t_far - t_near) + t_near\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = _get_inverse_fn(fn)\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. Represents the metric distances to be mapped to normalized distances.\n    :return: Tensor. The normalized distances corresponding to the input metric distances.\n    \"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    s = (t - t_near) / (t_far - t_near)\n    return fn(s)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. Represents the normalized distances to be mapped back to metric distances.\n    :return: Tensor. The metric distances corresponding to the input normalized distances.\n    \"\"\"\n    t = fn_inv(s) * (t_far - t_near) + t_near\n    return jnp.clip(t, t_near, t_far)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        contract: inv_contract,\n        contract3_isoscale: lambda x: 1 - x,\n        lambda x: x: lambda x: x,\n    }.get(fn, None)\n  if fn_inv is None:\n    raise ValueError(\n        f'fn_inv must be provided for fn={fn}. '\n        'Please provide it as a keyword argument.'\n    )\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. The input metric distances to be mapped to normalized distances.\n    :return: Tensor. The mapped normalized distances in the range [0, 1].\n    \"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    s = fn(t)\n    s = jnp.clip(s, 0, 1)\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. The input normalized distances to be mapped back to metric distances.\n    :return: Tensor. The mapped metric distances.\n    \"\"\"\n    s = jnp.clip(s, 0, 1)\n    t = fn_inv(s)\n    t = jnp.clip(t, t_near, t_far)\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        contract: inv_contract,\n        contract3_isoscale: inv_contract3_isoscale,\n    }.get(fn, None)\n\n  if fn_inv is None:\n    raise ValueError(f'No inverse for {fn}.')\n\n  def t_to_s(t):\n    t = jnp.clip(t, t_near, t_far)\n    return (t - t_near) / (t_far - t_near)\n\n  def s_to_t(s):\n    return s * (t_far - t_near) + t_near\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    :param t: Tensor. Represents the metric distances to be mapped.\n    :return: Tensor. The mapped normalized distances in the range [0, 1].\n    \"\"\"\n    return (t - t_near) / (t_far - t_near)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    :param s: Tensor. Represents the normalized distances to be mapped back to metric distances.\n    :return: Tensor. The mapped metric distances.\n    \"\"\"\n    return s * (t_far - t_near) + t_near\n\n  if fn_inv is None:\n    fn_inv = {\n        'contract': inv_contract,\n        'contract3_isoscale': lambda s: s * t_far + t_near,\n        'exp': lambda s: s * t_far + t_near,\n        'log': lambda s: s * (t_far - t_near) + t_near,\n        'pow': lambda s: s * (t_far - t_near) + t_near,\n        'sqrt': lambda s: s * t_far + t_near,\n        'tanh': lambda s: s * (t_far - t_near) + t_near,\n        'tanh_inv': lambda t: (t - t_near) / (t_far - t_near),\n    }.get(fn.__name__, None)\n\n  if fn_inv is None:\n    raise ValueError(f'No inverse for {fn.__name__}')\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Attempt to automatically determine the inverse function based on a mapping of functions to their inverses.\n    fn_inv = {\n        'contract': inv_contract,\n        'contract3_isoscale': lambda s: s**3,\n        'log': lambda s: jnp.exp(s),\n        'exp': lambda s: jnp.log(s),\n        'inv_log': lambda s: jnp.log(1 - s),\n        'inv_exp': lambda s: jnp.exp(1 - s),\n        'inv_log_isoscale': lambda s: jnp.log(1 - s),\n        'inv_exp_isoscale': lambda s: jnp.exp(1 - s),\n    }.get(fn.__name__, None)\n    if fn_inv is None:\n      raise ValueError(f'Unable to automatically determine inverse function for {fn.__name__}.')\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    :param t: Tensor. Input metric distances.\n    :return: Tensor. Output normalized distances in the range [0, 1].\n    \"\"\"\n    # Clip distances to ensure they fall within a valid range.\n    t = jnp.clip(t, t_near, t_far)\n    # Transform distances using the provided function.\n    s = fn(t)\n    # Normalize the transformed distances to the range [0, 1].\n    s = (s - t_near) / (t_far - t_near)\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    :param s: Tensor. Input normalized distances in the range [0, 1].\n    :return: Tensor. Output metric distances.\n    \"\"\"\n    # Denormalize the input distances to the range [t_near, t_far].\n    s = s * (t_far - t_near) +"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Attempt to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n    # This is a hard-coded mapping of functions to their inverses.\n    inverse_mapping = {\n        contract: inv_contract,\n        # Add more mappings as needed for other functions.\n    }\n    if fn in inverse_mapping:\n      fn_inv = inverse_mapping[fn]\n    else:\n      raise ValueError(f'No inverse found for function {fn}.')\n\n  # Construct the forward mapping from metric to normalized distances.\n  t_to_s = lambda t: jnp.maximum(0, jnp.minimum(1, (t - t_near) / (t_far - t_near)))\n\n  # Construct the backward mapping from normalized to metric distances.\n  s_to_t = lambda s: t_near + s * (t_far - t_near)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z])  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  # Calculate the cartesian coordinates (x, y, z) based on the given spherical coordinates (radius r, elevation theta, and azimuth phi).\n  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Ensure that the input data points are valid for a linear spline.\n  utils.assert_valid_linspline(t, w)\n\n  # Calculate the integral using the trapezoid rule.\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return math.trapz(t, w)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n\n  # Calculate the integral using the trapezoid rule\n  integral = (jnp.sum(w[1:] + w[:-1]) / 2) * (t[1] - t[0])\n\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  check_zero_endpoints(w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  # Check if the input data points are valid for a linear spline\n  utils.assert_valid_linspline(t, w)\n\n  # Calculate the integral using the trapezoid rule\n  integral = jnp.sum(w[:-1] + w[1:]) * (t[1] - t[0]) / 2\n\n  return integral\n\n"}
{"namespace": "linspline.integrate", "completion": "  check_zero_endpoints(w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, x=t)\n\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, x=t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  check_zero_endpoints(w)\n  checkify.check(jnp.all(t[1:] > t[:-1]), 't must be monotonically increasing.')\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  check_zero_endpoints(w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  t = jnp.asarray(t)\n  w = jnp.asarray(w)\n  n = t.shape[0]\n  h = t[1:] - t[:-1]\n  h = jnp.concatenate([h, h[-1:]])\n  return jnp.sum(h * (w[:-1] + w[1:]) / 2.0)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    weighted_scores = {}\n    for i, id_list in enumerate(ids):\n        for j, id_ in enumerate(id_list):\n            if id_ not in weighted_scores:\n                weighted_scores[id_] = 0\n            weighted_scores[id_] += scores[i][j] * weights[i]\n\n    # Normalize the scores\n    total_weight = sum(weights)\n    normalized_scores = {id_: score / total_weight for id_, score in weighted_scores.items()}\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(normalized_scores.keys(), key=lambda x: normalized_scores[x], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], [normalized_scores[id_] for id_ in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    ids_list = []\n    scores_list = []\n\n    for i in range(len(ids)):\n        ids_list.extend(ids[i])\n        scores_list.extend(scores[i])\n\n    df = pd.DataFrame({'id': ids_list, 'score': scores_list})\n    df['weighted_score'] = df['score'] * weights[i]\n    df = df.groupby('id')['weighted_score'].sum().reset_index()\n    df = df.sort_values('weighted_score', ascending=False)\n    df = df.head(top_k)\n\n    return df['id'].tolist(), df['weighted_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    ids_list = []\n    scores_list = []\n    for i in range(len(ids)):\n        ids_list.extend(ids[i])\n        scores_list.extend([score * weights[i] for score in scores[i]])\n\n    df = pd.DataFrame({'id': ids_list, 'score': scores_list})\n    df = df.groupby('id').sum().sort_values(by='score', ascending=False).reset_index()\n    return df['id'].tolist()[:top_k], df['score'].tolist()[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    ids_list = []\n    scores_list = []\n    for i in range(len(ids)):\n        ids_list.extend(ids[i])\n        scores_list.extend(scores[i])\n\n    df = pd.DataFrame({'id': ids_list, 'score': scores_list})\n    df['weighted_score'] = df['score'] * weights[i]\n    df = df.groupby('id')['weighted_score'].sum().reset_index()\n    df = df.sort_values('weighted_score', ascending=False)\n    df = df.head(top_k)\n\n    return df['id'].tolist(), df['weighted_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_scores = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            weighted_scores[ids[i][j]] = weighted_scores.get(ids[i][j], 0) + scores[i][j] * weights[i]\n\n    # Normalize the scores\n    max_score = max(weighted_scores.values())\n    min_score = min(weighted_scores.values())\n    normalized_scores = {id_: (score - min_score) / (max_score - min_score) for id_, score in weighted_scores.items()}\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(normalized_scores.keys(), key=lambda x: normalized_scores[x], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], [normalized_scores[id_] for id_ in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_list, score_list = [], []\n    for i in range(len(ids)):\n        id_list.extend(ids[i])\n        score_list.extend(scores[i])\n\n    df = pd.DataFrame({'id': id_list, 'score': score_list})\n    df['weighted_score'] = df['score'] * weights[i]\n    df = df.groupby('id').sum().sort_values('weighted_score', ascending=False).reset_index()\n    return df['id'].tolist()[:top_k], df['weighted_score'].tolist()[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    weighted_sums = {id_: sum(score * weight for score, weight in zip(scores, weights))\n                     for id_, scores in zip(ids, scores)}\n\n    # Normalize the weighted sums\n    total_weight = sum(weights)\n    normalized_weighted_sums = {id_: weighted_sum / total_weight\n                                for id_, weighted_sum in weighted_sums.items()}\n\n    # Sort the IDs by their normalized weighted sum in descending order\n    sorted_ids = sorted(normalized_weighted_sums, key=normalized_weighted_sums.get, reverse=True)\n\n    # Return the top K IDs and their corresponding normalized weighted sums\n    return sorted_ids[:top_k], [normalized_weighted_sums[id_] for id_ in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_list = []\n    score_list = []\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            id_list.append(ids[i][j])\n            score_list.append(scores[i][j] * weights[i])\n\n    df = pd.DataFrame({'id': id_list, 'score': score_list})\n    df = df.groupby('id').sum().sort_values('score', ascending=False)\n    df = df.reset_index()\n    return df['id'].tolist()[:top_k], df['score'].tolist()[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    weighted_scores = []\n    for id_list, score_list, weight in zip(ids, scores, weights):\n        for id, score in zip(id_list, score_list):\n            weighted_scores.append((id, score * weight))\n\n    weighted_scores.sort(key=lambda x: x[1], reverse=True)\n    top_ids = [id for id, _ in weighted_scores[:top_k]]\n    top_scores = [score for _, score in weighted_scores[:top_k]]\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_scores = [sum([score * weight for score, weight in zip(scores_list, weights)]) for scores_list in scores]\n\n    # Normalize the weighted scores\n    max_score = max(weighted_scores)\n    min_score = min(weighted_scores)\n    normalized_scores = [(score - min_score) / (max_score - min_score) for score in weighted_scores]\n\n    # Get the top K IDs and their corresponding scores\n    top_ids = [ids_list[i] for i in sorted(range(len(normalized_scores)), key=lambda i: normalized_scores[i], reverse=True)[:top_k]]\n    top_scores = [normalized_scores[i] for i in sorted(range(len(normalized_scores)), key=lambda i: normalized_scores[i], reverse=True)[:top_k]]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = []\n    for i in range(len(ids)):\n        id_list = ids[i]\n        score_list = scores[i]\n        weight = weights[i]\n        for j in range(len(id_list)):\n            id_ = id_list[j]\n            score = score_list[j]\n            if id_ not in weighted_sums:\n                weighted_sums.append({'id': id_, 'weighted_sum': score * weight})\n            else:\n                weighted_sums[id_list.index(id_)]['weighted_sum'] += score * weight\n\n    # Normalize the weighted sums\n    total_weighted_sum = sum([weighted_sum['weighted_sum'] for weighted_sum in weighted_sums])\n    normalized_weighted_sums = [weighted_sum['weighted_sum'] / total_weighted_sum for weighted_sum in weighted_sums]\n\n    # Sort the IDs by their normalized weighted sums in descending order\n    sorted_ids = [weighted_sum['id'] for weighted_sum in sorted(weighted_sums, key=lambda x: x['weighted_sum'], reverse=True)]\n    sorted_normalized_weighted_sums = sorted(normalized_weighted_sums, reverse=True)\n\n    # Return the top K IDs and their corresponding normalized weighted sums\n    return sorted_ids[:top_k], sorted_normalized_weighted_sums[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = {id_list[i]: sum(score_list[i] * weight for i, (score_list, weight) in enumerate(zip(scores, weights))) for id_list in ids for i in range(len(id_list))}\n\n    # Normalize the scores\n    max_weighted_sum = max(weighted_sums.values())\n    normalized_scores = {id_: score / max_weighted_sum for id_, score in weighted_sums.items()}\n\n    # Sort the IDs by their normalized scores in descending order\n    sorted_ids = sorted(normalized_scores.keys(), key=lambda x: normalized_scores[x], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], [normalized_scores[id_] for id_ in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = []\n    for id_list, score_list, weight in zip(ids, scores, weights):\n        weighted_sums.extend([score * weight for score in score_list])\n\n    # Normalize the weighted sums\n    normalized_sums = [sum / sum(weighted_sums) for sum in weighted_sums]\n\n    # Get the top K IDs and corresponding weighted sums\n    top_k_ids = [id_list[i] for i, _ in enumerate(normalized_sums) if i < top_k]\n    top_k_sums = [sum for _, sum in enumerate(normalized_sums) if _ < top_k]\n\n    return top_k_ids, top_k_sums"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = []\n    for id_list, score_list, weight in zip(ids, scores, weights):\n        weighted_sum = sum(score * weight for score in score_list)\n        weighted_sums.append((id_list, weighted_sum))\n\n    # Sort the IDs based on their weighted sums in descending order\n    weighted_sums.sort(key=lambda x: x[1], reverse=True)\n\n    # Extract the top K IDs and their corresponding weighted sums\n    top_ids = [id_list for id_list, _ in weighted_sums[:top_k]]\n    top_scores = [score for _, score in weighted_sums[:top_k]]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_list = []\n    score_list = []\n    for i in range(len(ids)):\n        assert len(ids[i]) == len(scores[i]), \"The length of ids and scores must be the same.\"\n        assert len(ids[i]) == len(weights), \"The length of weights must be the same as the length of ids.\"\n        id_list += ids[i]\n        score_list += [score * weights[i] for score in scores[i]]\n    df = pd.DataFrame({'id': id_list, 'score': score_list})\n    df = df.groupby('id')['score'].sum().reset_index()\n    df = df.sort_values('score', ascending=False)\n    df = df.head(top_k)\n    return df['id'].tolist(), df['score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_scores = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            weighted_scores[ids[i][j]] = weighted_scores.get(ids[i][j], 0) + scores[i][j] * weights[i]\n\n    # Normalize the scores\n    max_score = max(weighted_scores.values())\n    min_score = min(weighted_scores.values())\n    normalized_scores = {k: (v - min_score) / (max_score - min_score) for k, v in weighted_scores.items()}\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(normalized_scores.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return [id_score[0] for id_score in sorted_ids[:top_k]], [id_score[1] for id_score in sorted_ids[:top_k]]\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Calculate the weighted sum of scores for each ID\n    weighted_scores = [sum([score * weight for score, weight in zip(scores[i], weights)]) for i in range(len(ids))]\n\n    # Normalize the weighted scores\n    normalized_scores = [score / sum(weighted_scores) for score in weighted_scores]\n\n    # Get the top K IDs and their corresponding scores based on the normalized weighted scores\n    top_k_ids = [ids[i] for i in sorted(range(len(normalized_scores)), key=lambda x: normalized_scores[x], reverse=True)[:top_k]]\n    top_k_scores = [normalized_scores[i] for i in sorted(range(len(normalized_scores)), key=lambda x: normalized_scores[x], reverse=True)[:top_k]]\n\n    return top_k_ids, top_k_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    weighted_scores = [sum([score * weight for score, weight in zip(scores, weights)])]\n    for id_list, score_list in zip(ids[1:], scores[1:]):\n        weighted_scores.append(sum([score * weight for score, weight in zip(score_list, weights)]))\n\n    ids_list = [id_list for id_list in ids]\n    ids_list = [item for sublist in ids_list for item in sublist]\n    weighted_scores_list = [weighted_scores for weighted_scores in weighted_scores]\n    weighted_scores_list = [item for sublist in weighted_scores_list for item in sublist]\n\n    df = pd.DataFrame({'id': ids_list, 'weighted_score': weighted_scores_list})\n    df = df.sort_values(by='weighted_score', ascending=False)\n    df = df.head(top_k)\n\n    return df['id'].tolist(), df['weighted_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = {}\n    for i, (id_list, score_list, weight) in enumerate(zip(ids, scores, weights)):\n        for id_, score in zip(id_list, score_list):\n            weighted_sums[id_] = weighted_sums.get(id_, 0) + score * weight\n\n    # Normalize the scores\n    max_score = max(weighted_sums.values())\n    min_score = min(weighted_sums.values())\n    normalized_scores = {id_: (score - min_score) / (max_score - min_score) for id_, score in weighted_sums.items()}\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(normalized_scores.keys(), key=lambda x: normalized_scores[x], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], [normalized_scores[id_] for id_ in sorted_ids[:top_k]]\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Initialize an empty dictionary to store the weighted scores for each ID\n    weighted_scores = {}\n\n    # Iterate through each set of IDs and corresponding scores\n    for id_list, score_list, weight in zip(ids, scores, weights):\n        # Iterate through each ID and its corresponding score\n        for id, score in zip(id_list, score_list):\n            # If the ID is not in the dictionary, add it with its initial score\n            if id not in weighted_scores:\n                weighted_scores[id] = score * weight\n            # If the ID is already in the dictionary, add its score to the existing value\n            else:\n                weighted_scores[id] += score * weight\n\n    # Normalize the scores by dividing each score by the sum of all scores\n    total_score = sum(weighted_scores.values())\n    for id in weighted_scores:\n        weighted_scores[id] /= total_score\n\n    # Sort the IDs by their weighted scores in descending order\n    sorted_ids = sorted(weighted_scores.keys(), key=lambda x: weighted_scores[x], reverse=True)\n\n    # Return the top K IDs and their corresponding weighted scores\n    return sorted_ids[:top_k], [weighted_scores[id] for id in sorted_ids[:top_k]]"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  fn_cov = cov\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  jacobian = jax.jacfwd(fn)(mean)\n  fn_cov = jacobian @ cov @ jacobian.transpose((0, 2, 1))\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  fn_cov = cov\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  jac = jax.jacobian(fn)(mean)\n  fn_cov = jac @ cov @ jac.transpose(0, 2, 1)\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  fn_cov = jnp.linalg.solve(cov, fn(mean + cov) - fn_mean)\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  linearized_fn = jax.jacfwd(fn, argnums=0)\n  linearized_fn_cov = linearized_fn(mean)\n  linearized_fn_cov = jnp.einsum('...ij,...jk->...ik', linearized_fn_cov, cov)\n\n  # Transform the covariances using the linearized function\n  fn_cov = jnp.einsum('...ij,...jk->...ik', linearized_fn_cov, linearized_fn_cov)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  fn_cov = cov\n\n  # Apply the linearization to the covariances\n  for i in range(mean.shape[-1]):\n    for j in range(mean.shape[-1]):\n      fn_cov = fn_cov.at[..., i, j].set(fn_cov[..., i, j] + (mean[..., i] - fn_mean[..., i]) * (mean[..., j] - fn_mean[..., j]))\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  fn_cov = cov\n\n  # Compute the Jacobian of the function at the mean\n  jacobian = jax.jvp(fn, (mean,), (jnp.eye(mean.shape[-1]),))[1]\n\n  # Transform the covariances using the Jacobian\n  fn_cov = jacobian @ fn_cov @ jacobian.T\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  jac = jax.jacobian(fn, 0)(mean)\n  fn_cov = jac @ cov @ jac.transpose(0, 2, 1)\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  linearized_mean = fn(mean)\n\n  # Compute the Jacobian of the function at the mean\n  jac = jax.jacfwd(fn)(mean)\n\n  # Transform the covariances using the Jacobian\n  fn_cov = jnp.einsum('...ij,...jk,...lk->...il', jac, cov, jac)\n\n  return linearized_mean, fn_cov\n\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  fn_cov = jnp.einsum('...i,...j->...ij', cov, cov)\n\n  # Apply the linearized function to the mean and covariances\n  return fn_mean, fn_cov\n\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  fn_cov = cov\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  linearized_fn = jax.vmap(fn, in_axes=(-1, None))\n  fn_mean = linearized_fn(mean)\n\n  # Compute the Jacobian of the function\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=(-1, None))(mean)\n\n  # Transform the covariances using the Jacobian\n  fn_cov = jnp.einsum('...ij,...jk,...lk->...il', jac, cov, jac)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  # Compute the Jacobian of the function around the mean\n  jac = jax.jacfwd(fn)(mean)\n  # Transform the covariances using the Jacobian\n  fn_cov = jnp.einsum('...i,...ij,...j->...i', cov, jac, jac)\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean = fn(mean)\n  fn_cov = cov\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function\n  jac = jax.jacfwd(fn)(mean)\n\n  # Transform the covariances using the Jacobian\n  fn_cov = jac @ cov @ jac.T\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  # Calculate the Jacobian of the function at the mean\n  jacobian = jax.jacfwd(fn)(mean)\n  # Calculate the covariance of the linearized function\n  fn_cov = jacobian @ cov @ jacobian.T\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  linearized_fn = lambda x: jnp.dot(x - mean, jnp.linalg.pinv(cov)) + fn_mean\n\n  # Compute the Jacobian of the linearized function at the mean\n  jacobian = jax.jacfwd(linearized_fn)(mean)\n\n  # Compute the transformed covariances\n  fn_cov = jnp.dot(jnp.dot(jacobian, cov), jacobian.T)\n\n  return fn_mean, fn_cov\n\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jacobian = jax.jacfwd(fn)(mean)\n\n  # Compute the linearized function at the mean\n  fn_mean = fn(mean)\n\n  # Compute the linearized covariance\n  fn_cov = jnp.einsum('...i,...ij,...j', jacobian, cov, jacobian)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  jac = jax.jacfwd(fn)(mean)\n\n  # Transform the covariances using the linearization\n  fn_cov = jac @ cov @ jac.T\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if len(arr) > 1:\n            yield [arr[: len(arr) // 2], arr[len(arr) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if arr.size > 1:\n            yield arr[: len(arr) // 2], arr[len(arr) // 2 :]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for a in x:\n        if a.size > 1:\n            yield a[: a.size // 2], a[a.size // 2 :]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if arr.size > 1:\n            yield [arr[: arr.size // 2], arr[arr.size // 2 :]]\n        else:\n            yield [arr, arr]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.shape[0] > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n        else:\n            yield [i, i]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.shape[0] > 1:\n            yield [i[: i.shape[0] // 2], i[i.shape[0] // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield np.split(i, 2)\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]\n        else:\n            yield [xi, xi]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield np.split(i, 2)\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if arr.size > 1:\n            yield [arr[: arr.size // 2], arr[arr.size // 2 :]]\n        else:\n            yield [arr, arr]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n        else:\n            yield [i]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.shape[0] > 1:\n            yield [i[: i.shape[0] // 2], i[i.shape[0] // 2 :]]\n        else:\n            yield [i, i]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[: i.size // 2], i[i.size // 2 :]]\n        else:\n            yield [i]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield [x[i][: len(x[i]) // 2], x[i][len(x[i]) // 2 :]]\n        else:\n            yield [x[i], x[i]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[: i.size // 2], i[i.size // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if len(arr) > 1:\n            yield [arr[: len(arr) // 2], arr[len(arr) // 2 :]]\n        else:\n            yield [arr, arr]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"x must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"x must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The matrix must be 2d\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The matrix must be 2-dimensional\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if not x.shape[0] == x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if not np.allclose(x.shape[0], x.shape[1]):\n        raise ValueError(\"The matrix is not square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x_enc = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    x_enc = jnp.concatenate([x, x_enc], axis=-1)\n  return x_enc\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x_enc = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    x_enc = jnp.concatenate([x, x_enc], axis=-1)\n  return x_enc\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    encoded = jnp.concatenate([x, encoded], axis=-1)\n  return encoded\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  if append_identity:\n    x = jnp.concatenate([x, scaled_x], axis=-1)\n  else:\n    x = scaled_x\n  x = jnp.concatenate([jnp.sin(x), jnp.cos(x)], axis=-1)\n  return x\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x_enc = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    x_enc = jnp.concatenate([x, x_enc], axis=-1)\n  return x_enc\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x_enc = jnp.concatenate(\n      [jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1\n  )\n  if append_identity:\n    x_enc = jnp.concatenate([x, x_enc], axis=-1)\n  return x_enc\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    encoded = jnp.concatenate([x, encoded], axis=-1)\n  return encoded\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.concatenate(\n      [jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1\n  )\n  if append_identity:\n    encoded = jnp.concatenate([x, encoded], axis=-1)\n  return encoded\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x = jnp.concatenate([jnp.sin(x), jnp.cos(x)], axis=-1)\n  if append_identity:\n    x = jnp.concatenate([x, x[Ellipsis, :1]], axis=-1)\n  return x\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x_enc = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    x_enc = jnp.concatenate([x, x_enc], axis=-1)\n  return x_enc\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x_enc = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    x_enc = jnp.concatenate([x, x_enc], axis=-1)\n  return x_enc\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x_enc = jnp.concatenate(\n      [jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1\n  )\n  if append_identity:\n    x_enc = jnp.concatenate([x, x_enc], axis=-1)\n  return x_enc\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x = jnp.concatenate([jnp.sin(x), jnp.cos(x)], axis=-1)\n  if append_identity:\n    x = jnp.concatenate([x, jnp.broadcast_to(x[..., :1], x.shape)], axis=-1)\n  return x\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  x_scaled = x[..., None, :] * scales[None, :, None]\n  x_encoded = jnp.concatenate([jnp.sin(x_scaled), jnp.cos(x_scaled)], axis=-1)\n  if append_identity:\n    x_encoded = jnp.concatenate([x[..., None], x_encoded], axis=-1)\n  return x_encoded\n\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  x_scaled = x[..., None] * scales[None, :, None]\n  x_enc = jnp.concatenate([jnp.sin(x_scaled), jnp.cos(x_scaled)], axis=-1)\n  if append_identity:\n    x_enc = jnp.concatenate([x[..., None], x_enc], axis=-1)\n  return x_enc\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n\n  if append_identity:\n    x = jnp.concatenate([x, scaled_x], axis=-1)\n    return jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  else:\n    return jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n\n"}
{"namespace": "coord.pos_enc", "completion": "  if append_identity:\n    x = jnp.concatenate([x, jnp.ones_like(x[..., :1])], axis=-1)\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  x = (x[..., None, :] * scales[None, :, None])\n  x = jnp.concatenate([jnp.sin(x), jnp.cos(x)], axis=-1)\n  return x\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n\n  # Apply sine function to scaled input\n  pos_enc = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n\n  # Concatenate with original input if specified\n  if append_identity:\n    pos_enc = jnp.concatenate([x, pos_enc], axis=-1)\n\n  return pos_enc\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Generate scales based on the range of degrees\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Scale the input array\n  scaled_x = x[..., None, :] * scales[:, None]\n\n  # Apply sine function to the scaled input\n  encoded_x = jnp.sin(scaled_x)\n\n  # Concatenate the original input with the encoded features\n  if append_identity:\n    encoded_x = jnp.concatenate([x[..., None, :], encoded_x], axis=-1)\n\n  return encoded_x\n\n\n"}
{"namespace": "coord.pos_enc", "completion": "  # Generate scales based on the degree exponents\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Apply sine function to the input array\n  x_scaled = x[..., None, :] * scales[:, None]\n  x_encoded = jnp.concatenate([jnp.sin(x_scaled), jnp.cos(x_scaled)], axis=-1)\n\n  # Optionally append the original input array to the encoded array\n  if append_identity:\n    x_encoded = jnp.concatenate([x, x_encoded], axis=-1)\n\n  return x_encoded\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        else:\n            for i in range(len(values[field1])):\n                if values[field1][i].shape != values[field2][i].shape:\n                    raise ValueError(\n                        f\"{cls.__name__}: {field1} and {field2} shape mismatch for index {i}.\"\n                    )\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i, (arr1, arr2) in enumerate(zip(values[field1], values[field2])):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch in index {i}, \"\n                    f\"resp. {arr1.shape} and {arr2.shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch, resp. {len(values[field1])} and {len(values[field2])}\")\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1][i].shape} and {values[field2][i].shape}\")\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator\n\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                             f\"resp. {len(values[field1])} and {len(values[field2])}\")\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator\n\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model that is being validated.\n        :param values: Dict[str, np.ndarray]. A dictionary of values that include the two lists of numpy arrays to be compared.\n        :return: Dict[str, np.ndarray]. The validated values if the check passes.\n\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                             f\"resp. {len(values[field1])} and {len(values[field2])}\")\n\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and that all corresponding pairs of arrays have the same shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch for index {i}. \"\n                    f\"{values[field1][i].shape} and {values[field2][i].shape}\"\n                )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model being validated.\n        :param values: Dict[str, np.ndarray]. A dictionary of values to be validated. The keys of this dictionary should be the same as the field names specified in the input arguments.\n        :return: Dict[str, np.ndarray]. The validated values. If the check for shape equality passes, the original dictionary is returned.\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                    f\"resp. {values[field1][i].shape} and {values[field2][i].shape}\"\n                )\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function is a Pydantic validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model.\n        :param values: Dict[str, np.ndarray]. A dictionary of field names and corresponding numpy arrays. The field names are specified as the input arguments field1 and field2.\n        :return: Dict[str, np.ndarray]. The validated values, which are the same as the input values if the check passes.\n\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The type of the class being validated.\n        :param values: Dict[str, np.ndarray]. A dictionary of values to be validated. The keys of this dictionary correspond to the field names specified earlier. The values should be numpy arrays.\n        :return: Dict[str, np.ndarray]. The validated values dictionary. If the check passes, the function returns the input values unchanged.\n\n        \"\"\"\n\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the mesh\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the mesh using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera, eglctx)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    nomic_config = NomicBertConfig()\n\n    # Set the hidden_size attribute of the NomicBertConfig object to the same value as the hidden_size attribute of the BertConfig object\n    nomic_config.hidden_size = bert_config.hidden_size\n\n    # Set the num_attention_heads attribute of the NomicBertConfig object to the same value as the num_attention_heads attribute of the BertConfig object\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n\n    # Set the intermediate_size attribute of the NomicBertConfig object to the same value as the intermediate_size attribute of the BertConfig object\n    nomic_config.intermediate_size = bert_config.intermediate_size\n\n    # Set the hidden_act attribute of the NomicBertConfig object to the same value as the hidden_act attribute of the BertConfig object\n    nomic_config.hidden_act = bert_config.hidden_act\n\n    # Set the layer_norm_eps attribute of the NomicBertConfig object to the same value as the layer_norm_eps attribute of the BertConfig object\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n\n    # Set the max_position_embeddings attribute of the NomicBertConfig object to the same value as the max_position_embeddings attribute of the BertConfig object\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n\n    # Set the type_vocab_size attribute of the NomicBertConfig object to the same value as the type_vocab_size attribute of the BertConfig object\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n\n    # Set the vocab_size attribute of the NomicBertConfig object to the same value as the vocab_size attribute of the BertConfig object\n    nomic_config.vocab_size = bert_config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        num_labels=bert_config.num_labels,\n        num_special_tokens=bert_config.num_special_tokens,\n        num_segments=bert_config.num_segments,\n        num_spans=bert_config.num_spans,\n        num_spans_per_segment=bert_config.num_spans_per_segment,\n        num_spans_per_segment_per_span=bert_config.num_spans_per_segment_per_span,\n        num_spans_per_segment_per_span_per_span=bert_config.num_spans_per_segment_per_span_per_span,\n        num_spans_per_segment_per_span_per_span_per_span=bert_config.num_spans_per_segment_per_span_per_span_per_span,\n        num_spans_per_segment_per_span"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        use_cache=bert_config.use_cache,\n        num_labels=bert_config.num_labels,\n        num_beams=bert_config.num_beams,\n        max_length=bert_config.max_length,\n        min_length=bert_config.min_length,\n        do_sample=bert_config.do_sample,\n        temperature=bert_config.temperature,\n        top_k=bert_config.top_k,\n        top_p=bert_config.top_p,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        decoder_start_token_id=bert_config.decoder_start_token_id,\n        decoder_end_token_id=bert_config.decoder_end_token_id,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        e"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same values as the input BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        **bert_config.to_dict(),\n    )\n\n    # Set the classifier_dropout_prob attribute to the hidden_dropout_prob attribute\n    nomic_config.classifier_dropout_prob = bert_config.hidden_dropout_prob\n\n    # Set the classifier_activation attribute to the hidden_act attribute\n    nomic_config.classifier_activation = bert_config.hidden_act\n\n    # Set the classifier_activation_dropout_prob attribute to the hidden_dropout_prob attribute\n    nomic_config.classifier_activation_dropout_prob = bert_config.hidden_dropout_prob\n\n    # Set the classifier_activation_dropout_prob attribute to the hidden_dropout_prob attribute\n    nomic_config.classifier_activation_dropout_prob = bert_config.hidden_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    nomic_config = NomicBertConfig()\n\n    # Set the model_type attribute to \"bert\"\n    nomic_config.model_type = \"bert\"\n\n    # Set the num_attention_heads attribute to the value of the num_attention_heads attribute in the BertConfig object\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n\n    # Set the hidden_size attribute to the value of the hidden_size attribute in the BertConfig object\n    nomic_config.hidden_size = bert_config.hidden_size\n\n    # Set the num_hidden_layers attribute to the value of the num_hidden_layers attribute in the BertConfig object\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n\n    # Set the intermediate_size attribute to the value of the intermediate_size attribute in the BertConfig object\n    nomic_config.intermediate_size = bert_config.intermediate_size\n\n    # Set the max_position_embeddings attribute to the value of the max_position_embeddings attribute in the BertConfig object\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n\n    # Set the vocab_size attribute to the value of the vocab_size attribute in the BertConfig object\n    nomic_config.vocab_size = bert_config.vocab_size\n\n    # Set the type_vocab_size attribute to the value of the type_vocab_size attribute in the BertConfig object\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n\n    # Set the initializer_range attribute to the value of the initializer_range attribute in the BertConfig object\n    nomic_config.initializer_range = bert_config.initializer_range\n\n    # Set the hidden_dropout_prob attribute to the value of the hidden_dropout_prob attribute in the BertConfig object\n    nom"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same attributes as the original BertConfig\n    nomic_config = NomicBertConfig()\n    for attr, value in bert_config.items():\n        if hasattr(nomic_config, attr):\n            setattr(nomic_config, attr, value)\n\n    # Set the new attributes specific to the Nomic model\n    nomic_config.nomic_hidden_size = bert_config.hidden_size\n    nomic_config.nomic_num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.nomic_num_attention_heads = bert_config.num_attention_heads\n    nomic_config.nomic_intermediate_size = bert_config.intermediate_size\n    nomic_config.nomic_hidden_act = bert_config.hidden_act\n    nomic_config.nomic_vocab_size = bert_config.vocab_size\n    nomic_config.nomic_max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.nomic_type_vocab_size = bert_config.type_vocab_size\n    nomic_config.nomic_initializer_range = bert_config.initializer_range\n    nomic_config.nomic_layer_norm_eps = bert_config.layer_norm_eps\n    nomic_config.nomic_attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_config.nomic_hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.nomic_classifier_dropout = bert_config.classifier_dropout\n\n    return nomic_config\n\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Initialize a new NomicBertConfig object with default values\n    nomic_config = NomicBertConfig()\n\n    # Map the attributes from the BertConfig to the NomicBertConfig\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.position_embedding_type = bert_config.position_embedding_type\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.pad_token_id = bert_config.pad_token_id\n    nomic_config.bos_token_id = bert_config.bos_token_id\n    nomic_config.eos_token_id = bert_config.eos_token_id\n    nomic_config.sep_token_id = bert_config.sep_token_id\n    nomic_config.cls_token_id = bert_config.cls_token_id\n    nomic_config.mask_token_id = bert_config.mask_token_id\n    nomic_config.num_labels = bert_config.num_labels\n    nomic_config.num_special_tokens = bert_config.num_special_tokens"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same attributes as the original BertConfig\n    nomic_config = NomicBertConfig()\n    for attr, value in bert_config.__dict__.items():\n        if attr in nomic_config.__dict__:\n            setattr(nomic_config, attr, value)\n\n    # Set the new attributes specific to the Nomic model\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.hidden_act = bert_config.hidden_act\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n\n    # Set the new attributes specific to the Nomic model\n    nomic_config.num_layers = bert_config.num_hidden_layers\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.intermediate_size = bert_config.intermediate_size\n    nomic_config.hidden_act = bert_config.hidden_act\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.type_vocab_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        num_embeddings=bert_config.num_embeddings,\n        num_embedding_groups=bert_config.num_embedding_groups,\n        embedding_dim=bert_config.embedding_dim,\n        embedding_group_dim=bert_config.embedding_group_dim,\n        embedding_group_index=bert_config.embedding_group_index,\n        embedding_group_norm=bert_config.embedding_group_norm,\n        embedding_group_norm_type=bert_config.embedding_group_norm_type,\n        embedding_group_norm_eps=bert_config.embedding_group_norm_eps,\n        embedding_group_norm_affine=bert_config.embedding_group_norm_affine,\n        embedding_group_norm_elementwise_affine=bert_config.embedding_group_norm_elementwise_affine,\n        embedding_group_norm_scale_init_value=bert_config.embedding_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        max_seq_length=bert_config.max_position_embeddings,\n        num_labels_in_head=bert_config.num_labels_in_head,\n        num_labels_in_tail=bert_config.num_labels_in_tail,\n        label_embedding_size=bert_config.label_embedding_size,\n        label_head_hidden_size=bert_config.label_head_hidden_size,\n        label_tail_hidden_size=bert_config.label_tail_hidden_size,\n        label_head_num_layers=bert_config.label_head_num_layers,\n        label_tail_num_layers=bert_config.label_tail_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        is_decoder=bert_config.is_decoder,\n        is_encoder_decoder=bert_config.is_encoder_decoder,\n        is_split_head=bert_config.is_split_head,\n        num_special_tokens=bert_config.num_special_tokens,\n        model_type=bert_config.model_type,\n        model_name_or_path=bert_config.model_name_or_path,\n        model_revision=bert_config.model_revision,\n        use_auth_token=bert_config.use_auth_token,\n        cache_dir=bert_config.cache_dir,\n        bos_token_id"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        # additional arguments\n        num_labels=bert_config.num_labels,\n        max_length=bert_config.max_position_embeddings,\n        num_special_tokens=bert_config.num_special_tokens,\n        num_special_tokens_to_add=bert_config.num_special_tokens_to_add,\n        num_special_tokens_to_add_to_pooler=bert_config.num_special_tokens_to_add_to_pooler,\n        num_special_tokens_to_add_to_attention=bert_config.num_special_tokens_to_add_to_attention,\n        num_special_tokens_to_add_to_embedding=bert_config.num"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        is_decoder=bert_config.is_decoder,\n        is_encoder=bert_config.is_encoder,\n        is_nomic_model=True,\n        is_gpt_model=False,\n        is_bert_model=True,\n        is_roberta_model=False,\n        is_xlm_model=False,\n        is_xlm_roberta_model=False,\n        is_albert_model=False,\n        is_distilbert_model=False,\n        is_deberta_model=False,\n        is_deberta_v2_model=False,\n        is_xlnet_model=False,\n        is_longformer_model=False,\n        is_t5_model"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same attributes as the BertConfig\n    nomic_config = NomicBertConfig()\n    nomic_config.update(bert_config)\n\n    # Update the NomicBertConfig with specific mappings and additional configurations\n    nomic_config.num_labels = bert_config.vocab_size\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n    nomic_config.hidden_size = bert_config.hidden_size\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.layer_norm_eps = bert_config.layer_norm_eps\n    nomic_config.gradient_checkpointing = bert_config.gradient_checkpointing\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.pad_token_id = bert_config.pad_token_id\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.hidden_act = bert_config.hidden_act\n    nomic_config.initializer_range = bert_config.initializer_range\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n    nomic_config.num_attention_heads ="}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        scale_embedding=bert_config.scale_embedding,\n        scale_embedding_by_layer=bert_config.scale_embedding_by_layer,\n        # Additional attributes specific to the Nomic model\n        max_seq_length=bert_config.max_position_embeddings,\n        num_labels=bert_config.num_labels,\n        num_token_types=bert_config.type_vocab_size,\n        num_token_types_to_use=bert_config.type_vocab_size,\n        num_token_types_to_use_per_token=bert_config.type_vocab_size,\n        num_token_types_to_use_per_token_per_token=bert_config.type_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        is_decoder=bert_config.is_decoder,\n        is_encoder_decoder=bert_config.is_encoder_decoder,\n        use_scaled_init_for_decoder=bert_config.use_scaled_init_for_decoder,\n        # Nomic-specific configurations\n        num_classes=bert_config.num_classes,\n        num_labels=bert_config.num_labels,\n        num_special_tokens=bert_config.num_special_tokens,\n        num_token_type_embeddings=bert_config.num_token_type_embeddings,\n        num_token_type_embeddings_to_add=bert_config.num_token_type_embeddings_to_add,\n        num_token_type_embeddings_to_add_to_vocab=bert"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Define a mapping from BERT model argument names to Nomic model argument names\n    bert_to_nomic_argument_mapping = {\n        \"hidden_size\": \"hidden_size\",\n        \"num_hidden_layers\": \"num_layers\",\n        \"num_attention_heads\": \"num_heads\",\n        \"intermediate_size\": \"intermediate_size\",\n        \"hidden_act\": \"activation_function\",\n        \"hidden_dropout_prob\": \"dropout\",\n        \"attention_probs_dropout_prob\": \"attention_dropout\",\n        \"max_position_embeddings\": \"max_position_embeddings\",\n        \"type_vocab_size\": \"type_vocab_size\",\n        \"vocab_size\": \"vocab_size\",\n    }\n\n    # Create a new NomicBertConfig object with the same values as the BertConfig\n    nomic_config = NomicBertConfig()\n\n    # Update the NomicBertConfig with the mapped values from the BertConfig\n    for bert_argument, nomic_argument in bert_to_nomic_argument_mapping.items():\n        setattr(nomic_config, nomic_argument, getattr(bert_config, bert_argument))\n\n    # Set the new NomicBertConfig attributes\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n    nomic_config.vocab_size = bert_config.vocab_size\n    nomic_config.activation_function = bert_config.hidden_act\n    nomic_config.dropout = bert_config.hidden_dropout_prob\n    nomic_config.attention_dropout = bert_config.attention_probs_dropout_prob\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n    nomic_config.type_vocab_size = bert_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig()\n    config.vocab_size = bert_config.vocab_size\n    config.hidden_size = bert_config.hidden_size\n    config.num_hidden_layers = bert_config.num_hidden_layers\n    config.num_attention_heads = bert_config.num_attention_heads\n    config.intermediate_size = bert_config.intermediate_size\n    config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n    config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n    config.max_position_embeddings = bert_config.max_position_embeddings\n    config.type_vocab_size = bert_config.type_vocab_size\n    config.initializer_range = bert_config.initializer_range\n    config.layer_norm_eps = bert_config.layer_norm_eps\n    config.gradient_checkpointing = bert_config.gradient_checkpointing\n    config.max_position_embeddings = bert_config.max_position_embeddings\n    config.embedding_size = bert_config.hidden_size\n    config.embedding_dropout = bert_config.hidden_dropout_prob\n    config.embedding_type = \"bert\"\n    config.embedding_trainable = True\n    config.num_classes = 2\n    config.num_labels = 2\n    config.num_special_tokens = 2\n    config.pad_token_id = bert_config.pad_token_id\n    config.cls_token_id = bert_config.cls_token_id\n    config.sep_token_id = bert_config.sep_token_id\n    config.cls_token = \"[CLS]\"\n    config.sep_token = \"[SEP]\"\n    config.pad_token = \"[PAD]\"\n    config.unk_token = \"[UNK]\"\n    config.mask_token = \"[MASK]\"\n    config.mask_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object and copy the attributes from the original BertConfig\n    nomic_config = NomicBertConfig()\n    nomic_config.__dict__.update(bert_config.__dict__)\n\n    # Set the new attributes specific to the Nomic model\n    nomic_config.num_hidden_layers = 24\n    nomic_config.num_attention_heads = 12\n    nomic_config.hidden_size = 768\n    nomic_config.intermediate_size = 3072\n    nomic_config.hidden_act = \"gelu\"\n    nomic_config.attention_probs_dropout_prob = 0.1\n    nomic_config.hidden_dropout_prob = 0.1\n    nomic_config.max_position_embeddings = 512\n    nomic_config.type_vocab_size = 2\n    nomic_config.vocab_size = 30522\n    nomic_config.initializer_range = 0.02\n    nomic_config.layer_norm_eps = 1e-12\n    nomic_config.position_embedding_type = \"absolute\"\n    nomic_config.use_cache = True\n    nomic_config.classifier_dropout = 0.1\n\n    # Update the configuration with the new attributes\n    nomic_config.update(bert_config)\n\n    return nomic_config\n\n\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Define the mapping from BertConfig attributes to NomicBertConfig attributes\n    attribute_mapping = {\n        \"hidden_size\": \"n_embed\",\n        \"num_hidden_layers\": \"n_layer\",\n        \"num_attention_heads\": \"n_head\",\n        \"intermediate_size\": \"n_inner\",\n        \"hidden_act\": \"activation_function\",\n        \"attention_probs_dropout_prob\": \"dropout\",\n        \"hidden_dropout_prob\": \"dropout\",\n        \"max_position_embeddings\": \"n_ctx\",\n        \"type_vocab_size\": \"n_type\",\n        \"initializer_range\": \"initializer_range\",\n        \"layer_norm_eps\": \"layer_norm_eps\",\n        \"use_cache\": \"use_cache\",\n        \"num_labels\": \"n_labels\",\n        \"output_attentions\": \"output_attentions\",\n        \"output_hidden_states\": \"output_hidden_states\",\n        \"return_dict\": \"return_dict\",\n        \"bias\": \"bias\",\n    }\n\n    # Create a new NomicBertConfig object with the same attributes as the original BertConfig\n    nomic_config = NomicBertConfig(**{attribute_mapping.get(attr, attr): getattr(bert_config, attr) for attr in dir(bert_config) if not attr.startswith(\"__\")})\n\n    # Set the new attributes specific to the Nomic model\n    nomic_config.n_ctx = bert_config.max_position_embeddings\n    nomic_config.n_type = bert_config.type_vocab_size\n    nomic_config.n_labels = bert_config.num_labels\n    nomic_config.use_cache = bert_config.use_cache\n    nomic_config.output_attentions = bert_config.output_attentions\n    nomic_config.output_hidden_states = bert_config.output_hidden_states\n    nomic_config.return_dict = bert_"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Select shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n\n        # Unbind VAO"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            use_gl_program(self.point_program)\n        else:\n            use_gl_program(self.mesh_program)\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STR"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Setup shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self."}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        # Setup the shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.ebo is None:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, 0)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.ebo is None:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n            else:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, 0)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if self.ebo is None:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl."}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            use_gl_program(self.point_program)\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n            gl.glBindVertexArray(0)\n        else:\n            use_gl_program(self.mesh_program)\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            if self.render_type == Mesh.RenderType.LINES:\n                gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n            elif self.render_type == Mesh.RenderType.TRIS:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, None)\n            elif self.render_type == Mesh.RenderType.QUADS:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, None)\n            elif self.render_type == Mesh.RenderType.STRIPS:\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl.GL_UNSIGNED_INT, None)\n            gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Setup shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind vertex array object\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_SHORT, None)\n\n        # Unbind vertex array object\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Select shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Bind shader program\n        use_gl_program(program)\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glPointSize(self.point_radius)\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.ebo is None:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.ebo is None:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n            else:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if self.ebo is None:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n           "}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        # Set up the shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(program, camera)\n\n        # Bind the VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.faces.is_cuda:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, self.faces_data)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.faces.is_cuda:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, self.faces_data)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if self.faces.is_cuda:\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            else:\n                gl"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        # Select program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Issue draw call\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            if self.faces.shape[-1] == 3:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            elif self.faces.shape[-1] == 4:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            elif self.faces.shape[-1] == 5:\n                gl.glDrawElements(gl.GL_POLYGON, len(self.faces) * 5, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n\n        # Unbind VAO\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Select the shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the vertex array object\n        gl.glBindVertexArray(self.vao)\n\n        # Issue the draw call\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, None)\n\n        # Unbind the vertex array object\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        # Setup render type\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n            gl.glPointSize(self.point_radius)\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, None)\n        else:\n            raise NotImplementedError(f'Unsupported render type: {self.render_type}')\n\n        # Unbind VAO\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Select program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            if len(self.faces):\n                if self.render_type == Mesh.RenderType.TRIS:\n                    gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n                elif self.render_type == Mesh.RenderType.LINES:\n                    gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n                elif self.render_type == Mesh.RenderType.QUADS:\n                    gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n                elif self.render_type == Mesh.RenderType.STRIPS:\n                    gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n                else:\n                    raise NotImplementedError(f'Render type: {self.render_type} is not supported')\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n\n        # Unbind VAO\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the program\n        use_gl_program(program)\n\n        # Bind the VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh\n        if self.render_type == Mesh.RenderType.POINTS:\n            # Draw points\n            gl.glPointSize(self.point_radius)\n            gl.glDrawArrays(gl.GL_POINTS, 0, self.n_verts)\n        elif self.render_type == Mesh.RenderType.LINES:\n            # Draw lines\n            gl.glLineWidth(1)\n            gl.glDrawArrays(gl.GL_LINES, 0, self.n_verts)\n        elif self.render_type == Mesh.RenderType.TRIS:\n            # Draw triangles\n            gl.glDrawElements(gl.GL_TRIANGLES, self.n_faces, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            # Draw quads\n            gl.glDrawElements(gl.GL_QUADS, self.n_faces, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            # Draw triangle strips\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, self.n_faces, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n\n        # Unbind the VAO\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Setup shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Setup uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Setup VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Setup render type\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        else:\n            raise ValueError(f'Unsupported render type: {self.render_type}')\n\n        # Clean up\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        # Select the correct program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, self.verts.shape[0])\n        else:\n            if self.faces.shape[0] > 0:\n                if self.faces.shape[1] == 3:\n                    gl.glDrawElements(gl.GL_TRIANGLES, self.faces.shape[0] * 3, gl.GL_UNSIGNED_INT, None)\n                elif self.faces.shape[1] == 4:\n                    gl.glDrawElements(gl.GL_QUADS, self.faces.shape[0] * 4, gl.GL_UNSIGNED_INT, None)\n                elif self.faces.shape[1] == 5:\n                    gl.glDrawElements(gl.GL_POLYGON, self.faces.shape[0] * 5, gl.GL_UNSIGNED_INT, None)\n                else:\n                    gl.glDrawElements(gl.GL_TRIANGLE_STRIP, self.faces.shape[0] * self.faces.shape[1], gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, self.verts.shape[0])\n\n        # Unbind the VAO\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Setup the shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n            gl.glPointSize(self.point_radius)\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            if self.render_type == Mesh.RenderType.LINES:\n                gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n            elif self.render_type == Mesh.RenderType.TRIS:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n            elif self.render_type == Mesh.RenderType.QUADS:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n            elif self.render_type == Mesh.RenderType.STRIPS:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n            else:\n                raise NotImplementedError(f'Unknown render type: {self.render_type}')\n\n        # Unbind the VAO\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Select program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(self.render_type.value, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            elif self.faces.dtype == torch.int16:\n                gl.glDrawElements(self.render_type.value, len(self.faces), gl.GL_UNSIGNED_SHORT, None)\n            else:\n                gl.glDrawElements(self.render_type.value, len(self.faces), gl.GL_UNSIGNED_BYTE, None)\n\n        # Unbind VAO\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            use_gl_program(self.point_program)\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n            gl.glBindVertexArray(0)\n        else:\n            use_gl_program(self.mesh_program)\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            if self.render_type == Mesh.RenderType.LINES:\n                gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n            elif self.render_type == Mesh.RenderType.TRIS:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            elif self.render_type == Mesh.RenderType.QUADS:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            elif self.render_type == Mesh.RenderType.STRIPS:\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Select shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(program, camera)\n\n        # Bind VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, self.n_verts)\n        else:\n            if self.faces.is_cuda:\n                self.faces = self.faces.cpu()\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, self.n_faces, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, self.n_faces, gl.GL_UNSIGNED_SHORT, ctypes.c_void_p(0))\n\n        # Unbind VAO\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            use_gl_program(self.point_program)\n        else:\n            use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            if self.faces.is_cuda:\n                # gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, None)\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, self.faces.data_ptr())\n\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not self.use_quad_cuda:\n            w = w or self.W\n            h = h or self.H\n            if isinstance(ptr, torch.Tensor):\n                ptr = ptr.detach().cpu().numpy()\n            if ptr.shape[-1] == 3:\n                ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n            return\n\n        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        # assert self.use_quad_cuda, \"Need to enable cuda-opengl interop to copy from device to device, check creation of this Quad\"\n        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        if self.compose:\n            \"\"\"\n           "}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)  # add alpha channel\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert the input to a numpy array\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Resize the texture if necessary\n        w = w or self.W\n        h = h or self.H\n        self.resize_textures(h, w)\n\n        # Upload the data to the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not self.use_quad_cuda:\n            self.upload_to_texture_slow(ptr, x, y, w, h)\n            return\n\n        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.dtype == np.uint8:\n            ptr = ptr.astype(np.uint32)\n        elif ptr.dtype == np.float32:\n            ptr = (ptr * 255).astype(np.uint32)\n        elif ptr.dtype == np.float16:\n            ptr = (ptr * 255).astype(np.uint32)\n        else:\n            raise NotImplementedError(f'Unsupported dtype: {ptr.dtype}')\n\n        # assert self.use_quad_cuda, \"Need to enable cuda-opengl interop to copy from device to device, check creation of this Quad\"\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        if self.compose:\n            \"\"\"\n            Blit current framebuffer to this texture (self.tex)\n            Read content of this texture"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not self.use_quad_cuda:\n            w = w or self.W\n            h = h or self.H\n            if ptr.dtype == torch.half:\n                ptr = ptr.float()\n            ptr = ptr.numpy()\n            if ptr.shape[-1] == 3:\n                ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert PyTorch tensor to numpy array\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Set default values for width and height if not provided\n        w = w or self.W\n        h = h or self.H\n\n        # Upload the pixel data to the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.dtype == np.uint8:\n            ptr = ptr.astype(np.uint8)\n        elif ptr.dtype == np.float32:\n            ptr = np.round(ptr * 255).astype(np.uint8)\n        elif ptr.dtype == np.float16:\n            ptr = np.round(ptr * 255).astype(np.uint8)\n        else:\n            raise NotImplementedError\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)  # add alpha channel\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if isinstance(ptr, np.ndarray):\n            ptr = ptr.astype(np.uint8)\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not self.use_quad_cuda:\n            self.upload_to_texture_gl(ptr, x, y, w, h)\n            return\n\n        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        if not isinstance(ptr, torch.Tensor):\n            ptr = torch.as_tensor(ptr)\n        if ptr.dtype == torch.float16:\n            ptr = ptr.type(torch.float)\n        if ptr.dtype == torch.uint8:\n            ptr = ptr.type(torch.float) / 255.\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyDeviceToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * ptr.element_size(),\n                                                           y,\n                                                           ptr.data_ptr(),\n                                                           w * 4 * ptr.element_size(),  # differently sized\n                                                           w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        if not self.use_quad_cuda:\n            w = w or self.W\n            h = h or self.H\n            if isinstance(ptr, torch.Tensor):\n                ptr = ptr.detach().cpu().numpy()\n            if ptr.ndim == 2:\n                ptr = ptr[None, ...]\n            if ptr.shape[-1] == 3:\n                ptr = np.concatenate([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)\n            elif ptr.shape[-1] == 1:\n                ptr = np.concatenate([ptr, ptr, ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.ndim == 2:\n            ptr = ptr[None, ...]\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)\n        elif ptr.shape[-1] == 1:\n            ptr = np.concatenate([ptr, ptr, ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], axis=-1"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert the input to a numpy array\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Handle the case where the input is a PyTorch tensor\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Handle the case where the input is a numpy array\n        if isinstance(ptr, np.ndarray):\n            # Resize the texture if needed\n            self.resize_textures(ptr.shape[0], ptr.shape[1])\n\n            # Upload the data to the texture\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w or self.W, h or self.H, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n            gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not self.use_quad_cuda:\n            # This is a slow operation, we should avoid using this if possible\n            # The reason is that we're uploading the entire texture every time\n            # This is not a big deal if the texture is small\n            # But if the texture is large, this will be slow\n            # The reason is that we're uploading the entire texture every time\n            # This is not a big deal if the texture is small\n            # But if the texture is large, this will be slow\n            # The reason is that we're uploading the entire texture every time\n            # This is not a big deal if the texture is small\n            # But if the texture is large, this will be slow\n            # The reason is that we're uploading the entire texture every time\n            # This is not a big deal if the texture is small\n            # But if the texture is large, this will be slow\n            # The reason is that we're uploading the entire texture every time\n            # This is not a big deal if the texture is small\n            # But if the texture is large, this will be slow\n            # The reason is that we're uploading the entire texture every time\n            # This is not a big deal if the texture is small\n            # But if the texture is large, this will be slow\n            # The reason is that we're uploading the entire texture every time\n            # This is not a big deal if the texture is small\n            # But if the texture is large, this will be slow\n            # The reason is that we're uploading the entire texture every time\n            # This is not a big deal if the texture is small\n            # But if the texture is large, this will be slow\n            # The reason is that we're uploading the entire texture every time\n            # This is not a big deal if the texture is small\n            # But if the texture is large, this will be slow\n            # The reason is that we're uploading the entire texture every time\n            # This is not a big deal if the texture is small\n            # But if the texture is large, this will be slow\n            # The reason is that we're uploading the entire texture every time\n            #"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert input to numpy array if it's a tensor\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        # Set default values for width and height if not provided\n        w = w or self.W\n        h = h or self.H\n\n        # Check if the input data has the correct shape and dtype\n        assert ptr.shape[-1] == 3 or ptr.shape[-1] == 4, f\"The input data must be a numpy array with 3 or 4 channels, but got {ptr.shape[-1]} channels.\"\n        assert ptr.dtype == np.uint8, f\"The input data must be a numpy array with dtype uint8, but got {ptr.dtype}.\"\n\n        # Upload the data to the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert the input data to a numpy array if it is a PyTorch tensor\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Resize the texture if necessary\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)  # add alpha channel\n\n        # Bind the texture and update the pixels\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate input shapes and values\n    if not isinstance(R, torch.Tensor):\n        raise ValueError(\"R must be a torch.Tensor\")\n    if not isinstance(tvec, torch.Tensor):\n        raise ValueError(\"tvec must be a torch.Tensor\")\n    if not isinstance(camera_matrix, torch.Tensor):\n        raise ValueError(\"camera_matrix must be a torch.Tensor\")\n    if not isinstance(image_size, torch.Tensor):\n        raise ValueError(\"image_size must be a torch.Tensor\")\n    if not isinstance(znear, float):\n        raise ValueError(\"znear must be a float\")\n\n    # Ensure all inputs are batched\n    if len(R.shape) == 2:\n        R = R[None, ...]\n    if len(tvec.shape) == 2:\n        tvec = tvec[None, ...]\n    if len(camera_matrix.shape) == 2:\n        camera_matrix = camera_matrix[None, ...]\n    if len(image_size.shape) == 2:\n        image_size = image_size[None, ...]\n\n    # Validate batch sizes\n    if R.shape[0] != tvec.shape[0]:\n        raise ValueError(\"R and tvec must have the same batch size\")\n    if R.shape[0] != camera_matrix.shape[0]:\n        raise ValueError(\"R and camera_matrix must have the same batch size\")\n    if R.shape[0] != image_size.shape[0]:\n        raise ValueError(\"R and image_size must have the same batch size\")\n\n    # Validate input shapes\n    if R.shape[-2:] != (3, 3):\n        raise ValueError(\"R must have shape (..., 3, 3)\")\n    if tvec.shape[-1] != 3:\n        raise ValueError(\"tvec must have shape (..., 3)\")\n    if camera_matrix.shape[-2:] != (3, 3):"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate inputs\n    assert R.ndim == 3 and R.shape[-2:] == (3, 3), f\"R must be a batch of 3x3 rotation matrices, got {R.shape}\"\n    assert tvec.ndim == 2 and tvec.shape[-1] == 3, f\"tvec must be a batch of 3D translation vectors, got {tvec.shape}\"\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-2:] == (3, 3), f\"camera_matrix must be a batch of 3x3 camera matrices, got {camera_matrix.shape}\"\n    assert image_size.ndim == 2 and image_size.shape[-1] == 2, f\"image_size must be a batch of 2D image sizes, got {image_size.shape}\"\n\n    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Compute focal lengths\n    focal_lengths = camera_matrix[..., :2, :2].mean(dim=-1)\n    fx, fy = focal_lengths[..., 0], focal_lengths[..., 1]\n\n    # Compute principal points\n    principal_points = camera_matrix[..., :2, 2]\n    cx, cy = principal_points[..., 0], principal_points[..., 1]\n\n    # Compute sensor width\n    sensor_width = image_size[..., 0] / fx\n\n    # Compute camera position\n    camera_position = -R.transpose(-1, -2) @ tvec\n\n    # Compute rotation in 6D representation\n    rotation = matrix_to_"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if len(R.shape) == 3 else R\n    tvec = tvec.unsqueeze(0) if len(tvec.shape) == 2 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if len(camera_matrix.shape) == 3 else camera_matrix\n    image_size = image_size.unsqueeze(0) if len(image_size.shape) == 2 else image_size\n\n    # Validate shapes and values\n    assert R.shape[-2:] == (3, 3), f\"Expected shape (B, 3, 3), got {R.shape}\"\n    assert tvec.shape[-2:] == (3, 1), f\"Expected shape (B, 3, 1), got {tvec.shape}\"\n    assert camera_matrix.shape[-2:] == (3, 3), f\"Expected shape (B, 3, 3), got {camera_matrix.shape}\"\n    assert image_size.shape[-2:] == (2,), f\"Expected shape (B, 2), got {image_size.shape}\"\n\n    # Compute focal lengths and principal points\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n\n    # Compute sensor width\n    sensor_width = image_size[..., 1] / fx\n\n    # Compute focal length\n    focal_length = (fx + fy) / 2\n\n    # Compute principal point offsets\n    offset_x = (cx - image_size[..., 0] / 2) / image_size[..., 0]\n    offset_y = (cy - image_size[..., 1] / 2) / image_size[..., 1]\n\n    # Adjust focal length and principal point offsets\n   "}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched and have the same batch size\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n    batch_size = R.shape[0]\n    assert tvec.shape[0] == batch_size\n    assert camera_matrix.shape[0] == batch_size\n    assert image_size.shape[0] == batch_size\n\n    # Validate input shapes and values\n    assert R.shape[-2:] == (3, 3), f\"Expected rotation matrix shape (N, 3, 3), got {R.shape}\"\n    assert tvec.shape[-1] == 3, f\"Expected translation vector shape (N, 3), got {tvec.shape}\"\n    assert camera_matrix.shape[-2:] == (3, 3), f\"Expected camera matrix shape (N, 3, 3), got {camera_matrix.shape}\"\n    assert image_size.shape[-1] == 2, f\"Expected image size shape (N, 2), got {image_size.shape}\"\n    assert (camera_matrix[..., 0, 0] > 0).all(), f\"Invalid camera matrix: fx must be positive, got {camera_matrix[..., 0, 0]}\"\n    assert (camera_matrix[..., 1, 1] > 0).all(), f\"Invalid camera matrix: fy must be positive, got {camera_matrix[..., 1, 1]}\"\n    assert (image_size[..., 0] > 0).all(), f\"Invalid image size: width must be positive, got {image_size[..., 0]}\"\n    assert (image_size[..., 1] > 0).all(), f\"Invalid image size:"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if len(R.shape) == 2 else R\n    tvec = tvec.unsqueeze(0) if len(tvec.shape) == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if len(camera_matrix.shape) == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if len(image_size.shape) == 1 else image_size\n\n    # Validate input shapes and values\n    assert len(R.shape) == 3 and R.shape[1:] == (3, 3), \"R should be a batch of rotation matrices of shape (B, 3, 3)\"\n    assert len(tvec.shape) == 2 and tvec.shape[1] == 3, \"tvec should be a batch of translation vectors of shape (B, 3)\"\n    assert len(camera_matrix.shape) == 3 and camera_matrix.shape[1:] == (3, 3), \"camera_matrix should be a batch of camera intrinsic matrices of shape (B, 3, 3)\"\n    assert len(image_size.shape) == 2 and image_size.shape[1] == 2, \"image_size should be a batch of image sizes of shape (B, 2)\"\n\n    # Compute camera position\n    tvec = tvec.unsqueeze(-1)\n    camera_position = -R.transpose(-1, -2) @ tvec  # B, 3, 1\n    camera_position = camera_position.squeeze(-1)  # B, 3\n\n    # Compute camera rotation\n    camera_rotation = matrix_to_rotation_6d(R)  # B, 6\n\n    # Compute focal lengths\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    if not torch.allclose(fx, fy, atol=1e-2):\n        warn_once_about_"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate input shapes\n    assert R.shape[:-2] == tvec.shape[:-1] == camera_matrix.shape[:-2] == image_size.shape[:-1]\n\n    # Validate input values\n    assert torch.allclose(torch.linalg.det(R), torch.ones_like(R[..., 0, 0]))\n    assert torch.all(torch.isfinite(R))\n    assert torch.all(torch.isfinite(tvec))\n    assert torch.all(torch.isfinite(camera_matrix))\n    assert torch.all(torch.isfinite(image_size))\n\n    # Compute focal lengths and principal points\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    cx, cy = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n    image_width, image_height = image_size[..., 0], image_size[..., 1]\n\n    # Compute focal length and sensor width\n    focal_length = torch.sqrt(fx * fy)\n    sensor_width = image_width / (cx + 1)\n\n    # Compute principal point offsets\n    cx_offset = cx / image_width\n    cy_offset = cy / image_height\n\n    # Compute camera position\n    camera_position = -R.transpose(-1, -2) @ tvec\n\n    # Compute camera rotation"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if len(R.shape) == 2 else R\n    tvec = tvec.unsqueeze(0) if len(tvec.shape) == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if len(camera_matrix.shape) == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if len(image_size.shape) == 1 else image_size\n\n    # Validate shapes and values\n    assert R.shape == (1, 3, 3)\n    assert tvec.shape == (1, 3)\n    assert camera_matrix.shape == (1, 3, 3)\n    assert image_size.shape == (1, 2)\n\n    # Compute focal lengths\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    fx, fy = fx.unsqueeze(-1), fy.unsqueeze(-1)\n    fx, fy = fx.repeat(1, 1, 2), fy.repeat(1, 1, 2)\n\n    # Compute principal point offsets\n    cx, cy = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n    cx, cy = cx.unsqueeze(-1), cy.unsqueeze(-1)\n    cx, cy = cx.repeat(1, 1, 2), cy.repeat(1, 1, 2)\n\n    # Compute sensor width\n    w = image_size[..., 1] / fx\n\n    # Adjust focal lengths\n    fx = fx * znear / w\n    fy = fy * znear / w\n\n    # Normalize focal lengths\n    fx = fx / fx.mean(dim=-1, keepdim=True)\n    fy = fy / fy.mean("}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    R = R.unsqueeze(0) if R.ndim < 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim < 2 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim < 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim < 2 else image_size\n\n    # Validate inputs\n    if not torch.all(torch.isfinite(R)):\n        raise ValueError(\"Rotation matrix contains NaN or infinite values.\")\n    if not torch.all(torch.isfinite(tvec)):\n        raise ValueError(\"Translation vector contains NaN or infinite values.\")\n    if not torch.all(torch.isfinite(camera_matrix)):\n        raise ValueError(\"Camera matrix contains NaN or infinite values.\")\n    if not torch.all(torch.isfinite(image_size)):\n        raise ValueError(\"Image size contains NaN or infinite values.\")\n    if not (R.shape[-2:] == (3, 3)):\n        raise ValueError(\"Rotation matrix must be of shape (..., 3, 3).\")\n    if not (tvec.shape[-1] == 3):\n        raise ValueError(\"Translation vector must be of shape (..., 3).\")\n    if not (camera_matrix.shape[-2:] == (3, 3)):\n        raise ValueError(\"Camera matrix must be of shape (..., 3, 3).\")\n    if not (image_size.shape[-1] == 2):\n        raise ValueError(\"Image size must be of shape (..., 2).\")\n\n    # Convert rotation matrix to 6D representation\n    R6d = matrix_to_rotation_6d(R)\n\n    # Derive focal lengths and principal points\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate inputs\n    R = torch.as_tensor(R)\n    tvec = torch.as_tensor(tvec)\n    camera_matrix = torch.as_tensor(camera_matrix)\n    image_size = torch.as_tensor(image_size)\n\n    # Ensure inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 3 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 2 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim == 3 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 2 else image_size\n\n    # Validate shapes\n    assert R.shape[:-2] == tvec.shape[:-1] == camera_matrix.shape[:-3] == image_size.shape[:-1], \"Inputs must have same batch dimensions\"\n\n    # Validate values\n    assert torch.allclose(torch.linalg.det(R), torch.ones_like(R[..., 0, 0])), \"Rotation matrices must be valid\"\n    assert torch.all(tvec[..., 2] > 0), \"Translation vectors must be positive in z-direction\"\n    assert torch.all(camera_matrix[..., 0, 0] > 0), \"Camera matrix must be positive in x-direction\"\n    assert torch.all(camera_matrix[..., 1, 1] > 0), \"Camera matrix must be positive in y-direction\"\n    assert torch.all(image_size > 0), \"Image size must be positive\"\n\n    # Calculate focal lengths\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    fx_avg = (fx + fy) / 2\n    fy_avg = fx_avg\n\n    # Calculate principal points"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if len(R.shape) == 3 else R\n    tvec = tvec.unsqueeze(0) if len(tvec.shape) == 2 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if len(camera_matrix.shape) == 3 else camera_matrix\n    image_size = image_size.unsqueeze(0) if len(image_size.shape) == 2 else image_size\n\n    # Validate and reshape inputs\n    assert R.shape[-2:] == (3, 3), \"R must be a batch of rotation matrices of shape (*, 3, 3)\"\n    assert tvec.shape[-1] == 3, \"tvec must be a batch of translation vectors of shape (*, 3)\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"camera_matrix must be a batch of camera intrinsic matrices of shape (*, 3, 3)\"\n    assert image_size.shape[-1] == 2, \"image_size must be a batch of image sizes of shape (*, 2)\"\n\n    # Compute focal lengths and sensor width from camera intrinsic matrix\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    fx_fy_ratio = fx / fy\n    sensor_width = 2 * (fx + fy) / (image_size[..., 0] + image_size[..., 1])\n\n    # Adjust focal lengths based on near clipping plane distance\n    fx = fx / znear\n    fy = fy / znear\n\n    # Compute principal point offsets and normalize focal lengths\n    cx, cy = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n    cx = cx / image_size[..., 0]\n    cy = cy / image_size[..., 1]\n    fx = fx / sensor_width\n    f"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check input shapes and values\n    if R.ndim != 3 or R.shape[-1] != R.shape[-2]:\n        raise ValueError(\"Input 'R' must be a batch of square rotation matrices.\")\n    if tvec.ndim != 2 or tvec.shape[-1] != 3:\n        raise ValueError(\"Input 'tvec' must be a batch of translation vectors.\")\n    if camera_matrix.ndim != 3 or camera_matrix.shape[-1] != 3 or camera_matrix.shape[-2] != 3:\n        raise ValueError(\"Input 'camera_matrix' must be a batch of camera intrinsic matrices.\")\n    if image_size.ndim != 2 or image_size.shape[-1] != 2:\n        raise ValueError(\"Input 'image_size' must be a batch of image sizes.\")\n    if not torch.all(torch.isfinite(R)) or not torch.all(torch.isfinite(tvec)):\n        raise ValueError(\"Inputs 'R' and 'tvec' must be finite.\")\n    if not torch.all(torch.isfinite(camera_matrix)):\n        raise ValueError(\"Input 'camera_matrix' must be finite.\")\n    if not torch.all(torch.isfinite(image_size)):\n        raise ValueError(\"Input 'image_size' must be finite.\")\n    if not torch.all(image_size > 0):\n        raise ValueError(\"Input 'image_size' must be positive.\")\n\n    # Check if focal lengths are the same for x and y\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    if not torch.allclose(fx, fy, atol=1e-4):\n        warn_once_about_pulsar_fxfy()\n\n    # Compute focal length and sensor width\n    f = torch.mean(camera_matrix[..., 0, 0], dim=-1)\n    sensor_width = torch.mean(camera_matrix["}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim < 3 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim < 3 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim < 3 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim < 3 else image_size\n\n    # Validate shapes and values\n    assert R.shape[:-2] == tvec.shape[:-2] == camera_matrix.shape[:-2] == image_size.shape[:-2]\n    assert R.shape[-2:] == (3, 3)\n    assert tvec.shape[-1] == 3\n    assert camera_matrix.shape[-2:] == (3, 3)\n    assert image_size.shape[-1] == 2\n    assert znear > 0\n\n    # Compute focal lengths\n    focal_lengths = camera_matrix[..., 0, 0]\n    fy = focal_lengths\n    fx = camera_matrix[..., 1, 1]\n\n    # Check if focal lengths are the same\n    if not torch.allclose(fx, focal_lengths, atol=1e-4):\n        warn_once_about_pulsar_fxfy()\n        focal_lengths = (fx + fy) / 2\n\n    # Compute principal points\n    principal_points = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n\n    # Adjust principal point offsets and normalize focal length\n    principal_points = (\n        principal_points[0] - 0.5 * image_size[..., 0] + 0.5,\n        principal_points[1] - 0.5 * image_size[..., 1] + 0.5,\n    )\n    focal_lengths = focal_lengths / image_size[..., 0]\n\n    # Compute sensor width"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 3 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 2 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if camera_matrix.ndim == 3 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate input shapes\n    assert R.ndim == 3 and tvec.ndim == 2 and camera_matrix.ndim == 3\n    assert R.shape[:2] == tvec.shape[:2] == camera_matrix.shape[:2]\n    assert tvec.shape[-1] == 3\n    assert camera_matrix.shape[-1] == 3 and camera_matrix.shape[-2] == 3\n    assert image_size.shape[-1] == 2\n\n    # Validate input values\n    assert torch.allclose(torch.linalg.det(R), torch.ones_like(R[..., 0, 0]))\n\n    # Compute focal lengths\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    fx_avg = (fx + fy) / 2\n    fy_avg = fx_avg\n    if not torch.allclose(fx, fy, atol=1e-2):\n        warn_once_about_pulsar_fxfy()\n\n    # Compute principal points\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    cx_norm = cx / image_size[..., 1]\n    cy_norm = cy / image_size[..., 0]\n\n    # Compute sensor width\n    sensor_width = (image_size[..., 1] / camera_matrix[..., 0, "}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure that all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate input shapes and values\n    assert R.ndim == 3 and R.shape[-1] == 3 and R.shape[-2] == 3, \"Invalid rotation matrix shape\"\n    assert tvec.ndim == 2 and tvec.shape[-1] == 3, \"Invalid translation vector shape\"\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-1] == 3 and camera_matrix.shape[-2] == 3, \"Invalid camera matrix shape\"\n    assert image_size.ndim == 2 and image_size.shape[-1] == 2, \"Invalid image size shape\"\n\n    # Compute focal lengths from camera matrix\n    focal_lengths = camera_matrix[..., :2, :2].mean(dim=-1)\n    focal_lengths = focal_lengths / focal_lengths[..., 0:1]\n\n    # Compute principal points from camera matrix\n    principal_points = camera_matrix[..., :2, 2:3]\n\n    # Compute sensor width from image size\n    sensor_width = image_size[..., 0:1] / focal_lengths[..., 0:1]\n\n    # Compute camera position from translation vector\n    camera_position = -R.transpose(-2, -1) @ tvec[..., None]\n\n    # Compute camera rotation from rotation matrix\n    camera_rotation = matrix_to_rotation_6d(R)\n\n    # Compute focal length scale factor\n    focal_length_scale_factor = focal_lengths[..., 0:1] / focal_lengths[..., 1"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if len(R.shape) == 3 else R\n    tvec = tvec.unsqueeze(0) if len(tvec.shape) == 2 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if len(camera_matrix.shape) == 3 else camera_matrix\n    image_size = image_size.unsqueeze(0) if len(image_size.shape) == 2 else image_size\n\n    # Validate and broadcast shapes\n    batch_size, _, _ = R.shape\n    _, _, H, W = image_size.shape\n    _, _, _, fx, fy = camera_matrix.shape\n    assert batch_size == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n    assert fx == fy\n\n    # Compute focal length\n    f = camera_matrix[..., 0, 0]\n    fx, fy = f, f\n\n    # Compute principal point offsets\n    cx = (W - 1) / 2\n    cy = (H - 1) / 2\n    px, py = cx, cy\n\n    # Compute sensor width\n    sensor_width = 2 * fx / W\n\n    # Compute camera position\n    camera_position = -R.transpose(-1, -2) @ tvec\n\n    # Compute camera rotation\n    camera_rotation = matrix_to_rotation_6d(R)\n\n    # Compute focal length\n    focal_length = f / znear\n\n    # Create output tensor\n    camera_params = torch.cat([camera_position, camera_rotation, focal_length[..., None], sensor_width[..., None], px[..., None], py[..., None]], dim=-1)\n\n    return camera_params\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched and valid\n    R = R.unsqueeze(0) if len(R.shape) == 3 else R\n    tvec = tvec.unsqueeze(0) if len(tvec.shape) == 2 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if len(camera_matrix.shape) == 3 else camera_matrix\n    image_size = image_size.unsqueeze(0) if len(image_size.shape) == 2 else image_size\n\n    # Compute the focal length and principal point offsets\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    cx, cy = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n    image_width, image_height = image_size[..., 0], image_size[..., 1]\n    focal_length_x = fx * image_width / (cx + 1e-8)\n    focal_length_y = fy * image_height / (cy + 1e-8)\n    focal_length = (focal_length_x + focal_length_y) / 2\n    focal_length_x_diff = focal_length_x / focal_length - 1\n    focal_length_y_diff = focal_length_y / focal_length - 1\n    if abs(focal_length_x_diff) > 0.01 or abs(focal_length_y_diff) > 0.01:\n        warn_once_about_pulsar_fxfy()\n    principal_point_offset_x = (cx + 0.5) / image_width\n    principal_point_offset_y = (cy + 0.5) / image_height\n\n    # Compute the camera position\n    camera_position = -R.transpose(-1, -2) @ tvec\n\n    # Compute the rotation in a different representation\n    rotation = matrix_to_rotation_6d(R"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 3 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 2 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim == 3 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 2 else image_size\n\n    # Validate shapes and values\n    assert R.shape == tvec.shape == camera_matrix.shape[:-1] == image_size.shape, \\\n        f\"Input shapes do not match: R={R.shape}, tvec={tvec.shape}, camera_matrix={camera_matrix.shape}, image_size={image_size.shape}\"\n    assert (R.ndim == 3 and tvec.ndim == 2 and camera_matrix.ndim == 3 and image_size.ndim == 2), \\\n        f\"Inputs must be batched, but have shapes: R={R.shape}, tvec={tvec.shape}, camera_matrix={camera_matrix.shape}, image_size={image_size.shape}\"\n\n    # Convert rotation matrix to 6D representation\n    R6d = matrix_to_rotation_6d(R)\n\n    # Extract focal lengths from camera matrix\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    fx_fy_diff = torch.abs(fx - fy) / fy\n    if fx_fy_diff > 0.01:\n        warn_once_about_pulsar_fxfy()\n    focal_length = (fx + fy) / 2\n\n    # Compute principal point offsets and normalize focal length\n    principal_point_offset = torch.tensor([0.5, 0.5], device=R.device)\n    principal_point_offset = principal_point_offset.expand(R.shape["}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if len(R.shape) == 3 else R\n    tvec = tvec.unsqueeze(0) if len(tvec.shape) == 2 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if len(camera_matrix.shape) == 3 else camera_matrix\n    image_size = image_size.unsqueeze(0) if len(image_size.shape) == 1 else image_size\n\n    # Validate and broadcast input shapes\n    if R.shape != (1,) + tvec.shape:\n        raise ValueError(\n            \"R and tvec must have the same batch shape, \"\n            f\"but R.shape={R.shape} and tvec.shape={tvec.shape}.\"\n        )\n    if R.shape != (1,) + camera_matrix.shape[:-1]:\n        raise ValueError(\n            \"R and camera_matrix must have the same batch shape, \"\n            f\"but R.shape={R.shape} and camera_matrix.shape={camera_matrix.shape}.\"\n        )\n    if R.shape != (1,) + image_size.shape:\n        raise ValueError(\n            \"R and image_size must have the same batch shape, \"\n            f\"but R.shape={R.shape} and image_size.shape={image_size.shape}.\"\n        )\n\n    # Compute focal lengths and sensor width\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    if not torch.allclose(fx, fy, atol=1e-3):\n        warn_once_about_pulsar_fxfy()\n    f = torch.sqrt(fx * fy)\n    sensor_width = 2 * torch.atan2(image_size[..., 1] / f, 2)\n\n    # Compute principal point offsets\n    cx, cy = camera_matrix[..."}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if len(R.shape) == 2 else R\n    tvec = tvec.unsqueeze(0) if len(tvec.shape) == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if len(camera_matrix.shape) == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if len(image_size.shape) == 1 else image_size\n\n    # Validate shapes and values\n    assert R.shape == tvec.shape == camera_matrix.shape[:-2] + (3, 3)\n    assert image_size.shape == tvec.shape[:-1] + (2,)\n    assert znear > 0\n\n    # Derive focal lengths and sensor width\n    focal_lengths = camera_matrix[..., 0, 0]\n    sensor_width = camera_matrix[..., 0, 2] * 2 / image_size[..., 0]\n\n    # Adjust focal lengths for OpenCV convention (fx, fy)\n    if not torch.allclose(focal_lengths[..., 0], focal_lengths[..., 1], rtol=1e-2):\n        warn_once_about_pulsar_fxfy()\n    focal_lengths = focal_lengths.mean(dim=-1, keepdim=True)\n\n    # Adjust principal point offsets and normalize focal length\n    principal_point_offsets = torch.stack(\n        [\n            (camera_matrix[..., 0, 2] - image_size[..., 0] / 2) / focal_lengths[..., 0],\n            (camera_matrix[..., 1, 2] - image_size[..., 1] / 2) / focal_lengths[..., 0],\n        ],\n        dim=-1,\n    )\n    focal_lengths = focal_lengths / torch.maximum(\n        torch"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched and have the same batch size\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate the shapes of inputs\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0]\n\n    # Calculate focal lengths from camera matrix\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    # Check if focal lengths are different by more than 1%\n    if not torch.allclose(fx, fy, rtol=1e-2):\n        warn_once_about_pulsar_fxfy()\n\n    # Calculate focal length\n    focal_length = torch.mean(camera_matrix[..., 0, 0:1], dim=-1)\n\n    # Calculate sensor width from image size\n    sensor_width = image_size[..., 0] / focal_length\n\n    # Calculate principal point offsets from image size and focal length\n    cx_offset = (image_size[..., 0] - 1) / 2\n    cy_offset = (image_size[..., 1] - 1) / 2\n\n    # Calculate principal point\n    principal_point = torch.stack([cx_offset, cy_offset], dim=-1)\n\n    # Calculate camera position from translation vector\n    camera_position = -torch.linalg.inv(R)[..., :3, :3] @ tvec[..., :3, None]\n\n    # Calculate rotation from rotation matrix\n    rotation = matrix_to_rotation_"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box for rendering\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program and bind the texture\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral using vertex data\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n        if not self.use_quad_draw:\n            self.upload_to_texture(None, x, y, w, h)\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n            return\n\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit()\n            return\n\n        w = w or self.W\n        h = h or self.H\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glBindVertexArray(0)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n        use_gl_program(self.quad_program)\n\n        # Bind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box for rendering\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n        self.use_gl_program(self.quad_program)\n\n        # Bind the texture\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit()\n            return\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w or self.W, h or self.H)\n        gl.glScissor(x, y, w or self.W, h or self.H)\n\n        # Activate the shader program and bind the texture\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n\n        if self.use_quad_draw:\n            # Set up the viewport and scissor box\n            gl.glViewport(x, y, w, h)\n            gl.glScissor(x, y, w, h)\n\n            # Activate the shader program\n            gl.glUseProgram(self.quad_program)\n\n            # Bind the texture\n            gl.glActiveTexture(gl.GL_TEXTURE0)\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n            # Draw the quadrilateral\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n            # Restore the viewport and scissor box\n            gl.glViewport(0, 0, self.max_W, self.max_H)\n            gl.glScissor(0, 0, self.max_W, self.max_H)\n        else:\n            # Bind the texture\n            gl.glActiveTexture(gl.GL_TEXTURE0)\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n            # Draw the quadrilateral\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n        if self.use_quad_draw:\n            old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n            old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n            gl.glViewport(x, y, w, h)\n            gl.glScissor(x, y, w, h)\n\n            gl.glUseProgram(self.quad_program)\n            gl.glBindVertexArray(self.vao)\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n            gl.glBindVertexArray(0)\n\n            gl.glViewport(*old_viewport)\n            gl.glScissor(*old_scissor)\n        else:\n            gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # draw to screen\n            gl.glViewport(x, y, w, h)\n            gl.glScissor(x, y, w, h)\n            gl.glUseProgram(self.quad_program)\n            gl.glBindVertexArray(self.vao)\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n            gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit()\n            return\n\n        w = w or self.W\n        h = h or self.H\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glBindVertexArray(0)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit()\n            return\n\n        # Save the current viewport and scissor box\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        # Set the viewport and scissor box for the quadrilateral\n        gl.glViewport(x, y, w or self.W, h or self.H)\n        gl.glScissor(x, y, w or self.W, h or self.H)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the original viewport and scissor box\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit()\n            return\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w or self.W, h or self.H)\n        gl.glScissor(x, y, w or self.W, h or self.H)\n\n        # Activate the shader program and bind the texture\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral using vertex data\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box to their original sizes\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit()  # no uploading needed\n            return\n\n        # Set up the viewport and scissor box for rendering\n        gl.glViewport(x, y, w or self.W, h or self.H)\n        gl.glScissor(x, y, w or self.W, h or self.H)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program and bind the texture\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.max_W, self.max_H)\n        gl.glScissor(0, 0, self.max_W, self.max_H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glBindVertexArray(0)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n\n        # Setup the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program and bind the texture\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n\n        if not self.use_quad_draw:\n            # Fall back to blit\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n            return\n\n        # Set up viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate shader program\n        gl.glUseProgram(self.quad_program)\n        use_gl_program(self.quad_program)\n\n        # Bind texture\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n\n        if not self.use_quad_draw:\n            # blit the texture onto the screen\n            gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n            gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n            gl.glBlitFramebuffer(x, y, w, h,\n                                 x, y, w, h,\n                                 gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n            gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n            return\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Draw the quadrilateral\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit()\n            return\n\n        w = w or self.W\n        h = h or self.H\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glBindVertexArray(0)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # Adjust rotation and translation matrices for PyTorch3D\n    R = R.mT  # B, 3, 3\n    T = -T  # B, 3, 1\n    # Recalculate K for NDC\n    K = get_ndc_perspective_matrix(K, H, W, batch.meta.n[..., 0], batch.meta.f[..., 0])\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n\n    # Adjust R and T\n    R[..., 0] *= -1  # Flip x\n    R[..., 1] *= -1  # Flip y\n    T[..., 0] *= -1  # Flip x\n    T[..., 1] *= -1  # Flip y\n\n    # Recalculate K for NDC\n    K[..., 0, 2] = (K[..., 0, 2] + 0.5) / W\n    K[..., 1, 2] = (K[..., 1, 2] + 0.5) / H\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n\n    # Adjust the rotation matrix to match PyTorch3D's coordinate system\n    R = torch.cat([R[..., 0:1, :], R[..., 1:2, :], -R[..., 2:3, :]], dim=-2)  # B, 3, 3\n\n    # Adjust the translation vector to match PyTorch3D's coordinate system\n    T = torch.cat([T[..., 0:1, :], -T[..., 1:2, :], -T[..., 2:3, :]], dim=-2)  # B, 3, 1\n\n    # Calculate the camera intrinsic matrix for NDC\n    K = get_ndc_perspective_matrix(K, H, W)\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # Adjust R and T for PyTorch3D\n    R = R.mT  # B, 3, 3\n    T = -T  # B, 3, 1\n    # Recalculate K for NDC\n    K = get_ndc_perspective_matrix(K, H, W, batch.meta.n[..., 0], batch.meta.f[..., 0])  # B, 4, 4\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # Adjust the rotation matrix to match PyTorch3D's coordinate system\n    R = R.mT  # B, 3, 3\n    # Adjust the translation vector to match PyTorch3D's coordinate system\n    T = -T  # B, 3, 1\n    # Recalculate the intrinsic matrix for NDC\n    K = get_ndc_perspective_matrix(K, H, W, 0, 1)\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # Adjust camera parameters for PyTorch3D\n    R = R.mT  # B, 3, 3\n    T = -T  # B, 3, 1\n    # Recalculate K for NDC\n    K = get_ndc_perspective_matrix(K, H, W, torch.tensor([0.], device=K.device), torch.tensor([1.], device=K.device)).to(K.dtype)  # B, 4, 4\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n\n    # Adjust R and T to match PyTorch3D's coordinate system and conventions\n    R = R.mT  # R is applied on the right\n    R[:, 0] *= -1  # flip x\n    R[:, 1] *= -1  # flip y\n    R[:, 2] *= -1  # flip z\n    T *= -1  # flip z\n\n    # Recalculate K for NDC\n    K = get_ndc_perspective_matrix(K, H, W, K[..., 0], K[..., 1])\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # draw buffer defaults to 0\n        gl.glBlitFramebuffer(0, 0, W, H,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,\n                             0, 0, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # draw to default framebuffer\n        gl.glBlitFramebuffer(0, 0, self.W, self.H, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        old = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,  # read from\n                             x, y, w, h,  # write to\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # draw to default framebuffer\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,  # read from\n                             x, y, w, h,  # draw to\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        old = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # draw to default framebuffer\n        gl.glBlitFramebuffer(0, 0, W, H,  # read from\n                             x, y, w, h,  # write to\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # draw buffer\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # draw to default framebuffer\n        gl.glBlitFramebuffer(0, 0, W, H,  # src\n                             x, y, w, h,  # dst\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n\n        # Save the current read framebuffer\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n\n        # Bind the Quad instance's FBO as the read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Perform the pixel copy operation\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the previously bound read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glUseProgram(self.quad_program)  # use a different program\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        gl.glBindVertexArray(0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # default framebuffer\n        gl.glBlitFramebuffer(0, 0, W, H, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(0, 0, W, H, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # cumsum of y1\n    csum = torch.cat([torch.zeros(1, device=y1.device), torch.cumsum(y1, dim=0)], dim=0)\n\n    # cumsum of y1 * dt\n    csum_dt = torch.cat([torch.zeros(1, device=y1.device), torch.cumsum(y1 * (t1[1:] - t1[:-1]), dim=0)], dim=0)\n\n    # cumsum of y1 * dt^2\n    csum_dt2 = torch.cat([torch.zeros(1, device=y1.device), torch.cumsum(y1 * (t1[1:] - t1[:-1]) ** 2, dim=0)], dim=0)\n\n    # cumsum of y1 * dt^3\n    csum_dt3 = torch.cat([torch.zeros(1, device=y1.device), torch.cumsum(y1 * (t1[1:] - t1[:-1]) ** 3, dim=0)], dim=0)\n\n    # cumsum of y1 * dt^4\n    csum_dt4 = torch.cat([torch.zeros(1, device=y1.device), torch.cumsum(y1 * (t1[1:] - t1[:-1]) ** 4, dim=0)], dim=0)\n\n    # inner measure\n    inner = (csum[1:] - csum[:-1]) / (t1[1:] - t1[:-1])\n\n    # outer measure\n    outer = torch.zeros_like(t0)\n\n    for i in range(len(t0)):\n        idx = searchsorted(t1, t0[i])\n        outer[i] = (\n            csum[idx]\n            - csum[idx - 1]\n            - inner[idx - 1] * (t0[i] - t1[idx - 1])\n            - csum"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sums of y1\n    cumsum = torch.cumsum(y1, dim=-1)\n\n    # Find the indices where t0 lies between t1\n    idx_inner = searchsorted(t1, t0)\n    idx_outer = idx_inner + 1\n\n    # Compute the inner and outer measures\n    inner = torch.gather(cumsum, -1, idx_inner)\n    outer = torch.gather(cumsum, -1, idx_outer)\n\n    # Return the inner and outer measures\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the elements in t1 that are less than or equal to t0\n    indices = searchsorted(t1, t0)\n\n    # Construct the inner and outer measures\n    inner = y1[indices]\n    outer = y1[indices + 1]\n\n    # Return the inner and outer measures as a tuple\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0 = t0.detach()\n    t1 = t1.detach()\n    y1 = y1.detach()\n    t0_inds = searchsorted(t1, t0)\n    t0_inds = torch.clamp(t0_inds, 0, t1.shape[0] - 1)\n    t0_inds_g = torch.clamp(t0_inds + 1, 0, t1.shape[0] - 1)\n\n    y1_g = y1[t0_inds_g]\n    y1_l = y1[t0_inds]\n\n    t1_g = t1[t0_inds_g]\n    t1_l = t1[t0_inds]\n\n    inner = (y1_g - y1_l) / (t1_g - t1_l) * (t0 - t1_l) + y1_l\n    outer = (y1_g - y1_l) / (t1_g - t1_l) * (t0 - t1_l) + y1_g\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0 = t0.reshape(-1)\n    t1 = t1.reshape(-1)\n    y1 = y1.reshape(-1)\n\n    inner = torch.zeros(t0.shape[0], device=t0.device)\n    outer = torch.zeros(t0.shape[0], device=t0.device)\n\n    for i in range(t0.shape[0]):\n\n        # find the indices of the source times that are closest to the target time\n        idx = searchsorted(t1, t0[i])\n\n        # calculate the inner measure as the sum of the corresponding values\n        inner[i] = torch.sum(y1[idx])\n\n        # calculate the outer measure as the difference between the target time and the source times\n        outer[i] = t0[i] - t1[idx]\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t1 = t1.clone()\n    y1 = y1.clone()\n    t1.requires_grad = True\n    y1.requires_grad = True\n    inner = torch.zeros_like(t0)\n    outer = torch.zeros_like(t0)\n    for i in range(t0.shape[0]):\n        idx = searchsorted(t1, t0[i])\n        inner[i] = torch.sum(y1[:idx])\n        outer[i] = torch.sum(y1[idx:])\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # cumsums\n    c1 = torch.cumsum(y1, dim=0)\n    c2 = torch.cumsum(y1, dim=0)\n\n    # searchsorted\n    idx = searchsorted(t1, t0)\n\n    # inner\n    inner = c1[idx]\n\n    # outer\n    outer = c2[idx] - inner\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Construct inner and outer measures using cumulative sums\n    inner = torch.cumsum(y1, dim=-1)\n    outer = torch.cat([torch.zeros_like(inner[..., :1]), torch.cumsum(y1, dim=-1)], dim=-1)\n\n    # Find the indices of the source times (t1) that correspond to the target times (t0)\n    idx = searchsorted(t1, t0)\n\n    # Extract the inner and outer measures for the target times (t0)\n    inner = inner[..., idx]\n    outer = outer[..., idx]\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0 = t0.flatten()\n    t1 = t1.flatten()\n    y1 = y1.flatten()\n\n    # find the indices of the source times that are closest to the target times\n    idx = searchsorted(t1, t0)\n\n    # construct the inner measure using the values at the indices\n    inner = y1[idx]\n\n    # construct the outer measure by subtracting the values at the indices\n    outer = y1[idx] - y1[idx - 1]\n\n    # return the inner and outer measures as a tuple\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Construct the cumulative sums of y1\n    csum = torch.cumsum(y1, dim=0)\n\n    # Find the indices where t0 is greater than or equal to t1\n    idx = searchsorted(t1, t0)\n\n    # Compute the inner measure using the cumulative sums\n    inner = torch.gather(csum, 0, idx)\n\n    # Compute the outer measure by subtracting the inner measure from the cumulative sum\n    outer = csum[-1] - inner\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the closest times in t1 to t0\n    t1_idx = searchsorted(t1, t0)\n\n    # Clip the indices to the valid range\n    t1_idx = torch.clamp(t1_idx, 0, t1.size(0) - 1)\n\n    # Match up the channels of y1 and t1_idx\n    y1 = matchup_channels(y1, t1_idx)\n\n    # Compute the cumulative sums of y1\n    cumsum = torch.cumsum(y1, dim=0)\n\n    # Compute the inner measure\n    inner = cumsum[t1_idx]\n\n    # Compute the outer measure\n    outer = cumsum[-1] - inner\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t1_sort_idx = torch.argsort(t1)\n    t1_sort = t1[t1_sort_idx]\n    y1_sort = y1[t1_sort_idx]\n    y1_sort_cumsum = torch.cumsum(y1_sort, dim=0)\n    y1_sort_cumsum = torch.cat([torch.zeros(1, device=y1.device), y1_sort_cumsum[:-1]], dim=0)\n\n    t0_idx = searchsorted(t1_sort, t0)\n    y1_cumsum = y1_sort_cumsum[t0_idx]\n    y1_cumsum_prev = y1_sort_cumsum[t0_idx - 1]\n\n    inner = y1_cumsum - y1_cumsum_prev\n    outer = y1_sort_cumsum[t0_idx]\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t1_cumsum = torch.cumsum(y1, dim=0)\n    t1_cumsum = torch.cat([torch.zeros(1, device=t1_cumsum.device), t1_cumsum])\n\n    t0_cumsum = torch.cumsum(torch.ones_like(t0), dim=0)\n    t0_cumsum = torch.cat([torch.zeros(1, device=t0_cumsum.device), t0_cumsum])\n\n    inner_inds = searchsorted(t1, t0)\n    inner_inds = torch.clamp(inner_inds, 0, t1.size(0) - 1)\n\n    outer_inds = inner_inds + 1\n    outer_inds = torch.clamp(outer_inds, 0, t1.size(0) - 1)\n\n    inner = (t1_cumsum[outer_inds] - t1_cumsum[inner_inds]) / (t0_cumsum[outer_inds] - t0_cumsum[inner_inds])\n    outer = t1_cumsum[outer_inds] - inner * t0_cumsum[outer_inds]\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # find the index of the first time point that is greater than t0\n    idx = searchsorted(t1, t0)\n\n    # calculate the inner measure\n    inner = y1[idx] + (t0 - t1[idx]) * (y1[idx + 1] - y1[idx]) / (t1[idx + 1] - t1[idx])\n\n    # calculate the outer measure\n    outer = y1[idx] + (t0 - t1[idx]) * (y1[idx + 1] - y1[idx]) / (t1[idx + 1] - t1[idx])\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of t1 that are less than t0\n    idx = searchsorted(t1, t0)\n\n    # Construct the inner measure\n    inner = torch.cumsum(y1[idx], dim=0)\n\n    # Construct the outer measure\n    outer = torch.cat([torch.zeros_like(inner[0:1]), torch.cumsum(y1[idx], dim=0)[:-1]])\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Construct the cumulative sums for y1\n    csum_y1 = torch.cumsum(y1, dim=0)\n\n    # Find the indices where t0 lies between t1\n    idx = searchsorted(t1, t0)\n\n    # Calculate the inner and outer measures\n    inner = csum_y1[idx]\n    outer = csum_y1[idx + 1]\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t1_cumsum = torch.cat([torch.tensor([0]).to(t0.device), torch.cumsum(y1, dim=0)], dim=0)\n    t0_cumsum = torch.cat([torch.tensor([0]).to(t0.device), torch.cumsum(y1, dim=0)], dim=0)\n    t1_cumsum = t1_cumsum[1:]\n    t0_cumsum = t0_cumsum[1:]\n\n    t0_idx = searchsorted(t1, t0)\n    t0_idx = t0_idx.clamp(max=t0_cumsum.shape[0] - 1)\n    t0_idx_next = t0_idx + 1\n    t0_idx_next = t0_idx_next.clamp(max=t0_cumsum.shape[0] - 1)\n\n    inner = t0_cumsum[t0_idx]\n    outer = t0_cumsum[t0_idx_next] - t1_cumsum[t0_idx]\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # get cumulative sums\n    csum = torch.cumsum(y1, dim=-1)\n\n    # get indices\n    idx = searchsorted(t1, t0)\n\n    # get inner measure\n    inner = torch.gather(csum, -1, idx)\n\n    # get outer measure\n    outer = torch.cat([torch.zeros_like(inner[:, :1]), csum], dim=-1)\n    outer = torch.gather(outer, -1, idx + 1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and corresponding values\n    idx = torch.argsort(t1)\n    t1 = t1[idx]\n    y1 = y1[idx]\n\n    # Calculate cumulative sums\n    csum = torch.cumsum(y1, dim=0)\n\n    # Find the indices of the source times that are less than or equal to the target times\n    inds = searchsorted(t1, t0)\n\n    # Construct the inner and outer measures\n    inner = csum[inds]\n    outer = torch.zeros_like(t0)\n    outer[inds > 0] = csum[inds - 1]\n    outer[inds == 0] = 0\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # sort the source times and values\n    idx = torch.argsort(t1)\n    t1 = t1[idx]\n    y1 = y1[idx]\n\n    # compute the cumulative sums of the values\n    csum = torch.cumsum(y1, dim=0)\n\n    # compute the inner and outer measures\n    inner = torch.zeros_like(t0)\n    outer = torch.zeros_like(t0)\n    for i in range(t0.shape[0]):\n        # find the indices of the source times that are less than or equal to the current target time\n        j = searchsorted(t1, t0[i])\n        inner[i] = csum[j - 1] if j > 0 else 0.0\n        outer[i] = csum[j] if j < len(t1) else csum[-1]\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_outer, _ = inner_outer(t, t_env, w_env)\n\n    # calculate the difference between target weights and the upper envelope weights\n    w_diff = w - w_env_outer\n\n    # calculate the half-quadratic loss\n    loss = torch.where(w_diff > 0, w_diff**2, 0)\n\n    # scale the loss by the number of elements in the tensor\n    loss = loss / (w.shape[-1] - 1)\n\n    # add a small epsilon value to prevent division by zero\n    loss = loss + eps\n\n    return loss\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env_hi, w_env_lo = inner_outer(t, t_env, w_env)\n\n    w_env_hi = torch.maximum(w_env_hi, torch.zeros_like(w_env_hi))\n    w_env_lo = torch.maximum(w_env_lo, torch.zeros_like(w_env_lo))\n\n    w_env_hi = torch.where(w_env_hi > w, w, w_env_hi)\n    w_env_lo = torch.where(w_env_lo > w, w, w_env_lo)\n\n    w_env_hi = torch.where(w_env_hi > 0, w_env_hi, torch.zeros_like(w_env_hi))\n    w_env_lo = torch.where(w_env_lo > 0, w_env_lo, torch.zeros_like(w_env_lo))\n\n    w_env_hi = torch.where(w_env_hi > 0, w_env_hi, torch.zeros_like(w_env_hi))\n    w_env_lo = torch.where(w_env_lo > 0, w_env_lo, torch.zeros_like(w_env_lo))\n\n    w_env_hi = torch.where(w_env_hi > 0, w_env_hi, torch.zeros_like(w_env_hi))\n    w_env_lo = torch.where(w_env_lo > 0, w_env_lo, torch.zeros_like(w_env_lo))\n\n    w_env_hi = torch.where(w_env_hi > 0, w_env_hi, torch.zeros_like(w_env_hi))\n    w_env_lo = torch.where(w_env_lo > 0, w_env_lo, torch.zeros_like(w_env_lo))\n\n    w_env_hi = torch"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_outer, _ = inner_outer(t, t_env, w_env)\n\n    # calculate the difference between target weights and the upper envelope weights\n    diff = w - w_env_outer\n\n    # calculate the half-quadratic loss\n    loss = diff * torch.sqrt(torch.abs(diff) + eps)\n\n    # return the calculated loss\n    return loss\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t = t.detach()\n    # w = w.detach()\n    # t_env = t_env.detach()\n    # w_env = w_env.detach()\n\n    # calculate the upper envelope weights\n    w_env_outer, _ = inner_outer(t, t_env, w_env)\n\n    # calculate the difference between target weights and the upper envelope weights\n    diff = w - w_env_outer\n\n    # apply a half-quadratic loss function to the difference\n    loss = torch.sqrt(diff * diff + eps)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env_hi, w_env_lo = inner_outer(t, t_env, w_env)\n    w_hi, w_lo = inner_outer(t, t, w)\n\n    w_env_hi = torch.where(w_env_hi < 0, torch.zeros_like(w_env_hi), w_env_hi)\n    w_env_lo = torch.where(w_env_lo < 0, torch.zeros_like(w_env_lo), w_env_lo)\n    w_hi = torch.where(w_hi < 0, torch.zeros_like(w_hi), w_hi)\n    w_lo = torch.where(w_lo < 0, torch.zeros_like(w_lo), w_lo)\n\n    return (w_env_hi - w_hi)**2 + (w_env_lo - w_lo)**2\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    w_env = torch.cat([torch.zeros_like(w_env[..., :1]), w_env], dim=-1)\n    w_env_lo, w_env_hi = inner_outer(t, t_env, w_env)\n    w_env_lo = torch.where(w_env_lo < 0, 0, w_env_lo)\n    w_env_hi = torch.where(w_env_hi < 0, 0, w_env_hi)\n\n    w = torch.cat([torch.zeros_like(w[..., :1]), w], dim=-1)\n    w_lo, w_hi = inner_outer(t, t_env, w)\n    w_lo = torch.where(w_lo < 0, 0, w_lo)\n    w_hi = torch.where(w_hi < 0, 0, w_hi)\n\n    w_env_lo = torch.where(w_env_lo < eps, eps, w_env_lo)\n    w_env_hi = torch.where(w_env_hi < eps, eps, w_env_hi)\n    w_lo = torch.where(w_lo < eps, eps, w_lo)\n    w_hi = torch.where(w_hi < eps, eps, w_hi)\n\n    loss = torch.mean(0.5 * (w_env_lo - w_lo) ** 2 / w_env_lo + 0.5 * (w_env_hi - w_hi) ** 2 / w_env_hi)\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope\n    w_env_outer = torch.cat([torch.zeros_like(w_env[..., :1]), torch.cumsum(w_env, dim=-1)], dim=-1)  # 129\n\n    # calculate the inner and outer measures of the upper envelope\n    w_env_inner, w_env_outer = inner_outer(t, t_env, w_env)\n\n    # calculate the difference between the target weights and the upper envelope\n    w_diff = w - w_env_inner\n\n    # calculate the half-quadratic loss\n    loss = torch.where(w_diff > 0, w_diff * w_diff / (w_env_outer + eps), 0)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env = w_env[..., None]\n    t_env = t_env[..., None]\n\n    t_env_lo, t_env_hi = inner_outer(t, t_env, w_env)\n    w_env_lo, w_env_hi = inner_outer(t, t_env, w_env)\n\n    w_env_lo = w_env_lo / (t_env_hi - t_env_lo + eps)\n    w_env_hi = w_env_hi / (t_env_hi - t_env_lo + eps)\n\n    w_env_lo = torch.maximum(w_env_lo, torch.zeros_like(w_env_lo))\n    w_env_hi = torch.maximum(w_env_hi, torch.zeros_like(w_env_hi))\n\n    w_env_lo = torch.minimum(w_env_lo, torch.ones_like(w_env_lo))\n    w_env_hi = torch.minimum(w_env_hi, torch.ones_like(w_env_hi))\n\n    w_env = w_env_lo * (1 - w_env_hi) + w_env_hi * (1 - w_env_lo)\n\n    w = w[..., None]\n    t = t[..., None]\n\n    t_lo, t_hi = inner_outer(t, t_env, w_env)\n    w_lo, w_hi = inner_outer(t, t_env, w_env)\n\n    w_lo = w_lo / (t_hi - t_lo + eps)\n    w_hi = w_hi / (t_hi - t_lo + eps)\n\n    w_lo = torch.maximum(w_lo, torch.zeros_like(w_lo))\n    w_hi = torch.maximum(w_hi, torch.zeros_like(w_hi))\n\n    w_lo = torch"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    # calculate the upper envelope\n    w_env_outer, _ = inner_outer(t, t_env, w_env)\n\n    # calculate the difference between target weights and the upper envelope\n    diff = w - w_env_outer\n\n    # apply a half-quadratic loss function to the difference\n    loss = torch.max(torch.zeros_like(diff), diff) ** 2\n\n    # normalize the loss by the number of elements in the tensor\n    loss = loss / (diff.numel() + eps)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # assert t.shape[-1] == w.shape[-1] + 1\n    # assert t_env.shape[-1] == w_env.shape[-1] + 1\n\n    # compute outer measure\n    w_env_outer, w_env_inner = inner_outer(t, t_env, w_env)\n\n    # compute inner measure\n    w_inner, w_outer = inner_outer(t, t, w)\n\n    # compute scaled loss\n    loss = (w_env_outer - w_inner) ** 2 / (w_env_inner + eps)\n\n    # return loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n\n    # calculate upper envelope\n    w_env_lo, w_env_hi = inner_outer(t, t_env, w_env)\n\n    # calculate target weights\n    w_lo, w_hi = inner_outer(t, t, w)\n\n    # calculate loss\n    loss = torch.where(w_lo > w_env_lo, w_lo - w_env_lo, 0)\n    loss = torch.where(w_hi < w_env_hi, w_env_hi - w_hi, loss)\n    loss = loss / (w_env_hi - w_env_lo + eps)\n    loss = loss ** 2\n    loss = loss.mean()\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[:-1] == t_env.shape[:-1]\n\n    # calculate the upper envelope weights\n    w_env_upper = torch.maximum(w_env, torch.zeros_like(w_env))\n    w_env_upper = torch.maximum(w_env_upper, torch.zeros_like(w_env_upper))\n\n    # calculate the inner and outer measures for the target and environment weights\n    w_inner, w_outer = inner_outer(t, t_env, w_env_upper)\n\n    # calculate the difference between the target weights and the outer envelope weights\n    w_diff = w - w_outer\n\n    # calculate the half-quadratic loss\n    loss = w_diff ** 2 / (2 * torch.maximum(w_outer, eps))\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    # calculate upper envelope weights\n    w_env_inner, w_env_outer = inner_outer(t, t_env, w_env)\n    w_env_inner = torch.maximum(w_env_inner, w)\n    w_env_outer = torch.maximum(w_env_outer, w)\n\n    # calculate loss\n    loss = torch.maximum(w_env_outer - w, torch.zeros_like(w)) ** 2\n\n    # scale loss\n    loss = loss * (w_env_inner - w) / (w_env_outer - w_env_inner + eps)\n    return loss.sum(-1)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env_inner, w_env_outer = inner_outer(t, t_env, w_env)\n    w_inner, w_outer = inner_outer(t, t, w)\n    w_outer_min = torch.min(w_env_outer, w_outer)\n    w_outer_min = torch.where(w_outer_min < 0, 0, w_outer_min)\n    loss = torch.where(w_outer_min < w_inner, w_outer_min - w_inner, 0)\n    loss = loss * loss / (2 * (w_outer_min + eps))\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w_env.shape[-1] + 1\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    t_env, w_env = matchup_channels(t_env, w_env)\n    t_env = t_env.unsqueeze(0)\n    w_env = w_env.unsqueeze(0)\n\n    t = t.unsqueeze(0)\n    w = w.unsqueeze(0)\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    t_env = t_env.repeat(t.shape[0], 1, 1)\n    w_env = w_env.repeat(t.shape[0], 1, 1)\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    w_env = w_env.transpose(-1, -2)\n    w_env = w_env.transpose(-1, -2)\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    t_env = t_env.transpose(-1, -2)\n    t_env = t_env.transpose(-1, -2)\n\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    w_env = w_env.transpose(-1, -2)\n    w_env = w"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t = t.unsqueeze(-1)  # 129, 1\n    t_env = t_env.unsqueeze(0)  # 1, 129\n    w_env = w_env.unsqueeze(0)  # 1, 129\n    w = w.unsqueeze(-1)  # 129, 1\n    # 129, 129\n    w_env_scaled = w_env / (torch.abs(t_env - t) + eps)\n    w_env_scaled = torch.max(w_env_scaled, torch.zeros_like(w_env_scaled))\n    w_env_scaled = torch.min(w_env_scaled, w)\n    return torch.sum(w_env_scaled * (w_env_scaled - w) ** 2, dim=-1) / 2\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t_env = torch.cat([torch.zeros_like(t_env[..., :1]), t_env], dim=-1)\n    # w_env = torch.cat([torch.zeros_like(w_env[..., :1]), w_env], dim=-1)\n\n    # calculate the inner and outer measures for t_env and w_env\n    w_env_inner, w_env_outer = inner_outer(t, t_env, w_env)\n\n    # calculate the inner and outer measures for t and w\n    w_inner, w_outer = inner_outer(t, t, w)\n\n    # calculate the difference between the outer measures of t_env and w_env\n    outer_diff = w_env_outer - w_outer\n\n    # calculate the difference between the inner measures of t_env and w_env\n    inner_diff = w_env_inner - w_inner\n\n    # calculate the maximum of the outer_diff and inner_diff\n    max_diff = torch.maximum(outer_diff, inner_diff)\n\n    # calculate the loss using a half-quadratic loss function\n    loss = torch.where(max_diff > 0, max_diff**2 / 2, 0)\n\n    # return the calculated loss\n    return loss\n\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    w_env_lo, w_env_hi = inner_outer(t, t_env, w_env)\n    w_env_lo = torch.where(w_env_lo < 0, 0, w_env_lo)\n    w_env_hi = torch.where(w_env_hi < 0, 0, w_env_hi)\n\n    w_env = torch.maximum(w_env_lo, w_env_hi)\n    w_env = w_env / (torch.maximum(torch.sum(w_env, dim=-1, keepdim=True), eps))\n    w = w / (torch.maximum(torch.sum(w, dim=-1, keepdim=True), eps))\n\n    w_env = w_env / (torch.maximum(torch.sum(w_env, dim=-1, keepdim=True), eps))\n    w = w / (torch.maximum(torch.sum(w, dim=-1, keepdim=True), eps))\n\n    loss = torch.sum(torch.square(w - w_env), dim=-1)\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    w_env = matchup_channels(t, t_env, w_env)\n    w_env = w_env.view(-1, w.shape[-1] + 1)\n\n    t = t.view(-1, t.shape[-1])\n    w = w.view(-1, w.shape[-1])\n\n    # t_env.shape[-1] = w_env.shape[-1]\n    t_env = t_env.view(-1, t_env.shape[-1])\n    w_env = w_env.view(-1, w_env.shape[-1])\n\n    w_env_lo, w_env_hi = inner_outer(t, t_env, w_env)\n\n    # w_env_lo.shape[-1] = w_env_hi.shape[-1] = w.shape[-1]\n    w_env_lo = w_env_lo.view(-1, w.shape[-1])\n    w_env_hi = w_env_hi.view(-1, w.shape[-1])\n\n    # w_env_lo.shape[-1] = w_env_hi.shape[-1] = w.shape[-1]\n    w_env_lo = w_env_lo.view(-1, w.shape[-1])\n    w_env_hi = w_env_hi.view(-1, w.shape[-1])\n\n    w_env_lo = torch.max(w_env_lo, w)\n    w_env_hi = torch.min(w_env_hi, w)\n\n    loss = (w_env_lo - w) ** 2 + (w - w_env_hi) ** 2\n\n    # loss.shape[-1] = w.shape[-1]\n    loss = loss.view(-1, loss.shape[-1])\n\n    loss = torch.mean(loss, dim=-1)\n    loss = torch.mean(loss)\n\n    return loss\n\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t = t.detach()\n    w = w.detach()\n    t_env = t_env.detach()\n    w_env = w_env.detach()\n\n    # calculate upper envelope\n    w_env_outer, _ = inner_outer(t, t_env, w_env)\n    w_env_outer = torch.cumsum(w_env_outer, dim=-1)\n\n    # calculate loss\n    w_env_outer = w_env_outer.clamp(min=0)\n    w_env_outer = w_env_outer / (w_env_outer.sum(dim=-1, keepdim=True) + eps)\n    w_env_outer = w_env_outer.clamp(min=eps)\n\n    w_env_outer = w_env_outer.detach()\n    w_env_outer = w_env_outer / (w_env_outer.sum(dim=-1, keepdim=True) + eps)\n\n    # calculate loss\n    w_env_outer = w_env_outer.clamp(min=eps)\n    loss = (w - w_env_outer)**2 / (w_env_outer + eps)\n    loss = loss.sum(-1)\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n\n    w_inter = torch.diff(w_normalize, dim=-1)\n    w_intra = torch.diff(w_inter, dim=-1)\n\n    w_inter_loss = (w_inter.pow(2) / (w_inter.abs() + 1e-6)).mean()\n    w_intra_loss = (w_intra.pow(2) / (w_intra.abs() + 1e-6)).mean()\n\n    return w_inter_loss + w_intra_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n    w_inter = torch.abs(w_normalize[..., 1:] - w_normalize[..., :-1])\n    w_intra = torch.abs(w_normalize[..., 1:] - w_normalize[..., :-1].mean(-1, keepdim=True))\n    return (w_inter + w_intra).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Inter-interval loss\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n    area = 0.5 * (w_normalize[..., 1:] + w_normalize[..., :-1]) * (t[..., 1:] - t[..., :-1])\n    inter_loss = torch.sum(area, dim=-1)\n\n    # Intra-interval loss\n    intra_loss = torch.sum(w, dim=-1)\n\n    # Combine inter- and intra-interval losses\n    total_loss = inter_loss + intra_loss\n\n    return total_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Inter-interval loss\n    t_inter = (t[..., 1:] + t[..., :-1]) / 2\n    w_inter = (w[..., 1:] + w[..., :-1]) / 2\n    w_inter = torch.clip(w_inter, min=0.)\n    assert (w_inter >= 0.0).all()\n\n    # Intra-interval loss\n    w_intra = w[..., 1:] - w[..., :-1]\n    w_intra = torch.clip(w_intra, min=0.)\n    assert (w_intra >= 0.0).all()\n\n    # Combine losses\n    loss_inter = lossfun_outer(t, w, t_inter, w_inter)\n    loss_intra = lossfun_outer(t, w, t, w_intra)\n    loss = loss_inter + loss_intra\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    Computes the inter-interval loss for a given tensor of targets and weights. The function calculates the loss based on the provided tensors.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The target tensor for which the inter-interval loss is to be calculated. It is expected that the last dimension of 't' is one more than that of 'w'.\n    :param w: torch.Tensor. The weights tensor, used to weight the inter-interval loss calculation. The last dimension of 'w' should be one less than that of 't'.\n    :return: torch.Tensor. The calculated inter-interval loss as a tensor.\n    \"\"\"\n\n    # t.shape[-1] = w.shape[-1] + 1\n    w_inter = torch.diff(w, dim=-1)\n    t_inter = torch.diff(t, dim=-1)\n    loss_inter = (w_inter * t_inter).pow(2).sum(-1)\n\n    \"\"\"\n    Computes the intra-interval loss for a given tensor of targets and weights. The function calculates the loss based on the provided tensors.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The target tensor for which the intra-interval loss is to be calculated. It is expected that the last dimension of 't' is one more than that of 'w'.\n    :param w: torch.Tensor. The weights tensor, used to weight the intra-interval loss calculation. The last dimension of 'w' should be one less than that of 't'.\n    :return: torch.Tensor. The calculated intra-interval loss as a tensor.\n    \"\"\"\n\n    # t.shape[-1] = w.shape[-1] + 1\n    w_intra = torch.diff(w, dim=-1, prepend=torch.zeros_like(w[..., :1]))\n    t_intra = torch.diff(t, dim=-1, prepend=torch.zeros_like(t["}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], torch.finfo(torch.float32).eps)\n    t_ = torch.cat([t[..., :1], (t[..., 1:] + t[..., :-1]) / 2], dim=-1)\n    w_ = torch.cat([w_normalize[..., :1], (w_normalize[..., 1:] + w_normalize[..., :-1]) / 2], dim=-1)\n    return (w_[..., 1:] - w_[..., :-1]).pow(2).mean() + (t_[..., 1:] - t_[..., :-1]).pow(2).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    # inter-interval loss\n    w_inner, w_outer = inner_outer(t, t, w)\n    loss_inner = (w_inner.pow(2) / (w + torch.finfo(torch.float32).eps)).mean()\n    loss_outer = (w_outer.pow(2) / (w + torch.finfo(torch.float32).eps)).mean()\n    # intra-interval loss\n    w_inner, w_outer = inner_outer(t, t[..., 1:], w)\n    loss_inner_intra = (w_inner.pow(2) / (w + torch.finfo(torch.float32).eps)).mean()\n    loss_outer_intra = (w_outer.pow(2) / (w + torch.finfo(torch.float32).eps)).mean()\n    return loss_inner + loss_outer + loss_inner_intra + loss_outer_intra\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    \"\"\"\n    Computes the inter-interval loss for a given tensor of targets and weights. The function calculates the loss as the sum of the squared differences between adjacent intervals, weighted by the corresponding weights.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The target tensor for which the inter-interval loss is to be calculated. It is expected that the last dimension of 't' is one more than that of 'w'.\n    :param w: torch.Tensor. The weights tensor, used to weight the inter-interval loss calculation. The last dimension of 'w' should be one less than that of 't'.\n    :return: torch.Tensor. The calculated inter-interval loss as a tensor.\n    \"\"\"\n\n    t_inter = torch.cat([t[..., :1], (t[..., 1:] + t[..., :-1]) / 2, t[..., -1:]], dim=-1)\n    loss_inter = torch.sum((t_inter[..., 1:] - t_inter[..., :-1]) ** 2 * w, dim=-1)\n\n    \"\"\"\n    Computes the intra-interval loss for a given tensor of targets and weights. The function calculates the loss as the sum of the squared differences between the targets within each interval, weighted by the corresponding weights.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The target tensor for which the intra-interval loss is to be calculated. It is expected that the last dimension of 't' is one more than that of 'w'.\n    :param w: torch.Tensor. The weights tensor, used to weight the intra-interval loss calculation. The last dimension of 'w' should be one less than that of 't'.\n    :return: torch.Tensor. The calculated intra-interval loss as a tensor.\n    \"\"\"\n\n    loss_intra = torch.sum((t[..., 1:] - t[..., :-1]) ** 2 * w, dim=-1)\n\n    \"\"\""}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # inter-interval loss\n    w_inter = torch.diff(w, dim=-1)\n    t_inter = torch.diff(t, dim=-1)\n    loss_inter = (w_inter / t_inter).pow(2).mean()\n\n    # intra-interval loss\n    w_intra = w[..., 1:] - w[..., :-1]\n    t_intra = t[..., 1:] - t[..., :-1]\n    loss_intra = (w_intra / t_intra).pow(2).mean()\n\n    return loss_inter + loss_intra\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], torch.finfo(torch.float32).eps)\n    w_normalize = torch.cat([torch.zeros_like(w_normalize[..., :1]), w_normalize], dim=-1)\n\n    # inter-interval loss\n    w_inter = torch.abs(w_normalize[..., 1:] - w_normalize[..., :-1])\n    w_inter = torch.cat([w_inter, torch.zeros_like(w_inter[..., :1])], dim=-1)\n    loss_inter = torch.sum(w_inter * torch.abs(t[..., 1:] - t[..., :-1]), dim=-1)\n\n    # intra-interval loss\n    w_intra = torch.abs(w_normalize[..., 1:] + w_normalize[..., :-1])\n    loss_intra = torch.sum(w_intra * torch.abs(t[..., 1:] - t[..., :-1]), dim=-1)\n\n    # total loss\n    loss_distortion = torch.mean(loss_inter + loss_intra)\n\n    return loss_distortion\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"The proposal weight should be an upper envelope on the nerf weight.\"\"\"\n\n    w_inner, w_outer = inner_outer(t, t, w)\n    w_inner = torch.where(w_inner < 0, 0, w_inner)\n    w_outer = torch.where(w_outer < 0, 0, w_outer)\n\n    # Scaled half-quadratic loss that gives a constant gradient at w_outer = 0.\n    loss_inter = (w_outer - w_inner).pow(2) / (w_inner + 1e-6)\n\n    w_inner, w_outer = inner_outer(t, t, w_outer)\n    w_inner = torch.where(w_inner < 0, 0, w_inner)\n    w_outer = torch.where(w_outer < 0, 0, w_outer)\n\n    # Scaled half-quadratic loss that gives a constant gradient at w_outer = 0.\n    loss_intra = (w_outer - w_inner).pow(2) / (w_inner + 1e-6)\n\n    return (loss_inter + loss_intra).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], torch.finfo(torch.float32).eps)\n\n    # inter-interval loss\n    w_inter = torch.diff(w_normalize, dim=-1)\n    w_inter = torch.cat([w_inter, torch.zeros_like(w_inter[..., :1])], dim=-1)\n    w_inter = torch.abs(w_inter)\n    w_inter = w_inter.pow(2).mean()\n\n    # intra-interval loss\n    w_intra = torch.abs(w_normalize - w_normalize.mean(dim=-1, keepdim=True))\n    w_intra = w_intra.pow(2).mean()\n\n    return w_inter + w_intra\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t is the time of the intervals, w is the weight of the intervals\n    t, w = matchup_channels(t, w)\n\n    # Compute the inter-interval loss\n    inter_loss = (t[..., 1:] - t[..., :-1]) * w[..., :-1]\n    inter_loss = torch.sum(inter_loss, dim=-1)\n\n    # Compute the intra-interval loss\n    intra_loss = (t[..., 1:] - t[..., :-1]) * w[..., 1:]\n    intra_loss = torch.sum(intra_loss, dim=-1)\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_loss + intra_loss\n\n    return distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Calculate inter-interval loss\n    t, w = matchup_channels(t, w)\n    w_inner, w_outer = inner_outer(t, t, w)\n    loss_inter = w_inner.pow(2).mean() + w_outer.pow(2).mean()\n\n    # Calculate intra-interval loss\n    w_inner, w_outer = inner_outer(t[..., :-1], t[..., 1:], w[..., 1:])\n    loss_intra = w_inner.pow(2).mean() + w_outer.pow(2).mean()\n\n    return loss_inter + loss_intra\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n    w_normalize = torch.cat([torch.zeros_like(w_normalize[..., :1]), w_normalize], dim=-1)\n    return (w_normalize[..., 1:] - w_normalize[..., :-1]).pow(2).sum(dim=-1).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    t_intervals = torch.cat([t[..., :1], t[..., 1:]], dim=-1)\n    w_intervals = torch.cat([w[..., :1], w[..., :-1]], dim=-1)\n    w_intervals_outer = torch.cat([w[..., 1:], w[..., -1:]], dim=-1)\n    w_intervals_inner = torch.where(w_intervals_outer < w_intervals, w_intervals_outer, w_intervals)\n\n    # inter-interval loss\n    loss_inter_interval = (w_intervals_outer - w_intervals_inner).pow(2) / (w_intervals + torch.finfo(torch.float32).eps)\n    loss_inter_interval = loss_inter_interval.sum(-1)\n\n    # intra-interval loss\n    w_intervals_outer = torch.cat([w[..., 1:], torch.zeros_like(w[..., :1])], dim=-1)\n    w_intervals_inner = torch.cat([torch.zeros_like(w[..., :1]), w[..., :-1]], dim=-1)\n    loss_intra_interval = (w_intervals_outer - w_intervals_inner).pow(2) / (w_intervals + torch.finfo(torch.float32).eps)\n    loss_intra_interval = loss_intra_interval.sum(-1)\n\n    return loss_inter_interval + loss_intra_interval\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Calculate the inter-interval loss\n    inter_interval_loss = torch.sum(torch.diff(t, dim=-1) * w)\n\n    # Calculate the intra-interval loss\n    intra_interval_loss = torch.sum(torch.diff(t, dim=-1) * torch.diff(w, dim=-1))\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the inter-interval losses\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], torch.finfo(torch.float32).eps)\n    w_inter = torch.diff(w_normalize, dim=-1)\n    w_inter_loss = (w_inter - 1).pow(2).mean()\n\n    # Compute the intra-interval losses\n    w_intra = w_normalize[..., 1:] - w_normalize[..., :-1]\n    w_intra_loss = (w_intra - 1).pow(2).mean()\n\n    # Combine the losses\n    return w_inter_loss + w_intra_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # calculate inter-interval loss\n    t, w = matchup_channels(t, w)\n    t0 = t[..., :-1]\n    t1 = t[..., 1:]\n    w0 = w[..., :-1]\n    w1 = w[..., 1:]\n    w01 = w0 * w1\n\n    # calculate intra-interval loss\n    t01 = t0 * t1\n    w01_intra = w01 / (t1 - t0)\n    w01_intra = torch.where(t01 > 0, w01_intra, torch.zeros_like(w01_intra))\n\n    # combine inter-interval and intra-interval losses\n    loss = (w01_intra.sum(-1) + w01.sum(-1)) / w.sum(-1)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    t_ = torch.cat([t[..., :1], (t[..., 1:] + t[..., :-1]) / 2, t[..., -1:]], dim=-1)\n    w_ = torch.cat([w[..., :1], (w[..., 1:] + w[..., :-1]) / 2, w[..., -1:]], dim=-1)\n    # lossfun_outer\n    loss_outer = lossfun_outer(t, w, t_, w_, eps=torch.finfo(torch.float32).eps)\n    # lossfun_zip_outer\n    loss_zip_outer = lossfun_zip_outer(t, w, t_, w_, pulse_width=0.01)\n    return loss_outer + loss_zip_outer\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw0 = integrate_weights(w)\n    return interpolate(torch.tensor(ps), t, cw0)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw0 = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), t[..., :-1], cw0[..., :-1])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), t[..., :-1], cw[..., :-1])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps), t[..., :-1], cw[..., :-1])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps), t, cw)\n\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Match up channels\n    t, w = matchup_channels(t, w)\n    # Integrate the weights\n    cw = integrate_weights(w)\n    # Interpolate the integrated weights to find the weighted percentiles\n    return interpolate(torch.tensor(ps, device=t.device), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), t[..., :-1], cw[..., :-1])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Match up the channels of the tensors\n    t, w = matchup_channels(t, w)\n\n    # Integrate the weights\n    cw = integrate_weights(w)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    return interpolate(torch.tensor(ps, device=t.device), t, cw)\n\n\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), t[..., :-1], cw[..., :-1])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, dtype=t.dtype, device=t.device), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw0 = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), t[..., :-1], cw0[..., :-1])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    cw0 = integrate_weights(w)\n    cw0 = cw0[..., :-1]\n    cw1 = cw0 + w\n\n    ps = torch.tensor(ps, device=t.device)\n    if ps.ndim == 0:\n        ps = ps[None]\n\n    return interpolate(ps, cw0, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Sample uniformly over the bins.\n    u = torch.rand(num_samples, device=t.device)\n\n    # Invert the CDF to get the sampling points.\n    t_new = invert_cdf(u, t, w)\n\n    # Apply jitter to the samples.\n    if perturb:\n        if single_jitter:\n            j = torch.rand_like(t_new)\n        else:\n            j = torch.rand_like(t_new)\n        t_new = t_new + (j - 0.5)\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Normalize weights\n    w = w / torch.sum(w, dim=-1, keepdim=True)\n\n    # Sample uniformly in [0, 1]\n    u = torch.rand(num_samples, device=t.device)\n\n    # Invert CDF\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb samples\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand(num_samples, 1, device=t.device)\n        else:\n            jitter = torch.rand(num_samples, t.shape[-1], device=t.device)\n        t_new = t_new + jitter\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # 1. Sample uniformly in [0, 1]\n    u = torch.rand(num_samples, device=t.device)\n\n    # 2. Invert the CDF\n    t_new = invert_cdf(u, t, w)\n\n    # 3. Perturb the samples\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(t_new)\n            t_new = t_new + (jitter - 0.5)\n        else:\n            jitter = torch.rand_like(t_new)\n            t_new = t_new + (jitter - 0.5) / (w + 1e-5)\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Draw uniform samples in [0, 1] interval.\n    u = torch.rand(num_samples, device=t.device)\n\n    # Invert the CDF to get the sampling points.\n    t_new = invert_cdf(u, t, w)\n\n    # Apply jitter to the samples.\n    if perturb:\n        if single_jitter:\n            # Jitter all the samples together.\n            j = torch.rand_like(t_new)\n        else:\n            # Jitter each sample individually.\n            j = torch.rand_like(t_new)\n        t_new = t_new + (j - 0.5)\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Draw uniform samples in [0, 1]\n    if perturb:\n        u = torch.rand(num_samples, device=t.device)\n        if single_jitter:\n            j = torch.full((num_samples,), 1.0 / num_samples, device=t.device)\n        else:\n            j = torch.rand(num_samples, device=t.device) / num_samples\n    else:\n        u = torch.linspace(0.0, 1.0, num_samples, device=t.device)\n        j = torch.zeros(num_samples, device=t.device)\n\n    # Invert the CDF to get the sampling points.\n    t_new = invert_cdf(u + j, t, w)\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Draw uniform samples in [0, 1) from the upper envelop of the CDF.\n    u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n    if perturb:\n        # Apply jitter to the uniform samples to break ties.\n        if single_jitter:\n            j = torch.rand_like(u)\n        else:\n            j = torch.rand_like(u)\n        u = u + j / cw.shape[-1]\n\n    # Invert the CDF to get the sampling distribution.\n    t_new = invert_cdf(u, t, w)\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that t and w are sorted.\n    if not torch.all(torch.diff(t) > 0):\n        raise ValueError(\"t must be sorted\")\n    if not torch.all(torch.diff(w) > 0):\n        raise ValueError(\"w must be sorted\")\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Sample uniformly in [0, 1) from the CDF.\n    u = torch.rand(num_samples, device=t.device)\n    if perturb:\n        # Apply jittering to the samples.\n        if single_jitter:\n            u = u + torch.rand_like(u) / num_samples\n        else:\n            u = u + torch.rand_like(u) / num_samples\n\n        # Clamp the samples to the range [0, 1).\n        u = u.clamp(0, 1)\n\n    # Invert the CDF to obtain the samples.\n    t_new = invert_cdf(u, t, w)\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that the weights sum to 1.\n    w = w / torch.sum(w, dim=-1, keepdim=True)\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Generate random samples in [0, 1)\n    u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n\n    # Invert the CDF to get the sampling points.\n    t_new = invert_cdf(u, t, w)\n\n    # Apply jitter to the samples.\n    if perturb:\n        if single_jitter:\n            # Apply the same jitter to every sample along each dimension.\n            jitter = torch.rand_like(t_new)\n            t_new = t_new + jitter\n        else:\n            # Apply independent jitter to each sample.\n            jitter = torch.rand_like(t_new)\n            t_new = t_new + jitter\n\n    # Clamp the samples to the valid range of the PDF.\n    t_new = torch.clamp(t_new, t[..., 0], t[..., -1])\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that the weights sum to 1.\n    w = w / torch.sum(w, -1, keepdim=True)\n    # Ensure that the bin endpoints are sorted.\n    t = torch.sort(t, -1)[0]\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Sample uniformly from [0, 1) according to the CDF.\n    u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n    if perturb:\n        # Perturb the samples according to the PDF.\n        u = u + (torch.rand(t.shape[:-1] + (num_samples,), device=t.device) - 0.5) * (1 / num_samples)\n        u = torch.clip(u, 0, 1)\n    if single_jitter:\n        # Apply the same jitter to every sample along each dimension.\n        u = u + torch.rand(t.shape[:-1] + (1,), device=t.device) * (1 / num_samples)\n        u = torch.clip(u, 0, 1)\n    else:\n        # Apply independent jitter to each sample.\n        u = u + torch.rand(t.shape[:-1] + (num_samples,), device=t.device) * (1 / num_samples)\n        u = torch.clip(u, 0, 1)\n    # Interpolate into the PDF to get the sample values.\n    t_new = interpolate(u, cw, t)\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that t and w have the same number of dimensions.\n    t, w = matchup_channels(t, w)\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Compute the PDF and CDF for each weight vector.\n    cdf = cw[..., :-1]\n    pdf = cdf[..., 1:] - cdf[..., :-1]\n\n    # Draw uniform samples from the CDF.\n    u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n    if perturb:\n        u = u + (torch.rand(t.shape[:-1] + (num_samples,), device=t.device) - 0.5)\n\n    # Invert the CDF to obtain samples.\n    t_new = invert_cdf(u, t, w)\n\n    # Apply jitter to the samples.\n    if single_jitter:\n        jitter = torch.rand(t_new.shape, device=t.device)\n    else:\n        jitter = torch.rand_like(t_new)\n    t_new = t_new + jitter\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that t is sorted.\n    t, w = matchup_channels(t, w)\n    assert torch.all(t[..., 1:] >= t[..., :-1]), 't must be sorted'\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Draw uniform samples in [0, 1) from the CDF.\n    u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n    if perturb:\n        # Apply jitter to the samples to avoid clustering.\n        u = u + (torch.rand_like(u) - 0.5)\n        u = torch.clip(u, 0, 1)\n\n    # Invert the CDF to get the sampling points.\n    t_new = interpolate(u, cw, t)\n\n    if single_jitter:\n        # Apply the same jitter to all samples.\n        t_new = t_new + (torch.rand_like(t_new) - 0.5)\n    else:\n        # Apply independent jitter to each sample.\n        t_new = t_new + (torch.rand_like(t_new) - 0.5)\n\n    # Clip the samples to ensure they fall within the domain of the PDF.\n    t_new = torch.clip(t_new, t[..., 0], t[..., -1])\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that t and w are sorted.\n    t, w = matchup_channels(t, w)\n    w = w / torch.sum(w, -1, keepdim=True)\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Draw uniform samples.\n    u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n    # If we're perturbing, we sample uniformly within each interval\n    # and then shift the samples by a small amount.\n    if perturb:\n        u = u + (torch.rand_like(u) - 0.5) / t.shape[-1]\n    # If we're jittering, we add a small amount of noise to each sample.\n    if not single_jitter:\n        u = u + torch.rand_like(u) / t.shape[-1]\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Invert the CDF to get the quantile function, which maps from [0, 1) to [t[0], t[-1]].\n    q = invert_cdf(torch.rand(num_samples, device=t.device), t, w)\n    if perturb:\n        # Perturb the samples by adding a small amount of random noise.\n        u = torch.rand(num_samples, device=t.device)\n        if single_jitter:\n            # If single_jitter is True, use the same jitter for every sample along each dimension.\n            j = u\n        else:\n            # If single_jitter is False, use independent jitter for each sample.\n            j = u.reshape(num_samples, -1)\n        q = q + j * (cw[..., 1:] - cw[..., :-1])[..., None]\n    return q\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # 1. Generate samples from the CDF.\n    u = torch.rand(num_samples, device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    # 2. Optionally perturb the samples to avoid sample clustering at bin boundaries.\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(t_new)\n            t_new = t_new + (jitter - 0.5)\n        else:\n            jitter = torch.rand_like(t_new)\n            t_new = t_new + (jitter - 0.5)\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that t and w have the same shape\n    t, w = matchup_channels(t, w)\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Generate uniform samples in [0, 1)\n    u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n\n    # Apply jitter to the uniform samples\n    if single_jitter:\n        u = u + (torch.rand(t.shape[:-1] + (1,), device=t.device) - 0.5) / num_samples\n    else:\n        u = u + torch.rand(t.shape[:-1] + (num_samples,), device=t.device) / num_samples\n\n    # Clamp the uniform samples to [0, 1)\n    u = u.clip(max=1.0)\n\n    # Invert the CDF to get the samples according to the PDF\n    t_new = invert_cdf(u, t, w)\n\n    # Apply perturbation to the samples if requested\n    if perturb:\n        # Compute the bin widths\n        bin_widths = t[..., 1:] - t[..., :-1]\n\n        # Compute the midpoints of the bins\n        bin_midpoints = t[..., :-1] + 0.5 * bin_widths\n\n        # Compute the offsets for each sample\n        offsets = torch.zeros_like(t_new)\n        for i in range(t_new.shape[-1]):\n            # Find the indices of the bins that the sample belongs to\n            idx_lo, idx_hi = searchsorted(bin_midpoints[..., i], t_new[..., i])\n\n            # Compute the offset for the sample\n            offsets[..., i] = (t_new[..., i] - bin_midpoints[..., idx_lo]) / bin_widths[..., idx_lo]\n\n        #"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Normalize weights\n    w = w / torch.sum(w, -1, keepdim=True)\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Sample uniformly in [0, 1] and invert the CDF to get the sampling points.\n    u = torch.rand(num_samples, device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    # If perturb=True, add noise to the samples to avoid sample clustering at bin boundaries.\n    if perturb:\n        # Get the PDF value at the new sampling points.\n        w_new = torch.interpolate(w, t_new, mode='linear', align_corners=False)\n\n        # Apply jitter to the sampling points to avoid sample clustering.\n        if single_jitter:\n            t_new = t_new + torch.rand_like(t_new) / num_samples\n        else:\n            t_new = t_new + torch.rand_like(t_new) / num_samples\n\n        # Clamp the sampling points to the range of the bin endpoints.\n        t_new = torch.clamp(t_new, t[..., 0], t[..., -1])\n\n        # Invert the CDF to get the final sampling points.\n        t_new = invert_cdf(u, t_new, w_new)\n\n    return t_new\n\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the cumulative distribution function (CDF) from the weights\n    cdf = integrate_weights(w)\n\n    # Generate uniform samples in the range [0, 1)\n    u = torch.rand(num_samples, device=t.device)\n\n    # Sample from the CDF using the uniform samples\n    t_new = invert_cdf(u, t, w)\n\n    # Apply jitter to the samples\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(t_new)\n        else:\n            jitter = torch.rand_like(t_new).transpose(-2, -1)\n        t_new = t_new + jitter\n\n    # Clip the samples to the range [0, 1)\n    t_new = torch.clip(t_new, 0, 1)\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that t is sorted.\n    t = t.sort(dim=-1).values\n    # Ensure that w is normalized.\n    w = w / w.sum(dim=-1, keepdim=True)\n    # Ensure that the sum of w is 1.\n    w = w / w.sum(dim=-1, keepdim=True)\n    # Ensure that the last bin has weight 1.\n    w = torch.cat([w, torch.ones_like(w[..., :1])], dim=-1)\n\n    # Compute the CDF.\n    cw = integrate_weights(w)\n\n    # Compute the PDF.\n    pdf = (cw[..., 1:] - cw[..., :-1])\n\n    # Compute the PDF's CDF.\n    cdf = integrate_weights(pdf)\n\n    # Compute the CDF's inverse.\n    icdf = invert_cdf(torch.linspace(0, 1, num_samples, device=t.device), t, w)\n\n    # Add jitter to the samples.\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(icdf)\n        else:\n            jitter = torch.rand_like(icdf)\n        icdf = icdf + jitter / icdf.shape[-1]\n\n    # Clamp the samples to the range [0, 1].\n    icdf = icdf.clip(0, 1)\n\n    # Return the samples.\n    return icdf\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n    cw = integrate_weights(w)\n    # Get the PDF value at the CDF's bin midpoints\n    mid_inclusive = (cw[..., 1:] + cw[..., :-1]) / 2\n    # Invert the CDF\n    u = torch.rand(num_samples, device=t.device)\n    t_new = invert_cdf(u, t, w)\n    # Optionally, perturb these samples to avoid sample clustering at bin boundaries\n    if perturb:\n        # Get the PDF value at the CDF's bin midpoints\n        # Get the PDF value at the CDF's bin midpoints\n        mid_inclusive = (cw[..., 1:] + cw[..., :-1]) / 2\n        # Randomly sample indices from 0 to 11\n        indices = torch.randint(0, cw.shape[-1] - 1, (num_samples,), device=t.device)\n        # Get the PDF values at the randomly sampled indices\n        pdf_values = torch.gather(mid_inclusive, -1, indices[:, None])\n        # Apply jitter to the PDF values\n        if single_jitter:\n            jitter = torch.rand_like(pdf_values)\n        else:\n            jitter = torch.rand_like(pdf_values)\n        t_new = t_new + (pdf_values - jitter)\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    # Sample uniformly from [0, 1]\n    u = torch.rand(num_samples, device=t.device)\n\n    # Invert the CDF to get the PDF value at the randomly-generated points.\n    # This will be the x-value in the CDF; the y-value is the PDF which we\n    # take as our probability of sampling that x.\n    t_new = invert_cdf(u, t, w)\n\n    if perturb:\n        # Apply jitter to the samples to avoid sample clustering at bin boundaries.\n        if single_jitter:\n            # This method of jittering is meant to avoid sample clustering at bin\n            # boundaries, but it's also a bit too conservative for our purposes.\n            # TODO(jonbarron): figure out what the right amount of jitter is.\n            t_new += (torch.rand_like(t_new) - 0.5) / t_new.shape[-1]\n        else:\n            # This method of jittering is meant to be a bit less conservative\n            # for the purposes of our application.\n            t_new += (torch.rand_like(t_new) - 0.5) / (cw[..., 1:] - cw[..., :-1])\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = dilated_t.clamp(min=domain[0], max=domain[1])\n\n    # Compute the dilated weights\n    dilated_w = w / dilation\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    Performs dilation (via max-pooling) on a non-negative step function. It dilates the time steps based on the specified dilation parameter and clips the dilated time steps within a given domain. The weights are also adjusted accordingly to match the dilated time steps.\n\n    Input-Output Arguments\n    :param t: Tensor, the time steps of the step function, used as the basis for dilation.\n    :param w: Tensor, the weights associated with each time step of the step function.\n    :param dilation: Float, the amount by which to dilate the time steps.\n    :param domain: Tuple of two floats, the minimum and maximum values to clip the dilated time steps to. Defaults to negative and positive infinity, allowing all dilated time steps.\n    :return: A tuple of two Tensors. The first tensor is the dilated and clipped time steps, and the second tensor is the adjusted weights corresponding to the dilated time steps.\n    \"\"\"\n    t_dilated = torch.max_pool1d(t, dilation, stride=1, padding=dilation - 1)\n    t_dilated = torch.clamp(t_dilated, domain[0], domain[1])\n    w_dilated = torch.max_pool1d(w, dilation, stride=1, padding=dilation - 1)\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t_dilated = torch.max_pool1d(t, kernel_size=2, stride=1, padding=1, dilation=dilation)[..., 1:]\n    t_dilated = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n    w_dilated = pdf_to_weight(t_dilated, w)\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t_dilated = torch.max_pool1d(t.unsqueeze(-1), dilation, dilation)[..., 0]\n    t_dilated = torch.clamp(t_dilated, *domain)\n    w_dilated = torch.max_pool1d(w.unsqueeze(-1), dilation, dilation)[..., 0]\n    w_dilated = w_dilated / torch.max(t_dilated[..., 1:] - t_dilated[..., :-1], torch.finfo(torch.float32).eps)\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    Performs dilation (via max-pooling) on a non-negative step function. It dilates the time steps based on the specified dilation parameter and clips the dilated time steps within a given domain. The weights are also adjusted accordingly to match the dilated time steps.\n\n    Input-Output Arguments\n    :param t: Tensor, the time steps of the step function, used as the basis for dilation.\n    :param w: Tensor, the weights associated with each time step of the step function.\n    :param dilation: Float, the amount by which to dilate the time steps.\n    :param domain: Tuple of two floats, the minimum and maximum values to clip the dilated time steps to. Defaults to negative and positive infinity, allowing all dilated time steps.\n    :return: A tuple of two Tensors. The first tensor is the dilated and clipped time steps, and the second tensor is the adjusted weights corresponding to the dilated time steps.\n    \"\"\"\n\n    # Compute the dilated time steps\n    dilated_t = torch.max_pool1d(t[..., None], kernel_size=2, stride=1, padding=0)[..., 0] * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the dilated weights\n    dilated_w = torch.max_pool1d(w[..., None], kernel_size=2, stride=1, padding=0)[..., 0]\n\n    # Adjust the weights to match the dilated time steps\n    dilated_w = dilated_w * (dilated_t[..., 1:] - dilated_t[..., :-1]) / (t[..., 1:] - t[..., :-1])\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t = torch.cat([t[..., :1], (t[..., 1:] + t[..., :-1]) / 2, t[..., -1:]], dim=-1)\n    w = torch.cat([w[..., :1], (w[..., 1:] + w[..., :-1]) / 2, w[..., -1:]], dim=-1)\n    t = t * dilation\n    t = torch.clamp(t, *domain)\n    w = w / dilation\n    return t, w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Perform max-pooling on the time steps to dilate them\n    dilated_t = torch.nn.functional.max_pool1d(t, kernel_size=dilation, stride=dilation, padding=0)[..., :-1]\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    w = pdf_to_weight(dilated_t, w)\n\n    return dilated_t, w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    dilated_t = t * dilation\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n    w = w * dilation\n    w = w / torch.sum(w, dim=-1, keepdim=True)\n    return dilated_t, w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Perform dilation by taking the maximum of each consecutive pair of time steps\n    dilated_t = torch.maximum(t[..., :-1], t[..., 1:])\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    w = w / (t[..., 1:] - t[..., :-1])\n    w = w * (dilated_t[..., 1:] - dilated_t[..., :-1])\n\n    return dilated_t, w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t_dilated = torch.max_pool1d(t, dilation, stride=1, padding=dilation // 2)\n    t_dilated = t_dilated.clip(domain[0], domain[1])\n    w_dilated = w / dilation\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Calculate the dilated time steps.\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps within the specified domain.\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Calculate the dilated weights by dividing the weights by the dilation factor.\n    dilated_w = w / dilation\n\n    # Return the dilated and clipped time steps and the dilated weights.\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    Performs dilation (via max-pooling) on a non-negative step function. It dilates the time steps based on the specified dilation parameter and clips the dilated time steps within a given domain. The weights are also adjusted accordingly to match the dilated time steps.\n\n    Input-Output Arguments\n    :param t: Tensor, the time steps of the step function, used as the basis for dilation.\n    :param w: Tensor, the weights associated with each time step of the step function.\n    :param dilation: Float, the amount by which to dilate the time steps.\n    :param domain: Tuple of two floats, the minimum and maximum values to clip the dilated time steps to. Defaults to negative and positive infinity, allowing all dilated time steps.\n    :return: A tuple of two Tensors. The first tensor is the dilated and clipped time steps, and the second tensor is the adjusted weights corresponding to the dilated time steps.\n    \"\"\"\n\n    # Dilate the time steps\n    dilated_t = torch.max_pool1d(t, kernel_size=dilation, stride=dilation).squeeze(-1)\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the weights for the dilated time steps\n    dilated_w = torch.zeros_like(dilated_t)\n    for i in range(dilated_t.shape[0]):\n        start_idx = torch.searchsorted(t[i], dilated_t[i], right=False)\n        end_idx = torch.searchsorted(t[i], dilated_t[i], right=True)\n        dilated_w[i] = torch.sum(w[i, start_idx:end_idx])\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t = t.reshape(-1, t.shape[-1])\n    w = w.reshape(-1, w.shape[-1])\n    t_dilated = torch.max_pool1d(t, dilation, stride=dilation, padding=0)\n    w_dilated = torch.max_pool1d(w, dilation, stride=dilation, padding=0)\n    t_dilated = t_dilated.reshape(t.shape)\n    w_dilated = w_dilated.reshape(w.shape)\n    t_dilated = torch.clamp(t_dilated, domain[0], domain[1])\n    w_dilated = w_dilated * (t_dilated[..., 1:] - t_dilated[..., :-1])\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Check if the input tensors have the same shape\n    if t.ndim == w.ndim + 1:\n        t = t[..., 0]  # remove last dim\n\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)\n\n    # Compute the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the dilated weights\n    dilated_w = w / dilation\n\n    # Compute the indices of the dilated time steps within the original time steps\n    indices = torch.sum(torch.ge(dilated_t[..., :, None], t[..., None, :]), -1) - 1\n    indices = torch.clamp(indices, 0, w.shape[-1] - 1)\n\n    # Compute the dilated weights using the indices\n    dilated_w = dilated_w.gather(dim=-1, index=indices)\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Perform dilation by finding the maximum value within a window of size dilation.\n    dilated_t = torch.nn.functional.max_pool1d(t.unsqueeze(-1), dilation, stride=1, padding=dilation // 2).squeeze(-1)\n\n    # Clip the dilated time steps to the specified domain.\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the weights for the dilated time steps.\n    dilated_w = torch.zeros_like(dilated_t)\n    for i in range(dilated_t.shape[0]):\n        dilated_w[i] = torch.sum(w[i][(t[i] >= dilated_t[i] - dilation / 2) & (t[i] < dilated_t[i] + dilation / 2)])\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Ensure that the time steps are increasing.\n    t = t.clip(min=t[..., :-1])\n\n    # Dilate the time steps by the specified amount.\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain.\n    dilated_t = dilated_t.clip(*domain)\n\n    # Calculate the dilated weights based on the dilated time steps.\n    dilated_w = weight_to_pdf(dilated_t, w)\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    if dilation == 1:\n        return t, w\n\n    # Ensure that the time steps are sorted.\n    t = t.sort(dim=-1).values\n\n    # Compute the dilated time steps.\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain.\n    dilated_t = dilated_t.clamp(min=domain[0], max=domain[1])\n\n    # Compute the adjusted weights by dividing the original weights by the dilated time steps.\n    w_new = w / dilated_t\n\n    # Normalize the adjusted weights to sum to 1.\n    w_new = w_new / w_new.sum(dim=-1, keepdim=True)\n\n    return dilated_t, w_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # check if t is sorted\n    if not torch.all(t[..., 1:] >= t[..., :-1]):\n        raise ValueError(\"t must be sorted\")\n\n    # check if t is non-negative\n    if torch.any(t < 0):\n        raise ValueError(\"t must be non-negative\")\n\n    # check if w is non-negative\n    if torch.any(w < 0):\n        raise ValueError(\"w must be non-negative\")\n\n    # check if w sums to 1\n    if not torch.allclose(torch.sum(w, dim=-1), torch.ones_like(w[..., 0])):\n        raise ValueError(\"w must sum to 1\")\n\n    # check if dilation is positive\n    if dilation <= 0:\n        raise ValueError(\"dilation must be positive\")\n\n    # check if domain is valid\n    if domain[0] > domain[1]:\n        raise ValueError(\"domain must be a tuple of two floats, where the first float is less than or equal to the second float\")\n\n    # check if t is within the domain\n    if torch.any(t < domain[0]) or torch.any(t > domain[1]):\n        raise ValueError(\"t must be within the specified domain\")\n\n    # check if w is non-negative\n    if torch.any(w < 0):\n        raise ValueError(\"w must be non-negative\")\n\n    # check if w sums to 1\n    if not torch.allclose(torch.sum(w, dim=-1), torch.ones_like(w[..., 0])):\n        raise ValueError(\"w must sum to 1\")\n\n    # dilate t\n    t_dilated = torch.max_pool1d(t[..., None], kernel_size=2, stride=1, padding=0)[..., 0] * dilation\n\n    # clip t_dilated to the domain\n    t_dilated = torch.clamp(t_dilated, min=domain[0],"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Dilate the time steps\n    dilated_t = t[..., :-1] + (t[..., 1:] - t[..., :-1]) / 2 * dilation\n\n    # Clip the dilated time steps within the specified domain\n    dilated_t = torch.clamp(dilated_t, *domain)\n\n    # Compute the dilated weights\n    dilated_w = torch.zeros_like(dilated_t)\n    dilated_w[..., 1:] = dilated_t[..., 1:] - dilated_t[..., :-1]\n    dilated_w = dilated_w / torch.sum(dilated_w, dim=-1, keepdim=True)\n\n    # Adjust the weights based on the dilated time steps\n    w_new = torch.zeros_like(w)\n    w_new[..., 1:] = dilated_w[..., 1:] * (t[..., 1:] - t[..., :-1])\n    w_new = w_new / torch.sum(w_new, dim=-1, keepdim=True)\n\n    return dilated_t, w_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Dilate the time steps by the specified amount\n    dilated_t = torch.cat([t[..., :1] - 1, (t[..., 1:] + t[..., :-1]) / 2, t[..., -1:] + 1], dim=-1) * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = dilated_t.clip(*domain)\n\n    # Compute the widths of the dilated time steps\n    dilated_widths = torch.cat([t[..., 0:1], torch.diff(dilated_t, dim=-1)], dim=-1)\n\n    # Compute the adjusted weights based on the dilated time steps\n    dilated_w = w / dilated_widths\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # check if tq is a scalar\n    if tq.ndim == 0:\n        tq = tq[None]\n\n    # check if t is a scalar\n    if t.ndim == 0:\n        t = t[None]\n\n    # check if y is a scalar\n    if y.ndim == 0:\n        y = y[None]\n\n    # check if tq is a tensor\n    if not isinstance(tq, torch.Tensor):\n        tq = torch.tensor(tq)\n\n    # check if t is a tensor\n    if not isinstance(t, torch.Tensor):\n        t = torch.tensor(t)\n\n    # check if y is a tensor\n    if not isinstance(y, torch.Tensor):\n        y = torch.tensor(y)\n\n    # check if tq is a float\n    if torch.is_floating_point(tq):\n        tq = torch.tensor([tq])\n\n    # check if t is a float\n    if torch.is_floating_point(t):\n        t = torch.tensor([t])\n\n    # check if y is a float\n    if torch.is_floating_point(y):\n        y = torch.tensor([y])\n\n    # check if tq is a list\n    if isinstance(tq, list):\n        tq = torch.tensor(tq)\n\n    # check if t is a list\n    if isinstance(t, list):\n        t = torch.tensor(t)\n\n    # check if y is a list\n    if isinstance(y, list):\n        y = torch.tensor(y)\n\n    # check if tq is a numpy array\n    if isinstance(tq, np.ndarray):\n        tq = torch.from_numpy(tq)\n\n    # check if t is a numpy array\n    if isinstance(t, np.ndarray):\n        t = torch.from_numpy(t)\n\n    # check if y is"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices of the times in 't' that are less than or equal to the query times in 'tq'.\n    idx = torch.searchsorted(t, tq, right=True) - 1\n\n    # If the query time is exactly equal to a step change time, return the outside value.\n    # Otherwise, interpolate the value at the query time based on the step function.\n    return torch.where(idx == len(t) - 1, outside_value,\n                       (tq - t[idx]) / (t[idx + 1] - t[idx]) * y[idx + 1] + (t[idx + 1] - tq) / (t[idx + 1] - t[idx]) * y[idx])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a scalar\n    if torch.is_tensor(tq):\n        if tq.ndim == 0:\n            tq = tq[None]\n\n    # Check if t is a scalar\n    if torch.is_tensor(t):\n        if t.ndim == 0:\n            t = t[None]\n\n    # Check if y is a scalar\n    if torch.is_tensor(y):\n        if y.ndim == 0:\n            y = y[None]\n\n    # Check if tq is a 1D tensor\n    if tq.ndim != 1:\n        raise ValueError(\"tq must be a 1D tensor.\")\n\n    # Check if t is a 1D tensor\n    if t.ndim != 1:\n        raise ValueError(\"t must be a 1D tensor.\")\n\n    # Check if y is a 1D tensor\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1D tensor.\")\n\n    # Check if t and y have the same length\n    if t.shape[0] != y.shape[0]:\n        raise ValueError(\"t and y must have the same length.\")\n\n    # Check if t is sorted\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"t must be sorted.\")\n\n    # Check if tq is within the range of t\n    if not torch.all(torch.logical_and(tq >= t[0], tq <= t[-1])):\n        raise ValueError(\"tq must be within the range of t.\")\n\n    # Check if tq is a 1D tensor\n    if tq.ndim != 1:\n        raise ValueError(\"tq must be a 1D tensor.\")\n\n    # Check if t is a 1D tensor\n    if t.ndim != 1:\n        raise ValueError(\"t must be a 1D tensor.\")\n\n    # Check if y is a 1D tensor\n    if y.ndim != 1:\n        raise Value"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the query times are in the range of the step function\n    if torch.any(tq < t[..., 0]) or torch.any(tq > t[..., -1]):\n        return outside_value\n\n    # Find the indices of the intervals that contain the query times\n    idx_lo, idx_hi = searchsorted(t, tq)\n\n    # Interpolate the values at the query times\n    yq = (y[..., idx_hi] - y[..., idx_lo]) * (tq - t[..., idx_lo]) / (t[..., idx_hi] - t[..., idx_lo]) + y[..., idx_lo]\n\n    # If the query times exactly match a step change time, return the outside value\n    yq = torch.where(tq == t[..., idx_lo], outside_value, yq)\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a scalar or a tensor.\n    if tq.ndim == 0:\n        tq = tq.reshape(1)\n\n    # Check if t is a scalar or a tensor.\n    if t.ndim == 0:\n        t = t.reshape(1)\n\n    # Check if y is a scalar or a tensor.\n    if y.ndim == 0:\n        y = y.reshape(1)\n\n    # Check if tq is a tensor.\n    if tq.ndim == 1:\n        tq = tq.reshape(-1, 1)\n\n    # Check if t is a tensor.\n    if t.ndim == 1:\n        t = t.reshape(-1, 1)\n\n    # Check if y is a tensor.\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n\n    # Check if tq and t have the same number of dimensions.\n    if tq.ndim != t.ndim:\n        raise ValueError(\"tq and t must have the same number of dimensions.\")\n\n    # Check if t and y have the same number of dimensions.\n    if t.ndim != y.ndim:\n        raise ValueError(\"t and y must have the same number of dimensions.\")\n\n    # Check if tq and t have the same number of columns.\n    if tq.shape[1] != t.shape[1]:\n        raise ValueError(\"tq and t must have the same number of columns.\")\n\n    # Check if t and y have the same number of columns.\n    if t.shape[1] != y.shape[1]:\n        raise ValueError(\"t and y must have the same number of columns.\")\n\n    # Check if tq and t are sorted in ascending order.\n    if not torch.all(tq[1:] >= tq[:-1]):\n        raise ValueError(\"tq must be sorted in ascending order.\")\n\n    # Check if t and y are sorted in ascending order.\n    if not torch.all(t["}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the query times are within the range of the step function\n    if torch.any(tq < t[..., 0]) or torch.any(tq > t[..., -1]):\n        return outside_value\n\n    # Find the indices of the time intervals that enclose the query times\n    indices = torch.searchsorted(t, tq)\n\n    # Get the values of the step function at the left and right endpoints of the intervals\n    y0 = torch.gather(y, dim=-1, index=indices - 1)\n    y1 = torch.gather(y, dim=-1, index=indices)\n\n    # Interpolate the values at the query times\n    yq = y0 + (y1 - y0) * (tq - t[..., indices - 1]) / (t[..., indices] - t[..., indices - 1])\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Convert tq and t to the same device and dtype as y\n    tq = tq.to(y.device, y.dtype)\n    t = t.to(y.device, y.dtype)\n\n    # Check if tq is a scalar\n    if tq.ndim == 0:\n        tq = tq.unsqueeze(0)\n\n    # Check if t is a scalar\n    if t.ndim == 0:\n        t = t.unsqueeze(0)\n\n    # Check if tq and t are 1D tensors\n    if tq.ndim != 1 or t.ndim != 1:\n        raise ValueError(\"tq and t must be 1D tensors\")\n\n    # Check if tq and t have the same shape\n    if tq.shape != t.shape:\n        raise ValueError(\"tq and t must have the same shape\")\n\n    # Check if y is a scalar\n    if y.ndim == 0:\n        y = y.unsqueeze(0)\n\n    # Check if y is a 1D tensor\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1D tensor\")\n\n    # Check if t and y have the same shape\n    if t.shape[0] != y.shape[0]:\n        raise ValueError(\"t and y must have the same shape\")\n\n    # Check if t is sorted in ascending order\n    if not torch.all(t[1:] >= t[:-1]):\n        raise ValueError(\"t must be sorted in ascending order\")\n\n    # Check if tq is sorted in ascending order\n    if not torch.all(tq[1:] >= tq[:-1]):\n        raise ValueError(\"tq must be sorted in ascending order\")\n\n    # Check if tq and t have the same device and dtype\n    if tq.device != t.device or tq.dtype != t.dtype:\n        raise ValueError(\"tq and t must have the same device and dtype\")"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Ensure that tq is a 1D tensor\n    if tq.ndim == 1:\n        tq = tq[None]\n\n    # Ensure that t and y have the same number of dimensions\n    if t.ndim != y.ndim:\n        raise ValueError(\"'t' and 'y' must have the same number of dimensions.\")\n\n    # Ensure that tq and t have the same number of dimensions\n    if tq.ndim != t.ndim:\n        raise ValueError(\"'tq' and 't' must have the same number of dimensions.\")\n\n    # Ensure that t and y have the same number of elements in the last dimension\n    if t.shape[-1] != y.shape[-1]:\n        raise ValueError(\"'t' and 'y' must have the same number of elements in the last dimension.\")\n\n    # Ensure that tq and t have the same number of elements in the last dimension\n    if tq.shape[-1] != t.shape[-1]:\n        raise ValueError(\"'tq' and 't' must have the same number of elements in the last dimension.\")\n\n    # Ensure that t and tq are sorted in ascending order\n    if not torch.all(torch.diff(t, axis=-1) >= 0):\n        raise ValueError(\"'t' must be sorted in ascending order.\")\n\n    # Ensure that tq is a 1D tensor\n    if tq.ndim != 1:\n        raise ValueError(\"'tq' must be a 1D tensor.\")\n\n    # Ensure that tq is a 1D tensor\n    if tq.ndim != 1:\n        raise ValueError(\"'tq' must be a 1D tensor.\")\n\n    # Ensure that tq is a 1D tensor\n    if tq.ndim != 1:\n        raise ValueError(\"'tq' must be a 1D tensor.\")\n\n    # Ensure that tq is a 1D tensor\n    if tq.ndim != 1:\n        raise ValueError(\"'tq' must be"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices of the times in 't' that are less than or equal to the query times in 'tq'.\n    tq_indices = torch.searchsorted(t, tq)\n\n    # Clip the indices to ensure they are within the valid range of indices.\n    tq_indices = torch.clamp(tq_indices, 0, t.shape[-1] - 1)\n\n    # Look up the values of the step function at the query times.\n    yq = torch.gather(y, tq_indices)\n\n    # If the query times exactly match a step change time, return the outside value.\n    yq = torch.where(tq_indices < t.shape[-1] - 1, yq, outside_value)\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Pad the time values with a value that is guaranteed to be outside the range of the input times.\n    t = torch.cat([t.new_full((t.shape[0], 1), -torch.inf), t, t.new_full((t.shape[0], 1), torch.inf)], dim=1)\n\n    # Find the indices of the times that are less than or equal to the query times.\n    idx = torch.searchsorted(t, tq, right=True)\n\n    # Check if the query times exactly match a step change time.\n    exact_match = torch.eq(t[..., idx], tq)\n\n    # If the query times exactly match a step change time, return the outside value.\n    yq = torch.where(exact_match, outside_value, y[..., idx - 1])\n\n    # If the query times do not exactly match a step change time, interpolate the values.\n    yq = torch.where(exact_match, yq, torch.lerp(y[..., idx - 1], y[..., idx], (tq - t[..., idx - 1]) / (t[..., idx] - t[..., idx - 1])))\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Compute the indices of the rightmost interval containing each query time.\n    indices = torch.searchsorted(t, tq) - 1\n\n    # Clip the indices to the valid range of the step function.\n    indices = torch.clamp(indices, 0, y.shape[-1] - 1)\n\n    # Extract the corresponding values from the step function.\n    values = y.gather(-1, indices)\n\n    # If the query time exactly matches a step change time, return the outside value.\n    values = torch.where(tq == t, outside_value, values)\n\n    return values\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices of the times in 't' that are less than or equal to each query time in 'tq'.\n    idx = torch.searchsorted(t, tq, right=True) - 1\n\n    # If the query time exactly matches a step change time, return the outside value.\n    outside = tq == t[idx]\n    if outside.any():\n        yq = torch.where(outside, outside_value, y[idx])\n    else:\n        # Otherwise, interpolate the value at the query time.\n        yq = torch.where(idx < 0, y[0], y[idx]) + (tq - t[idx]) * (y[idx + 1] - y[idx]) / (t[idx + 1] - t[idx])\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a scalar\n    if tq.ndim == 0:\n        tq = tq[None]\n\n    # Check if t and y have the same shape\n    if t.ndim == y.ndim + 1:\n        t = t[..., 0]\n    if t.shape != y.shape:\n        raise ValueError(\"t and y must have the same shape\")\n\n    # Check if t is sorted\n    if not torch.all(t[..., 1:] >= t[..., :-1]):\n        raise ValueError(\"t must be sorted\")\n\n    # Check if tq is within the range of t\n    if torch.any(tq < t[..., 0]) or torch.any(tq > t[..., -1]):\n        raise ValueError(\"tq must be within the range of t\")\n\n    # Find the indices of the step changes that are less than or equal to tq\n    indices = torch.searchsorted(t, tq, right=True)\n\n    # If tq exactly matches a step change, return the outside value\n    if torch.any(tq == t[indices]):\n        return outside_value\n\n    # If tq is less than the first step change, return the first value\n    if torch.any(tq < t[..., 0]):\n        return y[..., 0]\n\n    # If tq is greater than the last step change, return the last value\n    if torch.any(tq > t[..., -1]):\n        return y[..., -1]\n\n    # Interpolate the values at the query times\n    return torch.lerp(y[..., indices - 1], y[..., indices], (tq - t[..., indices - 1]) / (t[..., indices] - t[..., indices - 1]))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a scalar and convert to a tensor if it is\n    if torch.is_tensor(tq):\n        tq = tq.unsqueeze(0)\n    else:\n        tq = torch.tensor(tq, dtype=t.dtype)\n\n    # Check if tq is a 1D tensor and convert to a 2D tensor if it is\n    if len(tq.shape) == 1:\n        tq = tq.unsqueeze(1)\n\n    # Check if t and y have the same shape\n    if t.shape != y.shape:\n        raise ValueError(\"t and y must have the same shape\")\n\n    # Check if tq has the same shape as t\n    if tq.shape[1] != t.shape[1]:\n        raise ValueError(\"tq must have the same shape as t\")\n\n    # Check if t is sorted\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"t must be sorted\")\n\n    # Check if tq is sorted\n    if not torch.all(torch.diff(tq) >= 0):\n        raise ValueError(\"tq must be sorted\")\n\n    # Check if t and tq have the same dtype\n    if t.dtype != tq.dtype:\n        raise ValueError(\"t and tq must have the same dtype\")\n\n    # Check if t and tq have the same device\n    if t.device != tq.device:\n        raise ValueError(\"t and tq must be on the same device\")\n\n    # Check if t and tq have the same dtype\n    if t.dtype != y.dtype:\n        raise ValueError(\"t and tq must have the same dtype\")\n\n    # Check if t and tq have the same device\n    if t.device != y.device:\n        raise ValueError(\"t and tq must be on the same device\")\n\n    # Check if t and tq have the same dtype\n    if t.dtype != outside_"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq = tq.unsqueeze(-1)\n    t = t.unsqueeze(0)\n    y = y.unsqueeze(0)\n\n    # Find the indices of the time intervals that contain the query times\n    t_indices = torch.searchsorted(t, tq, right=True) - 1\n\n    # Create a mask to indicate which query times exactly match a step change time\n    exact_matches = torch.all(tq == t, dim=-1)\n\n    # Interpolate the values at the query times using the step function\n    y_interp = (y[..., 1:] - y[..., :-1]) / (t[..., 1:] - t[..., :-1]) * (tq - t[..., :-1]) + y[..., :-1]\n\n    # Set the interpolated values to the outside value for query times that exactly match a step change time\n    y_interp = torch.where(exact_matches, outside_value, y_interp)\n\n    # Set the interpolated values to the outside value for query times outside the range of the step function\n    y_interp = torch.where((tq < t[..., 0]) | (tq > t[..., -1]), outside_value, y_interp)\n\n    # Set the interpolated values to the outside value for query times that exactly match a step change time\n    y_interp = torch.where(exact_matches, outside_value, y_interp)\n\n    return y_interp\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # tq: [..., 1]\n    # t: [..., N]\n    # y: [..., N]\n    # outside_value: float\n\n    # Check if the query times are outside the range of the step function\n    outside = (tq < t[..., 0]) | (tq > t[..., -1])\n\n    # If the query times are outside the range, return the outside value\n    if outside.any():\n        tq_outside = tq[outside]\n        t_outside = t[outside]\n        y_outside = y[outside]\n        tq_outside = tq_outside.reshape([-1, 1])\n        t_outside = t_outside.reshape([-1, t_outside.shape[-1]])\n        y_outside = y_outside.reshape([-1, y_outside.shape[-1]])\n        yq_outside = interpolate(tq_outside, t_outside, y_outside)\n        yq_outside = yq_outside.reshape(tq.shape[:-1] + (1,))\n        yq = yq.masked_fill(outside, yq_outside)\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq, t, y = matchup_channels(tq, t, y)\n    # Ensure that tq is sorted.\n    tq = torch.sort(tq, dim=-1)[0]\n\n    # Find the indices of the times in 't' that are greater than or equal to 'tq'.\n    idx_lo, idx_hi = searchsorted(t, tq)\n\n    # Compute the interpolated values of the step function at the query times.\n    y_lo = torch.gather(y, dim=-1, index=idx_lo)\n    y_hi = torch.gather(y, dim=-1, index=idx_hi)\n    t_lo = torch.gather(t, dim=-1, index=idx_lo)\n    t_hi = torch.gather(t, dim=-1, index=idx_hi)\n    y_q = y_lo + (y_hi - y_lo) * (tq - t_lo) / (t_hi - t_lo + 1e-8)\n\n    # If the query times exactly match a step change time, return the outside value.\n    mask = torch.gather(t, dim=-1, index=idx_lo) == tq\n    y_q = torch.where(mask, outside_value, y_q)\n\n    return y_q\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices of the times in 't' that are less than or equal to the query times in 'tq'.\n    idx = torch.searchsorted(t, tq, side='right') - 1\n\n    # Mask out the indices that are outside the range of 't'.\n    idx = torch.where(idx < 0, 0, idx)\n    idx = torch.where(idx >= len(t) - 1, len(t) - 2, idx)\n\n    # Interpolate the values of the step function at the query times.\n    t0 = t[idx]\n    t1 = t[idx + 1]\n    y0 = y[idx]\n    y1 = y[idx + 1]\n    tq0 = tq[idx]\n    tq1 = tq[idx + 1]\n    yq = (y0 * (t1 - tq1) + y1 * (tq0 - t0)) / (t1 - t0)\n\n    # Return the outside value for query times that exactly match a step change time.\n    yq = torch.where(tq0 == t0, outside_value, yq)\n    yq = torch.where(tq1 == t1, outside_value, yq)\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Ensure tq and t are 1D tensors\n    tq = tq.reshape(-1)\n    t = t.reshape(-1)\n\n    # Find the indices of the time intervals in 't' that contain 'tq'\n    indices = torch.searchsorted(t, tq)\n\n    # Clip the indices to the valid range\n    indices = indices.clip(0, t.shape[0] - 1)\n\n    # Extract the start and end indices of the time intervals that contain 'tq'\n    start_indices = indices - 1\n    end_indices = indices\n\n    # Extract the corresponding start and end times and values from 't' and 'y'\n    start_times = t[start_indices]\n    end_times = t[end_indices]\n    start_values = y[start_indices]\n    end_values = y[end_indices]\n\n    # Calculate the slope of the step function at each interval\n    slopes = (end_values - start_values) / (end_times - start_times)\n\n    # Calculate the query values by interpolating between the start and end values using the slope\n    query_values = start_values + slopes * (tq - start_times)\n\n    # Set the query values to the outside value if 'tq' exactly matches a step change time\n    query_values = torch.where(tq == t, outside_value, query_values)\n\n    return query_values\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Pad the step function with an endpoint at the maximum query time\n    t = torch.cat([t, tq.max()[None]])\n    y = torch.cat([y, outside_value])\n\n    # Interpolate the step function at the query times\n    return torch.interp(tq, t, y)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Calculate the annealing factor\n    anneal_factor = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Adjust the weights\n    w_adjusted = torch.max(w * anneal_factor, eps)\n\n    # Normalize the weights\n    w_adjusted /= torch.sum(w_adjusted, dim=-1, keepdim=True).clip(eps)\n\n    return w_adjusted\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Calculate the annealing factor based on the training fraction and slope\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Calculate the annealed weights by multiplying the weights with the annealing factor\n    w_annealed = w * anneal_factor\n\n    # Handle cases where adjacent intervals have zero distance\n    w_annealed = torch.where(\n        t[..., 1:] - t[..., :-1] <= eps,\n        torch.zeros_like(w_annealed),\n        w_annealed,\n    )\n\n    # Ensure stability in the computation by using a softmax operation on the adjusted weights\n    w_annealed = torch.softmax(w_annealed, dim=-1)\n\n    return w_annealed\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Adjust the weights based on the training fraction and annealing slope.\n    w = w * (1 - torch.exp(-anneal_slope * train_frac))\n\n    # Handle cases where adjacent intervals have zero distance.\n    w = torch.where(torch.isclose(t[..., 1:] - t[..., :-1], 0), 0, w)\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights.\n    w = torch.nn.functional.softmax(w, dim=-1)\n\n    # Normalize the weights to sum to 1.\n    w = w / torch.sum(w, dim=-1, keepdim=True).clip(eps)\n\n    return w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Calculate the annealing factor using Schlick's bias function\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Adjust the weights based on the annealing factor\n    w_annealed = w * anneal_factor\n\n    # Handle adjacent intervals with zero distance\n    w_annealed = torch.where(\n        (t[..., 1:] - t[..., :-1]).clip(eps) == 0,\n        torch.zeros_like(w_annealed),\n        w_annealed,\n    )\n\n    # Renormalize the weights to sum to 1\n    w_annealed /= torch.sum(w_annealed, dim=-1, keepdim=True).clip(eps)\n\n    return w_annealed\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Calculate the annealing function\n    anneal_fn = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Adjust the weights based on the annealing function\n    w_anneal = w * anneal_fn\n\n    # Handle adjacent intervals with zero distance\n    w_anneal = torch.where(\n        (t[..., 1:] - t[..., :-1]).abs() < eps,\n        torch.zeros_like(w_anneal),\n        w_anneal,\n    )\n\n    # Normalize the weights to sum to 1\n    w_anneal = torch.softmax(w_anneal, dim=-1)\n\n    return w_anneal\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Ensure t.shape[-1] = w.shape[-1] + 1\n    if t.ndim == w.ndim + 1:\n        t = t[..., 0]  # remove last dimension\n\n    # Adjust weights based on training fraction and anneal slope\n    w = w * torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Handle cases where adjacent intervals have zero distance\n    w = torch.where(t[..., 1:] - t[..., :-1] == 0, 0, w)\n\n    # Prevent NaN values due to log of zero\n    w = torch.where(w == 0, eps, w)\n\n    # Apply softmax to adjust weights\n    w = torch.softmax(w, dim=-1)\n\n    return w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    This function anneals the weights based on the training fraction and an annealing slope using Schlick's bias function. It adjusts the weights of a tensor based on the progression of training, making the weight adjustment more dynamic as training progresses. It ensures stability in the computation by handling cases where adjacent intervals have zero distance, setting their weight to zero, and preventing NaN values by using a softmax operation on the adjusted weights.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The tensor representing time or another sequential dimension, used to align with the weights tensor.\n    :param w: torch.Tensor. The weights tensor that will be adjusted based on the training fraction and anneal slope.\n    :param train_frac: float. The fraction of training completed, used to calculate the annealing effect on weights.\n    :param anneal_slope: float, optional. The slope of the annealing function, determining how sharply weights are adjusted. Defaults to 10.0.\n    :param eps: torch.float32, optional. A very small number added to prevent division by zero and log of zero in computations. Defaults to a small epsilon value squared.\n    :return: torch.Tensor. The adjusted weights tensor after applying the annealing process.\n    \"\"\"\n\n    # Calculate the annealing factor using Schlick's bias function\n    anneal_factor = torch.sigmoid((train_frac - 0.5) * anneal_slope)\n\n    # Adjust the weights using the annealing factor\n    w_annealed = w * anneal_factor\n\n    # Handle cases where adjacent intervals have zero distance\n    w_annealed = torch.where(\n        torch.isclose(t[..., 1:] - t[..., :-1], 0, atol=eps),\n        0.0,\n        w_annealed\n    )\n\n    # Ensure stability by using a softmax operation on the adjusted weights\n    w_annealed = torch.nn.function"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    This function anneals the weights based on the training fraction and an annealing slope using Schlick's bias function. It adjusts the weights of a tensor based on the progression of training, making the weight adjustment more dynamic as training progresses. It ensures stability in the computation by handling cases where adjacent intervals have zero distance, setting their weight to zero, and preventing NaN values by using a softmax operation on the adjusted weights.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The tensor representing time or another sequential dimension, used to align with the weights tensor.\n    :param w: torch.Tensor. The weights tensor that will be adjusted based on the training fraction and anneal slope.\n    :param train_frac: float. The fraction of training completed, used to calculate the annealing effect on weights.\n    :param anneal_slope: float, optional. The slope of the annealing function, determining how sharply weights are adjusted. Defaults to 10.0.\n    :param eps: torch.float32, optional. A very small number added to prevent division by zero and log of zero in computations. Defaults to a small epsilon value squared.\n    :return: torch.Tensor. The adjusted weights tensor after applying the annealing process.\n    \"\"\"\n\n    # Calculate the annealing factor based on the training fraction and anneal slope\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Calculate the annealed weights using Schlick's bias function\n    w_annealed = torch.where(\n        (t[..., 1:] - t[..., :-1]).clip(eps) == 0,\n        torch.zeros_like(w),\n        torch.exp(\n            -torch.log(\n                torch.clamp(\n                    w / (t[..., 1:] - t[..., :-1]).clip(eps),\n                    min=eps,\n                    max"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Calculate the annealing factor based on the training fraction and slope\n    anneal_factor = torch.sigmoid(anneal_slope * (1.0 - train_frac))\n\n    # Adjust the weights by multiplying with the annealing factor\n    w_anneal = w * anneal_factor\n\n    # Ensure stability in the computation by handling cases where adjacent intervals have zero distance\n    w_anneal = torch.where(t[..., 1:] - t[..., :-1] < eps,\n                           torch.zeros_like(w_anneal),\n                           w_anneal)\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights\n    w_anneal = torch.softmax(w_anneal, dim=-1)\n\n    return w_anneal\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Ensure that t.shape[-1] = w.shape[-1] + 1\n    if t.ndim == w.ndim + 1:\n        t = t[..., 0]  # remove last dim\n\n    # Calculate the annealing factor using Schlick's bias function\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Adjust the weights by multiplying with the annealing factor\n    w_anneal = w * anneal_factor\n\n    # Handle cases where adjacent intervals have zero distance\n    w_anneal = torch.where(\n        torch.abs(t[..., 1:] - t[..., :-1]) < eps,\n        torch.zeros_like(w_anneal),\n        w_anneal\n    )\n\n    # Apply softmax to the adjusted weights to ensure stability in the computation\n    w_anneal = torch.nn.functional.softmax(w_anneal, dim=-1)\n\n    return w_anneal\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the bias based on the training fraction and anneal slope\n    bias = torch.sigmoid((train_frac - 0.5) * anneal_slope)\n\n    # Adjust the weights based on the bias\n    w = torch.softmax(w * bias, dim=-1)\n\n    # Ensure that adjacent intervals have zero distance\n    w[..., 1:] -= w[..., :-1]\n    w[..., 1:] = w[..., 1:].clip(min=eps)\n\n    return t, w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Calculate the annealing factor based on the training fraction and slope.\n    anneal_factor = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Calculate the annealed weights using the bias function.\n    bias = torch.exp(-anneal_factor * (t[..., 1:] - t[..., :-1]).clip(eps))\n    w_annealed = w * bias\n\n    # Prevent weights from being zero or negative.\n    w_annealed = torch.max(w_annealed, eps)\n\n    # Normalize the weights to sum to 1.\n    w_annealed /= torch.sum(w_annealed, dim=-1, keepdim=True).clip(eps)\n\n    return w_annealed\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Ensure that adjacent intervals have non-zero distance.\n    t_diff = t[..., 1:] - t[..., :-1]\n    t_diff = torch.cat([t_diff, torch.zeros_like(t_diff[..., :1])], dim=-1)\n    t_diff = t_diff.clip(eps)\n\n    # Calculate the annealing effect using Schlick's bias function.\n    anneal_effect = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Adjust the weights using the annealing effect.\n    w_annealed = w * anneal_effect\n\n    # Ensure stability in the computation by handling cases where adjacent intervals have zero distance.\n    w_annealed = torch.where(t_diff == 0, torch.zeros_like(w_annealed), w_annealed)\n\n    # Normalize the adjusted weights to sum to 1.\n    w_annealed = torch.softmax(w_annealed, dim=-1)\n\n    return w_annealed\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Initialize the weights tensor with the same shape as the input weights tensor\n    w_anneal = torch.zeros_like(w)\n\n    # Calculate the annealing factor based on the training fraction and annealing slope\n    anneal_factor = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Adjust the weights based on the annealing factor\n    w_anneal = anneal_factor * w\n\n    # Ensure stability in the computation by handling cases where adjacent intervals have zero distance\n    w_anneal = torch.where(\n        (t[..., 1:] - t[..., :-1]) < eps,\n        torch.zeros_like(w_anneal),\n        w_anneal,\n    )\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights\n    w_anneal = torch.softmax(w_anneal, dim=-1)\n\n    return w_anneal\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Ensure that the input tensors have the same shape\n    t, w = matchup_channels(t, w)\n\n    # Calculate the annealing factor based on the training fraction and slope\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Adjust the weights based on the annealing factor\n    w_annealed = w * anneal_factor\n\n    # Ensure that adjacent intervals have non-zero distance\n    w_annealed = torch.where(t[..., 1:] - t[..., :-1] == 0,\n                             torch.zeros_like(w_annealed),\n                             w_annealed)\n\n    # Prevent NaN values due to log of zero\n    w_annealed = torch.where(w_annealed == 0, torch.ones_like(w_annealed) * eps, w_annealed)\n\n    # Apply a softmax operation to the adjusted weights to ensure stability in the computation\n    w_annealed = torch.nn.functional.softmax(torch.log(w_annealed), dim=-1)\n\n    return w_annealed\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Compute the bias using the annealing slope\n    bias = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Compute the annealed weights by adjusting the weights using the bias\n    annealed_weights = torch.max(w * bias, torch.finfo(w.dtype).eps)\n\n    # Normalize the annealed weights using softmax\n    annealed_weights = annealed_weights / torch.sum(annealed_weights, dim=-1, keepdim=True).clip(eps)\n\n    return annealed_weights\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"Anneal weights based on training fraction.\"\"\"\n    # Ensure that adjacent intervals have nonzero distance.\n    t = torch.cat([t[..., :1], t[..., :-1] + (t[..., 1:] - t[..., :-1]).clip(eps), t[..., -1:]], dim=-1)\n    # Compute the annealing factor.\n    a = torch.clamp(anneal_slope * (1 - train_frac), min=0)\n    # Adjust the weights using Schlick's bias function.\n    w_anneal = torch.softmax(a * torch.log(w.clip(eps) + 1), dim=-1)\n    # Ensure that weights sum to 1.\n    w_anneal /= torch.sum(w_anneal, dim=-1, keepdim=True).clip(eps)\n    return w_anneal\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Calculate the slope of the annealing function\n    slope = -anneal_slope\n\n    # Calculate the annealing effect on weights using Schlick's bias function\n    annealing_effect = torch.exp(slope * (1 - train_frac))\n\n    # Adjust the weights by multiplying by the annealing effect\n    adjusted_weights = w * annealing_effect\n\n    # Handle cases where adjacent intervals have zero distance\n    adjusted_weights = torch.where(t[..., 1:] - t[..., :-1] == 0, 0, adjusted_weights)\n\n    # Prevent NaN values by using a softmax operation\n    adjusted_weights = torch.nn.functional.softmax(adjusted_weights, dim=-1)\n\n    # Ensure that the sum of weights is 1\n    adjusted_weights /= torch.sum(adjusted_weights, dim=-1, keepdim=True).clip(eps)\n\n    return adjusted_weights\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # t: [B, P, I]\n    # w: [B, P, I - 1]\n\n    t, w = matchup_channels(t, w)\n\n    # Calculate the bias factor based on the training fraction and annealing slope\n    bias = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Apply the bias factor to the weights\n    w_anneal = w * bias\n\n    # Handle cases where adjacent intervals have zero distance\n    w_anneal = torch.where(t[..., 1:] - t[..., :-1] > 0, w_anneal, torch.zeros_like(w_anneal))\n\n    # Prevent NaN values due to log of zero\n    w_anneal = torch.where(w_anneal > 0, w_anneal, torch.ones_like(w_anneal) * eps)\n\n    # Apply a softmax operation to the adjusted weights\n    w_anneal = torch.softmax(torch.log(w_anneal), dim=-1)\n\n    # Reshape the tensor to match the original shape\n    w_anneal = w_anneal.reshape(w.shape)\n\n    return w_anneal\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Calculate the annealing factor using Schlick's bias function.\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Adjust the weights by multiplying with the annealing factor.\n    w_annealed = w * anneal_factor\n\n    # Handle cases where adjacent intervals have zero distance.\n    w_annealed = torch.where(\n        t[..., 1:] - t[..., :-1] == 0,\n        torch.zeros_like(w_annealed),\n        w_annealed\n    )\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights.\n    w_annealed = torch.softmax(w_annealed, dim=-1)\n\n    # Renormalize the weights to sum to 1.\n    w_annealed = w_annealed / torch.sum(w_annealed, dim=-1, keepdim=True).clip(eps)\n\n    return w_annealed\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(x, device, ignore_list) for x in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(x, device, ignore_list) for x in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple) or isinstance(batch, list):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(x, device, ignore_list) for x in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(x, device, ignore_list) for x in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple):\n        return tuple([to_cuda(b, device, ignore_list) for b in batch])\n    elif isinstance(batch, list):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(b, device, ignore_list) for b in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        if 'meta' in batch:\n            return batch\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple) or isinstance(batch, list):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != 'meta'}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device) if k != \"meta\" else v for k, v in batch.items()}\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(x, device) for x in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(x, device) for x in batch]\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(item, device, ignore_list) for item in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(item, device, ignore_list) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: to_cuda(value, device, ignore_list) if key != \"meta\" else value for key, value in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(elem, device, ignore_list) for elem in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(elem, device, ignore_list) for elem in batch]\n    elif isinstance(batch, dict):\n        return {key: to_cuda(val, device, ignore_list) for key, val in batch.items() if key != \"meta\"}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device) if k != \"meta\" else v for k, v in batch.items()}\n    elif isinstance(batch, list):\n        return [to_cuda(v, device) for v in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(v, device) for v in batch)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, dict):\n        if \"meta\" in batch:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, list):\n        return [to_cuda(v, device, ignore_list) for v in batch]\n    elif isinstance(batch, tuple):\n        return tuple([to_cuda(v, device, ignore_list) for v in batch])\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):\n        return torch.from_numpy(batch).to(device, non_blocking=True)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(elem, device, ignore_list) for elem in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(elem, device, ignore_list) for elem in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(b, device, ignore_list) for b in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, str):\n        return batch\n    else:\n        raise ValueError(f\"Unsupported type: {type(batch)}\")\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, tuple):\n        return tuple(to_cuda(elem, device=device, ignore_list=ignore_list) for elem in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(elem, device=device, ignore_list=ignore_list) for elem in batch]\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return batch\n        else:\n            return {key: to_cuda(value, device=device, ignore_list=ignore_list) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        if batch.device == torch.device(\"cpu\"):\n            return batch.to(device, non_blocking=True)\n        else:\n            return batch\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, dict):\n        if \"meta\" in batch:\n            return batch\n        else:\n            return {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, list):\n        return [to_cuda(v, device, ignore_list) for v in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(v, device, ignore_list) for v in batch)\n    elif isinstance(batch, torch.Tensor):\n        if ignore_list:\n            return batch.to(device, non_blocking=True)\n        else:\n            return batch.to(device, non_blocking=True).contiguous()\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, tuple) or isinstance(batch, list):\n        return [to_cuda(x, device=device, ignore_list=ignore_list) for x in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device=device, ignore_list=ignore_list) for k, v in batch.items() if k != \"meta\"}\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(item, device, ignore_list) for item in batch)\n    elif isinstance(batch, list):\n        if ignore_list:\n            return batch\n        return [to_cuda(item, device, ignore_list) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: to_cuda(value, device, ignore_list) for key, value in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, dict):\n        if \"meta\" in batch:\n            batch[\"meta\"] = to_cuda(batch[\"meta\"], device, ignore_list)\n        else:\n            for k, v in batch.items():\n                batch[k] = to_cuda(v, device, ignore_list)\n        return batch\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(x, device, ignore_list) for x in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(x, device, ignore_list) for x in batch]\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(x, device, ignore_list) for x in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(x, device, ignore_list) for x in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) if k != \"meta\" else v for k, v in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, dict):\n        if \"meta\" in batch:\n            batch[\"meta\"] = to_cuda(batch[\"meta\"], device, ignore_list=True)\n        else:\n            batch = {k: to_cuda(v, device, ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, tuple) or isinstance(batch, list):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):\n        batch = torch.from_numpy(batch).to(device, non_blocking=True)\n    elif isinstance(batch, Image.Image):\n        batch = torch.from_numpy(np.array(batch)).to(device, non_blocking=True)\n    elif isinstance(batch, BytesIO):\n        batch = torch.from_numpy(np.array(Image.open(batch))).to(device, non_blocking=True)\n    elif isinstance(batch, str):\n        batch = torch.from_numpy(np.array(Image.open(batch))).to(device, non_blocking=True)\n    else:\n        raise NotImplementedError(f\"Unsupported type: {type(batch)}\")\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # expand f to match v's batch dim\n    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(f.shape[:dim] + v.shape[dim:] + f.shape[dim:])\n\n    # gather vertices for each face\n    v1 = multi_gather(v, f[..., 0], dim)\n    v2 = multi_gather(v, f[..., 1], dim)\n    v3 = multi_gather(v, f[..., 2], dim)\n\n    # compute face normals\n    face_normals = torch.cross(v2 - v1, v3 - v1, dim=dim)\n\n    # reshape to maintain original faces tensor structure\n    return face_normals.reshape(f.shape)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # Adjust the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[dim] != v.shape[0]:\n        f = f.expand(v.shape[0], *f.shape[1:])\n\n    # Reshape the faces tensor to match the batch dimension of the vertices tensor\n    f = f.reshape(v.shape[0], -1, 3)\n\n    # Gather the vertices for each face\n    v1 = multi_gather(v, f[:, :, 0], dim=dim)\n    v2 = multi_gather(v, f[:, :, 1], dim=dim)\n    v3 = multi_gather(v, f[:, :, 2], dim=dim)\n\n    # Compute the face normals\n    e1 = v2 - v1\n    e2 = v3 - v1\n    n = torch.cross(e1, e2, dim=dim)\n    n = n / torch.norm(n, dim=dim, keepdim=True)\n\n    # Reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    n = n.reshape(*f.shape[:2], *v.shape[1:])\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # expand faces to match the batch dimension of the vertices\n    if f.shape[dim] != v.shape[dim]:\n        expand_shape = list(f.shape)\n        expand_shape[dim] = v.shape[dim]\n        f = f.expand(*expand_shape)\n\n    # gather vertices to form triangles\n    tris = v.gather(dim, f)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return tris.reshape(*(tris.shape[:dim] + (-1, 3) + tris.shape[dim + 1:]))\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimension of f to match the batch dimension of v\n    f = f.unsqueeze(-1) if f.ndim == v.ndim - 1 else f\n\n    # gather the vertices at the specified dimension\n    v1 = multi_gather(v, f[..., 0], dim=dim)\n    v2 = multi_gather(v, f[..., 1], dim=dim)\n    v3 = multi_gather(v, f[..., 2], dim=dim)\n\n    # compute the normals of the triangles\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    n = n / (torch.norm(n, dim=-1, keepdim=True) + 1e-10)\n\n    # reshape the result to maintain the structure of the original faces tensor\n    return n.reshape(*f.shape[:-1], 3)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    f = f.expand(v.shape[0], -1, -1) if f.ndim == 2 else f\n    # gather the triangles from the vertices tensor\n    v = multi_gather(v, f, dim)\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    v = v.reshape(v.shape[0], -1, 3, 3)\n    return v\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # expand f to match v\n    if f.shape[dim] != v.shape[dim]:\n        expand_shape = list(f.shape)\n        expand_shape[dim] = v.shape[dim]\n        f = f.expand(*expand_shape)\n\n    # gather vertices\n    v1 = multi_gather(v, f[..., 0], dim)\n    v2 = multi_gather(v, f[..., 1], dim)\n    v3 = multi_gather(v, f[..., 2], dim)\n\n    # compute face normals\n    v12 = v2 - v1\n    v13 = v3 - v1\n    face_normals = torch.cross(v12, v13, dim=dim)\n\n    # reshape face normals to maintain original faces structure\n    face_normals = face_normals.reshape(*f.shape[:-1], 3)\n\n    return face_normals\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # Adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(f.shape[:dim] + v.shape[dim:] + f.shape[dim:])\n\n    # Gather the vertices forming each face\n    v1 = multi_gather(v, f[:, 0], dim)\n    v2 = multi_gather(v, f[:, 1], dim)\n    v3 = multi_gather(v, f[:, 2], dim)\n\n    # Compute the face normals\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    n = n / torch.norm(n, dim=dim, keepdim=True)\n\n    # Reshape the result to maintain the structure of the original faces tensor\n    return n.reshape(f.shape[:dim] + (3,) + f.shape[dim:])\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[dim] != v.shape[0]:\n        f = f.expand(v.shape[0], *f.shape[1:])\n\n    # gather the vertices to form triangles\n    f = f.reshape(-1, 3)\n    v = v.reshape(v.shape[0], -1, v.shape[-1])\n    v = multi_gather(v, f, dim=dim)\n\n    # compute the normals of the triangles\n    v1 = v[:, 1] - v[:, 0]\n    v2 = v[:, 2] - v[:, 0]\n    n = torch.cross(v1, v2, dim=-1)\n    n = n / (n.norm(dim=-1, keepdim=True) + 1e-6)\n\n    # reshape the result to maintain the original faces tensor structure\n    n = n.reshape(*f.shape, n.shape[-1])\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # check if the batch dimension of f is the same as v\n    if f.shape[dim] != v.shape[0]:\n        # expand f to match the batch dimension of v\n        f = f.expand(v.shape[0], -1, -1)\n\n    # reshape f to match the batch dimension of v\n    f = f.reshape(v.shape[0], -1, 3)\n\n    # gather the vertices of the triangles\n    tris = multi_gather(v, f, dim)\n\n    # compute the normals of the triangles\n    normals = torch.cross(tris[:, :, 1] - tris[:, :, 0], tris[:, :, 2] - tris[:, :, 0], dim=-1)\n\n    # normalize the normals\n    normals = torch.nn.functional.normalize(normals, dim=-1)\n\n    # reshape the normals to match the original faces tensor structure\n    normals = normals.reshape(*f.shape[:-1], 3)\n\n    return normals\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust f to match v's batch dim\n    f = f.expand(v.shape[0], -1, -1) if f.ndim == 2 else f\n\n    # gather the vertices that form the triangles\n    v1 = multi_gather(v, f[:, :, 0], dim=dim)\n    v2 = multi_gather(v, f[:, :, 1], dim=dim)\n    v3 = multi_gather(v, f[:, :, 2], dim=dim)\n\n    # compute the normals of the triangles\n    e1 = v2 - v1\n    e2 = v3 - v1\n    n = torch.cross(e1, e2, dim=dim)\n    n = n / torch.norm(n, dim=dim, keepdim=True)\n\n    # reshape the result to maintain the original faces tensor structure\n    return n.reshape(f.shape)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimension of the faces tensor to match the batch dimension of the vertices tensor\n    if v.ndim > f.ndim: f = f.unsqueeze(-1)\n    # gather the vertices corresponding to the faces\n    v = multi_gather(v, f, dim=dim)\n    # compute the normals of the faces\n    v1 = v[:, :, 1] - v[:, :, 0]\n    v2 = v[:, :, 2] - v[:, :, 0]\n    n = torch.cross(v1, v2)\n    n = n / torch.norm(n, dim=-1, keepdim=True)\n    # reshape the result to maintain the original faces tensor structure\n    return n.reshape(*f.shape[:-1], *n.shape[1:])\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    if v.ndim > f.ndim:\n        f = f.expand((*v.shape[:-1], *f.shape))\n    elif v.ndim < f.ndim:\n        f = f.expand((*v.shape[:-2], *v.shape[-2:], *f.shape[v.ndim - 2:]))\n\n    # gather the vertices at the specified indices and reshape to maintain the original structure of the faces tensor\n    return v.gather(dim, f).reshape(*f.shape[:-2], 3, *f.shape[-2:])\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimension of the faces tensor to match the batch dimension of the vertices tensor\n    f = f.expand(v.shape[0], -1, -1) if f.ndim < v.ndim else f\n\n    # gather the vertices at the specified dimension\n    v1 = multi_gather(v, f[:, :, 0], dim=dim)\n    v2 = multi_gather(v, f[:, :, 1], dim=dim)\n    v3 = multi_gather(v, f[:, :, 2], dim=dim)\n\n    # compute the face normals\n    v12 = v2 - v1\n    v13 = v3 - v1\n    n = torch.cross(v12, v13, dim=dim)\n\n    # reshape the result to maintain the structure of the original faces tensor\n    n = n.reshape(*f.shape, -1)\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # f will be expanded to match the batch dimension of v\n    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(*v.shape[:dim], *f.shape[dim:])\n    # gather the vertices\n    v1 = multi_gather(v, f[..., 0], dim)\n    v2 = multi_gather(v, f[..., 1], dim)\n    v3 = multi_gather(v, f[..., 2], dim)\n    # compute the face normals\n    v12 = v2 - v1\n    v13 = v3 - v1\n    n = torch.cross(v12, v13, dim=dim)\n    # normalize the normals\n    n = n / (n.norm(dim=dim, keepdim=True) + 1e-8)\n    # reshape the result to maintain the original faces tensor structure\n    return n.reshape(*f.shape)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # Gather the value at the -2th dim of values, augment index shape on the back\n    # Example: values: B, P, 3, index: B, N, -> B, N, 3\n\n    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # first, adjust the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[0] != v.shape[0]:\n        f = f.expand(v.shape[0], -1, -1)\n\n    # reshape the faces tensor to maintain the original structure with additional dimensions for batch processing\n    f = f.reshape(-1, 3)\n\n    # gather the vertices corresponding to the faces\n    v = multi_gather(v, f, dim=dim)\n\n    # compute the normals of the faces\n    v1 = v[:, 1] - v[:, 0]\n    v2 = v[:, 2] - v[:, 0]\n    n = torch.cross(v1, v2)\n\n    # reshape the normals to maintain the original structure with additional dimensions for batch processing\n    n = n.reshape(-1, 3)\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim == v.ndim:\n        f = f\n    elif f.ndim == v.ndim + 1:\n        f = f.unsqueeze(0)\n    elif f.ndim == v.ndim - 1:\n        f = f.unsqueeze(-1)\n    else:\n        raise ValueError('Invalid faces tensor shape')\n\n    # gather the vertices at the specified indices\n    fv = multi_gather(v, f, dim=dim)\n\n    # compute the face normals\n    fv = fv.reshape(-1, 3, 3)\n    fv = fv[:, :, 1:] - fv[:, :, 0].unsqueeze(1)\n    fv = torch.cross(fv[:, 0], fv[:, 1])\n    fv = fv / torch.norm(fv, dim=-1, keepdim=True)\n\n    # reshape the result to maintain the original faces tensor structure\n    fv = fv.reshape(*f.shape[:-2], *fv.shape[1:])\n\n    return fv\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[0] != v.shape[0]:\n        f = f.expand(v.shape[0], -1, -1)\n\n    # gather the vertices for each triangle in the faces tensor\n    f = multi_gather(v, f, dim=dim)\n\n    # compute the normals of the faces by subtracting the first vertex from the other two vertices\n    f = f[:, :, 1:] - f[:, :, [0]]\n\n    # reshape the result to maintain the structure of the original faces tensor\n    f = f.reshape(*f.shape[:-2], 3, *f.shape[-2:])\n\n    return f\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust f to match v\n    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(*v.shape[:dim], *f.shape[dim:])\n\n    # gather the triangles\n    f = f.reshape(f.shape[0], -1, 3)\n    f = multi_gather(v, f, dim)\n\n    # compute normals\n    v1 = f[:, :, 1] - f[:, :, 0]\n    v2 = f[:, :, 2] - f[:, :, 0]\n    n = torch.cross(v1, v2, dim=-1)\n    n = n / (torch.norm(n, dim=-1, keepdim=True) + 1e-10)\n\n    # reshape to maintain the original structure\n    n = n.reshape(f.shape)\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # make sure that the faces tensor is of the same batch dimension as the vertices tensor\n    if f.shape[0] != v.shape[0]:\n        # expand the faces tensor to match the batch dimension of the vertices tensor\n        f = f.expand(v.shape[0], *f.shape[1:])\n\n    # gather the vertices for each face\n    v1 = multi_gather(v, f[:, 0], dim=dim)\n    v2 = multi_gather(v, f[:, 1], dim=dim)\n    v3 = multi_gather(v, f[:, 2], dim=dim)\n\n    # compute the normals of the faces\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    n = torch.nn.functional.normalize(n, dim=dim)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    n = n.reshape(*f.shape[:-1], 3)\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # This function gathers triangles from a batch of vertices and faces tensors, computing the faces normals with respect to the vertices.\n    # It adjusts the dimensions of the faces tensor to match the batch dimension of the vertices tensor if necessary,\n    # and then reshapes the result to maintain the original faces tensor structure with additional dimensions for batch processing.\n\n    # Adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim == v.ndim: f = f.unsqueeze(-1)  # add a batch dim to f\n    if f.shape[-2] != v.shape[dim]: f = f.expand(*f.shape[:-2], v.shape[dim], f.shape[-1])  # expand f to match the batch dimension of v\n\n    # Gather the vertices corresponding to the faces\n    v1 = multi_gather(v, f[..., 0], dim)\n    v2 = multi_gather(v, f[..., 1], dim)\n    v3 = multi_gather(v, f[..., 2], dim)\n\n    # Compute the face normals\n    e1 = v2 - v1\n    e2 = v3 - v1\n    n = torch.cross(e1, e2, dim=dim)\n    n = n / torch.linalg.norm(n, dim=-1, keepdim=True)\n\n    # Reshape the result to maintain the structure of the original faces tensor with additional dimensions for batch processing\n    return n.view(*f.shape[:-2], *f.shape[-2:], *n.shape[1:])\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.asarray(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.asarray(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.asarray(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.asarray(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.asarray(batch)\n        batch = batch[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        batch = np.expand_dims(batch, axis=0)\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.asarray(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.expand_dims(batch, 0)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.expand_dims(batch, 0)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = np.expand_dims(batch, 0)\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.origin, batch.meta.world_up, batch.meta.movement_speed, batch.meta.movement_force, batch.meta.drag_coeff_mult, batch.meta.constant_drag, batch.meta.mass, batch.meta.moment_of_inertia, batch.meta.movement_torque, batch.meta.angular_friction, batch.meta.constant_torque, batch.meta.min_interval, batch.meta.pause_physics = self.origin, self.world_up, self.movement_speed, self.movement_force, self.drag_coeff_mult, self.constant_drag, self.mass, self.moment_of_inertia, self.movement_torque, self.angular_friction, self.constant_torque, self.min_interval, self.pause_physics\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Direct mapping\n        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        # Nested 'meta' dictionary\n        batch.meta = dotdict()\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Direct mapping\n        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = torch.as_tensor(self.K)\n        batch.R = torch.as_tensor(self.R)\n        batch.T = torch.as_tensor(self.T)\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = torch.as_tensor(self.bounds)\n\n        # Nested 'meta' dictionary\n        batch.meta = dotdict()\n        batch.meta.origin = torch.as_tensor(self.origin)\n        batch.meta.world_up = torch.as_tensor(self.world_up)\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Direct mapping\n        batch = dotdict()\n        batch.H = torch.as_tensor(self.H, dtype=torch.int)\n        batch.W = torch.as_tensor(self.W, dtype=torch.int)\n        batch.K = torch.as_tensor(self.K, dtype=torch.float)\n        batch.R = torch.as_tensor(self.R, dtype=torch.float)\n        batch.T = torch.as_tensor(self.T, dtype=torch.float)\n        batch.n = torch.as_tensor(self.n, dtype=torch.float)\n        batch.f = torch.as_tensor(self.f, dtype=torch.float)\n        batch.t = torch.as_tensor(self.t, dtype=torch.float)\n        batch.v = torch.as_tensor(self.v, dtype=torch.float)\n        batch.bounds = torch.as_tensor(self.bounds, dtype=torch.float)\n\n        # Nested 'meta' dictionary\n        batch.meta = dotdict()\n        batch.meta.origin = torch.as_tensor(self.origin, dtype=torch.float)\n        batch.meta.world_up = torch.as_tensor(self.world_up, dtype=torch.float)\n        batch.meta.movement_speed = torch.as_tensor(self.movement_speed, dtype=torch.float)\n        batch.meta.movement_force = torch.as_tensor(self.movement_force, dtype=torch.float)\n        batch.meta.drag_coeff_mult = torch.as_tensor(self.drag_coeff_mult, dtype=torch.float)\n        batch.meta.constant_drag = torch.as_tensor(self.constant_drag, dtype=torch.float)\n        batch.meta.mass = torch.as_tensor(self.mass, d"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Direct mapping\n        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        # Nested 'meta' dictionary\n        batch.meta = dotdict()\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W = self.H, self.W\n        batch.K = torch.as_tensor(self.K)\n        batch.R = torch.as_tensor(self.R)\n        batch.T = torch.as_tensor(self.T)\n        batch.n, batch.f = self.n, self.f\n        batch.t, batch.v = self.t, self.v\n        batch.bounds = torch.as_tensor(self.bounds)\n\n        batch.meta = dotdict()\n        batch.meta.origin = torch.as_tensor(self.origin)\n        batch.meta.world_up = torch.as_tensor(self.world_up)\n        batch.meta.movement_speed = torch.as_tensor(self.movement_speed)\n        batch.meta.movement_force = torch.as_tensor(self.movement_force)\n        batch.meta.drag_coeff_mult = torch.as_tensor(self.drag_coeff_mult)\n        batch.meta.constant_drag = torch.as_tensor(self.constant_drag)\n        batch.meta.mass = torch.as_tensor(self.mass)\n        batch.meta.moment_of_inertia = torch.as_tensor(self.moment_of_inertia)\n        batch.meta.movement_torque = torch.as_tensor(self.movement_torque)\n        batch.meta.angular_friction = torch.as_tensor(self.angular_friction)\n        batch.meta.constant_torque = torch.as_tensor(self.constant_torque)\n        batch.meta.min_interval = torch.as_tensor(self.min_interval)\n        batch.meta.pause_physics = torch.as_tensor(self.pause_physics)\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert all parameters to tensors\n        H, W, K, R, T, n, f, t, v, bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        H, W, K, R, T, n, f, t, v, bounds = torch.as_tensor(H), torch.as_tensor(W), torch.as_tensor(K), torch.as_tensor(R), torch.as_tensor(T), torch.as_tensor(n), torch.as_tensor(f), torch.as_tensor(t), torch.as_tensor(v), torch.as_tensor(bounds)\n\n        # Convert all parameters to tensors\n        origin, world_up, movement_speed, movement_force, drag_coeff_mult, constant_drag, mass, moment_of_inertia, movement_torque, angular_friction, constant_torque = self.origin, self.world_up, self.movement_speed, self.movement_force, self.drag_coeff_mult, self.constant_drag, self.mass, self.moment_of_inertia, self.movement_torque, self.angular_friction, self.constant_torque\n        origin, world_up, movement_speed, movement_force, drag_coeff_mult, constant_drag, mass, moment_of_inertia, movement_torque, angular_friction, constant_torque = torch.as_tensor(origin), torch.as_tensor(world_up), torch.as_tensor(movement_speed), torch.as_tensor(movement_force), torch.as_tensor(drag_coeff_mult), torch.as_tensor(constant_drag), torch.as_tensor(mass), torch.as_tensor(moment_of_inertia), torch.as_tensor(movement_torque), torch.as_tensor(angular_friction), torch."}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.origin, batch.meta.world_up, batch.meta.movement_speed, batch.meta.movement_force, batch.meta.drag_coeff_mult, batch.meta.constant_drag, batch.meta.mass, batch.meta.moment_of_inertia, batch.meta.movement_torque, batch.meta.angular_friction, batch.meta.constant_torque = self.origin, self.world_up, self.movement_speed, self.movement_force, self.drag_coeff_mult, self.constant_drag, self.mass, self.moment_of_inertia, self.movement_torque, self.angular_friction, self.constant_torque\n        batch.meta.min_interval, batch.meta.pause_physics = self.min_interval, self.pause_physics\n\n        # Convert to tensors\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = to_tensor(batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds)\n        batch.meta.origin, batch.meta.world_up, batch.meta.movement_speed, batch.meta.movement_force, batch.meta.drag_coeff_mult, batch.meta.constant_drag, batch.meta.mass, batch.meta.moment_of_inertia, batch.meta.movement_torque, batch.meta.angular_friction, batch."}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Direct mapping of parameters\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        # Nested 'meta' dictionary\n        batch.meta = dotdict()\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n\n        # Convert tensors to numpy arrays\n        batch.H = int(batch.H)\n        batch.W = int(batch.W)\n        batch.K = to_numpy(batch.K)\n        batch.R = to_numpy(batch.R)\n        batch.T = to_numpy(batch.T)\n        batch.n = float(batch.n)\n        batch.f = float(batch.f)\n        batch.t = float(batch.t)\n        batch.v = float(batch.v)\n        batch.bounds = to_numpy(batch.bounds)\n        batch.meta.origin = to_numpy"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert to tensors\n        H, W, K, R, T, n, f, t, v, bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        H, W, K, R, T, n, f, t, v, bounds = torch.as_tensor(H), torch.as_tensor(W), torch.as_tensor(K), torch.as_tensor(R), torch.as_tensor(T), torch.as_tensor(n), torch.as_tensor(f), torch.as_tensor(t), torch.as_tensor(v), torch.as_tensor(bounds)\n\n        # Construct batch\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = H, W, K, R, T, n, f, t, v, bounds\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = H, W, K, R, T, n, f, t, v, bounds\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Directly convert parameters to tensors\n        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = torch.as_tensor(self.K)\n        batch.R = torch.as_tensor(self.R)\n        batch.T = torch.as_tensor(self.T)\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = torch.as_tensor(self.bounds)\n\n        # Convert GUI related elements to tensors\n        batch.origin = torch.as_tensor(self.origin)\n        batch.world_up = torch.as_tensor(self.world_up)\n        batch.movement_speed = self.movement_speed\n        batch.movement_force = self.movement_force\n        batch.drag_coeff_mult = self.drag_coeff_mult\n        batch.constant_drag = self.constant_drag\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_torque = self.movement_torque\n        batch.angular_friction = self.angular_friction\n        batch.constant_torque = self.constant_torque\n        batch.min_interval = self.min_interval\n        batch.pause_physics = self.pause_physics\n\n        # Create a nested 'meta' dictionary\n        batch.meta = dotdict()\n        batch.meta.H = batch.H\n        batch.meta.W = batch.W\n        batch.meta.K = batch.K\n        batch.meta.R = batch.R\n        batch.meta.T = batch.T\n        batch.meta.n = batch.n\n        batch.meta.f = batch.f\n        batch.meta.t = batch.t\n        batch.meta.v = batch.v\n        batch.meta.bounds = batch.bounds\n        batch."}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = torch.as_tensor(self.K)\n        batch.R = torch.as_tensor(self.R)\n        batch.T = torch.as_tensor(self.T)\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = torch.as_tensor(self.bounds)\n\n        # Add meta\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        # Other configurables\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n        batch.movement_speed = self.movement_speed\n        batch.movement_force = self.movement_force\n        batch.drag_coeff_mult = self.drag_coeff_mult\n        batch.constant_drag = self.constant_drag\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_torque = self.movement_torque\n        batch.angular_friction = self.angular_friction\n        batch.constant_torque = self.constant_torque\n        batch.min_interval = self.min_interval\n        batch.pause_physics = self.pause_physics\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert parameters to tensors\n        H = torch.tensor(self.H, dtype=torch.int)\n        W = torch.tensor(self.W, dtype=torch.int)\n        K = torch.tensor(self.K, dtype=torch.float)\n        R = torch.tensor(self.R, dtype=torch.float)\n        T = torch.tensor(self.T, dtype=torch.float)\n        n = torch.tensor(self.n, dtype=torch.float)\n        f = torch.tensor(self.f, dtype=torch.float)\n        t = torch.tensor(self.t, dtype=torch.float)\n        v = torch.tensor(self.v, dtype=torch.float)\n        bounds = torch.tensor(self.bounds, dtype=torch.float)\n\n        # Create a structured dictionary with all the parameters\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = H, W, K, R, T, n, f, t, v, bounds\n\n        # Create a nested 'meta' dictionary with the same content\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = H, W, K, R, T, n, f, t, v, bounds\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Directly convert to tensors\n        K = torch.as_tensor(self.K)\n        R = torch.as_tensor(self.R)\n        T = torch.as_tensor(self.T)\n        bounds = torch.as_tensor(self.bounds)\n\n        # Convert to tensors\n        n = torch.as_tensor(self.n)\n        f = torch.as_tensor(self.f)\n        t = torch.as_tensor(self.t)\n        v = torch.as_tensor(self.v)\n\n        # Construct a batch\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, K, R, T, n, f, t, v, bounds\n        batch.meta = dotdict()\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert all parameters into tensors\n        H, W = self.H, self.W\n        K, R, T = self.K, self.R, self.T\n        n, f = self.n, self.f\n        bounds = self.bounds\n\n        # Convert all parameters into tensors\n        K = torch.as_tensor(K, dtype=torch.float)\n        R = torch.as_tensor(R, dtype=torch.float)\n        T = torch.as_tensor(T, dtype=torch.float)\n        n = torch.as_tensor(n, dtype=torch.float)\n        f = torch.as_tensor(f, dtype=torch.float)\n        bounds = torch.as_tensor(bounds, dtype=torch.float)\n\n        # Create a structured dictionary with the converted parameters\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.bounds = H, W, K, R, T, n, f, bounds\n\n        # Add a nested 'meta' dictionary with the same content as the original dictionary\n        batch.meta = dotdict()\n        for k, v in batch.items():\n            batch.meta[k] = v\n\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        # GUI related elements\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n        batch.movement_speed = self.movement_speed\n        batch.movement_force = self.movement_force\n        batch.drag_coeff_mult = self.drag_coeff_mult\n        batch.constant_drag = self.constant_drag\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_torque = self.movement_torque\n        batch.angular_friction = self.angular_friction\n        batch.constant_torque = self.constant_torque\n        batch.min_interval = self.min_interval\n        batch.pause_physics = self.pause_physics\n\n        return batch\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize_agent(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        serialized_agent = AgentSerializer.serialize(agent)\n        self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize_agent(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize_agent(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        # Serialize the agent's state\n        serialized_agent = AgentSerializer.serialize(agent)\n\n        # Save the serialized agent state\n        self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        serialized_agent = AgentSerializer.serialize_agent(agent)\n        self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working() and not agent.is_prime():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_serializer = AgentSerializer(agent)\n            serialized_agent = agent_serializer.serialize()\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        # Serialize the agent state\n        serialized_agent = AgentSerializer.serialize(agent)\n\n        # Save the serialized agent state\n        self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent():\n            if not agent.is_prime_agent():\n                serialized_agent = AgentSerializer.serialize(agent)\n                self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_working_agent():\n            return\n\n        serialized_agent = AgentSerializer.serialize_agent(agent)\n        self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_prime_agent:\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize_agent(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -np.inf\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) == 0:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            max_index = similarities.index(max_similarity)\n            return self.agents[max_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) < 250:\n                return self.agents[0], 0.999\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            best_index = np.argmax(similarities)\n            return self.agents[best_index], similarities[best_index]\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([e], [purpose_embedding])[0][0] for e in [agent.purpose_embedding for agent in self.agents]]\n            closest_agent_index = np.argmax(similarities)\n            if similarities[closest_agent_index] > 0:\n                return self.agents[closest_agent_index], similarities[closest_agent_index]\n            else:\n                return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) == 0:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            max_similarity_index = similarities.index(max_similarity)\n            if max_similarity > self.calculate_similarity_threshold():\n                return self.agents[max_similarity_index], max_similarity\n            else:\n                return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) < 2:\n                return None, -np.inf\n\n            embeddings=[]\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                   agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                embeddings.append(agent.purpose_embedding)\n\n            similarities = [cosine_similarity([purpose_embedding], [e])[0][0] for e in embeddings]\n            if similarities:\n                max_similarity = max(similarities)\n                max_similarity_index = similarities.index(max_similarity)\n                return self.agents[max_similarity_index], max_similarity\n            else:\n                return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) == 0:\n                logger.exception(\"No agents found\")\n                return None, -np.inf\n\n            if purpose_embedding is None:\n                logger.exception(\"Invalid purpose embedding\")\n                return None, -np.inf\n\n            similarity_threshold = self.calculate_similarity_threshold()\n            max_similarity = -np.inf\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n                if similarity > max_similarity and similarity > similarity_threshold:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, -np.inf"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not self.agents:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            if max_similarity > 0:\n                closest_agent = self.agents[similarities.index(max_similarity)]\n                return closest_agent, max_similarity\n            else:\n                return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarity_threshold = self.calculate_similarity_threshold()\n            closest_agent = None\n            max_similarity = -np.inf\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n                if similarity > max_similarity and similarity > similarity_threshold:\n                    max_similarity = similarity\n                    closest_agent = agent\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarity_threshold = self.calculate_similarity_threshold()\n            closest_agent = None\n            max_similarity = -np.inf\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n                if similarity > similarity_threshold and similarity > max_similarity:\n                    closest_agent = agent\n                    max_similarity = similarity\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([e], [purpose_embedding])[0][0] for e in [agent.purpose_embedding for agent in self.agents]]\n            if similarities:\n                max_similarity = max(similarities)\n                if max_similarity > 0:\n                    closest_agent_index = similarities.index(max_similarity)\n                    return self.agents[closest_agent_index], max_similarity\n            return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not purpose_embedding:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            if max_similarity == 0:\n                return None, -np.inf\n            closest_agent_index = similarities.index(max_similarity)\n            return self.agents[closest_agent_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not self.agents:\n                return None, -np.inf\n\n            # Calculate the cosine similarity between the purpose embedding and the purpose embeddings of each agent\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n\n            # Find the index of the agent with the highest similarity score\n            max_index = np.argmax(similarities)\n\n            # Return the agent with the highest similarity score\n            return self.agents[max_index], similarities[max_index]\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not isinstance(purpose_embedding, np.ndarray):\n                raise ValueError(\"Invalid input type for purpose_embedding. Expected np.ndarray.\")\n\n            if len(purpose_embedding) != 1536:\n                raise ValueError(\"Invalid input shape for purpose_embedding. Expected (1, 1536).\")\n\n            if len(self.agents) == 0:\n                logger.warning(\"No agents found.\")\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            max_similarity_index = similarities.index(max_similarity)\n            closest_agent = self.agents[max_similarity_index]\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) < 250:\n                return self.agents[0], 1.0\n\n            max_similarity = -np.inf\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([e], [purpose_embedding])[0][0] for e in [agent.purpose_embedding for agent in self.agents]]\n            max_similarity = max(similarities)\n            if max_similarity < 0.999:\n                return None, -np.inf\n            closest_agent_index = similarities.index(max_similarity)\n            return self.agents[closest_agent_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, -np.inf\n\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([e], [purpose_embedding])[0][0] for e in [agent.purpose_embedding for agent in self.agents]]\n            if similarities:\n                max_similarity = max(similarities)\n                max_similarity_index = similarities.index(max_similarity)\n                return self.agents[max_similarity_index], max_similarity\n            else:\n                return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not isinstance(purpose_embedding, np.ndarray):\n                logger.exception(\"Invalid input type. Expected np.ndarray\")\n                raise ValueError(\"Invalid input type. Expected np.ndarray\")\n\n            if len(self.agents) < 2:\n                logger.exception(\"Insufficient agents to compare\")\n                raise ValueError(\"Insufficient agents to compare\")\n\n            max_similarity = -np.inf\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                   agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not isinstance(purpose_embedding, np.ndarray) or len(purpose_embedding) != 1536:\n                logger.exception(\"Invalid purpose embedding format\")\n                raise ValueError(\"Invalid purpose embedding format\")\n\n            # Calculate the cosine similarity between the purpose embedding and the purpose embeddings of each agent\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n\n            # Find the index of the agent with the highest similarity score\n            if len(similarities) > 0:\n                closest_agent_index = np.argmax(similarities)\n                return self.agents[closest_agent_index], similarities[closest_agent_index]\n            else:\n                return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarity_threshold = self.calculate_similarity_threshold()\n            logger.info(f\"Similarity threshold: {similarity_threshold}\")\n\n            closest_agent = None\n            highest_similarity = -np.inf\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n                logger.info(f\"Similarity between {agent.purpose} and purpose: {similarity}\")\n\n                if similarity > highest_similarity and similarity > similarity_threshold:\n                    highest_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, highest_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, -np.inf\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            self.openai_wrapper,\n            PRIME_PROMPT,\n            PRIME_NAME,\n            PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(MicroAgent(PRIME_PROMPT, PRIME_NAME, PRIME_AGENT_WEIGHT, True, False))\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            self.openai_wrapper,\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True,\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            flags={\"prime\": True, \"unspecified\": True},\n            openai_wrapper=self.openai_wrapper\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        logger.info(\"Creating prime agent\")\n        prime_agent = MicroAgent(self.openai_wrapper, PRIME_PROMPT, PRIME_NAME, PRIME_AGENT_WEIGHT, True, False)\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            flags={\"prime\": True, \"unspecified\": True},\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            flags={\"prime\": True, \"unspecified\": True},\n            openai_wrapper=self.openai_wrapper,\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        logger.info(\"Creating prime agent\")\n        prime_agent = MicroAgent(\n            name=PRIME_NAME,\n            prompt=PRIME_PROMPT,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True,\n            openai_wrapper=self.openai_wrapper\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            self.openai_wrapper,\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        # Create the prime agent\n        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            is_prime=True,\n            unspecified_flag=True,\n            openai_wrapper=self.openai_wrapper\n        )\n\n        # Add the prime agent to the agent list\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            name=PRIME_NAME,\n            prompt=PRIME_PROMPT,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        # Create the prime agent\n        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, PRIME_AGENT_WEIGHT, True, False)\n\n        # Add the prime agent to the agent list\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        # Create the prime agent\n        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            flags={\"prime\": True, \"unspecified\": True}\n        )\n\n        # Add the prime agent to the agent list\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(MicroAgent(\n            self.openai_wrapper,\n            PRIME_PROMPT,\n            PRIME_NAME,\n            PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True\n        ))\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            name=PRIME_NAME,\n            prompt=PRIME_PROMPT,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True,\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(MicroAgent(\n            name=PRIME_NAME,\n            prompt=PRIME_PROMPT,\n            weight=PRIME_AGENT_WEIGHT,\n            flags={\"prime\": True, \"unspecified\": True},\n            openai_wrapper=self.openai_wrapper,\n            agent_persistence=self.agent_persistence\n        ))\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            self.openai_wrapper,\n            PRIME_PROMPT,\n            PRIME_NAME,\n            PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified_flag=True\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(\n            MicroAgent(\n                prompt=PRIME_PROMPT,\n                name=PRIME_NAME,\n                weight=PRIME_AGENT_WEIGHT,\n                flags={\"prime\": True, \"unspecified\": True},\n            )\n        )\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            flags={\"prime\": True, \"unspecified\": True},\n            openai_wrapper=self.openai_wrapper\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is None:\n            return None\n        return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is None:\n            return None\n        else:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent_with_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent_with_purpose(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n        return None\n\n    def load_agent_with_id(self, agent_id, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified ID from the database. If an agent with the given ID is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_id: str, The ID of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        agent_data = self.persistence.load_agent(purpose)\n        if agent_data is None:\n            return None\n        return AgentSerializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is None:\n            return None\n        return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n\n    def get_all_agents(self):\n        \"\"\"\n        Get all agents from the database.\n        \"\"\"\n        return self.persistence.get_all_agents()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent_with_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent_with_purpose(purpose)\n        if serialized_agent is not None:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n\n    \"\"\"\n    Loads an agent with a specified id from the database. If an agent with the given id is found, it is deserialized and returned; otherwise, None is returned.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_id: str, The id of the agent to be loaded. It is used to identify the agent in the database.\n    :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n    :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n    :return: An instance of the deserialized agent if found, otherwise None.\n    \"\"\"\n    @memoize_to_sqlite\n    def load_agent_with_id(self, agent_id, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent_with_id(agent_id)\n        if serialized_agent is not None:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n\n    \"\"\"\n    Loads an agent with a specified id from the database. If an agent with the given id is found, it is deserialized and returned; otherwise, None is returned.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent_by_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent_by_purpose(purpose)\n        if serialized_agent is not None:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n\n    def load_agent_by_id(self, agent_id, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent_by_id(agent_id)\n        if serialized_agent is not None:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n        else:\n            return None\n\n    def clear_all_agents(self):\n        \"\"\"\n        Clear all agents from the database.\n        \"\"\"\n        self.persistence.clear_all_agents()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n\n    def get_all_agents(self):\n        \"\"\"\n        Retrieve all agents from the database.\n        \"\"\"\n        serialized_agents = self.persistence.get_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.deserialize(serialized_agent)\n            agents.append(agent)\n        return agents"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent_by_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent_by_purpose(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n        return None\n\n    \"\"\"\n    Loads an agent with a specified id from the database. If an agent with the given id is found, it is deserialized and returned; otherwise, None is returned.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_id: str, The id of the agent to be loaded. It is used to identify the agent in the database.\n    :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n    :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n    :return: An instance of the deserialized agent if found, otherwise None.\n    \"\"\"\n    @memoize_to_sqlite\n    def load_agent_by_id(self, agent_id, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent_by_id(agent_id)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n        return None"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n        else:\n            return None\n\n    def load_agent_with_memoization(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database and applies memoization to it. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            agent = memoize_to_sqlite(agent)\n            return agent\n        else:\n            return None"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent_with_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent_with_purpose(purpose)\n        if serialized_agent is not None:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n\n    \"\"\"\n    Loads a prime agent from the database. If a prime agent is found, it is deserialized and returned; otherwise, None is returned.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n    :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n    :return: An instance of the deserialized agent if found, otherwise None.\n    \"\"\"\n\n    @memoize_to_sqlite\n    def load_prime_agent(self, agent_lifecycle, openai_"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite(db_filename=\"agents.db\", table_name=\"agents\")\n    def load_agent_with_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent_with_purpose(purpose)\n        if serialized_agent is None:\n            return None\n        return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n\n    def load_agent_with_id(self, agent_id, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent_with_id(agent_id)\n        if serialized_agent is None:\n            return None\n        return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent_by_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n\n        serialized_agent = self.persistence.load_agent_by_purpose(purpose)\n        if serialized_agent is None:\n            return None\n\n        return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and deserializes them.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: A list of deserialized agents.\n        \"\"\"\n\n        serialized_agents = self.persistence.load_all_agents()\n        return [AgentSerializer.deserialize(serialized_agent, agent_l"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n\n    def get_all_agents(self):\n        return self.persistence.get_all_agents()\n\n    def get_agent_by_id(self, agent_id):\n        return self.persistence.get_agent_by_id(agent_id)\n\n    def get_agent_by_purpose(self, purpose):\n        return self.persistence.get_agent_by_purpose(purpose)\n\n    def get_agent_by_purpose_and_lifecycle(self, purpose, lifecycle):\n        return self.persistence.get_agent_by_purpose_and_lifecycle(purpose, lifecycle)\n\n    def get_agent_by_purpose_and_lifecycle_and_id(self, purpose, lifecycle, agent_id):\n        return self.persistence.get_agent_by_purpose_and_lifecycle_and_id(purpose, lifecycle, agent_id)\n\n    def get_agent_by_purpose_and_lifecycle_and_id_and_version(self, purpose, lifecycle, agent_id, version):\n        return self.persistence.get_agent_by_purpose_and_lifecycle_and_id_and_version(purpose, lifecycle, agent_id, version)\n\n    def get_agent_by_purpose_and_lifecycle_and_id_and_version_and_model(self, purpose, lifecycle, agent_id, version, model):\n        return self.persistence.get_agent_by_purpose_and_lifecycle_and_id_and_version_and_model(purpose, lifecycle, agent_id, version, model"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite(db_filename=\"agents.db\")\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n        else:\n            return None\n\n    def list_agents(self):\n        \"\"\"\n        Returns a list of all agents in the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :return: list of str. A list of agent purposes.\n        \"\"\"\n        return self.persistence.list_agents()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite(db_filename=\"agents.db\")\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is None:\n            return None\n\n        return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n\n    def list_agents(self):\n        \"\"\"\n        Returns a list of all agent purposes in the database.\n        \"\"\"\n        return self.persistence.list_agents()\n\n    def get_agent_count(self):\n        \"\"\"\n        Returns the total number of agents in the database.\n        \"\"\"\n        return self.persistence.get_agent_count()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite(db_filename=\"agents.db\")\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is None:\n            return None\n        return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n\n    def load_all_agents(self):\n        serialized_agents = self.persistence.load_all_agents()\n        return [AgentSerializer.deserialize(agent) for agent in serialized_agents]\n\n    def load_agent_by_id(self, agent_id):\n        serialized_agent = self.persistence.load_agent_by_id(agent_id)\n        if serialized_agent is None:\n            return None\n        return AgentSerializer.deserialize(serialized_agent)\n\n    def load_agent_by_purpose(self, purpose):\n        serialized_agent = self.persistence.load_agent_by_purpose(purpose)\n        if serialized_agent is None:\n            return None\n        return AgentSerializer.deserialize(serialized_agent)\n\n    def load_agent_by_purpose_and_lifecycle(self, purpose, lifecycle):\n        serialized_agent = self.persistence.load_agent_by_purpose_and_lifecycle(purpose, lifecycle)\n        if serialized_agent is None:\n            return None\n        return AgentSerializer.deserialize(serialized_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n\n        if serialized_agent is None:\n            return None\n\n        agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n\n        return agent\n\n    def save_agent_to_sqlite(self, agent):\n        \"\"\"\n        Save an agent to the SQLite database.\n        \"\"\"\n        serialized_agent = AgentSerializer.serialize(agent)\n        self.persistence.save_agent_to_sqlite(serialized_agent)\n\n    def load_agent_from_sqlite(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load an agent from the SQLite database.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent_from_sqlite(purpose)\n\n        if serialized_agent is None:\n            return None\n\n        agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n\n        return agent\n\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load all agents from the SQLite database.\n        \"\"\"\n        serialized_agents = self.persistence.load_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            agents.append(agent)\n        return agents\n\n    def load_agent_by_id(self, agent_id, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load an agent from the SQLite database by its ID.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent_by_id(agent_id)\n\n        if serialized_agent is None:\n            return None"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        return [AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper) for serialized_agent in serialized_agents]"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def clear_all_agents(self):\n        \"\"\"\n        Clear all agents from the database.\n        \"\"\"\n        self.persistence.clear_all_agents()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        if serialized_agents:\n            return [AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper) for serialized_agent in serialized_agents]\n        return []"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def clear_agents(self):\n        self.persistence.clear_agents()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite(\"agents.db\")\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n\n        return agents\n\n    \"\"\"\n    Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n    :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n    :return: list. A list of agents that have been successfully loaded from the database.\n    \"\"\""}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n\n        return agents\n\n    def clear_database(self):\n        \"\"\"\n        Clear the database of all agents.\n        \"\"\"\n        self.persistence.clear_database()\n\n    def close(self):\n        \"\"\"\n        Close the persistence manager.\n        \"\"\"\n        self.persistence.close()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite(\"load_all_agents\")\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def remove_all_agents(self):\n        \"\"\"\n        Removes all agents from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        \"\"\"\n        self.persistence.remove_all_agents()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load all agents from the database and return a list of these agents if they are successfully loaded.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def save_all_agents(self, agents):\n        \"\"\"\n        Save all agents to the database.\n        \"\"\"\n        serialized_agents = [AgentSerializer.serialize(agent) for agent in agents]\n        self.persistence.save_agents(serialized_agents)\n\n    def remove_all_agents(self):\n        \"\"\"\n        Remove all agents from the database.\n        \"\"\"\n        self.persistence.remove_all_agents()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def load_all_agents_with_memoization(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process. Additionally, memoization is applied to the agent's behavior to improve performance.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database and"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load all agents from the database and return a list of these agents if they are successfully loaded.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(\n                serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def load_all_agents_with_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load all agents with the given purpose from the database and return a list of these agents if they are successfully loaded.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents_with_purpose(purpose)\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(\n                serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def load_all_agents_with_purpose_and_state(self, purpose, state, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load all agents with the given purpose and state from the database and return a list of these agents if they are successfully loaded.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents_with_purpose_and_state(\n            purpose, state)\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(\n                serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def load_all_agents_with_purpose_and_state_and_role(self, purpose, state, role, agent_lifecycle,"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def get_all_agent_purposes(self):\n        \"\"\"\n        Retrieves all agent purposes from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :return: list. A list of agent purposes that have been retrieved from the database.\n        \"\"\"\n        return self.persistence.fetch_all_agent_purposes()"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def load_all_agents_with_purpose(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents with a specific purpose from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str. The purpose of the agents to be loaded from the database.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database with the specified purpose.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents_with_purpose(purpose)\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def load_all_agents_with_purpose_and_state(self, purpose, state, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents with a specific purpose and state from the database and returns a list of these agents if they are successfully loaded. Each agent"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def save_agent_state(self, agent):\n        \"\"\"\n        Serialize and save the agent state if it is a working agent and not a prime agent.\n        \"\"\"\n        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n\n    def remove_agent_state(self, agent):\n        \"\"\"\n        Remove an agent from the database.\n        \"\"\"\n        self.persistence.remove_agent(agent.id)\n\n    def save_agent_state_if_exists(self, agent):\n        \"\"\"\n        Serialize and save the agent state if it is a working agent and not a prime agent.\n        \"\"\"\n        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n        agents = self.persistence.fetch_all_agents()\n        loaded_agents = []\n        for agent in agents:\n            loaded_agent = AgentSerializer.from_dict(agent, agent_lifecycle, openai_wrapper)\n            if loaded_agent:\n                loaded_agents.append(loaded_agent)\n        return loaded_agents\n\n    def save_agent_response(self, agent_id, response):\n        \"\"\"\n        Save the response of an agent to the database.\n        \"\"\"\n        self.persistence.save_agent_response(agent_id, response)\n\n    def load_agent_response(self, agent_id):\n        \"\"\"\n        Load the response of an agent from the database.\n        \"\"\"\n        return self.persistence.load_agent_response(agent_id)\n\n    def save_agent_request(self, agent_id, request):\n        \"\"\"\n        Save the request of an agent to the database.\n        \"\"\"\n        self.persistence.save_agent_request(agent_id, request)\n\n    def load_agent_request(self, agent_id):\n        \"\"\"\n        Load the request of an agent from the database.\n        \"\"\"\n        return self.persistence.load_"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite(\"agents.db\")\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    \"\"\"\n    Saves the agent state to the database.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent: The agent to be saved.\n    \"\"\"\n\n    def save_agent_state(self, agent):\n        self.persistence.save_agent_state(agent.id, agent.state)\n\n    \"\"\"\n    Loads the agent state from the database.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_id: The ID of the agent whose state is to be loaded.\n    :return: dict. The state of the agent with the specified ID.\n    \"\"\"\n\n    def load_agent_state(self, agent_id):\n        return self.persistence.fetch_agent_state(agent_id)\n\n    \"\"\"\n    Saves the agent state to the database.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n    :param agent_id: The ID of the agent whose state is to be saved.\n    :param state: The state of the agent to be saved.\n    \"\"\"\n\n    def save_agent_state(self, agent_id, state):\n        self.persistence.save_agent_state(agent_id, state)\n\n    \"\"\"\n    Loads the agent state from the database.\n\n    Input-Output Arguments\n    :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def save_agent_lifecycle(self, agent):\n        \"\"\"\n        Serialize and save the agent lifecycle state.\n        \"\"\"\n        self.persistence.save_agent_lifecycle(agent.id, agent.lifecycle.get_state())\n\n    def load_agent_lifecycle(self, agent):\n        \"\"\"\n        Load the agent lifecycle state.\n        \"\"\"\n        state = self.persistence.fetch_agent_lifecycle(agent.id)\n        if state:\n            agent.lifecycle.set_state(state)\n\n    def save_agent_memory(self, agent):\n        \"\"\"\n        Serialize and save the agent memory state.\n        \"\"\"\n        self.persistence.save_agent_memory(agent.id, agent.memory.get_state())\n\n    def load_agent_memory(self, agent):\n        \"\"\"\n        Load the agent memory state.\n        \"\"\"\n        state = self.persistence.fetch_agent_memory(agent.id)\n        if state:\n            agent.memory.set_state(state)\n\n    def save_agent_conversation_history(self, agent):\n        \"\"\"\n        Serialize and save the agent conversation history state.\n        \"\"\"\n        self.persistence.save_agent_conversation_history(agent.id, agent.conversation_history.get_state())\n\n    def load_agent_conversation_history(self, agent):\n        \"\"\"\n        Load the agent conversation history state.\n        \"\"\"\n        state = self.persistence.fetch_agent_conversation_history(agent.id"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(\"Error while saving agent: %s\", e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(\"Error saving agent: %s\", str(e))\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(\"Error while saving agent: %s\", e)\n            raise\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error saving agent {agent.id}: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(\"Error while saving agent: %s\", e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error while saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error occurred while saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(\"Error occurred while saving agent: %s\", str(e))\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error occurred while saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error saving agent {agent.id}: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(\"Error saving agent: %s\", e)\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error while saving agent: {e}\")\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(\"Error saving agent: %s\", str(e))\n            raise e\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(\"Error saving agent: %s\", e)\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up the agents\n        self.agent_lifecycle.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agent_lifecycle.agents\n\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up agents\n        self.agent_lifecycle.cleanup_agents()\n        # Return the current list of agents\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up the agents\n        self.agent_lifecycle.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                examples=EXAMPLES,\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                sample_input=sample_input\n            )\n\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input\n        )\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                examples=EXAMPLES,\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                sample_input=sample_input,\n            )\n\n            return prompt\n\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                examples=EXAMPLES,\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                sample_input=sample_input\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                examples=EXAMPLES,\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                sample_input=sample_input\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                examples=EXAMPLES,\n                sample_input=sample_input\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                goal=goal,\n                examples=EXAMPLES,\n                sample_input=sample_input\n            )\n\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                examples=EXAMPLES,\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                sample_input=sample_input\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        goal = goal.replace(\"\\n\", \" \")\n        sample_input = sample_input.replace(\"\\n\", \" \")\n        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            sample_input=sample_input,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT\n        )\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                prompt_engineering_system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                goal=goal,\n                examples=EXAMPLES,\n                sample_input=sample_input\n            )\n\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                goal=goal,\n                sample_input=sample_input,\n                examples=EXAMPLES\n            )\n            logger.info(f\"Prompt: {prompt}\")\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                examples=EXAMPLES,\n                sample_input=sample_input\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            examples=EXAMPLES,\n            sample_input=sample_input\n        )\n\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt_engineering_prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                examples=EXAMPLES,\n                prompt_engineering_system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT\n            )\n            prompt_engineering_response = self.openai_wrapper.get_chat_completion(prompt_engineering_prompt)\n            prompt = prompt_engineering_response.choices[0].message.content\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            examples=EXAMPLES,\n            sample_input=sample_input\n        )\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                goal=goal,\n                examples=EXAMPLES,\n                sample_input=sample_input\n            )\n\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt_template = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT\n        )\n\n        try:\n            prompt = self.openai_wrapper.get_chat_completion(prompt_template, sample_input)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n\n        return prompt\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                examples=EXAMPLES,\n                goal=goal,\n                sample_input=sample_input\n            )\n\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                examples=EXAMPLES,\n                sample_input=sample_input,\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input\n        )\n\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n                         (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict[\"data\"])))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))\n            conn.commit()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            result = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,)).fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is None:\n                return None\n            return json.loads(row[0])"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is None:\n                return None\n            return json.loads(row[0])"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            result = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,)).fetchone()\n            if result is not None:\n                return json.loads(result[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = cursor.fetchall()\n            return [purpose[0] for purpose in purposes]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = cursor.fetchall()\n            return [purpose[0] for purpose in purposes]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = [row[0] for row in cursor.fetchall()]\n        return purposes\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\"SELECT result FROM cache WHERE hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\"SELECT result FROM cache WHERE hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            result = json.loads(result[0])\n        return result\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        result = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        ).fetchone()\n        if result is not None:\n            return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash=?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            result = json.loads(result[0])\n        return result\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            result = json.loads(result[0])\n        return result\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash=?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash=?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        result = json.loads(row[0])\n        return result\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            result = json.loads(result[0])\n        return result\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT OR REPLACE INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT OR REPLACE INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache (hash, result) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config_from_args(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of printing it to the terminal\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, \"log.txt\"), \"w\") as f:\n            with redirect_stdout(f):\n                CommandLine(deepcopy(ARGS)).run()\n    else:\n        CommandLine(deepcopy(ARGS)).run()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(vars(args))\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of printing it to the terminal\n    if quiet_mode:\n        with open(\"output.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                CommandLine(ARGS).run()\n    else:\n        CommandLine(ARGS).run()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config_from_args(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of printing it to the terminal\n    if quiet_mode:\n        with open(os.path.join(CONFIG.get_record_dir(), \"log.txt\"), \"w\") as f:\n            with redirect_stdout(f):\n                CommandLine(CONFIG, ARGS).run()\n    else:\n        CommandLine(CONFIG, ARGS).run()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(ARGS, vars(args))\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.get('record_dir'), 'log.txt'), 'w') as f:\n            with redirect_stdout(f):\n                # Execute the command line process\n                CommandLine(deepcopy(CONFIG)).run()\n    else:\n        # Execute the command line process\n        CommandLine(deepcopy(CONFIG)).run()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config_params(args)\n    ARGS.update_args(args)\n\n    # Create a CommandLine object\n    command_line = CommandLine(CommandLineParam(CONFIG, ARGS))\n\n    # If quiet mode is enabled, redirect output to a file\n    if quiet_mode:\n        with open('log.txt', 'w') as f:\n            with redirect_stdout(f):\n                command_line.run()\n    else:\n        command_line.run()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, \"output.txt\"), \"w\") as f:\n            with redirect_stdout(f):\n                CommandLine(ARGS).run()\n    else:\n        CommandLine(ARGS).run()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(ARGS)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of printing it to the terminal\n    if quiet_mode:\n        with open(os.path.join(CONFIG[\"record_dir\"], \"output.txt\"), \"w\") as f:\n            with redirect_stdout(f):\n                CommandLine(args).run()\n    else:\n        CommandLine(args).run()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    if args.config_file:\n        CONFIG.update_config_file(args.config_file)\n\n    if args.quiet:\n        with open(\"output.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                CommandLine.run(\n                    task=args.task,\n                    upload_files=args.upload_files,\n                    model=args.model,\n                    record_dir=args.record_dir,\n                    mode=args.mode,\n                    max_subtask_chain_length=args.max_subtask_chain_length,\n                    enable_ask_human_for_help=args.enable_ask_human_for_help,\n                    max_plan_refine_chain_length=args.max_plan_refine_chain_length,\n                    max_plan_tree_depth=args.max_plan_tree_depth,\n                    max_plan_tree_width=args.max_plan_tree_width,\n                    max_retry_times=args.max_retry_times\n                )\n    else:\n        CommandLine.run(\n            task=args.task,\n            upload_files=args.upload_files,\n            model=args.model,\n            record_dir=args.record_dir,\n            mode=args.mode,\n            max_subtask_chain_length=args.max_subtask_chain_length,\n            enable_ask_human_for_help=args.enable_ask_human_for_help,\n            max_plan_refine_chain_length=args.max_plan_refine_chain_length,\n            max_plan_tree_depth=args.max_plan_tree_depth,\n            max_plan_tree_width=args.max_plan_tree_width,\n            max_retry_times=args.max_retry_times\n        )\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_from_dict(args.__dict__)\n\n    # Create a CommandLine object with the provided arguments\n    command_line = CommandLine(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, 'log.txt'), 'w') as f:\n            with redirect_stdout(f):\n                command_line.run()\n    else:\n        # Otherwise, run the command line process\n        command_line.run()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_from_args(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(\"log.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                # Execute the command line process\n                CommandLine(ARGS).start()\n    else:\n        # Execute the command line process\n        CommandLine(ARGS).start()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(args)\n\n    # Check if quiet mode is enabled and redirect output to a file if necessary\n    if quiet_mode:\n        with open(CONFIG.get_config_param(\"quiet_mode_output_file\"), \"w\") as f:\n            with redirect_stdout(f):\n                # Execute the command line process\n                CommandLine(args).execute()\n    else:\n        # Execute the command line process\n        CommandLine(args).execute()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config_from_args(args)\n    # Create a CommandLine instance with the provided arguments\n    command_line = CommandLine(args)\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, 'output.log'), 'w') as f:\n            with redirect_stdout(f):\n                # Execute the command line process\n                command_line.execute()\n    else:\n        # Execute the command line process\n        command_line.execute()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(args.__dict__)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, \"log.txt\"), \"w\") as f:\n            with redirect_stdout(f):\n                CommandLine.execute_command_line_process()\n    else:\n        # Otherwise, execute the command line process normally\n        CommandLine.execute_command_line_process()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(args.__dict__)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(os.path.join(ARGS.record_dir, 'log.txt'), 'w') as f:\n            with redirect_stdout(f):\n                # Execute the command line process\n                CommandLine(ARGS).execute()\n    else:\n        # Execute the command line process\n        CommandLine(ARGS).execute()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_from_args(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of printing it to the terminal\n    if quiet_mode:\n        with open(CONFIG.get_value('record_dir') + '/stdout.txt', 'w') as f:\n            with redirect_stdout(f):\n                CommandLine().execute()\n    else:\n        CommandLine().execute()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_from_args(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of printing it to the terminal\n    if quiet_mode:\n        with open(\"output.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                CommandLine(args.task, args.model, args.upload_files, args.record_dir, args.mode).execute()\n    else:\n        CommandLine(args.task, args.model, args.upload_files, args.record_dir, args.mode).execute()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config_params(\n        args,\n        ARGS.task,\n        ARGS.upload_files,\n        ARGS.model,\n        ARGS.record_dir,\n        ARGS.mode,\n        ARGS.max_subtask_chain_length,\n        ARGS.enable_ask_human_for_help,\n        ARGS.max_plan_refine_chain_length,\n        ARGS.max_plan_tree_depth,\n        ARGS.max_plan_tree_width,\n        ARGS.max_retry_times,\n        ARGS.config_file\n    )\n\n    # Create a CommandLine object\n    command_line = CommandLine()\n\n    # If quiet mode is enabled, redirect output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.RECORD_DIR, 'output.txt'), 'w') as f:\n            with redirect_stdout(f):\n                # Execute the command line process\n                command_line.execute_command_line_process()\n    else:\n        # Execute the command line process\n        command_line.execute_command_line_process()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(args)\n\n    # Create a CommandLine object\n    command_line = CommandLine()\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, 'log.txt'), 'w') as f:\n            with redirect_stdout(f):\n                # Execute the command line process\n                command_line.execute()\n    else:\n        # Execute the command line process\n        command_line.execute()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    global CONFIG\n    global ARGS\n\n    # Update global configuration parameters with the provided arguments\n    CONFIG.update(args)\n    ARGS = deepcopy(CONFIG)\n\n    # Create a CommandLine object\n    command_line = CommandLine(CONFIG)\n\n    # Set the output file path for the command line process\n    output_file_path = os.path.join(CONFIG['record_dir'], 'command_line.txt')\n\n    # If quiet mode is enabled, redirect the standard output to the output file\n    if quiet_mode:\n        with open(output_file_path, 'w') as f:\n            with redirect_stdout(f):\n                command_line.execute()\n    else:\n        # If quiet mode is not enabled, execute the command line process\n        command_line.execute()\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_params(\n        model=args.model,\n        record_dir=args.record_dir,\n        mode=args.mode,\n        max_subtask_chain_length=args.max_subtask_chain_length,\n        enable_ask_human_for_help=args.enable_ask_human_for_help,\n        max_plan_refine_chain_length=args.max_plan_refine_chain_length,\n        max_plan_tree_depth=args.max_plan_tree_depth,\n        max_plan_tree_width=args.max_plan_tree_width,\n        max_retry_times=args.max_retry_times,\n    )\n\n    # Create a CommandLine object with the provided arguments\n    command_line = CommandLine(\n        task=args.task,\n        upload_files=args.upload_files,\n        quiet_mode=quiet_mode,\n        config_file=args.config_file,\n    )\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, \"log.txt\"), \"w\") as f:\n            with redirect_stdout(f):\n                command_line.execute()\n    else:\n        command_line.execute()\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.user_message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = open"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.user_message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.user_message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.user_message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.user_message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or (self._last_time and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or (self._last_time and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or (time() - self._last_time) > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or (\n            time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or (time() - self._last_time) > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None\n            and time() - self._last_time >= self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The state_dict method should not be called from a DataLoader worker process. \"\n                \"Please use the state_dict method from the StreamingDataLoader class instead.\"\n            )\n\n        state: Dict[str, Any] = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        self._state_dict = state\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"This method should be called from the main process.\")\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return self._state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"state_dict() can only be called from the main process.\")\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"input_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return self._state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"Calling `state_dict` from a DataLoader worker process is not allowed.\")\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"input_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader is not None else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return self._state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The state_dict method should not be called within a DataLoader worker process.\")\n\n        if self.cache is None:\n            self.worker_env = _WorkerEnv.detect()\n            self.cache = self._create_cache(worker_env=self.worker_env)\n            self.shuffler = self._create_shuffler(self.cache)\n\n        if self.shuffler is None:\n            raise ValueError(\"The shuffler is not initialized.\")\n\n        if self.item_loader is None:\n            raise ValueError(\"The item loader is not initialized.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"input_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict(),\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        self._state_dict = state\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The state_dict method should not be called from a DataLoader worker process.\")\n\n        state: Dict[str, Any] = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        if self.serializers:\n            state[\"serializers\"] = {k: v.state_dict() for k, v in self.serializers.items()}\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method cannot be called from a DataLoader worker process. \"\n                \"Please use the `state_dict` method from the main process instead.\"\n            )\n\n        if self._state_dict is not None:\n            state: Dict[str, Any] = self._state_dict\n            state[\"num_samples_yielded\"] = num_samples_yielded\n            state[\"num_workers\"] = num_workers\n            state[\"batch_size\"] = batch_size\n            return state\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader is not None else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return self._state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The `state_dict` method is not supported within a DataLoader worker process.\")\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"input_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return self._state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"The state_dict method should not be called from a DataLoader worker process.\")\n\n        if self.item_loader is not None:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader_state\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"The `state_dict` method can only be called from the main process.\")\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path if self.input_dir.path else self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return self._state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"state_dict can only be called from the main process\")\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return self._state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The state_dict method should not be called from a DataLoader worker process. \"\n                \"Please ensure that it is only called from the main process.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        if self.item_loader:\n            state[\"item_loader_state\"] = self.item_loader.state_dict()\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The `state_dict` method should only be called from the main process. \"\n                \"This method is used to save the state of the dataset when it is used within a DataLoader. \"\n                \"It is not intended to be called from a DataLoader worker process.\"\n            )\n\n        state: Dict[str, Any] = {}\n        state[\"num_samples_yielded\"] = num_samples_yielded\n        state[\"num_workers\"] = num_workers\n        state[\"batch_size\"] = batch_size\n        state[\"current_epoch\"] = self.current_epoch\n        state[\"input_dir\"] = self.input_dir.path\n        state[\"input_dir_url\"] = self.input_dir.url\n        state[\"item_loader_state\"] = self.item_loader.state_dict() if self.item_loader else None\n        state[\"last_batch_dropped\"] = self.drop_last\n        state[\"seed\"] = self.seed\n        state[\"world_size\"] = self.distributed_env.world_size\n        state[\"shuffle\"] = self.shuffle\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"state_dict should only be called from the main process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path if self.input_dir.path else self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        self._state_dict = state\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"state_dict() can only be called from the main process.\")\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"input_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return self._state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"state_dict() can only be called from the main process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"input_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        self._state_dict = state\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The `state_dict` method should not be called from a DataLoader worker process.\"\n                \" This method is used to generate the state_dict for the StreamingDataset, which is then used to resume the dataset when the StreamingDataLoader is re-initialized.\"\n            )\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader is not None else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return self._state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"The state_dict method should not be called from a DataLoader worker process.\")\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n        return self._state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"state_dict should be called from the main process.\")\n\n        state: Dict[str, Any] = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        self._state_dict = state\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"You can only call `state_dict` on the main process. \"\n                \"If you are using a distributed DataLoader, you can call `state_dict` on the main process \"\n                \"and use the returned state_dict to restart the StreamingDataset on the worker processes.\"\n            )\n\n        # Check if the input directory is a local directory\n        is_local_dir = os.path.isdir(self.input_dir.path)\n\n        # Check if the input directory is a remote directory\n        is_remote_dir = self.input_dir.url is not None\n\n        # If the input directory is a local directory, get the directory path\n        if is_local_dir:\n            dir_path = self.input_dir.path\n        # If the input directory is a remote directory, get the directory path\n        elif is_remote_dir:\n            dir_path = self.input_dir.url\n        # If the input directory is neither a local nor a remote directory, raise an error\n        else:\n            raise ValueError(\"The input directory is neither a local nor a remote directory.\")\n\n        # Get the state of the item loader\n        item_loader_state = None\n        if self.item_loader is not None:\n            item_loader_state = self.item_loader.state_dict()\n\n        # Get the state of the shuffler\n        shuffler_state = None\n        if self.shuffler is not None:\n            shuffler_state = self.shuffler.state_dict()\n\n        # Get the state of the serializers\n        serializers_state = None\n        if self.serializers is not None:\n            serializers_state = {\n                serializer_name: serializer.state_dict() for serializer_name, serializer in self.serializers.items()\n            }\n\n        # Get the state of the cache\n        cache_state = None\n        if self.cache is not None:\n            cache_state = self.cache.state_dict()\n\n        # Get the state of the distributed environment"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if not _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_workers = state_dict[\"num_workers\"]\n        batch_size = state_dict[\"batch_size\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, self.worker_chunks, self.worker_intervals\n        )\n        indexes = _replay_sampling(state_dict[\"num_samples_yielded\"], batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        current_indexes = current_indexes[indexes[worker_rank] :]\n        self.current_indexes = current_indexes\n\n        # bump the chunk_index\n        self"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if self._state_dict is not None:\n            raise RuntimeError(\"The method `load_state_dict` should only be called once.\")\n\n        self._validate_state_dict(state_dict)\n        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._validate_state_dict(state_dict)\n        self._state_dict = state_dict\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.input_dir.path = state_dict[\"input_dir_path\"]\n        self.input_dir.url = state_dict[\"input_dir_url\"]\n        self.item_loader = self.item_loader.load_state_dict(state_dict[\"item_loader\"]) if self.item_loader else None\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.distributed_env = _DistributedEnv(state_dict[\"world_size\"])\n        self.shuffle = state_dict[\"shuffle\"]\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            raise RuntimeError(\"The method `load_state_dict` should only be called once.\")\n\n        self._validate_state_dict(state_dict)\n\n        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        # Handle restart\n        if self._state_dict:\n            self._validate_state_dict()\n            state: Dict[str, Any] = self._state_dict\n\n            self.current_epoch = state[\"current_epoch\"]\n            self.input_dir.path = state[\"input_dir_path\"]\n            self.input_dir.url = state[\"input_dir_url\"]\n            self.item_loader = self.item_loader.load_state_dict(state[\"item_loader\"]) if self.item_loader else None\n            self.drop_last = state[\"drop_last\"]\n            self.seed = state[\"seed\"]\n            self.distributed_env.world_size = state[\"world_size\"]\n            self.shuffle = state[\"shuffle\"]\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        self._validate_state_dict()\n\n        state: Dict[str, Any] = self._state_dict\n\n        self.input_dir.path = state[\"input_dir_path\"]\n        self.input_dir.url = state[\"input_dir_url\"]\n        self.item_loader = self.item_loader.load_state_dict(state[\"item_loader\"]) if self.item_loader else None\n        self.drop_last = state[\"drop_last\"]\n        self.seed = state[\"seed\"]\n        self.distributed_env.world_size = state[\"world_size\"]\n        self.shuffle = state[\"shuffle\"]\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if not _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._validate_state_dict(state_dict)\n        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._validate_state_dict(state_dict)\n        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n\n        self.input_dir = Dir(path=state_dict[\"input_dir_path\"], url=state_dict[\"input_dir_url\"])\n\n        self.item_loader = BaseItemLoader.from_state_dict(state_dict[\"item_loader\"])\n\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.distributed_env = _DistributedEnv(state_dict[\"world_size\"])\n        self.shuffle = state_dict[\"shuffle\"]\n\n        self.cache = self._create_cache(worker_env=_WorkerEnv.detect())\n        self.shuffler = self._create_shuffler(self.cache)\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._validate_state_dict(state_dict)\n        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if self._state_dict is not None:\n            raise RuntimeError(\"The method `load_state_dict` should only be called once.\")\n\n        self._validate_state_dict(state_dict)\n        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        self._validate_state_dict()\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.seed = state_dict[\"seed\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.shuffle = state_dict[\"shuffle\"]\n\n        self.input_dir.path = state_dict[\"input_dir_path\"]\n        self.input_dir.url = state_dict[\"input_dir_url\"]\n\n        if state_dict[\"item_loader\"]:\n            self.item_loader = BaseItemLoader.from_state_dict(state_dict[\"item_loader\"])\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Check if the state_dict is a dictionary\n        if not isinstance(state_dict, dict):\n            raise ValueError(\"The provided state_dict is not a dictionary.\")\n\n        # Check if the state_dict contains the required keys\n        required_keys = [\"num_samples_yielded\", \"num_workers\", \"batch_size\", \"current_epoch\", \"input_dir_path\", \"input_dir_url\", \"item_loader\", \"drop_last\", \"seed\", \"world_size\", \"shuffle\"]\n        missing_keys = [key for key in required_keys if key not in state_dict]\n        if missing_keys:\n            raise ValueError(f\"The state_dict is missing the following required keys: {missing_keys}\")\n\n        # Check if the input_dir_path and input_dir_url keys match\n        if state_dict[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\"The input_dir_path in the state_dict does not match the current StreamingDataset instance.\")\n        if state_dict[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\"The input_dir_url in the state_dict does not match the current StreamingDataset instance.\")\n\n        # Check if the world_size key matches the current distributed environment\n        if state_dict[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\"The world_size in the state_dict does not match the current distributed environment.\")\n\n        # Check if the shuffle key matches the current StreamingDataset instance\n        if state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"The shuffle key in the state_dict does not match the current StreamingDataset instance.\")\n\n        # Load the state into the StreamingDataset instance\n        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        state: Dict[str, Any] = self._state_dict\n\n        self.input_dir = Dir(state[\"input_dir_path\"], state[\"input_dir_url\"])\n        self.item_loader = self.item_loader.load_state_dict(state[\"item_loader\"]) if self.item_loader else None\n        self.shuffle = state[\"shuffle\"]\n        self.drop_last = state[\"drop_last\"]\n        self.seed = state[\"seed\"]\n        self.current_epoch = state[\"current_epoch\"]\n        self.distributed_env = _DistributedEnv(state[\"world_size\"])\n\n        self._create_cache(worker_env=_WorkerEnv(self.distributed_env.world_size, 0))\n        self.shuffler = self._create_shuffler(self.cache)\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._validate_state_dict(state_dict)\n        self._state_dict = state_dict\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self.distributed_env = _DistributedEnv.detect()\n\n        if self.distributed_env.world_size > 1:\n            if state_dict[\"drop_last\"]:\n                logger.warn(\n                    \"You're operating within a distributed environment and have enabled the `drop_last` option. \"\n                    \"Please note that this configuration may lead to training interruptions if your system depends \"\n                    \"on distributed collectives.\"\n                )\n            else:\n                state_dict[\"drop_last\"] = True\n\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.max_cache_size = state_dict[\"max_cache_size\"]\n\n        self.input_dir.path = state_dict[\"input_dir_path\"]\n        self.input_dir.url = state_dict[\"input_dir_url\"]\n\n        self.item_loader = BaseItemLoader.from_state_dict(state_dict[\"item_loader\"])\n\n        self.shuffle = state_dict[\"shuffle\"]\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        self._validate_state_dict()\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.distributed_env = _DistributedEnv(state_dict[\"world_size\"])\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self.item_loader = self.item_loader.load_state_dict(state_dict[\"item_loader\"]) if self.item_loader else None\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.shuffle = state_dict[\"shuffle\"]\n\n        # Handle restart\n        if self._state_dict:\n            self._resume(\n                chunks_replica=self._state_dict[\"chunks_replica\"],\n                intervals_replica=self._state_dict[\"intervals_replica\"],\n            )\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if self._state_dict is not None:\n            raise RuntimeError(\"The StreamingDataset has already been loaded.\")\n\n        self._validate_state_dict(state_dict)\n\n        self._state_dict = state_dict\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n\n        # Restore the input directory\n        self.input_dir = Dir(\n            path=state_dict[\"input_dir_path\"],\n            url=state_dict[\"input_dir_url\"],\n        )\n\n        # Restore the item loader\n        if state_dict[\"item_loader\"] is not None:\n            self.item_loader = self.item_loader.load_state_dict(state_dict[\"item_loader\"])\n\n        # Restore the drop_last\n        self.drop_last = state_dict[\"drop_last\"]\n\n        # Restore the seed\n        self.seed = state_dict[\"seed\"]\n\n        # Restore the shuffle\n        self.shuffle = state_dict[\"shuffle\"]\n\n        # Restore the distributed env\n        self.distributed_env = _DistributedEnv(\n            world_size=state_dict[\"world_size\"],\n            rank=0,\n            local_rank=0,\n            global_rank=0,\n        )\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._validate_state_dict(state_dict)\n        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n\n        # Re-create the cache\n        self.input_dir = Dir(path=state_dict[\"input_dir_path\"], url=state_dict[\"input_dir_url\"])\n        self.cache = self._create_cache(worker_env=_WorkerEnv.detect())\n        self.shuffler = self._create_shuffler(self.cache)\n\n        # Re-create the item loader\n        if state_dict[\"item_loader\"] is not None:\n            self.item_loader = BaseItemLoader.from_state_dict(state_dict[\"item_loader\"])\n\n        # Re-create the shuffler\n        if state_dict[\"shuffle\"]:\n            self.shuffle = True\n            self.shuffler = FullShuffle(self.cache, state_dict[\"seed\"], state_dict[\"drop_last\"])\n        else:\n            self.shuffle = False\n            self.shuffler = NoShuffle(self.cache, state_dict[\"seed\"], state_dict[\"drop_last\"])\n\n        # Re-create the distributed env\n        self.distributed_env = _DistributedEnv(state_dict[\"world_size\"], state_dict[\"rank\"])\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._validate_state_dict(state_dict)\n\n        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.distributed_env = _DistributedEnv(\n            world_size=state_dict[\"world_size\"],\n            global_rank=state_dict[\"global_rank\"],\n            local_rank=state_dict[\"local_rank\"],\n        )\n        self.seed = state_dict[\"seed\"]\n        self.shuffle = state_dict[\"shuffle\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.input_dir = Dir(state_dict[\"input_dir_path\"], state_dict[\"input_dir_url\"])\n\n        if self.item_loader is not None:\n            self.item_loader.load_state_dict(state_dict[\"item_loader\"])\n\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state dictionary is not set.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path in the state dictionary ({state['input_dir_path']}) \"\n                f\"does not match the current input directory path ({self.input_dir.path}).\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL in the state dictionary ({state['input_dir_url']}) \"\n                f\"does not match the current input directory URL ({self.input_dir.url}).\"\n            )\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle flag in the state dictionary ({state['shuffle']}) \"\n                f\"does not match the current shuffle flag ({self.shuffle}).\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed in the state dictionary ({state['seed']}) \"\n                f\"does not match the current seed ({self.seed}).\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last flag in the state dictionary ({state['drop_last']}) \"\n                f\"does not match the current drop_last flag ({self.drop_last}).\"\n            )\n\n        if state[\"num_workers\"] !="}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The StreamingDataset is set to shuffle={self.shuffle} but the state_dict is set to shuffle={state['shuffle']}\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The StreamingDataset is set to num_workers={self.worker_env.world_size} but the state_dict is set to num_workers={state['num_workers']}\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The StreamingDataset is set to input_dir_path={self.input_dir.path} but the state_dict is set to input_dir_path={state['input_dir_path']}\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The StreamingDataset is set to input_dir_url={self.input_dir.url} but the state_dict is set to input_dir_url={state['input_dir_url']}\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The StreamingDataset is set to seed={self.seed} but the state_dict is set to seed={state['seed']}\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The StreamingDataset is set to drop_last={self.drop_last} but the state_dict is set to drop_last={state['drop_last']}\"\n            )\n\n        if state[\"item_loader\"] is not None:\n            if self.item_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n        input_dir_path = state[\"input_dir_path\"]\n        input_dir_url = state[\"input_dir_url\"]\n        seed = state[\"seed\"]\n        drop_last = state[\"drop_last\"]\n        shuffle = state[\"shuffle\"]\n        item_loader_state = state[\"item_loader\"]\n\n        if num_workers != self.worker_env.world_size:\n            raise ValueError(\n                f\"The number of workers ({num_workers}) in the state dict does not match the current number of workers ({self.worker_env.world_size}).\"\n            )\n\n        if batch_size != self.worker_env.batch_size:\n            raise ValueError(\n                f\"The batch size ({batch_size}) in the state dict does not match the current batch size ({self.worker_env.batch_size}).\"\n            )\n\n        if input_dir_path != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path ({input_dir_path}) in the state dict does not match the current input directory path ({self.input_dir.path}).\"\n            )\n\n        if input_dir_url != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL ({input_dir_url}) in the state dict does not match the current input directory URL ({self.input_dir.url}).\"\n            )\n\n        if seed != self.seed:\n            raise ValueError(\n                f\"The seed ({seed}) in the state dict does not match the current seed ({self.seed}).\"\n            )\n\n        if drop_last != self.drop_last:\n            raise ValueError(\n                f\"The drop_last flag ({drop_last}) in the state dict does not"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The `world_size` in the state_dict ({state['world_size']}) \"\n                f\"does not match the current world_size ({self.distributed_env.world_size})\"\n            )\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The `shuffle` in the state_dict ({state['shuffle']}) \"\n                f\"does not match the current shuffle ({self.shuffle})\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The `input_dir_path` in the state_dict ({state['input_dir_path']}) \"\n                f\"does not match the current input_dir_path ({self.input_dir.path})\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The `input_dir_url` in the state_dict ({state['input_dir_url']}) \"\n                f\"does not match the current input_dir_url ({self.input_dir.url})\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The `seed` in the state_dict ({state['seed']}) \"\n                f\"does not match the current seed ({self.seed})\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The `drop_last` in the state_dict ({state['drop_last']}) \"\n                f\"does not match the current drop_last"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        if self._state_dict is None:\n            raise ValueError(\"The state_dict is empty.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path in the state_dict ({state['input_dir_path']}) \"\n                f\"does not match the current input directory path ({self.input_dir.path}).\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL in the state_dict ({state['input_dir_url']}) \"\n                f\"does not match the current input directory URL ({self.input_dir.url}).\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The number of workers in the state_dict ({state['num_workers']}) \"\n                f\"does not match the current number of workers ({self.worker_env.world_size}).\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed in the state_dict ({state['seed']}) \"\n                f\"does not match the current seed ({self.seed}).\"\n            )\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state_dict is not initialized.\")\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The state_dict is not compatible with the current shuffle setting. \"\n                f\"The state_dict is set to {state['shuffle']} but the current shuffle setting is {self.shuffle}.\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The state_dict is not compatible with the current number of workers. \"\n                f\"The state_dict is set to {state['num_workers']} workers but the current number of workers is {self.worker_env.world_size}.\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The state_dict is not compatible with the current input directory path. \"\n                f\"The state_dict is set to {state['input_dir_path']} but the current input directory path is {self.input_dir.path}.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The state_dict is not compatible with the current input directory URL. \"\n                f\"The state_dict is set to {state['input_dir_url']} but the current input directory URL is {self.input_dir.url}.\"\n            )\n\n        if state[\"item_loader\"] != self.item_loader.state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        state: Dict[str, Any] = self._state_dict\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The number of workers is different between the state dict and the current state of the StreamingDataset. \"\n                f\"Expected: {state['num_workers']}, Found: {self.worker_env.world_size}\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path is different between the state dict and the current state of the StreamingDataset. \"\n                f\"Expected: {state['input_dir_path']}, Found: {self.input_dir.path}\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL is different between the state dict and the current state of the StreamingDataset. \"\n                f\"Expected: {state['input_dir_url']}, Found: {self.input_dir.url}\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed is different between the state dict and the current state of the StreamingDataset. \"\n                f\"Expected: {state['seed']}, Found: {self.seed}\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last flag is different between the state dict and the current state of the StreamingDataset. \"\n                f\"Expected: {state['drop_last']}, Found: {self.drop_last}\"\n            )\n\n        if state[\"shuffle\"] !="}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n        input_dir_path = state[\"input_dir_path\"]\n        input_dir_url = state[\"input_dir_url\"]\n        drop_last = state[\"drop_last\"]\n        seed = state[\"seed\"]\n        shuffle = state[\"shuffle\"]\n        world_size = state[\"world_size\"]\n\n        if self.worker_env.world_size != world_size:\n            raise ValueError(\n                f\"The world size of the worker environment ({self.worker_env.world_size}) \"\n                f\"does not match the world size of the StreamingDataset ({world_size}).\"\n            )\n\n        if self.distributed_env.world_size != world_size:\n            raise ValueError(\n                f\"The world size of the distributed environment ({self.distributed_env.world_size}) \"\n                f\"does not match the world size of the StreamingDataset ({world_size}).\"\n            )\n\n        if self.shuffle != shuffle:\n            raise ValueError(\n                f\"The shuffle flag of the StreamingDataset ({self.shuffle}) \"\n                f\"does not match the shuffle flag of the state dictionary ({shuffle}).\"\n            )\n\n        if self.seed != seed:\n            raise ValueError(\n                f\"The seed of the StreamingDataset ({self.seed}) \"\n                f\"does not match the seed of the state dictionary ({seed}).\"\n            )\n\n        if self.drop_last != drop_last:\n            raise ValueError(\n                f\"The drop_last flag of the StreamingDataset ({self.drop_last})"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state_dict is not set.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The shuffle state is not consistent. Expected: {self.shuffle}, Found: {state['shuffle']}\")\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The num_workers state is not consistent. Expected: {self.worker_env.world_size}, Found: {state['num_workers']}\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input_dir_path state is not consistent. Expected: {self.input_dir.path}, Found: {state['input_dir_path']}\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input_dir_url state is not consistent. Expected: {self.input_dir.url}, Found: {state['input_dir_url']}\"\n            )\n\n        if state[\"item_loader\"] is not None and self.item_loader is not None:\n            self.item_loader.load_state_dict(state[\"item_loader\"])\n        elif state[\"item_loader\"] is not None and self.item_loader is None:\n            raise ValueError(\"The item_loader state is not consistent.\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last state is not consistent. Expected: {self.drop_last}, Found: {state['drop_last']}\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed state is not consistent. Expected: {self.seed}, Found: {state['seed']}\")\n\n        if state[\"world_size"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        state = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle state is different from the current shuffle state. \"\n                f\"Expected {self.shuffle}, got {state['shuffle']}\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The num_workers state is different from the current num_workers state. \"\n                f\"Expected {self.worker_env.world_size}, got {state['num_workers']}\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input_dir_path state is different from the current input_dir_path state. \"\n                f\"Expected {self.input_dir.path}, got {state['input_dir_path']}\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input_dir_url state is different from the current input_dir_url state. \"\n                f\"Expected {self.input_dir.url}, got {state['input_dir_url']}\"\n            )\n\n        if state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"The item_loader state is different from the current item_loader state. \"\n                f\"Expected {self.item_loader.state_dict()}, got {state['item_loader']}\"\n            )\n\n        if"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle argument is different from the one used to create the dataset. \"\n                f\"Expected {self.shuffle} but found {state['shuffle']}\"\n            )\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The num_workers argument is different from the one used to create the dataset. \"\n                f\"Expected {self.worker_env.world_size} but found {state['num_workers']}\"\n            )\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input_dir_path argument is different from the one used to create the dataset. \"\n                f\"Expected {self.input_dir.path} but found {state['input_dir_path']}\"\n            )\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input_dir_url argument is different from the one used to create the dataset. \"\n                f\"Expected {self.input_dir.url} but found {state['input_dir_url']}\"\n            )\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed argument is different from the one used to create the dataset. \"\n                f\"Expected {self.seed} but found {state['seed']}\"\n            )\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last argument is different from the one used to create the dataset. \"\n                f\"Expected {self.drop_last} but found {state['drop_last']}\"\n            )\n        if state[\"item_loader\"] != self.item_loader.state_dict():"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state_dict is empty.\")\n\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The state_dict contains a different shuffle state ({self._state_dict['shuffle']}) than the current shuffle state ({self.shuffle}).\"\n            )\n\n        if self._state_dict[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The state_dict contains a different number of workers ({self._state_dict['num_workers']}) than the current number of workers ({self.worker_env.world_size}).\"\n            )\n\n        if self._state_dict[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The state_dict contains a different input directory path ({self._state_dict['input_dir_path']}) than the current input directory path ({self.input_dir.path}).\"\n            )\n\n        if self._state_dict[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The state_dict contains a different input directory URL ({self._state_dict['input_dir_url']}) than the current input directory URL ({self.input_dir.url}).\"\n            )\n\n        if self._state_dict[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"The state_dict contains a different item_loader state ({self._state_dict['item_loader']}) than the current item_loader state ({self.item_loader.state_dict()}).\"\n            )\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The state_dict contains a different seed ({self._state_dict['seed']}) than the current seed ({self.seed}).\"\n            )\n\n        if self._state_dict[\"drop_last\"] != self"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        # check the shuffle\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle value in the state_dict is {state['shuffle']} but the current shuffle value is {self.shuffle}.\"\n            )\n\n        # check the num_workers\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The num_workers value in the state_dict is {state['num_workers']} but the current num_workers value is {self.worker_env.world_size}.\"\n            )\n\n        # check the input directory path\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input_dir_path value in the state_dict is {state['input_dir_path']} but the current input_dir_path value is {self.input_dir.path}.\"\n            )\n\n        # check the input directory URL\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input_dir_url value in the state_dict is {state['input_dir_url']} but the current input_dir_url value is {self.input_dir.url}.\"\n            )\n\n        # check the item_loader state\n        if state[\"item_loader\"] is not None and self.item_loader is not None:\n            if state[\"item_loader\"] != self.item_loader.state_dict():\n                raise ValueError(\n                    f\"The item_loader state value in the state_dict is {state['item_loader']} but the current item_loader state value is {self.item_loader.state_dict()}.\"\n                )\n\n        # check the seed\n        if state[\"seed\"] != self.seed:\n            raise ValueError"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The StreamingDataset instance does not have a state dictionary.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if self.worker_env is None:\n            raise ValueError(\"The StreamingDataset instance does not have a worker environment.\")\n\n        if self.cache is None:\n            raise ValueError(\"The StreamingDataset instance does not have a cache.\")\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\"The shuffle flag does not match the state dictionary.\")\n\n        if self.worker_env.world_size != state[\"world_size\"]:\n            raise ValueError(\"The world size does not match the state dictionary.\")\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\"The input directory path does not match the state dictionary.\")\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\"The input directory URL does not match the state dictionary.\")\n\n        if self.item_loader is not None and state[\"item_loader\"] is not None:\n            self.item_loader.load_state_dict(state[\"item_loader\"])\n        elif self.item_loader is not None and state[\"item_loader\"] is None:\n            raise ValueError(\"The item loader state dictionary is missing.\")\n        elif self.item_loader is None and state[\"item_loader\"] is not None:\n            raise ValueError(\"The item loader state dictionary is not missing.\")\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\"The seed does not match the state dictionary.\")\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\"The drop_last flag does not match the state dictionary.\")\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if not self._state_dict:\n            raise ValueError(\"The state_dict is not available.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The shuffle state is different from the current shuffle state: {state['shuffle']} != {self.shuffle}\")\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(f\"The num_workers state is different from the current num_workers: {state['num_workers']} != {self.worker_env.world_size}\")\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(f\"The input_dir_path state is different from the current input_dir_path: {state['input_dir_path']} != {self.input_dir.path}\")\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(f\"The input_dir_url state is different from the current input_dir_url: {state['input_dir_url']} != {self.input_dir.url}\")\n\n        if state[\"item_loader\"] is not None and self.item_loader is not None:\n            if state[\"item_loader\"][\"class_name\"] != self.item_loader.class_name:\n                raise ValueError(f\"The item_loader class name state is different from the current item_loader class name: {state['item_loader']['class_name']} != {self.item_loader.class_name}\")\n            if state[\"item_loader\"][\"state_dict\"] != self.item_loader.state_dict():\n                raise ValueError(f\"The item_loader state_dict state is different from the current item_loader state_dict: {state['item_loader']['state_dict']} != {self.item_loader.state_dict()}\")\n        elif state[\"item_loader\"] is not None"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        state: Dict[str, Any] = self._state_dict\n\n        # Validate the state_dict\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The state_dict is inconsistent with the current shuffle state: {state['shuffle']}\")\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(f\"The state_dict is inconsistent with the current num_workers: {state['num_workers']}\")\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(f\"The state_dict is inconsistent with the current input_dir_path: {state['input_dir_path']}\")\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(f\"The state_dict is inconsistent with the current input_dir_url: {state['input_dir_url']}\")\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The state_dict is inconsistent with the current seed: {state['seed']}\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(f\"The state_dict is inconsistent with the current drop_last: {state['drop_last']}\")\n\n        if state[\"item_loader\"] is not None:\n            if self.item_loader is None:\n                raise ValueError(\"The state_dict is inconsistent with the current item_loader: None\")\n            if state[\"item_loader\"] != self.item_loader.state_dict():\n                raise ValueError(f\"The state_dict is inconsistent with the current item_loader: {state['item_loader']}\")\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed is not the same: {state['seed']} != {self.seed}\")\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The shuffle is not the same: {state['shuffle']} != {self.shuffle}\")\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(f\"The drop_last is not the same: {state['drop_last']} != {self.drop_last}\")\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(f\"The input_dir_path is not the same: {state['input_dir_path']} != {self.input_dir.path}\")\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(f\"The input_dir_url is not the same: {state['input_dir_url']} != {self.input_dir.url}\")\n        if state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(f\"The item_loader is not the same: {state['item_loader']} != {self.item_loader.state_dict()}\")\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        # Validate the shuffle\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\"The shuffle parameter does not match the state_dict\")\n\n        # Validate the num_workers\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\"The num_workers parameter does not match the state_dict\")\n\n        # Validate the input directory path\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\"The input directory path does not match the state_dict\")\n\n        # Validate the input directory URL\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\"The input directory URL does not match the state_dict\")\n\n        # Validate the seed\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\"The seed parameter does not match the state_dict\")\n\n        # Validate the item_loader state\n        if self.item_loader and state[\"item_loader\"] and not self.item_loader.is_equal(state[\"item_loader\"]):\n            raise ValueError(\"The item_loader state does not match the state_dict\")\n\n        # Validate the drop_last flag\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\"The drop_last parameter does not match the state_dict\")\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The StreamingDataset has not been initialized yet.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\"The number of workers has changed.\")\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\"The input directory path has changed.\")\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\"The input directory URL has changed.\")\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\"The seed has changed.\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\"The drop_last flag has changed.\")\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"The shuffle flag has changed.\")\n\n        if state[\"item_loader\"] is not None and self.item_loader is not None:\n            if state[\"item_loader\"][\"class_name\"] != self.item_loader.class_name:\n                raise ValueError(\"The item_loader class name has changed.\")\n            if state[\"item_loader\"][\"args\"] != self.item_loader.args:\n                raise ValueError(\"The item_loader arguments have changed.\")\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        state: Dict[str, Any] = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\"shuffle is inconsistent\")\n        if self.distributed_env.world_size != state[\"world_size\"]:\n            raise ValueError(\"world_size is inconsistent\")\n        if self.worker_env.world_size != state[\"num_workers\"]:\n            raise ValueError(\"num_workers is inconsistent\")\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\"input_dir_path is inconsistent\")\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\"input_dir_url is inconsistent\")\n        if self.item_loader != state[\"item_loader\"]:\n            raise ValueError(\"item_loader is inconsistent\")\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\"seed is inconsistent\")\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\"drop_last is inconsistent\")\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if not input_dir:\n        input_dir = \"\"\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.md5(input_dir.encode()).hexdigest())\n    if not os.path.exists(cache_dir):\n        try:\n            os.makedirs(cache_dir)\n        except Exception:\n            return None\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Create a unique cache directory based on the input directory\n    cache_dir = _DEFAULT_CACHE_DIR\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n\n    cache_dir = os.path.join(cache_dir, hashlib.md5(input_dir.encode()).hexdigest())\n\n    # Create the cache directory if it doesn't exist\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    dir_name = hashlib.md5(input_dir.encode()).hexdigest()\n\n    # Check if the environment variables are set\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\", _DEFAULT_CACHE_DIR)\n    cache_dir = os.path.join(cache_dir, dir_name)\n\n    # Create the cache directory if it doesn't exist\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError as e:\n        logger.warn(f\"Could not create cache directory: {e}\")\n        return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = _DEFAULT_CACHE_DIR\n\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n\n    cache_dir = os.path.join(cache_dir, hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest())\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"):\n        cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\", _DEFAULT_CACHE_DIR)\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        cache_dir = os.path.join(cache_dir, hashlib.md5(input_dir.encode()).hexdigest())\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        return cache_dir\n    else:\n        return None\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    if not os.path.exists(_DEFAULT_CACHE_DIR):\n        os.makedirs(_DEFAULT_CACHE_DIR)\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.md5(input_dir.encode()).hexdigest())\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    if cache_dir:\n        cache_dir = os.path.join(cache_dir, hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest())\n    else:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest())\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir = os.path.join(\n        os.environ.get(\"DATA_OPTIMIZER_CACHE_DIR\", _DEFAULT_CACHE_DIR),\n        hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest(),\n    )\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except Exception:\n        cache_dir = None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique cache directory name based on the input directory\n    cache_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Get the cache directory path from the environment variable\n    cache_dir_path = os.environ.get(\"DATA_OPTIMIZER_CACHE_DIR\", _DEFAULT_CACHE_DIR)\n\n    # Create the cache directory if it doesn't exist\n    cache_dir = os.path.join(cache_dir_path, cache_dir_name)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hash_object = hashlib.sha256(input_dir.encode())\n    hash_string = hash_object.hexdigest()\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_string)\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    try:\n        os.makedirs(cache_dir)\n    except OSError:\n        logger.warn(\n            f\"Could not create cache directory {cache_dir}. \"\n            \"Please make sure you have the correct permissions.\"\n        )\n        return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Check if the cache directory has already been created\n    if _should_replace_path(input_dir):\n        return input_dir\n\n    # Generate a unique directory name by hashing the input directory\n    hashed_dir = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Get the cache directory path\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashed_dir)\n\n    # Create the cache directory if it doesn't exist\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hash_obj = hashlib.sha1(input_dir.encode())\n    hash_str = hash_obj.hexdigest()\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_str)\n\n    if not os.path.exists(cache_dir):\n        try:\n            os.makedirs(cache_dir, exist_ok=True)\n        except OSError as e:\n            logger.error(f\"Error creating cache directory: {e}\")\n            return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name by hashing the input directory\n    dir_hash = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Create the cache directory in a default location if certain environment variables are not set\n    if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\") is None:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, dir_hash)\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n\n    # Create the cache directory in a specified location if certain environment variables are set\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    if cache_dir is None:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, dir_hash)\n\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if not input_dir:\n        return None\n\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    else:\n        cache_dir = _DEFAULT_CACHE_DIR\n\n    hash_object = hashlib.sha256(input_dir.encode())\n    hashed_input_dir = hash_object.hexdigest()\n    cache_dir = os.path.join(cache_dir, hashed_input_dir)\n\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError:\n        logger.error(f\"Unable to create the cache directory at {cache_dir}\")\n        return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.path.join(\n        os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\", _DEFAULT_CACHE_DIR),\n        hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest(),\n    )\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    try:\n        os.makedirs(cache_dir)\n        return cache_dir\n    except OSError:\n        return None\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hash_key = hashlib.md5(input_dir.encode()).hexdigest()\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_key)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir, exist_ok=True)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if not input_dir:\n        return None\n\n    if not _should_replace_path(input_dir):\n        return input_dir\n\n    # Create a unique cache directory for the input directory\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest())\n\n    # Create the cache directory if it doesn't exist\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hash_object = hashlib.sha256(input_dir.encode())\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_object.hexdigest())\n\n    try:\n        if os.path.exists(cache_dir):\n            return cache_dir\n        os.makedirs(cache_dir, exist_ok=True)\n    except Exception as e:\n        logger.warn(f\"Failed to create cache directory at {cache_dir}. Exception: {e}\")\n        return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir = os.environ.get(\"DATA_OPTIMIZER_CACHE_DIR\", _DEFAULT_CACHE_DIR)\n    if cache_dir is None:\n        cache_dir = _DEFAULT_CACHE_DIR\n\n    # If the cache dir is a relative path, we need to resolve it to an absolute path.\n    if not os.path.isabs(cache_dir):\n        cache_dir = os.path.abspath(cache_dir)\n\n    # If the cache dir is a relative path, we need to resolve it to an absolute path.\n    if not os.path.isabs(input_dir):\n        input_dir = os.path.abspath(input_dir)\n\n    if _should_replace_path(input_dir):\n        cache_dir = os.path.join(cache_dir, hashlib.sha256(input_dir.encode()).hexdigest())\n\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        logger.warning(f\"Failed to create the cache directory. Reason: {e}\")\n        return None\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # If the input directory is a URL, we can't use it for hashing\n    # since it might contain special characters that can't be hashed.\n    # We'll just use the URL as the input directory.\n    if input_dir.startswith(\"http://\") or input_dir.startswith(\"https://\"):\n        return input_dir\n\n    # Generate a unique directory name by hashing the input directory.\n    hash_input = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_input)\n\n    # Create the cache directory if it doesn't exist.\n    os.makedirs(cache_dir, exist_ok=True)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"Invalid S3 URL: {remote_filepath}\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Use s5cmd to download the file if available\n        if self._s5cmd_available:\n            cmd = f\"s5cmd cp {remote_filepath} {local_filepath}\"\n            subprocess.run(cmd, shell=True, check=True)\n            return\n\n        # Use boto3 to download the file if s5cmd is not available\n        bucket_name = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n        self._client.download_file(bucket_name, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            self._download_file_with_s5cmd(remote_filepath, local_filepath)\n        else:\n            self._download_file_with_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Parse the S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        # Download the file using s5cmd (if available)\n        if self._s5cmd_available:\n            s5cmd_cmd = f\"s5cmd cp s3://{bucket}/{key} {local_filepath}\"\n            subprocess.run(s5cmd_cmd, shell=True, check=True)\n            return\n\n        # Download the file using boto3\n        self._client.download_file(bucket, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(\"Remote file path must be an S3 URL\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        if self._s5cmd_available:\n            cmd = f\"s5cmd --endpoint-url={self._client.endpoint_url} cp s3://{bucket}/{key} {local_filepath}\"\n            subprocess.run(cmd, shell=True, check=True)\n        else:\n            self._client.download_file(bucket, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Remote file path {remote_filepath} must start with 's3://'.\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        with FileLock(local_filepath + \".lock\", timeout=10):\n            if not os.path.exists(local_filepath):\n                parsed_url = parse.urlparse(remote_filepath)\n                bucket = parsed_url.netloc\n                key = parsed_url.path.lstrip(\"/\")\n\n                if self._s5cmd_available:\n                    cmd = f\"s5cmd cp s3://{bucket}/{key} {local_filepath}\"\n                    subprocess.run(cmd, shell=True, check=True)\n                else:\n                    self._client.download_file(bucket, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"Remote file path must be an S3 URL, got {remote_filepath}\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Check if the s5cmd command-line tool is available\n        if self._s5cmd_available:\n            # Use s5cmd to download the file\n            s5cmd_command = f\"s5cmd cp {remote_filepath} {local_filepath}\"\n            subprocess.run(s5cmd_command, shell=True, check=True)\n        else:\n            # Use boto3 to download the file\n            self._client.download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Parse the S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket_name = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        # Check if s5cmd is available\n        if self._s5cmd_available:\n            # Download the file using s5cmd\n            s5cmd_cmd = f\"s5cmd cp s3://{bucket_name}/{key} {local_filepath}\"\n            subprocess.run(s5cmd_cmd, shell=True, check=True)\n            return\n\n        # Download the file using boto3\n        with FileLock(local_filepath + \".lock\", timeout=5):\n            self._client.download_file(bucket_name, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Remote filepath must be an S3 URL, got {remote_filepath}\")\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_filepath = local_filepath + \".lock\"\n        with FileLock(lock_filepath, timeout=10):\n            if self._s5cmd_available:\n                cmd = f\"s5cmd cp s3://{bucket}/{key} {local_filepath}\"\n                subprocess.run(cmd, shell=True, check=True)\n            else:\n                self._client.download_file(bucket, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"{remote_filepath} is not an S3 URL\")\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket_name = parsed_url.netloc\n        key = parsed_url.path[1:]\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_filepath = local_filepath + \".lock\"\n        with FileLock(lock_filepath, timeout=10):\n            if self._s5cmd_available:\n                self._download_file_with_s5cmd(bucket_name, key, local_filepath)\n            else:\n                self._download_file_with_boto3(bucket_name, key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n\n        # Parse the S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Check if the s5cmd command-line tool is available\n        if self._s5cmd_available:\n            # Use s5cmd to download the file\n            s5cmd_cmd = f\"s5cmd cp s3://{bucket}/{key} {local_filepath}\"\n            subprocess.run(s5cmd_cmd, shell=True, check=True)\n        else:\n            # Use boto3 to download the file\n            self._client.download_file(bucket, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid S3 URL: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path[1:]\n\n        if self._s5cmd_available:\n            try:\n                subprocess.run(\n                    [\"s5cmd\", \"cp\", remote_filepath, local_filepath],\n                    check=True,\n                    capture_output=True,\n                )\n            except subprocess.CalledProcessError as e:\n                raise ValueError(\n                    f\"Failed to download file {remote_filepath} with s5cmd: {e.stderr.decode()}\"\n                )\n        else:\n            try:\n                self._client.download_file(bucket, key, local_filepath)\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to download file {remote_filepath} with boto3: {e}\"\n                )\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"Invalid S3 URL: {remote_filepath}\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Attempt to download the file using s5cmd (if available)\n        if self._s5cmd_available:\n            s3_url = f\"s3://{parsed_url.netloc}/{parsed_url.path}\"\n            try:\n                subprocess.run([\"s5cmd\", \"cp\", s3_url, local_filepath], check=True)\n                return\n            except subprocess.CalledProcessError as e:\n                print(f\"Failed to download file using s5cmd: {e}\")\n\n        # Attempt to download the file using boto3\n        bucket_name = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n        self._client.download_file(bucket_name, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Remote filepath must be an s3 url: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        lock_filepath = local_filepath + \".lock\"\n        with FileLock(lock_filepath, timeout=10):\n            if os.path.exists(local_filepath):\n                return\n\n            if self._s5cmd_available:\n                s5cmd_command = f\"s5cmd --endpoint-url={self._client.endpoint_url} cp s3://{bucket}/{key} {local_filepath}\"\n                subprocess.run(s5cmd_command, shell=True, check=True)\n            else:\n                self._client.download_file(bucket, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not self._s5cmd_available:\n            # Use boto3 to download the file\n            self._client.download_file(remote_filepath, local_filepath)\n            return\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Check if the remote file path is an S3 URL\n        url = parse.urlparse(remote_filepath)\n        if url.scheme != \"s3\":\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n\n        # Use s5cmd to download the file\n        cmd = f\"s5cmd cp {remote_filepath} {local_filepath}\"\n        subprocess.run(cmd, shell=True, check=True)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not self._s5cmd_available:\n            self._client.download_file(remote_filepath, local_filepath)\n            return\n\n        if os.path.exists(local_filepath):\n            return\n\n        with FileLock(local_filepath + \".lock\", timeout=5):\n            if os.path.exists(local_filepath):\n                return\n\n            parsed_url = parse.urlparse(remote_filepath)\n\n            if parsed_url.scheme != \"s3\":\n                raise ValueError(f\"Invalid S3 URL: {remote_filepath}\")\n\n            try:\n                subprocess.run(\n                    [\n                        \"s5cmd\",\n                        \"cp\",\n                        remote_filepath,\n                        local_filepath,\n                    ],\n                    check=True,\n                )\n            except subprocess.CalledProcessError as e:\n                raise ValueError(f\"Failed to download {remote_filepath}: {e}\")\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"Remote file path {remote_filepath} is not an S3 URL\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Create the local directory if it doesn't exist\n        os.makedirs(os.path.dirname(local_filepath), exist_ok=True)\n\n        # Use s5cmd to download the file if available\n        if self._s5cmd_available:\n            cmd = f\"s5cmd cp '{remote_filepath}' '{local_filepath}'\"\n            subprocess.run(cmd, shell=True, check=True)\n            return\n\n        # Otherwise, use boto3 to download the file\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n        with FileLock(local_filepath + \".lock\", timeout=10):\n            try:\n                self._client.download_file(bucket, key, local_filepath)\n            except Timeout:\n                raise Timeout(f\"Timeout while downloading {remote_filepath} to {local_filepath}\")\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        with FileLock(f\"{local_filepath}.lock\", timeout=30):\n            if not os.path.exists(local_filepath):\n                if self._s5cmd_available:\n                    self._download_s5cmd(remote_filepath, local_filepath)\n                else:\n                    self._download_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not self._s5cmd_available:\n            self._client.download_file(remote_filepath, local_filepath)\n            return\n\n        # check if the file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # parse the S3 URL\n        url = parse.urlparse(remote_filepath)\n        if url.scheme != \"s3\":\n            raise ValueError(f\"Invalid S3 URL: {remote_filepath}\")\n\n        # create the cache directory if it doesn't exist\n        os.makedirs(self._cache_dir, exist_ok=True)\n\n        # create a file lock to prevent multiple processes from downloading the same file simultaneously\n        lock_file = os.path.join(self._cache_dir, \"s3_download.lock\")\n        lock = FileLock(lock_file, timeout=60)\n\n        # acquire the lock\n        try:\n            lock.acquire()\n        except Timeout:\n            raise Timeout(f\"Could not acquire lock for {remote_filepath}\")\n\n        # download the file using s5cmd\n        cmd = f\"s5cmd cp {remote_filepath} {local_filepath}\"\n        subprocess.run(cmd, shell=True, check=True)\n\n        # release the lock\n        lock.release()\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not self._s5cmd_available:\n            if not remote_filepath.startswith(\"s3://\"):\n                raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n            parsed_url = parse.urlparse(remote_filepath)\n            bucket = parsed_url.netloc\n            key = parsed_url.path.lstrip(\"/\")\n            self._client.download_file(bucket, key, local_filepath)\n        else:\n            if not os.path.exists(local_filepath):\n                with FileLock(local_filepath + \".lock\"):\n                    try:\n                        subprocess.run(\n                            [\n                                \"s5cmd\",\n                                \"cp\",\n                                \"--endpoint\",\n                                \"https://s3.amazonaws.com\",\n                                \"--no-sign-request\",\n                                remote_filepath,\n                                local_filepath,\n                            ],\n                            check=True,\n                            capture_output=True,\n                        )\n                    except subprocess.CalledProcessError as e:\n                        print(e.stderr)\n                        raise e\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"Invalid S3 URL: {remote_filepath}\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Use s5cmd to download the file if available\n        if self._s5cmd_available:\n            s5cmd_command = f\"s5cmd cp s3://{parsed_url.netloc}/{parsed_url.path} {local_filepath}\"\n            subprocess.run(s5cmd_command, shell=True, check=True)\n            return\n\n        # Use boto3 to download the file\n        bucket_name = parsed_url.netloc\n        object_key = parsed_url.path[1:]  # Remove the leading '/' from the object key\n        self._client.download_file(bucket_name, object_key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = [[] for _ in range(num_workers)]\n    workers_intervals = [[] for _ in range(num_workers)]\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % num_workers\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = [[] for _ in range(num_workers)]\n    workers_intervals = [[] for _ in range(num_workers)]\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    # Define the distribution strategy\n    if worker_env.rank % 2 == 0:\n        distribution_strategy = \"even\"\n    else:\n        distribution_strategy = \"odd\"\n\n    # Distribute chunks and intervals to workers\n    for i in range(num_workers):\n        if distribution_strategy == \"even\":\n            if i % 2 == 0:\n                workers_chunks[i] = chunks_replica[i // 2]\n                workers_intervals[i] = intervals_replica[i // 2]\n            else:\n                workers_chunks[i] = chunks_replica[(i + 1) // 2]\n                workers_intervals[i] = intervals_replica[(i + 1) // 2]\n        elif distribution_strategy == \"odd\":\n            if i % 2 == 0:\n                workers_chunks[i] = chunks_replica[(i + 1) // 2]\n                workers_intervals[i] = intervals_replica[(i + 1) // 2]\n            else:\n                workers_chunks[i] = chunks_replica[i // 2]\n                workers_intervals[i] = intervals_replica[i // 2]\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n    for i in range(num_workers):\n        workers_chunks[i] = []\n        workers_intervals[i] = []\n\n    # Distribute chunks and intervals across workers\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_rank = i % worker_env.world_size\n        workers_chunks[worker_rank].append(chunk_index)\n        workers_intervals[worker_rank].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n    for worker_rank in range(num_workers):\n        workers_chunks[worker_rank] = []\n        workers_intervals[worker_rank] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_rank = i % worker_env.world_size\n        workers_chunks[worker_rank].append(chunk_index)\n        workers_intervals[worker_rank].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {i: [] for i in range(num_workers)}\n    workers_intervals = {i: [] for i in range(num_workers)}\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {i: [] for i in range(num_workers)}\n    workers_intervals = {i: [] for i in range(num_workers)}\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        if i % worker_env.world_size != worker_env.rank:\n            continue\n        workers_chunks[worker_env.rank].append(chunk_index)\n        workers_intervals[worker_env.rank].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize the dictionaries to store the chunks and intervals for each worker\n    workers_chunks = {i: [] for i in range(num_workers)}\n    workers_intervals = {i: [] for i in range(num_workers)}\n\n    # Distribute the chunks and intervals based on the worker environment\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for i in range(num_workers):\n        workers_chunks[i] = []\n        workers_intervals[i] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize the dictionaries to store the chunks and intervals for each worker\n    workers_chunks = {i: [] for i in range(num_workers)}\n    workers_intervals = {i: [] for i in range(num_workers)}\n\n    # Calculate the number of chunks to be distributed to each worker\n    chunks_per_worker = len(chunks_replica) // num_workers\n\n    # Distribute the chunks and intervals to each worker\n    for i, (chunk_index, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % num_workers\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Create a list of worker indices in the range [0, num_workers)\n    workers = list(range(num_workers))\n\n    # Calculate the number of chunks per worker\n    chunks_per_worker = len(chunks_replica) // num_workers\n\n    # Distribute the chunks and intervals among the workers based on the worker index\n    workers_chunks = {worker: chunks_replica[worker * chunks_per_worker : (worker + 1) * chunks_per_worker] for worker in workers}\n    workers_intervals = {worker: intervals_replica[worker * chunks_per_worker : (worker + 1) * chunks_per_worker] for worker in workers}\n\n    # Handle the case when the number of chunks is not evenly divisible by the number of workers\n    if len(chunks_replica) % num_workers != 0:\n        # Calculate the number of chunks to be distributed to the remaining workers\n        remaining_chunks = len(chunks_replica) % num_workers\n\n        # Distribute the remaining chunks to the remaining workers\n        for worker in workers[-remaining_chunks:]:\n            workers_chunks[worker].append(chunks_replica[-1])\n            workers_intervals[worker].append(intervals_replica[-1])\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = [[] for _ in range(num_workers)]\n    workers_intervals = [[] for _ in range(num_workers)]\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n    for i in range(num_workers):\n        workers_chunks[i] = []\n        workers_intervals[i] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_rank = i % worker_env.world_size\n        workers_chunks[worker_rank].append(chunk_index)\n        workers_intervals[worker_rank].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n    for i in range(num_workers):\n        workers_chunks[i] = []\n        workers_intervals[i] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        if i % worker_env.world_size != worker_env.rank:\n            continue\n        workers_chunks[i].append(chunk_index)\n        workers_intervals[i].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for i in range(num_workers):\n        workers_chunks[i] = []\n        workers_intervals[i] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_rank in range(num_workers):\n        worker_chunks = []\n        worker_intervals = []\n\n        for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            if i % worker_env.world_size != worker_rank:\n                continue\n            worker_chunks.append(chunk_index)\n            worker_intervals.append(chunk_interval)\n\n        workers_chunks[worker_rank] = worker_chunks\n        workers_intervals[worker_rank] = worker_intervals\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for i in range(num_workers):\n        workers_chunks[i] = []\n        workers_intervals[i] = []\n\n    # distribute chunks and intervals to workers\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        if i % worker_env.world_size != worker_env.rank:\n            continue\n        workers_chunks[i % worker_env.world_size].append(chunk_index)\n        workers_intervals[i % worker_env.world_size].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Define the distribution strategy based on the worker index and world size\n    if worker_env.rank < num_workers:\n        distribution_strategy = \"round_robin\"\n    else:\n        distribution_strategy = \"modulo\"\n\n    # Initialize the dictionaries to store the chunks and intervals per worker\n    workers_chunks: Dict[int, List[int]] = {i: [] for i in range(num_workers)}\n    workers_intervals: Dict[int, List[Any]] = {i: [] for i in range(num_workers)}\n\n    # Distribute the chunks and intervals based on the distribution strategy\n    for i, (chunk_index, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        if distribution_strategy == \"round_robin\":\n            worker_index = i % num_workers\n        elif distribution_strategy == \"modulo\":\n            worker_index = (i + worker_env.rank) % num_workers\n        else:\n            raise ValueError(f\"Invalid distribution strategy: {distribution_strategy}\")\n\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {worker_index: [] for worker_index in range(num_workers)}\n    workers_intervals = {worker_index: [] for worker_index in range(num_workers)}\n\n    # Distribute the chunks and intervals to the workers\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {worker_index: [] for worker_index in range(num_workers)}\n    workers_intervals = {worker_index: [] for worker_index in range(num_workers)}\n\n    # Distribute chunks and intervals evenly across workers\n    for chunk_index, chunk_interval in zip(chunks_replica, intervals_replica):\n        worker_index = chunk_index % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n\n        super().download_file(remote_filepath, local_filepath)\n\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL.Image.Image, got {type(item)}\")\n\n        with io.BytesIO() as f:\n            item.save(f, format=\"PNG\")\n            f.seek(0)\n            data = f.read()\n\n        return data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL.Image.Image, got {type(item)}\")\n\n        with io.BytesIO() as f:\n            item.save(f, format=\"PNG\")\n            return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image's dimensions, mode, and raw pixel data\n        width, height = item.size\n        mode = item.mode\n        raw_data = item.tobytes()\n\n        # Serialize the image's dimensions, mode, and raw pixel data into a bytes object\n        serialized_data = pickle.dumps((width, height, mode, raw_data))\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Convert the PIL Image object to a bytes object\n        img_byte_arr = io.BytesIO()\n        item.save(img_byte_arr, format='PNG')\n        img_byte_arr = img_byte_arr.getvalue()\n\n        # Convert the PIL Image object to a bytes object\n        img_byte_arr = io.BytesIO()\n        item.save(img_byte_arr, format='PNG')\n        img_byte_arr = img_byte_arr.getvalue()\n\n        # Serialize the image dimensions, mode, and raw pixel data\n        width, height = item.size\n        mode_length = len(item.mode).to_bytes(1, byteorder='big')\n        mode_bytes = item.mode.encode('utf-8')\n        serialized_data = width.to_bytes(4, byteorder='big') + height.to_bytes(4, byteorder='big') + mode_length + mode_bytes + img_byte_arr\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode_length = len(item.mode).to_bytes(1, byteorder=\"big\")\n        mode_bytes = item.mode.encode(\"utf-8\")\n        raw_bytes = item.tobytes()\n        return (\n            width.to_bytes(4, byteorder=\"big\")\n            + height.to_bytes(4, byteorder=\"big\")\n            + mode_length\n            + mode_bytes\n            + raw_bytes,\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL.Image.Image, got {type(item)}\")\n        width, height = item.size\n        mode = item.mode\n        raw_data = item.tobytes()\n        mode_bytes = mode.encode(\"utf-8\")\n        mode_length = len(mode_bytes).to_bytes(4, \"little\")\n        return (\n            mode_length\n            + mode_bytes\n            + width.to_bytes(4, \"little\")\n            + height.to_bytes(4, \"little\")\n            + raw_data,\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode_len = len(item.mode).to_bytes(1, byteorder='big')\n        mode_bytes = item.mode.encode('utf-8')\n        raw_pixels = item.tobytes()\n\n        serialized_data = width.to_bytes(4, byteorder='big') + height.to_bytes(4, byteorder='big') + mode_len + mode_bytes + raw_pixels\n\n        return serialized_data, None\n\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Convert the PIL Image object to a bytes object\n        img_bytes = io.BytesIO()\n        item.save(img_bytes, format=\"PNG\")\n        img_bytes = img_bytes.getvalue()\n\n        # Convert the image's dimensions, mode, and raw pixel data to bytes objects\n        width_bytes = np.array([item.width], dtype=np.int32).tobytes()\n        height_bytes = np.array([item.height], dtype=np.int32).tobytes()\n        mode_bytes = item.mode.encode(\"utf-8\")\n\n        # Combine the serialized data into a single bytes object\n        serialized_data = width_bytes + height_bytes + len(mode_bytes).to_bytes(1, byteorder=\"big\") + mode_bytes + img_bytes\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Convert the image to a bytes object\n        img_byte_arr = io.BytesIO()\n        item.save(img_byte_arr, format=item.format)\n        img_byte_arr = img_byte_arr.getvalue()\n\n        # Convert the image's dimensions to bytes\n        width_bytes = np.array(item.width, dtype=np.int32).tobytes()\n        height_bytes = np.array(item.height, dtype=np.int32).tobytes()\n\n        # Convert the image mode to bytes\n        mode_bytes = item.mode.encode(\"utf-8\")\n\n        # Combine the serialized data into a single bytes object\n        serialized_data = width_bytes + height_bytes + mode_bytes + img_byte_arr\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode_length = len(item.mode).to_bytes(1, \"big\")\n        mode_bytes = item.mode.encode(\"utf-8\")\n        raw_bytes = item.tobytes()\n        return width.to_bytes(4, \"big\") + height.to_bytes(4, \"big\") + mode_length + mode_bytes + raw_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL.Image.Image, got {type(item)}\")\n\n        # Convert PIL Image to bytes\n        img_byte_arr = io.BytesIO()\n        item.save(img_byte_arr, format=\"PNG\")\n        img_byte_arr = img_byte_arr.getvalue()\n\n        # Serialize the image data\n        serialized_data = b\"\"\n        serialized_data += len(img_byte_arr).to_bytes(4, byteorder=\"big\")\n        serialized_data += item.mode.encode(\"utf-8\")\n        serialized_data += img_byte_arr\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get the image dimensions and mode\n        width, height = item.size\n        mode = item.mode\n        mode_length = len(mode).to_bytes(1, byteorder=\"big\")\n\n        # Convert the PIL Image object to a bytes object containing the raw pixel data\n        raw_data = item.tobytes()\n\n        # Combine the image dimensions, mode length, mode, and raw pixel data into a single bytes object\n        serialized_data = width.to_bytes(4, byteorder=\"big\") + height.to_bytes(4, byteorder=\"big\") + mode_length + mode.encode(\n            \"utf-8\") + raw_data\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Get image dimensions and mode\n        width, height = item.size\n        mode = item.mode\n\n        # Encode mode in UTF-8\n        mode_bytes = mode.encode(\"utf-8\")\n\n        # Get raw pixel data\n        raw_pixels = item.tobytes()\n\n        # Combine dimensions, mode length, mode, and raw pixel data\n        serialized_data = width.to_bytes(4, \"big\") + height.to_bytes(4, \"big\") + len(mode_bytes).to_bytes(\n            4, \"big\") + mode_bytes + raw_pixels\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode_length = len(item.mode)\n        mode_bytes = item.mode.encode(\"utf-8\")\n        raw_bytes = item.tobytes()\n        return width.to_bytes(4, \"big\") + height.to_bytes(4, \"big\") + mode_length.to_bytes(4, \"big\") + mode_bytes + raw_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL.Image.Image, got {type(item)}\")\n\n        # Get the image's mode and dimensions\n        mode = item.mode\n        width, height = item.size\n\n        # Serialize the image's dimensions and mode\n        dim_bytes = width.to_bytes(4, byteorder=\"big\") + height.to_bytes(4, byteorder=\"big\")\n        mode_bytes = mode.encode(\"utf-8\")\n\n        # Get the raw pixel data\n        raw_data = item.tobytes()\n\n        # Combine the serialized data into a single bytes object\n        serialized_data = dim_bytes + len(mode_bytes).to_bytes(4, byteorder=\"big\") + mode_bytes + raw_data\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL.Image.Image, got {type(item)}\")\n\n        width, height = item.size\n        mode = item.mode\n        mode_length = len(mode)\n        mode_bytes = mode.encode(\"utf-8\")\n        data = item.tobytes()\n\n        # Concatenate the width, height, mode length, mode, and data\n        serialized_data = width.to_bytes(4, \"big\")\n        serialized_data += height.to_bytes(4, \"big\")\n        serialized_data += mode_length.to_bytes(4, \"big\")\n        serialized_data += mode_bytes\n        serialized_data += data\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL.Image.Image object, got {type(item)}\")\n\n        width, height = item.size\n        mode = item.mode\n        mode_length = len(mode).to_bytes(1, byteorder=\"big\")\n        mode_bytes = mode.encode(\"utf-8\")\n        raw_bytes = item.tobytes()\n        return mode_length + mode_bytes + raw_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Serialize the image into a bytes object\n        with io.BytesIO() as f:\n            item.save(f, format=\"PNG\")\n            img_bytes = f.getvalue()\n        # Return the serialized data and None\n        return img_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(\"Expected PIL.Image.Image, got %s\" % type(item))\n        width, height = item.size\n        mode = item.mode\n        if mode == \"P\":\n            palette = item.getpalette()\n            item = item.convert(\"RGBA\")\n        else:\n            palette = None\n        item_bytes = item.tobytes()\n        if palette:\n            item_bytes += palette\n        mode_length = len(mode).to_bytes(1, \"big\")\n        mode_bytes = mode.encode(\"utf-8\")\n        return b\"\".join([width.to_bytes(4, \"big\"), height.to_bytes(4, \"big\"), mode_length, mode_bytes, item_bytes]), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL.Image.Image, got {type(item)}\")\n\n        if isinstance(item, GifImageFile):\n            raise TypeError(f\"GIF images are not supported\")\n\n        if item.mode not in (\"L\", \"RGB\", \"RGBA\"):\n            raise ValueError(f\"Unsupported mode: {item.mode}\")\n\n        width, height = item.size\n        mode = item.mode\n        mode_length = len(mode.encode(\"utf-8\"))\n\n        # Convert the image to bytes\n        buf = io.BytesIO()\n        item.save(buf, format=\"PNG\")\n        img_bytes = buf.getvalue()\n\n        # Combine the serialized data\n        serialized_data = width.to_bytes(4, byteorder=\"big\")\n        serialized_data += height.to_bytes(4, byteorder=\"big\")\n        serialized_data += mode_length.to_bytes(4, byteorder=\"big\")\n        serialized_data += mode.encode(\"utf-8\")\n        serialized_data += img_bytes\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n        else:\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image.Image) and isinstance(item, JpegImageFile):\n            return item.tobytes(), None\n        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected Image.Image, got {type(item)}\")\n        with io.BytesIO() as f:\n            item.save(f, format=\"JPEG\")\n            return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise TypeError(f\"JPEG file {item.filename} does not exist\")\n        elif isinstance(item, Image.Image):\n            with io.BytesIO() as output:\n                item.save(output, format=\"JPEG\")\n                return output.getvalue(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image.Image):\n            if isinstance(item, JpegImageFile) and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                with io.BytesIO() as f:\n                    item.save(f, format=\"JPEG\")\n                    return f.getvalue(), None\n        else:\n            raise TypeError(f\"Expected Image, got {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not self.can_serialize(item):\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n\n        if isinstance(item, JpegImageFile):\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n\n        with io.BytesIO() as output:\n            item.save(output, format=\"JPEG\")\n            return output.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            return item.read(), None\n\n        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected Image, got {type(item)}\")\n\n        with io.BytesIO() as f:\n            item.save(f, \"JPEG\")\n            return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image.Image):\n            with io.BytesIO() as output:\n                item.save(output, format=\"JPEG\")\n                return output.getvalue(), None\n        elif isinstance(item, str) and os.path.exists(item):\n            with open(item, \"rb\") as f:\n                return f.read(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise ValueError(\"JPEG image file is not found.\")\n        elif isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, \"JPEG\")\n                return f.getvalue(), None\n        else:\n            raise TypeError(\"Unsupported image type.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected Image, got {type(item)}\")\n        if isinstance(item, JpegImageFile):\n            return item.tobytes(), None\n        else:\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image.Image):\n            if isinstance(item, JpegImageFile):\n                with io.BytesIO() as output:\n                    item.save(output, format=\"JPEG\")\n                    return output.getvalue(), None\n            else:\n                with io.BytesIO() as output:\n                    item.save(output, format=\"JPEG\")\n                    return output.getvalue(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.tobytes(), None\n        elif isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n        else:\n            raise TypeError(f\"Cannot serialize item of type {type(item)} to JPEG\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(\"Item must be an instance of Image class or its subclasses.\")\n\n        if isinstance(item, JpegImageFile):\n            filename = item.filename\n            if os.path.exists(filename):\n                with open(filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise FileNotFoundError(f\"File {filename} not found.\")\n        else:\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected Image.Image, got {type(item)}\")\n\n        if not isinstance(item, JpegImageFile):\n            item = item.convert(\"RGB\")\n\n        with io.BytesIO() as f:\n            item.save(f, format=\"JPEG\")\n            return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            return item.tobytes(), None\n\n        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL.Image.Image or its subclasses, got {type(item)}\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".jpg\") as f:\n            item.save(f.name, format=\"JPEG\")\n            with open(f.name, \"rb\") as fp:\n                return fp.read(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not self.can_serialize(item):\n            raise TypeError(f\"Cannot serialize {type(item)}\")\n\n        if isinstance(item, JpegImageFile):\n            filename = item.filename\n            if filename is not None and os.path.isfile(filename):\n                with open(filename, \"rb\") as f:\n                    return f.read(), None\n\n        buffer = io.BytesIO()\n        item.save(buffer, format=\"JPEG\")\n        return buffer.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not self.can_serialize(item):\n            raise TypeError(f\"{item} is not a supported image type\")\n        if isinstance(item, JpegImageFile):\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n        with io.BytesIO() as f:\n            item.save(f, format=\"JPEG\")\n            return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n\n        if not self.can_serialize(item):\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n\n        if isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n\n        raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise ValueError(f\"JPEG image file {item.filename} does not exist.\")\n        elif isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n        else:\n            raise TypeError(f\"Expected Image.Image or JpegImageFile, got {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n        if isinstance(item, JpegImageFile):\n            return item.tobytes(), None\n        else:\n            with io.BytesIO() as output:\n                item.save(output, format=\"JPEG\")\n                return output.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise ValueError(f\"JPEG image file {item.filename} does not exist.\")\n        else:\n            if not isinstance(item, Image.Image):\n                raise TypeError(f\"Expected Image, got {type(item)}\")\n            if not item.mode == \"RGB\":\n                item = item.convert(\"RGB\")\n            with io.BytesIO() as f:\n                item.save(f, \"JPEG\")\n                return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombuffer(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_len = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_len]\n        raw = data[12 + mode_len :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_len = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_len].decode(\"utf-8\")\n        raw = data[12 + mode_len :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_len = np.frombuffer(data[:12], np.uint32)\n        mode = data[12:12 + mode_len]\n        raw = data[12 + mode_len:]\n        return Image.frombytes(mode.decode(\"utf-8\"), (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12:12 + mode_size]\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombuffer(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        item = Image.frombytes(mode, (width, height), raw)\n        return item\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12:12 + mode_size]\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode.decode(\"utf-8\"), (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        data = data[4:]\n        shape_len = np.frombuffer(data[:4], np.uint32)[0]\n        data = data[4:]\n        shape = np.frombuffer(data[: shape_len * 4], np.uint32)\n        data = data[shape_len * 4 :]\n        return torch.frombuffer(data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = np.frombuffer(data[8 : 8 + shape_size * 4], np.uint32)\n        tensor = torch.frombuffer(data[8 + shape_size * 4 :], dtype=dtype)\n        return tensor.reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_len = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = np.frombuffer(data[8 : 8 + shape_len * 4], np.uint32)\n        raw = data[8 + shape_len * 4 :]\n        return torch.frombuffer(raw, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        data = data[4:]\n        shape_len = np.frombuffer(data[:4], np.uint32)[0]\n        data = data[4:]\n        shape = np.frombuffer(data[: shape_len * 4], np.uint32)\n        data = data[shape_len * 4 :]\n        return torch.frombuffer(data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        data = data[4:]\n        shape_size = np.frombuffer(data[:4], np.uint32)[0]\n        shape = np.frombuffer(data[4 : 4 + shape_size * 4], np.uint32)\n        data = data[4 + shape_size * 4 :]\n        return torch.frombuffer(data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        idx = 4\n        shape_size = np.frombuffer(data[idx : idx + 4], np.uint32)[0]\n        idx += 4\n        shape = np.frombuffer(data[idx : idx + shape_size * 4], np.uint32)\n        idx += shape_size * 4\n        array = np.frombuffer(data[idx:], dtype=dtype)\n        return torch.from_numpy(array.reshape(shape))\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], dtype=np.uint32)[0]\n        dtype = list(_TORCH_DTYPES_MAPPING.keys())[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], dtype=np.uint32)[0]\n        shape = np.frombuffer(data[8 : 8 + shape_size * 4], dtype=np.uint32)\n        tensor = np.frombuffer(data[8 + shape_size * 4 :], dtype=dtype)\n        return torch.from_numpy(tensor).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = np.frombuffer(data[8 : 8 + shape_size * 4], np.uint32)\n        tensor_data = data[8 + shape_size * 4 :]\n        return torch.frombuffer(tensor_data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        data = data[4:]\n        shape_size = np.frombuffer(data[:4], np.uint32)[0]\n        data = data[4:]\n        shape = []\n        for i in range(shape_size):\n            shape.append(np.frombuffer(data[:4], np.uint32)[0])\n            data = data[4:]\n        tensor = torch.frombuffer(data, dtype=dtype)\n        tensor = tensor.reshape(shape)\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], dtype=np.uint32)[0]\n        dtype = list(_TORCH_DTYPES_MAPPING.keys())[dtype_indice]\n        data = data[4:]\n        shape_size = np.frombuffer(data[:4], dtype=np.uint32)[0]\n        data = data[4:]\n        shape = np.frombuffer(data[:shape_size * 4], dtype=np.uint32)\n        data = data[shape_size * 4 :]\n        return torch.frombuffer(data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = int.from_bytes(data[:4], \"little\")\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        data = data[4:]\n        shape_size = int.from_bytes(data[:4], \"little\")\n        data = data[4:]\n        shape = []\n        for _ in range(shape_size):\n            shape.append(int.from_bytes(data[:4], \"little\"))\n            data = data[4:]\n        return torch.frombuffer(data, dtype=dtype).reshape(tuple(shape))\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_len = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = np.frombuffer(data[8:8 + shape_len * 4], np.uint32)\n\n        # Construct the tensor from the remaining bytes\n        tensor = torch.frombuffer(data[8 + shape_len * 4 :], dtype=dtype)\n        tensor = tensor.reshape(shape)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        idx = 4\n        shape_size = np.frombuffer(data[idx: idx + 4], np.uint32)[0]\n        idx += 4\n        shape = np.frombuffer(data[idx: idx + shape_size * 4], np.uint32)\n        idx += shape_size * 4\n        tensor = np.frombuffer(data[idx:], dtype=dtype)\n        tensor = tensor.reshape(shape)\n        return torch.from_numpy(tensor)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        idx = 4\n        dtype_indice = np.frombuffer(data[:idx], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        idx2 = idx + 4\n        dims = np.frombuffer(data[idx:idx2], np.uint32)\n        idx3 = idx2 + dims.size * 4\n        raw = data[idx3:]\n        return torch.frombuffer(raw, dtype=dtype).reshape(dims)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        idx = 4\n        dim_size = np.frombuffer(data[idx : idx + 4], np.uint32)[0]\n        idx += 4\n        shape = np.frombuffer(data[idx : idx + 4 * dim_size], np.uint32)\n        idx += 4 * dim_size\n        array = np.frombuffer(data[idx:], dtype=dtype)\n        return torch.from_numpy(array.reshape(shape))\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        idx = 0\n        dtype_indice = np.frombuffer(data[idx : idx + 4], np.uint32)[0]\n        idx += 4\n        dim_size = np.frombuffer(data[idx : idx + 4], np.uint32)[0]\n        idx += 4\n        shape = np.frombuffer(data[idx : idx + 4 * dim_size], np.uint32)\n        idx += 4 * dim_size\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        tensor = torch.frombuffer(data[idx:], dtype=dtype)\n        tensor = tensor.reshape(shape)\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = int.from_bytes(data[:4], \"little\")\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_len = int.from_bytes(data[4:8], \"little\")\n        shape = []\n        for i in range(shape_len):\n            shape.append(int.from_bytes(data[8 + i * 4 : 12 + i * 4], \"little\"))\n        tensor_data = data[12 + shape_len * 4 :]\n        return torch.frombuffer(tensor_data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n\n        dim_len = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = np.frombuffer(data[8:8 + dim_len * 4], np.uint32)\n\n        return torch.frombuffer(data[8 + dim_len * 4:], dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information\n        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_length = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = np.frombuffer(data[8 : 8 + shape_length * 4], np.uint32)\n\n        # Extract the tensor data and reconstruct the tensor\n        tensor_data = data[8 + shape_length * 4 :]\n        tensor = torch.frombuffer(tensor_data, dtype=dtype).reshape(shape)\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the input byte array\n        dtype_indice = np.frombuffer(data[:4], dtype=np.uint32)[0]\n        dtype = list(_TORCH_DTYPES_MAPPING.keys())[dtype_indice]\n        shape_len = np.frombuffer(data[4:8], dtype=np.uint32)[0]\n        shape = np.frombuffer(data[8 : 8 + shape_len * 4], dtype=np.uint32)\n\n        # Reconstruct the tensor from the remaining bytes\n        tensor_data = data[8 + shape_len * 4 :]\n        return torch.frombuffer(tensor_data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = self._dtype_to_indices[item.dtype]\n        shape = item.shape\n        raw = item.numpy().tobytes()\n        ints = np.array([dtype, *shape], np.uint32)\n        return ints.tobytes() + raw, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = self._dtype_to_indices[item.dtype]\n        shape = item.shape\n        raw = item.numpy().tobytes()\n        ints = np.array([dtype, len(shape)], np.uint32)\n        return ints.tobytes() + shape.tobytes() + raw, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = item.dtype\n        shape = item.shape\n        raw = item.numpy().tobytes()\n        ints = np.array([dtype.itemsize, len(shape)], np.uint32)\n        return ints.tobytes() + shape.tobytes() + dtype.tobytes() + raw, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = self._dtype_to_indices[item.dtype]\n        shape = item.shape\n        data = item.numpy().tobytes()\n        return dtype.to_bytes(1, byteorder=\"big\") + shape.tobytes() + data, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Get the dtype of the tensor\n        dtype = item.dtype\n\n        # Convert the dtype to a string representation\n        dtype_str = str(dtype)\n\n        # Convert the shape of the tensor to a list of integers\n        shape = list(item.shape)\n\n        # Convert the tensor to a numpy array\n        data = item.numpy()\n\n        # Serialize the dtype, shape, and data into a bytes object\n        serialized_data = pickle.dumps((dtype_str, shape, data))\n\n        # Return the serialized data and None as the second element of the tuple\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        array = item.numpy()\n\n        # Get the dtype and shape of the tensor\n        dtype = array.dtype\n        shape = array.shape\n\n        # Convert the dtype to a string\n        dtype_str = str(dtype)\n\n        # Convert the shape to a string\n        shape_str = str(shape)\n\n        # Serialize the dtype and shape\n        dtype_bytes = dtype_str.encode()\n        shape_bytes = shape_str.encode()\n\n        # Serialize the raw data\n        data_bytes = array.tobytes()\n\n        # Concatenate the serialized dtype, shape, and raw data\n        serialized_bytes = dtype_bytes + shape_bytes + data_bytes\n\n        # Return the serialized bytes and None for metadata\n        return serialized_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = item.dtype\n        shape = item.shape\n        raw = item.numpy().tobytes()\n        ints = np.array([dtype.itemsize, len(shape), *shape], np.uint32)\n        return ints.tobytes() + raw, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        array = item.numpy()\n\n        # Get the dtype of the tensor\n        dtype = item.dtype\n\n        # Get the shape of the tensor\n        shape = item.shape\n\n        # Convert the NumPy array to a bytes object\n        array_bytes = array.tobytes()\n\n        # Create a bytes object with the serialized data\n        serialized_data = array_bytes\n\n        # Add the dtype and shape to the serialized data\n        serialized_data += dtype.tobytes()\n        serialized_data += np.array(shape, dtype=np.uint32).tobytes()\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Get the dtype and shape of the tensor\n        dtype = item.dtype\n        shape = item.shape\n\n        # Convert the tensor to a numpy array\n        array = item.numpy()\n\n        # Serialize the dtype and shape\n        dtype_index = self._dtype_to_indices.get(dtype, 0)\n        shape_bytes = np.array(shape, np.uint32).tobytes()\n\n        # Serialize the raw data\n        raw_bytes = array.tobytes()\n\n        # Concatenate the serialized data\n        serialized_data = shape_bytes + raw_bytes\n\n        # Return the serialized data and None for the metadata\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        array = item.detach().cpu().numpy()\n        # Convert the NumPy array to a bytes object\n        data = array.tobytes()\n        # Serialize the dtype and shape of the tensor\n        dtype = array.dtype\n        shape = array.shape\n        dtype_index = self._dtype_to_indices[dtype]\n        shape = np.array(shape, dtype=np.uint32)\n        shape_bytes = shape.tobytes()\n        # Concatenate the serialized dtype and shape with the raw data\n        serialized = np.array([dtype_index, *shape], dtype=np.uint32).tobytes() + shape_bytes + data\n        return serialized, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        array = item.cpu().numpy()\n\n        # Get the dtype and shape of the array\n        dtype = array.dtype\n        shape = array.shape\n\n        # Convert the dtype to a string representation\n        dtype_str = str(dtype)\n\n        # Convert the shape to a string representation\n        shape_str = \",\".join(str(s) for s in shape)\n\n        # Serialize the NumPy array to bytes\n        serialized_array = array.tobytes()\n\n        # Concatenate the dtype, shape, and serialized array into a single bytes object\n        serialized_tensor = dtype_str.encode() + b\"\\n\" + shape_str.encode() + b\"\\n\" + serialized_array\n\n        # Return the serialized tensor and None as the second element of the tuple\n        return serialized_tensor, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a numpy array\n        data = item.numpy()\n\n        # Convert the numpy array to a bytes object\n        data_bytes = data.tobytes()\n\n        # Convert the tensor's dtype to its corresponding index in the dtype_to_indices dictionary\n        dtype_index = self._dtype_to_indices[item.dtype]\n\n        # Convert the tensor's shape to a bytes object\n        shape_bytes = np.array(item.shape, dtype=np.int32).tobytes()\n\n        # Combine the dtype index, shape bytes, and data bytes into a single bytes object\n        serialized_bytes = dtype_index.to_bytes(1, byteorder=\"big\") + shape_bytes + data_bytes\n\n        # Return the serialized bytes object and None as the second element of the tuple\n        return serialized_bytes, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        array = item.numpy()\n\n        # Determine the dtype of the tensor\n        dtype = self._dtype_to_indices[item.dtype]\n\n        # Serialize the NumPy array to a bytes object\n        serialized_array = array.tobytes()\n\n        # Combine the dtype, shape, and serialized array into a single bytes object\n        serialized_tensor = np.array([dtype, *array.shape], dtype=np.uint32).tobytes() + serialized_array\n\n        # Return the serialized tensor and None as the second element of the tuple\n        return serialized_tensor, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        array = item.numpy()\n\n        # Serialize the NumPy array into a bytes object\n        serialized_array = pickle.dumps(array)\n\n        # Get the dtype and shape of the tensor\n        dtype = item.dtype\n        shape = item.shape\n\n        # Convert the dtype to an index\n        dtype_index = self._dtype_to_indices[dtype]\n\n        # Convert the shape to a bytes object\n        serialized_shape = pickle.dumps(shape)\n\n        # Combine the serialized array, dtype index, and shape into a single bytes object\n        serialized_tensor = serialized_array + dtype_index.to_bytes(1, byteorder=\"big\") + serialized_shape\n\n        # Return the serialized tensor and None as the second element of the tuple\n        return serialized_tensor, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a byte array\n        array = item.numpy().tobytes()\n\n        # Get the tensor's dtype and shape as integers\n        dtype = self._dtype_to_indices[item.dtype]\n        shape = item.shape\n\n        # Pack the dtype, shape, and raw data into a bytes object\n        packed_data = np.array([dtype, *shape], dtype=np.uint32).tobytes() + array\n\n        return packed_data, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        np_array = item.numpy()\n\n        # Determine the dtype and shape of the tensor\n        dtype = np_array.dtype\n        shape = np_array.shape\n\n        # Convert the NumPy array to bytes\n        serialized_data = np_array.tobytes()\n\n        # Convert the dtype to a string representation\n        dtype_str = str(dtype)\n\n        # Convert the shape to a string representation\n        shape_str = str(shape)\n\n        # Combine the serialized data, dtype, and shape into a single bytes object\n        serialized_tensor = b\"\".join([serialized_data, dtype_str.encode(), shape_str.encode()])\n\n        return serialized_tensor, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        array = item.numpy()\n\n        # Get the dtype and shape of the array\n        dtype = array.dtype\n        shape = array.shape\n\n        # Convert the NumPy array to bytes\n        array_bytes = array.tobytes()\n\n        # Convert the dtype to a string\n        dtype_str = str(dtype)\n\n        # Convert the shape to a string\n        shape_str = str(shape)\n\n        # Create a bytes object containing the dtype, shape, and raw data\n        serialized_data = (dtype_str + shape_str + array_bytes).encode()\n\n        # Return the serialized data and None as the second element of the tuple\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        data = item.numpy()\n\n        # Determine the data type of the tensor\n        dtype = item.dtype\n\n        # Convert the NumPy array to a bytes object\n        data_bytes = data.tobytes()\n\n        # Convert the shape of the tensor to a bytes object\n        shape_bytes = np.array(item.shape, dtype=np.int64).tobytes()\n\n        # Convert the data type of the tensor to a bytes object\n        dtype_bytes = np.array(dtype.name, dtype=np.object_).tobytes()\n\n        # Concatenate the data, shape, and data type bytes objects\n        serialized_data = shape_bytes + dtype_bytes + data_bytes\n\n        # Return the serialized data and None for the metadata\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a numpy array\n        array = item.numpy()\n\n        # Get the dtype and shape of the tensor\n        dtype = array.dtype\n        shape = array.shape\n\n        # Convert the dtype to a string representation\n        dtype_str = str(dtype)\n\n        # Convert the shape to a string representation\n        shape_str = str(shape)\n\n        # Serialize the dtype and shape as bytes\n        dtype_bytes = dtype_str.encode(\"utf-8\")\n        shape_bytes = shape_str.encode(\"utf-8\")\n\n        # Serialize the raw data as bytes\n        data_bytes = array.tobytes()\n\n        # Concatenate the dtype, shape, and data bytes to form the serialized tensor\n        serialized_tensor = dtype_bytes + shape_bytes + data_bytes\n\n        # Return the serialized tensor and None for the metadata\n        return serialized_tensor, None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        tensor_np = item.numpy()\n        # Get the dtype of the tensor\n        dtype = item.dtype\n        # Convert the NumPy array to a byte string\n        tensor_bytes = tensor_np.tobytes()\n        # Get the shape of the tensor\n        shape = item.shape\n        # Serialize the dtype and shape\n        dtype_index = self._dtype_to_indices[dtype]\n        shape_bytes = np.array(shape, np.int32).tobytes()\n        # Concatenate the serialized dtype and shape with the tensor data\n        serialized = shape_bytes + dtype_index.to_bytes(1, \"big\") + tensor_bytes\n        # Return the serialized tensor data and None for additional metadata\n        return serialized, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        # fallback to PIL\n        image = Image.open(io.BytesIO(data))\n        if _TORCH_VISION_AVAILABLE:\n            return pil_to_tensor(image)\n        return image\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        with tempfile.NamedTemporaryFile(suffix=\".jpg\") as f:\n            f.write(data)\n            f.flush()\n            pil_image = Image.open(f.name)\n\n        if _TORCH_VISION_AVAILABLE:\n            return pil_to_tensor(pil_image)\n\n        return pil_image\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"PNG with JPEG extension\" in str(e):\n                    return pil_to_tensor(Image.open(io.BytesIO(data)))\n                else:\n                    raise e\n        else:\n            return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n\n        raise ValueError(\"Unable to deserialize JPEG image. Please install PIL or torchvision.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                jpeg_image = decode_jpeg(data)\n                return jpeg_image\n            except RuntimeError as e:\n                if \"Expected JPEG image\" in str(e):\n                    pil_image = Image.open(io.BytesIO(data))\n                    if _TORCH_AVAILABLE:\n                        return pil_to_tensor(pil_image)\n                    else:\n                        return pil_image\n                else:\n                    raise e\n        else:\n            pil_image = Image.open(io.BytesIO(data))\n            return pil_image\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"Expected JPEG\" not in str(e):\n                    raise e\n\n        image = Image.open(io.BytesIO(data))\n        if _TORCH_VISION_AVAILABLE:\n            return pil_to_tensor(image)\n        return image\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                # attempt to decode the data as a JPEG image using torchvision\n                return decode_jpeg(data)\n            except RuntimeError:\n                # if the decoding fails, fall back to using PIL to deserialize the data\n                return Image.open(io.BytesIO(data))\n        else:\n            # if torchvision is not available, just use PIL to deserialize the data\n            return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n\n        raise TypeError(f\"The provided data should be of type {JpegImageFile}. Found {data}.\")\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"JPEG\" not in str(e):\n                    raise e\n\n        if _PIL_AVAILABLE:\n            try:\n                pil_image = Image.open(io.BytesIO(data))\n                return pil_to_tensor(pil_image)\n            except Exception as e:\n                raise e\n\n        raise TypeError(\"The provided data should be of type bytes.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"Expected JPEG\" in str(e):\n                    # fallback to PIL\n                    pass\n                else:\n                    raise e\n\n        image = Image.open(io.BytesIO(data))\n        if _TORCH_VISION_AVAILABLE:\n            return pil_to_tensor(image)\n        return image\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                jpeg_image = decode_jpeg(data)\n                return jpeg_image\n            except RuntimeError as e:\n                if \"Unknown JPEG format\" in str(e):\n                    # fallback to PIL\n                    pass\n                else:\n                    raise e\n\n        if _PIL_AVAILABLE:\n            pil_image = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                pil_image = pil_to_tensor(pil_image)\n            return pil_image\n\n        raise TypeError(f\"The provided data should be of type {JpegImageFile}.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE and _TORCH_VISION_AVAILABLE.is_installed():\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"Expected JPEG\" in str(e):\n                    pass\n                else:\n                    raise e\n\n        if _PIL_AVAILABLE:\n            image = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                image = pil_to_tensor(image)\n            return image\n\n        raise TypeError(f\"The provided data should be of type {JpegImageFile} or {torch.Tensor}.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                image = decode_jpeg(data)\n            except RuntimeError:\n                # fall back to PIL\n                image = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                image = pil_to_tensor(image)\n            return image\n        else:\n            return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            image = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                return pil_to_tensor(image)\n            return image\n\n        raise ValueError(\"No supported serializer found.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                # attempt to decode the data as a JPEG image using torchvision\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        # if torchvision is not available or the decoding fails, fall back to using PIL\n        pil_image = Image.open(io.BytesIO(data))\n\n        if _TORCH_VISION_AVAILABLE:\n            # convert the PIL image to a PyTorch tensor if torchvision is available\n            return pil_to_tensor(pil_image)\n        else:\n            return pil_image\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"Expected image with 3 channels\" in str(e):\n                    pass\n                else:\n                    raise e\n\n        if _PIL_AVAILABLE:\n            image = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                return pil_to_tensor(image)\n            return image\n\n        raise TypeError(f\"The provided data should be of type {JpegImageFile}.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            image = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                return pil_to_tensor(image)\n            return image\n\n        raise RuntimeError(\"No available image library to deserialize the data.\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The dtype is not defined. HINT: Call the setup method with the data_format argument.\")\n\n        dtype_indice = self._dtype_to_indices[self._dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if self._dtype is not None:\n            item = item.to(self._dtype)\n        data = item.numpy().tobytes(order=\"C\")\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return data, f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        data = item.numpy().tobytes(order=\"C\")\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return data, f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The dtype is not set. HINT: Call setup() with the data format.\")\n        dtype_indice = self._dtype_to_indices[self._dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The dtype is not defined.\")\n        dtype_indice = self._dtype_to_indices[self._dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if self._dtype is None:\n            dtype_indice = self._dtype_to_indices[item.dtype]\n        else:\n            dtype_indice = self._dtype_to_indices[self._dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        data = item.numpy().tobytes(order=\"C\")\n        dtype_indice = self._dtype_to_indices[self._dtype]\n        data_format = f\"no_header_tensor:{dtype_indice}\"\n        return data, data_format\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        np_array = item.numpy()\n\n        # Get the data type of the tensor\n        dtype = item.dtype\n\n        # Map the data type to an index\n        dtype_index = self._dtype_to_indices[dtype]\n\n        # Convert the NumPy array to bytes\n        tensor_bytes = np_array.tobytes()\n\n        # Return the serialized tensor data and the data type index as a string\n        return tensor_bytes, f\"no_header_tensor:{dtype_index}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The dtype is not defined. Please call setup() method with the data format.\")\n        dtype_indice = self._dtype_to_indices[self._dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        tensor_data = item.numpy()\n\n        # Map the tensor's data type to an index\n        dtype_index = self._dtype_to_indices[item.dtype]\n\n        # Convert the NumPy array to bytes\n        tensor_bytes = tensor_data.tobytes()\n\n        # Create the data type string\n        data_type_string = f\"no_header_tensor:{dtype_index}\"\n\n        # Return the serialized data and the data type string\n        return tensor_bytes, data_type_string\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. Please call the setup method before deserializing.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype is not None\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. Please call setup() first.\")\n\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not set. Please call setup() method before deserializing.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not set. Please call the setup method before deserializing.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. Please call setup() to set it.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. Please call setup() with the data format.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not set. Please call setup() method before deserializing the data.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. Please call the setup method to define the data type.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. Please call the setup() method with the data type.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. HINT: Call the setup method with the correct data type.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. Please call setup() method to set it.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. Please call the setup method with the data format first.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. Please call setup() with the data format before deserializing.\")\n\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. HINT: Call the setup() method to set the data type.\")\n\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return tensor.reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        return np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = np.shape(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = np.shape(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        return np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = np.array(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n        return {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        if isinstance(self.dataset, StreamingDataset):\n            return state\n\n        state[\"num_samples_yielded\"] = self._num_samples_yielded_combined\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": self.dataset,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        if isinstance(self.dataset, CombinedStreamingDataset):\n            state_dict[\"num_samples_yielded_combined\"] = deepcopy(self._num_samples_yielded_combined)\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\"dataset\": deepcopy(self.dataset), \"current_epoch\": self.current_epoch}\n\n        if isinstance(self.dataset, StreamingDataset):\n            state_dict[\"num_samples_yielded\"] = self._num_samples_yielded_streaming\n        else:\n            state_dict[\"num_samples_yielded\"] = self._num_samples_yielded_combined\n\n        state_dict[\"latest_worker_idx\"] = self._latest_worker_idx\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\"dataset\": deepcopy(self.dataset), \"current_epoch\": self.current_epoch}\n\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_streaming\n        else:\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_combined\n\n        state[\"latest_worker_idx\"] = self._latest_worker_idx\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\"dataset\": self.dataset, \"current_epoch\": self.current_epoch}\n\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_streaming\n            state[\"latest_worker_idx\"] = self._latest_worker_idx\n        else:\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_combined\n            state[\"latest_worker_idx\"] = self._latest_worker_idx\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict() if isinstance(self.dataset, StreamingDataset) else deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\"torchvision is not installed.\")\n        if not _AV_AVAILABLE:\n            raise RuntimeError(\"av is not installed.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"torchvision is not installed.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is not installed.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n            os.remove(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE or not _AV_AVAILABLE:\n            raise RuntimeError(\"VideoSerializer requires torchvision and av to be installed.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\"torchvision is not installed.\")\n\n        if not _AV_AVAILABLE:\n            raise RuntimeError(\"av is not installed.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\"torchvision is not installed. Please install torchvision to use this function.\")\n        if not _AV_AVAILABLE:\n            raise RuntimeError(\"av is not installed. Please install av to use this function.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\"torchvision is not installed\")\n        if not _AV_AVAILABLE:\n            raise RuntimeError(\"av is not installed\")\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", delete=False) as f:\n            f.write(data)\n            video_object, _, _ = torchvision.io.read_video(f.name)\n        return video_object\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n                f.write(data)\n                f.seek(0)\n                video, _, _ = torchvision.io.read_video(f.name)\n                return video\n        else:\n            raise RuntimeError(\"torchvision is not installed\")\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"torchvision is not installed. Please install it to use this function.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is not installed. Please install it to use this function.\")\n\n        # Write the data to a temporary file\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            video_path = f.name\n\n        # Deserialize the video file using torchvision's read_video function\n        video, _, _ = torchvision.io.read_video(video_path)\n\n        # Delete the temporary file\n        os.unlink(video_path)\n\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\"torchvision is not installed. Please install torchvision to use this function.\")\n        if not _AV_AVAILABLE:\n            raise RuntimeError(\"av is not installed. Please install av to use this function.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\"torchvision is not installed. Please install torchvision to use this function.\")\n\n        if not _AV_AVAILABLE:\n            raise RuntimeError(\"av is not installed. Please install av to use this function.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"torchvision is not installed.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is not installed.\")\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name)\n            os.remove(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE and _AV_AVAILABLE:\n            import av\n            from torchvision.io import read_video\n\n            with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n                f.write(data)\n                f.seek(0)\n                video_object, _, _ = read_video(f.name, pts_unit=\"pts\")\n                return video_object\n        else:\n            raise RuntimeError(\"torchvision and av are required to deserialize video data.\")\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"Torchvision is not installed. Please install torchvision to use this function.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"AV is not installed. Please install AV to use this function.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"torchvision is not installed. Please install it to use this function.\")\n\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is not installed. Please install it to use this function.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not (_TORCH_VISION_AVAILABLE and _AV_AVAILABLE):\n            raise ImportError(\"torchvision and av must be installed to use VideoSerializer\")\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name)\n            os.remove(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise RuntimeError(\"The VideoSerializer requires the av library to be installed.\")\n\n        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\"The VideoSerializer requires the torchvision library to be installed.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as f:\n            f.write(data)\n            video = torch.utils.data.datapipes.iter.FileOpener(f.name)\n            video = torch.utils.data.datapipes.iter.VideoReader(video)\n            video = torch.utils.data.datapipes.iter.VideoDecoder(video)\n            video = torch.utils.data.datapipes.iter.VideoToTensor(video)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is not installed.\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is not installed.\")\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n            os.remove(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The torchvision library is not installed.\")\n\n        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is not installed.\")\n\n        # Create a temporary file to write the data to\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            filepath = f.name\n\n        # Use torchvision's read_video function to deserialize the video file into a video object\n        video, _, _ = torchvision.io.read_video(filepath)\n\n        # Delete the temporary file\n        os.remove(filepath)\n\n        return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"torchvision is not installed. Please install it to use this feature.\")\n\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is not installed. Please install it to use this feature.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n            os.remove(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\"torchvision is not installed. Please install it to use this function.\")\n        if not _AV_AVAILABLE:\n            raise RuntimeError(\"av is not installed. Please install it to use this function.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n        while self._should_write():\n            self.write_chunk()\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, self.get_chunk_filename()) for _ in range(self._chunk_index)]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        while self._should_write():\n            self.write_chunk()\n\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        filepath = self.write_chunks_index()\n        self._is_done = True\n        return [filepath]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n        if self._should_write():\n            self.write_chunk(on_done=True)\n        self._is_done = True\n        return self.write_chunks_index()\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        while self._should_write():\n            self.write_chunk(on_done=True)\n\n        if not self.filled:\n            sleep(1)\n\n        if self.filled:\n            self.write_chunks_index()\n            self._is_done = True\n\n        return [os.path.join(self._cache_dir, self.get_chunk_filename()) for _ in range(self._chunk_index)]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write any remaining chunks\n        while self._should_write():\n            self.write_chunk(on_done=True)\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writer as done\n        self._is_done = True\n\n        # Return the list of written chunks\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        if self._should_write():\n            self.write_chunk(on_done=True)\n\n        if not self.filled:\n            return []\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, self.get_chunk_filename()) for _ in range(self._chunk_index)]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        if self._should_write():\n            self.write_chunk(on_done=True)\n\n        filepaths = []\n        for i in range(self._chunk_index):\n            filepaths.append(self.write_chunk(on_done=True))\n\n        self.write_chunks_index()\n        self._is_done = True\n        return filepaths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self.filled:\n            while self._should_write():\n                self.write_chunk()\n            self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, self.get_chunk_filename()) for _ in range(self._chunk_index)]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        if self._should_write():\n            self.write_chunk(on_done=True)\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, f) for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write any remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of written chunks\n        return [os.path.join(self._cache_dir, f\"chunk-{self.rank}-{i}.bin\") for i in range(self._chunk_index)]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write any remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of chunk file paths\n        return [os.path.join(self._cache_dir, f\"chunk-{self.rank}-{i}.bin\") for i in range(self._chunk_index)]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        while self._should_write():\n            self.write_chunk()\n\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\")]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        while self._should_write():\n            self.write_chunk()\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write any remaining chunks\n        while self._should_write():\n            self.write_chunk(on_done=True)\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of written chunks\n        return [os.path.join(self._cache_dir, self.get_chunk_filename()) for _ in self._chunks_info]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n        while self._should_write():\n            self.write_chunk()\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\")]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            raise RuntimeError(\"The writing process is already marked as complete.\")\n\n        while self._should_write():\n            self.write_chunk(on_done=True)\n\n        filepaths = []\n        for chunk_index in range(self._chunk_index):\n            filepaths.append(self.write_chunk(on_done=True))\n\n        filepath = self.write_chunks_index()\n        filepaths.append(filepath)\n\n        self._is_done = True\n        return filepaths\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        while self._should_write():\n            self.write_chunk()\n\n        # If there are still items, write them as a chunk\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        # Write the index file\n        filepath = self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        return [filepath]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write any remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of chunks file paths\n        return [os.path.join(self._cache_dir, f\"chunk-{self.rank}-{i}.bin\") for i in range(self._chunk_index)]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        while self._should_write():\n            self.write_chunk()\n        if not self.filled:\n            # Wait for other workers to finish writing\n            sleep(1)\n            if not self.filled:\n                raise RuntimeError(\"The workers didn't finish writing.\")\n\n        self.write_chunks_index()\n        self._is_done = True\n\n        return [os.path.join(self._cache_dir, self.get_chunk_filename()) for _ in self._chunks_info]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self.filled:\n            while self._should_write():\n                self.write_chunk()\n            self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\")]\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\")\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {type(self.dataset)}.\"\n            )\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self._worker_idx_iter = iter(self._worker_idx)\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx_iter = iter(cycle(list(range(self.num_workers if self.num_workers > 0 else 1))))\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx_iter = iter(cycle(list(range(self.num_workers if self.num_workers > 0 else 1))))\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n        else:\n            raise RuntimeError(\"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\")\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self._worker_idx_iter = iter(self._worker_idx)\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n\n        if self.num_workers == 0:\n            self._worker_idx_iter = None\n        else:\n            self._worker_idx_iter = iter(self._worker_idx)\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                f\"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset. Found {self.dataset}.\"\n            )\n\n        self.dataset._set_use_streaming_dataloader(True)\n        self.restore = True\n\n        if self.num_workers > 0:\n            self._worker_idx = cycle(list(range(self.num_workers)))\n            self._worker_idx_iter = iter(self._worker_idx)\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._worker_idx_iter = iter(cycle(list(range(self.num_workers if self.num_workers > 0 else 1))))\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._worker_idx_iter = iter(cycle(list(range(self.num_workers if self.num_workers > 0 else 1))))\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        assert num_samples_yielded is not None\n\n        state_dict: Dict[str, Any] = {}\n        for i, dataset in enumerate(self._datasets):\n            state_dict[f\"{i}\"] = dataset.state_dict(num_workers, batch_size, num_samples_yielded[i])\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        assert num_samples_yielded is not None\n        return {\n            __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n            __SAMPLES_KEY__: [d.state_dict(num_workers, batch_size) for d in self._datasets],\n        }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        state_dict = {}\n        for dataset in self._datasets:\n            state_dict.update(dataset.state_dict(num_workers, batch_size))\n        return state_dict\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n\n        return {\n            __NUM_SAMPLES_YIELDED_KEY__: self._num_samples_yielded,\n            __SAMPLES_KEY__: [dataset.state_dict() for dataset in self._datasets],\n        }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return {\n                __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n                __SAMPLES_KEY__: [dataset.state_dict(num_workers, batch_size) for dataset in self._datasets],\n            }\n\n        return self._iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is None:\n            return self._iterator.state_dict()\n\n        return {\n            __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n            __SAMPLES_KEY__: self._iterator.state_dict(),\n        }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return {__NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded}\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            state_dict = self._iterator.state_dict()\n        else:\n            assert num_samples_yielded is not None\n            state_dict = {\n                __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n                __SAMPLES_KEY__: [dataset.state_dict() for dataset in self._datasets],\n            }\n\n        return {\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"state_dict\": state_dict,\n        }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict()\n\n        assert num_samples_yielded is not None\n\n        return {\n            __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n            __SAMPLES_KEY__: [d.state_dict(num_workers, batch_size) for d in self._datasets],\n        }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return {\n            __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n            __SAMPLES_KEY__: [d.state_dict(num_workers, batch_size) for d in self._datasets],\n        }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n        return {\n            __NUM_SAMPLES_YIELDED_KEY__: self._num_samples_yielded,\n            __SAMPLES_KEY__: self._iterator.state_dict(num_workers, batch_size),\n        }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return {\n                __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n                __SAMPLES_KEY__: [d.state_dict(num_workers, batch_size) for d in self._datasets],\n            }\n\n        return self._iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return {\n                __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n                __SAMPLES_KEY__: [d.state_dict(num_workers, batch_size) for d in self._datasets],\n            }\n\n        return self._iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        assert num_samples_yielded is not None\n        return {\n            __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n            __SAMPLES_KEY__: [d.state_dict() for d in self._datasets],\n        }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        assert num_samples_yielded is not None\n\n        return {\n            __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n            __SAMPLES_KEY__: [dataset.state_dict(num_workers, batch_size) for dataset in self._datasets],\n        }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n        else:\n            assert num_samples_yielded is not None\n            return {__NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded}\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            assert num_samples_yielded is not None\n            return {\n                __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n                __SAMPLES_KEY__: [d.state_dict(num_workers, batch_size) for d in self._datasets],\n            }\n\n        return self._iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return {\n                __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n                __SAMPLES_KEY__: [\n                    dataset.state_dict(num_workers, batch_size, num_samples_yielded[i])\n                    for i, dataset in enumerate(self._datasets)\n                ],\n            }\n\n        return self._iterator.state_dict()\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict()\n\n        if num_samples_yielded is None:\n            raise RuntimeError(\n                \"The num_samples_yielded must be provided when the CombinedStreamingDataset is not iterated over\"\n            )\n\n        datasets_state_dict = {}\n        for i, dataset in enumerate(self._datasets):\n            datasets_state_dict[i] = dataset.state_dict(num_workers, batch_size, num_samples_yielded[i])\n\n        return datasets_state_dict\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is None:\n            return self._iterator.state_dict()\n\n        return {\n            __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n            __SAMPLES_KEY__: self._iterator.state_dict(),\n        }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)\n            return\n\n        if __NUM_SAMPLES_YIELDED_KEY__ not in state_dict:\n            return\n\n        num_samples_yielded = state_dict.pop(__NUM_SAMPLES_YIELDED_KEY__)\n\n        if __SAMPLES_KEY__ not in state_dict:\n            return\n\n        samples = state_dict.pop(__SAMPLES_KEY__)\n\n        for dataset in self._datasets:\n            dataset.load_state_dict(samples[dataset.__class__.__name__])\n\n        self._num_samples_yielded = num_samples_yielded\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            del state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n            for dataset_state_dict in state_dict.values():\n                if __SAMPLES_KEY__ in dataset_state_dict:\n                    del dataset_state_dict[__SAMPLES_KEY__]\n\n            for dataset in self._datasets:\n                dataset.load_state_dict(state_dict)\n\n            if self._use_streaming_dataloader:\n                self._num_samples_yielded = num_samples_yielded\n        else:\n            for dataset in self._datasets:\n                dataset.load_state_dict(state_dict)\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__, None)\n            self._use_streaming_dataloader = state_dict.get(__USE_STREAMING_DATALOADER_KEY__, False)\n            self._datasets = state_dict.get(__SAMPLES_KEY__, [])\n            self._iterator = _CombinedDatasetIterator(\n                self._datasets,\n                self._seed,\n                self._weights,\n                self._use_streaming_dataloader,\n                self._num_samples_yielded,\n            )\n        else:\n            self._iterator.load_state_dict(state_dict)\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__)\n        self._use_streaming_dataloader = state_dict.get(__SAMPLES_KEY__)\n\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict[dataset.__class__.__name__])\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            for dataset in self._datasets:\n                if __SAMPLES_KEY__ in state_dict[dataset.__class__.__name__]:\n                    dataset.load_state_dict(state_dict[dataset.__class__.__name__])\n            self._num_samples_yielded = num_samples_yielded\n        else:\n            for dataset in self._datasets:\n                if __SAMPLES_KEY__ in state_dict[dataset.__class__.__name__]:\n                    dataset.load_state_dict(state_dict[dataset.__class__.__name__])\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = state_dict.pop(__NUM_SAMPLES_YIELDED_KEY__, None)\n        samples = state_dict.pop(__SAMPLES_KEY__, None)\n\n        if samples is not None:\n            for dataset, sample in zip(self._datasets, samples):\n                dataset.load_state_dict(sample)\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            del state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n        if __SAMPLES_KEY__ in state_dict:\n            for dataset_state in state_dict[__SAMPLES_KEY__]:\n                dataset = dataset_state[\"dataset\"]\n                num_samples_yielded = dataset_state[\"num_samples_yielded\"]\n                dataset.load_state_dict(num_samples_yielded)\n            del state_dict[__SAMPLES_KEY__]\n\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict)\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            self._datasets = state_dict[__SAMPLES_KEY__]\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n        else:\n            self._iterator.load_state_dict(state_dict)\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n        if __SAMPLES_KEY__ in state_dict:\n            for dataset in self._datasets:\n                dataset.load_state_dict(state_dict[__SAMPLES_KEY__])\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            del state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n        if __SAMPLES_KEY__ in state_dict:\n            samples = state_dict[__SAMPLES_KEY__]\n            del state_dict[__SAMPLES_KEY__]\n            for dataset in self._datasets:\n                dataset.load_state_dict(samples)\n\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict)\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            for dataset in self._datasets:\n                dataset.load_state_dict(state_dict[__SAMPLES_KEY__][dataset.dataset_name])\n            self._num_samples_yielded = num_samples_yielded\n        else:\n            for dataset in self._datasets:\n                dataset.load_state_dict(state_dict[dataset.dataset_name])\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__)\n        self._use_streaming_dataloader = state_dict.get(__SAMPLES_KEY__) is not None\n\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict)\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict[dataset.__class__.__name__])\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        for dataset in self._datasets:\n            dataset_state_dict = state_dict.get(dataset.__class__.__name__)\n            if dataset_state_dict is not None:\n                dataset.load_state_dict(dataset_state_dict)\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            self._iterator = None\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._check_datasets(state_dict[__SAMPLES_KEY__])\n        for dataset in state_dict[__SAMPLES_KEY__]:\n            dataset.load_state_dict(state_dict[__SAMPLES_KEY__][dataset])\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None or not state_dict:\n            return\n\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict.get(dataset.name, {}))\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            del state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n        for dataset in self._datasets:\n            if dataset.__class__.__name__ in state_dict:\n                dataset.load_state_dict(state_dict[dataset.__class__.__name__])\n                del state_dict[dataset.__class__.__name__]\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if not state_dict:\n            return\n\n        for dataset in self._datasets:\n            dataset_state_dict = state_dict.get(dataset.__class__.__name__)\n            if dataset_state_dict is not None:\n                dataset.load_state_dict(dataset_state_dict)\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._datasets = state_dict[__SAMPLES_KEY__]\n        self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict[dataset.name])\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict[__SAMPLES_KEY__][dataset.dataset_name])\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path.replace(\"file://\", \"\"))\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path."}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if not dir_path:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"$\"):\n        return Dir(path=os.environ.get(dir_path[1:]))\n\n    if dir_path.startswith(\"${\") and dir_path.endswith(\"}\"):\n        return Dir(path=os.environ.get(dir_path[2:-1]))\n\n    if dir_path.startswith(\"${\"):\n        return Dir(path=os.environ.get(dir_path[2:].split(\"}\")[0]))\n\n    if dir_path.startswith(\"$\"):\n        return Dir(path=os.environ.get(dir_path[1:].split(\"}\")[0]))\n\n    if dir_path.startswith(\"$\"):\n        return Dir(path=os.environ.get(dir_path[1:].split(\"}\")[0]))\n\n    if dir_path.startswith(\"$\"):\n        return Dir(path=os.environ.get(dir_path[1:].split(\"}\")[0]))\n\n    if dir_path.startswith(\"$\"):\n        return Dir(path=os.environ.get(dir_path[1:].split(\"}\")[0]))\n\n    if dir_path.startswith(\"$\""}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"https://\") or dir_path.startswith(\"http://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"hdfs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"ssh://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"sftp://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"ftp://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"ftps://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"smb://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"nfs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"cifs://\"):\n        return Dir(url=dir_path"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:])\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(path=dir_path, url=dir_path)\n        elif dir_path.startswith(\"file://\"):\n            return Dir(path=dir_path[7:])\n        elif dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"s3://\"):\n            return Dir(path=dir_path, url=dir_path)\n        elif dir_path.startswith(\"file://\"):\n            return Dir(path=dir_path[7:])\n        elif dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n            return Dir(url=dir_path)\n        elif dir_path.startswith(\"s3://\"):\n            return Dir(path=dir_path, url=dir_path)\n        elif dir_path.startswith(\"file://\"):\n            return Dir(path=dir_path[7:])\n        elif dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n            return Dir(url=dir_path)\n        else:\n            return Dir(path=dir_path)\n    elif isinstance(dir_path, Dir):\n        return dir_path\n    else:\n        raise ValueError(f\"Invalid directory path or Dir object: {dir_path}\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    dir_path = str(dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path))\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=os.path.join(os.path.expanduser(\"~\"), dir_path[2:]))\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path))\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=os.path.join(os.path.expanduser(\"~\"), dir_path[2:]))\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"https://\") or dir_path.startswith(\"http://\"):\n        return Dir(url=dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, str):\n        dir_path = Dir(path=dir_path)\n    elif isinstance(dir_path, Dir):\n        pass\n    else:\n        raise TypeError(f\"Unsupported type for dir_path: {type(dir_path)}\")\n\n    if dir_path.path is None:\n        return dir_path\n\n    if dir_path.path.startswith(\"s3://\"):\n        dir_path.url = dir_path.path\n        dir_path.path = None\n    elif dir_path.path.startswith(\"https://\"):\n        dir_path.url = dir_path.path\n        dir_path.path = None\n    elif dir_path.path.startswith(\"gs://\"):\n        dir_path.url = dir_path.path\n        dir_path.path = None\n    elif dir_path.path.startswith(\"file://\"):\n        dir_path.url = dir_path.path\n        dir_path.path = None\n    elif dir_path.path.startswith(\"/\"):\n        dir_path.url = dir_path.path\n        dir_path.path = None\n    elif dir_path.path.startswith(\"./\"):\n        dir_path.path = dir_path.path[2:]\n    elif dir_path.path.startswith(\"../\"):\n        dir_path.path = dir_path.path[3:]\n    elif dir_path.path.startswith(\"~/\"):\n        dir_path.path = dir_path.path[2:]\n    elif dir_path.path.startswith(\"~\"):\n        dir_path.path = dir_path.path[1:]\n    elif dir_path.path.startswith(\"./\"):\n        dir_path.path = dir_path.path[2:]\n    elif dir_path.path.startswith(\"../\"):\n        dir_path.path = dir_path.path[3:]\n    elif dir_path.path.startswith(\"~/\"):\n        dir_path.path = dir"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[3:]))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path[2:]))\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, str):\n        dir_path = Dir(path=dir_path)\n\n    if dir_path.path is None:\n        raise ValueError(\"dir_path must be a string or a Dir object with a path attribute.\")\n\n    if dir_path.path.startswith(\"s3://\"):\n        dir_path.url = dir_path.path\n        dir_path.path = None\n    elif dir_path.path.startswith(\"https://\"):\n        dir_path.url = dir_path.path\n        dir_path.path = None\n    elif dir_path.path.startswith(\"s3://\") or dir_path.path.startswith(\"https://\"):\n        dir_path.url = dir_path.path\n        dir_path.path = None\n    elif dir_path.path.startswith(\"/\"):\n        dir_path.url = f\"file://{dir_path.path}\"\n        dir_path.path = None\n    elif dir_path.path.startswith(\"~\"):\n        dir_path.path = os.path.expanduser(dir_path.path)\n\n    return dir_path\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(path=dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:])\n\n    if dir_path.startswith(\"project://\"):\n        project_path = dir_path[10:]\n        if project_path.startswith(\"projects/\"):\n            project_path = project_path[9:]\n        return Dir(path=project_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"projects/\") or dir_path.startswith(\"project/\") or dir_path.startswith(\"project\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        dir_path = os.path.expanduser(dir_path)\n\n    if os.path.isabs(dir_path):\n        return Dir(path=dir_path)\n\n    return Dir(path=os.path.abspath(dir_path))\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path[2:])\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path[1:])\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path[2:])\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path[3:])\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(url=dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:])\n\n    if dir_path.startswith(\"projects\"):\n        return Dir(path=dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        dir_path = os.path.expanduser(dir_path)\n\n    if os.path.isdir(dir_path):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"projects\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"http\"):\n        return Dir(url=dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        dir_path = os.path.expanduser(dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"project://\"):\n        return Dir(url=dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(url=dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if output_dir.url is not None:\n        if not output_dir.url.startswith(\"s3://\"):\n            raise ValueError(f\"`output_dir` must start with `s3://`, got: {output_dir.url}\")\n\n        # Check if the directory is empty\n        s3_client = boto3.client(\"s3\")\n        bucket_name, key = parse.urlparse(output_dir.url).netloc, parse.urlparse(output_dir.url).path.lstrip(\"/\")\n        try:\n            response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=key)\n            if response.get(\"Contents\"):\n                raise ValueError(\n                    f\"The directory `{output_dir.url}` is not empty. Use `append=True` or `overwrite=True` to append or overwrite data.\"\n                )\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"NoSuchBucket\":\n                raise ValueError(f\"Bucket `{bucket_name}` does not exist.\")\n            elif e.response[\"Error\"][\"Code\"] == \"404\":\n                raise ValueError(f\"The directory `{output_dir.url}` does not exist.\")\n            else:\n                raise e\n\n    if output_dir.path is not None:\n        if not os.path.exists(output_dir.path):\n            raise ValueError(f\"The directory `{output_dir.path}` does not exist.\")\n\n        if os.listdir(output_dir.path):\n            raise ValueError(\n                f\"The directory `{output_dir.path}` is not empty. Use `append=True` or `overwrite=True` to append or overwrite data.\"\n            )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if output_dir.url is None and output_dir.path is None:\n        raise ValueError(f\"`output_dir` must have either a `url` or a `path`, got: {output_dir}\")\n\n    if output_dir.url is not None and not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must start with `s3://`, got: {output_dir}\")\n\n    if output_dir.url is not None:\n        if not _BOTO3_AVAILABLE:\n            raise ValueError(\"`output_dir` must be an S3 bucket, but `boto3` is not installed.\")\n\n        parsed_url = parse.urlparse(output_dir.url)\n        bucket_name = parsed_url.netloc\n        bucket_path = parsed_url.path\n\n        s3 = boto3.resource(\"s3\")\n        bucket = s3.Bucket(bucket_name)\n        objects = list(bucket.objects.filter(Prefix=bucket_path))\n\n        if len(objects) > 0:\n            raise ValueError(f\"`output_dir` must be an empty S3 bucket, got: {output_dir}\")\n\n    if output_dir.path is not None:\n        if not os.path.isdir(output_dir.path):\n            raise ValueError(f\"`output_dir` must be an existing directory, got: {output_dir}\")\n\n        if len(os.listdir(output_dir.path)) > 0:\n            raise ValueError(f\"`output_dir` must be an empty directory, got: {output_dir}\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be an instance of `Dir`, got: {output_dir}\")\n\n    if output_dir.path is not None:\n        if not os.path.exists(output_dir.path):\n            os.makedirs(output_dir.path)\n\n        if not os.path.isdir(output_dir.path):\n            raise ValueError(f\"`output_dir` must be a directory, got: {output_dir.path}\")\n\n        if os.listdir(output_dir.path):\n            raise ValueError(f\"`output_dir` must be an empty directory, got: {output_dir.path}\")\n\n    elif output_dir.url is not None:\n        if not output_dir.url.startswith(\"s3://\"):\n            raise ValueError(f\"`output_dir` must start with `s3://`, got: {output_dir.url}\")\n\n        if _BOTO3_AVAILABLE:\n            bucket_name, *prefix = parse.urlparse(output_dir.url).path.split(\"/\")[1:]\n            prefix = \"/\".join(prefix)\n\n            s3 = boto3.resource(\"s3\")\n\n            try:\n                bucket = s3.Bucket(bucket_name)\n                objects = bucket.objects.filter(Prefix=prefix)\n                if any(objects):\n                    raise ValueError(f\"`output_dir` must be an empty directory, got: {output_dir.url}\")\n            except botocore.exceptions.ClientError as e:\n                if e.response[\"Error\"][\"Code\"] == \"NoSuchBucket\":\n                    raise ValueError(f\"`output_dir` must be an existing bucket, got: {output_dir.url}\")\n                else:\n                    raise e\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be an instance of `Dir`, got: {output_dir}\")\n\n    if output_dir.url is None:\n        raise ValueError(f\"`output_dir` must be an `s3://` directory, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must be an `s3://` directory, got: {output_dir}\")\n\n    if append or overwrite:\n        raise NotImplementedError(f\"`append` and `overwrite` are not implemented yet.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\n            \"`_assert_dir_is_empty` requires `boto3` to be installed. Please install it with `pip install boto3`.\"\n        )\n\n    # Parse the S3 URL to get the bucket name and object key\n    parsed_url = parse.urlparse(output_dir.url)\n    bucket_name = parsed_url.netloc\n    object_key = parsed_url.path.lstrip(\"/\")\n\n    # Create an S3 client\n    s3_client = boto3.client(\"s3\")\n\n    # Check if the object exists in the bucket\n    try:\n        s3_client.head_object(Bucket=bucket_name, Key=object_key)\n        raise ValueError(f\"The directory `{output_dir}` is not empty.\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            # The object does not exist, so the directory is empty\n            return\n        else:\n            # Some other error occurred, raise it\n            raise e\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if output_dir.path is not None:\n        raise ValueError(f\"`output_dir` must be an `s3://` URL, got: {output_dir}\")\n\n    if output_dir.url is None:\n        raise ValueError(f\"`output_dir` must be an `s3://` URL, got: {output_dir}\")\n\n    if append or overwrite:\n        raise ValueError(\"`append` and `overwrite` are not supported yet.\")\n\n    if _BOTO3_AVAILABLE:\n        s3 = boto3.resource(\"s3\")\n        bucket_name, key = _parse_s3_url(output_dir.url)\n        bucket = s3.Bucket(bucket_name)\n        objects = list(bucket.objects.filter(Prefix=key))\n        if len(objects) > 0:\n            raise ValueError(f\"`output_dir` is not empty: {output_dir}\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if output_dir.url is None:\n        raise ValueError(\"`output_dir` must be an S3 URL.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\"`boto3` is not installed.\")\n\n    if not append and not overwrite:\n        s3_client = boto3.client(\"s3\")\n        bucket, key = parse.urlparse(output_dir.url).netloc, parse.urlparse(output_dir.url).path.lstrip(\"/\")\n        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=key)\n        if \"Contents\" in response:\n            raise ValueError(f\"`output_dir` already contains data.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if output_dir.path is not None:\n        raise ValueError(f\"`output_dir` must be an S3 URL, got: {output_dir.path}\")\n\n    if output_dir.url is None:\n        raise ValueError(f\"`output_dir` must be an S3 URL, got: {output_dir.url}\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The `boto3` library is required to use `_assert_dir_is_empty`. Please install it with `pip install boto3`.\"\n        )\n\n    parsed_url = parse.urlparse(output_dir.url)\n    bucket_name = parsed_url.netloc\n    prefix = parsed_url.path[1:]\n\n    s3_client = boto3.client(\"s3\")\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n\n    if page_iterator.search(\"Contents\"):\n        raise ValueError(f\"The directory {output_dir.url} is not empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be an instance of the Dir class, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must start with 's3://', got: {output_dir.url}\")\n\n    if append:\n        raise NotImplementedError(\"Append mode is not implemented yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwrite mode is not implemented yet.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\"`boto3` is required to check if a directory is empty.\")\n\n    s3 = boto3.resource(\"s3\")\n    bucket_name, prefix = _parse_s3_url(output_dir.url)\n    bucket = s3.Bucket(bucket_name)\n    objects = list(bucket.objects.filter(Prefix=prefix))\n    if objects:\n        raise ValueError(f\"The directory {output_dir.url} is not empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\"`_assert_dir_is_empty` requires `boto3` to be installed.\")\n\n    if output_dir.url is None:\n        raise ValueError(f\"`output_dir` must be an `s3://` url, got: {output_dir.url}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must be an `s3://` url, got: {output_dir.url}\")\n\n    bucket, *prefix = output_dir.url[5:].split(\"/\", 1)\n\n    prefix = \"/\".join(prefix)\n\n    s3 = boto3.resource(\"s3\")\n    bucket = s3.Bucket(bucket)\n\n    try:\n        objects = list(bucket.objects.filter(Prefix=prefix))\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchBucket\":\n            raise ValueError(f\"Bucket {bucket} does not exist.\")\n        else:\n            raise e\n\n    if objects:\n        raise ValueError(f\"Directory {output_dir.url} is not empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The `output_dir` must be an instance of the `Dir` class, got: {output_dir}\")\n\n    if output_dir.url is None:\n        raise ValueError(f\"The `output_dir` must be an S3 URL, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"The `output_dir` must be an S3 URL, got: {output_dir}\")\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\n            \"The `_assert_dir_is_empty` function requires the `boto3` package. Please install it with `pip install boto3`.\"\n        )\n\n    if not append and not overwrite:\n        s3_client = boto3.client(\"s3\")\n\n        # Extract the bucket name and key prefix from the S3 URL\n        parsed_url = parse.urlparse(output_dir.url)\n        bucket_name = parsed_url.netloc\n        key_prefix = parsed_url.path.lstrip(\"/\")\n\n        # Check if the directory is empty\n        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=key_prefix)\n        if \"Contents\" in response and len(response[\"Contents\"]) > 0:\n            raise ValueError(f\"The `output_dir` must be empty, got: {output_dir}\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The output_dir must be a Dir object, not {type(output_dir)}\")\n\n    if output_dir.url is None:\n        raise ValueError(f\"The output_dir must have a url, not {output_dir.url}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"The output_dir must start with s3://, not {output_dir.url}\")\n\n    if not _BOTO3_AVAILABLE:\n        raise ValueError(\"boto3 is not installed, please install it with `pip install boto3`\")\n\n    bucket_name, key = parse.urlsplit(output_dir.url).netloc, parse.urlsplit(output_dir.url).path.lstrip(\"/\")\n\n    s3 = boto3.resource(\"s3\")\n    bucket = s3.Bucket(bucket_name)\n\n    try:\n        response = bucket.meta.client.list_objects_v2(Bucket=bucket_name, Prefix=key)\n        if response[\"KeyCount\"] > 0:\n            raise ValueError(f\"The output_dir {output_dir.url} is not empty\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchBucket\":\n            raise ValueError(f\"The bucket {bucket_name} does not exist\")\n        else:\n            raise ValueError(f\"An error occurred while checking the output_dir {output_dir.url}: {e}\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir` object, got: {output_dir}\")\n\n    if output_dir.path is not None:\n        raise ValueError(f\"`output_dir.path` must be None, got: {output_dir.path}\")\n\n    if not output_dir.url:\n        raise ValueError(f\"`output_dir.url` must be a string, got: {output_dir.url}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir.url` must start with 's3://', got: {output_dir.url}\")\n\n    if append or overwrite:\n        raise ValueError(\"`append` and `overwrite` are not implemented yet.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The `boto3` package is not installed.\")\n\n    bucket_name, *key_parts = parse.urlparse(output_dir.url).path.lstrip(\"/\").split(\"/\")\n\n    s3_client = boto3.client(\"s3\")\n\n    try:\n        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=\"/\".join(key_parts))\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchBucket\":\n            raise ValueError(f\"Bucket {bucket_name} does not exist.\")\n        raise e\n\n    if response[\"KeyCount\"] > 0:\n        raise ValueError(f\"Directory {output_dir.url} is not empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The provided `output_dir` must be an instance of the Dir class. Got: {output_dir}\")\n\n    if output_dir.url is not None:\n        if not output_dir.url.startswith(\"s3://\"):\n            raise ValueError(f\"The provided `output_dir` must start with `s3://`. Got: {output_dir.url}\")\n\n        if not _BOTO3_AVAILABLE:\n            raise RuntimeError(\n                \"The `boto3` package is required to check if a directory in S3 is empty. Please install it with `pip install boto3`.\"\n            )\n\n        bucket_name, *prefix = output_dir.url.split(\"/\", 3)[2:]\n\n        s3 = boto3.resource(\"s3\")\n        bucket = s3.Bucket(bucket_name)\n        objects = list(bucket.objects.filter(Prefix=\"/\".join(prefix)))\n\n        if objects:\n            raise ValueError(f\"The provided `output_dir` is not empty. Got: {output_dir.url}\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if output_dir.url is None:\n        return\n\n    if output_dir.path is not None:\n        raise ValueError(f\"`output_dir` must be an S3 bucket, got: {output_dir.path}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must be an S3 bucket, got: {output_dir.url}\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"You need to install `boto3` to use `_assert_dir_is_empty`. You can do so by running `pip install boto3`.\"\n        )\n\n    parsed_url = parse.urlparse(output_dir.url)\n    bucket_name = parsed_url.netloc\n    prefix = parsed_url.path.lstrip(\"/\")\n\n    s3_client = boto3.client(\"s3\")\n\n    try:\n        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchBucket\":\n            raise ValueError(f\"Bucket {bucket_name} does not exist.\")\n        raise e\n\n    if \"Contents\" in response:\n        if not append and not overwrite:\n            raise ValueError(f\"Directory {output_dir.url} is not empty. Use append or overwrite to specify behavior.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if output_dir.url is not None:\n        if not output_dir.url.startswith(\"s3://\"):\n            raise ValueError(f\"`output_dir` must start with `s3://`, got: {output_dir.url}\")\n\n        if not _BOTO3_AVAILABLE:\n            raise RuntimeError(\n                \"You need to install `boto3` to use the `output_dir` argument with the `s3://` protocol.\"\n            )\n\n        bucket_name, *rest = output_dir.url[5:].split(\"/\")\n\n        s3 = boto3.resource(\"s3\")\n        bucket = s3.Bucket(bucket_name)\n\n        if not append and not overwrite:\n            if list(bucket.objects.filter(Prefix=\"/\".join(rest))):\n                raise ValueError(\n                    f\"The `output_dir` already contains data, use `append=True` or `overwrite=True` to proceed.\"\n                )\n\n    if output_dir.path is not None:\n        if not os.path.isdir(output_dir.path):\n            raise ValueError(f\"`output_dir` must be a valid directory, got: {output_dir.path}\")\n\n        if not append and not overwrite:\n            if os.listdir(output_dir.path):\n                raise ValueError(\n                    f\"The `output_dir` already contains data, use `append=True` or `overwrite=True` to proceed.\"\n                )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be an instance of the Dir class, got: {output_dir}\")\n\n    if output_dir.url is None and output_dir.path is None:\n        raise ValueError(\"`output_dir` must contain either a `url` or a `path`.\")\n\n    if output_dir.url is not None and output_dir.url.startswith(\"s3://\"):\n        if _BOTO3_AVAILABLE:\n            s3 = boto3.resource(\"s3\")\n            bucket_name, key = parse.urlparse(output_dir.url).netloc, parse.urlparse(output_dir.url).path.lstrip(\"/\")\n            bucket = s3.Bucket(bucket_name)\n            objects = list(bucket.objects.filter(Prefix=key))\n            if len(objects) > 0:\n                raise ValueError(\n                    f\"The directory {output_dir.url} is not empty. Please provide an empty directory or use the `append` or `overwrite` arguments.\"\n                )\n\n    elif output_dir.path is not None:\n        if os.path.exists(output_dir.path) and os.listdir(output_dir.path):\n            raise ValueError(\n                f\"The directory {output_dir.path} is not empty. Please provide an empty directory or use the `append` or `overwrite` arguments.\"\n            )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The `output_dir` must be an instance of the Dir class.\")\n\n    if output_dir.url is not None and not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(\"The `output_dir` must start with 's3://'.\")\n\n    if output_dir.path is not None:\n        raise ValueError(\"The `output_dir` cannot be a local path.\")\n\n    if output_dir.url is not None:\n        s3_client = boto3.client(\"s3\")\n        s3_resource = boto3.resource(\"s3\")\n\n        bucket_name, key = parse.urlparse(output_dir.url).netloc, parse.urlparse(output_dir.url).path.lstrip(\"/\")\n\n        try:\n            s3_resource.Object(bucket_name, key).load()\n            raise ValueError(\n                f\"The `output_dir` directory {output_dir.url} is not empty. Please use an empty directory or specify the `overwrite` argument.\"\n            )\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                return\n            else:\n                raise e\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be an instance of `Dir`, got: {output_dir}\")\n\n    if output_dir.url is None:\n        raise ValueError(f\"`output_dir` must have an `url`, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must have an `url` starting with `s3://`, got: {output_dir}\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The `boto3` package is not installed.\")\n\n    if append or overwrite:\n        raise NotImplementedError(\n            \"The `append` and `overwrite` arguments are not implemented yet. Please use the `output_dir` argument to specify an empty directory.\"\n        )\n\n    s3_resource = boto3.resource(\"s3\")\n    bucket_name, *path = output_dir.url[5:].split(\"/\")\n    bucket = s3_resource.Bucket(bucket_name)\n    objects = bucket.objects.filter(Prefix=\"/\".join(path))\n\n    if list(objects.all()):\n        raise ValueError(f\"The directory `{output_dir}` is not empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if output_dir.path is not None:\n        if not Path(output_dir.path).is_dir():\n            raise ValueError(f\"`output_dir` must be a directory, got: {output_dir}\")\n\n    if output_dir.url is not None:\n        if not output_dir.url.startswith(\"s3://\"):\n            raise ValueError(f\"`output_dir` must start with 's3://', got: {output_dir}\")\n\n        if not _BOTO3_AVAILABLE:\n            raise ImportError(\"boto3 is not installed\")\n\n        bucket_name, *rest = output_dir.url[5:].split(\"/\", 1)\n        key = \"/\".join(rest)\n\n        s3 = boto3.resource(\"s3\")\n        bucket = s3.Bucket(bucket_name)\n\n        if bucket.objects.filter(Prefix=key).count() > 0:\n            raise ValueError(f\"`output_dir` must be empty, got: {output_dir}\")\n\n    if append:\n        raise NotImplementedError(\"Append is not implemented yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwrite is not implemented yet.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be an instance of `Dir`, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must be an S3 URL, got: {output_dir.url}\")\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\"boto3 is not installed.\")\n\n    if not append and not overwrite:\n        raise NotImplementedError(\"Append and overwrite are not implemented.\")\n\n    if not append and overwrite:\n        raise NotImplementedError(\"Append and overwrite are not implemented.\")\n\n    bucket_name, key = parse.urlsplit(output_dir.url).netloc, parse.urlsplit(output_dir.url).path.lstrip(\"/\")\n    s3_client = boto3.client(\"s3\")\n    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=key)\n\n    if \"Contents\" in response:\n        raise ValueError(f\"Directory {output_dir.url} is not empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"CommonPrefixes\" in objects and len(objects[\"CommonPrefixes\"]) > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if \"Contents\" in objects and len(objects[\"Contents\"]) > 0:\n        for content in objects[\"Contents\"]:\n            if content[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file. Datasets are meant to be immutable.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"CommonPrefixes\" in objects:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if \"Contents\" in objects:\n        for content in objects[\"Contents\"]:\n            if content[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        for content in objects[\"Contents\"]:\n            if content[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index.json file. \"\n                    \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n        # Delete all objects within the specified prefix in the bucket\n        s3.delete_objects(\n            Bucket=obj.netloc,\n            Delete={\n                \"Objects\": [{\"Key\": obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\" + obj[\"Key\"]} for obj in objects[\"Contents\"]]\n            },\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects:\n        for obj in objects[\"Contents\"]:\n            if obj[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n    if \"CommonPrefixes\" in objects:\n        for obj in objects[\"CommonPrefixes\"]:\n            if obj[\"Prefix\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n    # If there are no objects, we don't need to delete anything\n    if \"Contents\" not in objects:\n        return\n\n    # Delete all objects within the specified prefix\n    s3.delete_objects(\n        Bucket=obj.netloc,\n        Delete={\n            \"Objects\": [{\"Key\": obj[\"Key\"]} for obj in objects[\"Contents\"]],\n        },\n    )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` is empty.\"\n        )\n\n    if \"index.json\" in [obj[\"Key\"] for obj in objects[\"Contents\"]]:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index.json file.\"\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects:\n        for content in objects[\"Contents\"]:\n            if content[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index.json file and datasets are meant to be immutable.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if objects[\"KeyCount\"] > 0 and any(obj[\"Key\"].endswith(\"index.json\") for obj in objects.get(\"Contents\", [])):\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'. \"\n            \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if objects[\"KeyCount\"] > 0:\n        # Delete all objects within the specified prefix in the bucket\n        for obj in objects.get(\"Contents\", []):\n            s3.delete_object(Bucket=obj[\"Bucket\"], Key=obj[\"Key\"])\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # Check if the index.json file already exists\n    index_file_exists = False\n    for obj in objects[\"Contents\"]:\n        if obj[\"Key\"] == \"index.json\":\n            index_file_exists = True\n            break\n\n    # If the index.json file exists, delete all objects within the prefix\n    if index_file_exists:\n        for obj in objects[\"Contents\"]:\n            s3.delete_object(Bucket=obj[\"Bucket\"], Key=obj[\"Key\"])\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # Check if the directory already contains an index.json file.\n    index_file_exists = False\n    for content in objects.get(\"Contents\", []):\n        if content[\"Key\"] == \"index.json\":\n            index_file_exists = True\n            break\n\n    if index_file_exists:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index.json file.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # If the directory already contains an index file, we raise an error.\n    if \"CommonPrefixes\" in objects and \"index.json\" in [prefix[\"Prefix\"] for prefix in objects[\"CommonPrefixes\"]]:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains a file named 'index.json'. \"\n            \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # If the directory does not contain an index file, we delete all objects within the specified prefix.\n    if \"Contents\" in objects:\n        for object in objects[\"Contents\"]:\n            s3.delete_object(Bucket=obj.netloc, Key=object[\"Key\"])\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects:\n        for content in objects[\"Contents\"]:\n            if content[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'. \"\n                    \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n    if \"CommonPrefixes\" in objects:\n        for prefix in objects[\"CommonPrefixes\"]:\n            s3.delete_objects(\n                Bucket=obj.netloc,\n                Delete={\n                    \"Objects\": [{\"Key\": prefix[\"Prefix\"]}],\n                    \"Quiet\": True,\n                },\n            )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # If the directory already contains an index file, we raise an error.\n    if \"Contents\" in objects:\n        for content in objects[\"Contents\"]:\n            if content[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index.json file and datasets are meant to be immutable.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n    # If the directory does not contain an index file, we delete all objects within the specified prefix.\n    if \"Contents\" in objects:\n        for content in objects[\"Contents\"]:\n            s3.delete_object(Bucket=obj.netloc, Key=content[\"Key\"])\n\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # Check if the directory contains an index.json file\n    index_file_exists = False\n    for obj in objects[\"Contents\"]:\n        if obj[\"Key\"].endswith(\"index.json\"):\n            index_file_exists = True\n            break\n\n    if not index_file_exists:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` does not contain an index.json file.\"\n            \" HINT: Did you consider adding a index.json file to your output_dir?\"\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        return\n\n    for content in objects[\"Contents\"]:\n        if content[\"Key\"].endswith(\"index.json\"):\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains a file named 'index.json'.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    # Delete all objects within the specified prefix\n    if \"Contents\" in objects:\n        s3.delete_objects(\n            Bucket=obj.netloc,\n            Delete={\n                \"Objects\": [{\"Key\": obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\" + content[\"Key\"]} for content in objects[\"Contents\"]]\n            },\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        for obj in objects[\"Contents\"]:\n            if obj[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json' and datasets are meant to be immutable.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects and any(obj[\"Key\"] == \"index.json\" for obj in objects[\"Contents\"]):\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'. \"\n            \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if \"Contents\" in objects:\n        s3.delete_objects(\n            Bucket=obj.netloc,\n            Delete={\"Objects\": [{\"Key\": obj[\"Key\"]} for obj in objects[\"Contents\"]]},\n        )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects:\n        for object in objects[\"Contents\"]:\n            if object[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n    # Delete all objects within the specified prefix in the bucket\n    if \"Contents\" in objects:\n        for object in objects[\"Contents\"]:\n            s3.delete_object(Bucket=obj.netloc, Key=object[\"Key\"])\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"CommonPrefixes\" in objects:\n        for prefix in objects[\"CommonPrefixes\"]:\n            if prefix[\"Prefix\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects:\n        for content in objects[\"Contents\"]:\n            if content[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'. Datasets are meant to be immutable. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all parts to be available\n        while True:\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(1)\n\n        # Only merge on the master node\n        if node_rank is not None and node_rank != 0:\n            while True:\n                index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(1)\n\n        # Merge the index files\n        chunks_info = []\n        for index_file in index_files:\n            with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                index = json.load(f)\n                chunks_info.extend(index[\"chunks\"])\n\n        # Sort the chunks by index\n        chunks_info = sorted(chunks_info, key=lambda x: x[\"filename\"])\n\n        # Write the merged index file\n        with open(os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\"), \"w\") as out:\n            json.dump({\"chunks\": chunks_info, \"config\": self.get_config()}, out, sort_keys=True)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index files to be available\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        # Merge only on the master node\n        if node_rank is not None and node_rank != 0:\n            while True:\n                if os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\")):\n                    break\n                sleep(0.1)\n\n        # Merge index files\n        chunks_info = []\n        for i in range(num_workers):\n            with open(os.path.join(self._cache_dir, f\"{i}.{_INDEX_FILENAME}\"), \"r\") as f:\n                index_info = json.load(f)\n                chunks_info.extend(index_info[\"chunks\"])\n\n        # Write down the index file\n        self.write_chunks_index()\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait until all parts are available\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        # Merge only if it's the master node (rank 0)\n        if node_rank == 0:\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            index_files.sort()\n            chunks_info = []\n            for index_file in index_files:\n                with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                    index_data = json.load(f)\n                chunks_info.extend(index_data[\"chunks\"])\n            chunks_info.sort(key=lambda x: x[\"filename\"])\n\n            with open(os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\"), \"w\") as out:\n                json.dump({\"chunks\": chunks_info, \"config\": self.get_config()}, out, sort_keys=True)\n\n        # Wait until the merged index file is available\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        # Wait for all index files to be available\n        while not self.filled:\n            sleep(1)\n\n        if node_rank != 0:\n            # Wait for the master node to merge the index files\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\")):\n                sleep(1)\n            return\n\n        # Merge the index files\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        index_files.sort()\n        chunks_info = []\n        for index_file in index_files:\n            with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                index_data = json.load(f)\n                chunks_info.extend(index_data[\"chunks\"])\n\n        # Write the merged index file\n        with open(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\"), \"w\") as f:\n            json.dump({\"chunks\": chunks_info}, f)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        # Wait until all parts are available\n        while True:\n            index_files = os.listdir(self._cache_dir)\n            index_files = [f for f in index_files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        # Only the master node performs the merge\n        if node_rank == 0:\n            # Merge the index files\n            index_files = os.listdir(self._cache_dir)\n            index_files = [f for f in index_files if f.endswith(_INDEX_FILENAME)]\n            index_files = sorted(index_files)\n            index_files = [os.path.join(self._cache_dir, f) for f in index_files]\n            chunks_info = []\n            for index_file in index_files:\n                with open(index_file, \"r\") as f:\n                    index_data = json.load(f)\n                    chunks_info.extend(index_data[\"chunks\"])\n\n            # Write the merged index file\n            with open(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\"), \"w\") as out:\n                json.dump({\"chunks\": chunks_info, \"config\": index_data[\"config\"]}, out, sort_keys=True)\n\n        # Wait until the merged index file is available\n        while True:\n            index_files = os.listdir(self._cache_dir)\n            index_files = [f for f in index_files if f.endswith(_INDEX_FILENAME)]\n            if f\"{node_rank}.{_INDEX_FILENAME}\" in index_files:\n                break\n            sleep(0.1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index files to be available\n        while True:\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        # Only the master node (rank 0) performs the actual merge\n        if node_rank is not None and node_rank != 0:\n            # Wait for the merged index file to be available\n            while True:\n                merged_index_file = f\"{node_rank}.{_INDEX_FILENAME}\"\n                if os.path.exists(os.path.join(self._cache_dir, merged_index_file)):\n                    break\n                sleep(0.1)\n\n        # The master node (rank 0) performs the actual merge\n        if node_rank == 0:\n            # Load the index files\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            indexes = [json.load(open(os.path.join(self._cache_dir, index_file))) for index_file in index_files]\n\n            # Merge the indexes\n            merged_index = {}\n            for index in indexes:\n                merged_index.update(index)\n\n            # Write the merged index file\n            with open(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\"), \"w\") as out:\n                json.dump(merged_index, out, sort_keys=True)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        # Wait for all index parts to be available\n        while True:\n            index_files = os.listdir(self._cache_dir)\n            index_files = [f for f in index_files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        # Only the master node (rank 0) performs the merge\n        if node_rank == 0:\n            # Load all index files and merge them\n            index_files = os.listdir(self._cache_dir)\n            index_files = [f for f in index_files if f.endswith(_INDEX_FILENAME)]\n            index_files = sorted(index_files)\n            indices = []\n            for index_file in index_files:\n                with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                    index = json.load(f)\n                indices.append(index)\n\n            # Merge indices\n            merged_index = {\"chunks\": [], \"config\": {}}\n            for index in indices:\n                merged_index[\"chunks\"].extend(index[\"chunks\"])\n                if index[\"config\"][\"data_format\"] is not None:\n                    merged_index[\"config\"][\"data_format\"] = index[\"config\"][\"data_format\"]\n                if index[\"config\"][\"data_spec\"] is not None:\n                    merged_index[\"config\"][\"data_spec\"] = index[\"config\"][\"data_spec\"]\n\n            # Write merged index to file\n            with open(os.path.join(self._cache_dir, \"index.json\"), \"w\") as f:\n                json.dump(merged_index, f)\n\n            # Wait for merged index to be available\n            while True:\n                merged_index_file = os.path.join(self._cache_dir, \"index.json\")\n                if os.path.exists(merged_index_file):\n                    break\n                sleep(0.1)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index files to be available\n        while True:\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(1)\n\n        # Check if this is the master node (rank 0)\n        if node_rank is None:\n            node_rank = get_worker_rank()\n        if node_rank != 0:\n            # Wait for the merged index file to be available\n            while True:\n                merged_index_file = f\"merged.{_INDEX_FILENAME}\"\n                if merged_index_file in os.listdir(self._cache_dir):\n                    break\n                sleep(1)\n\n        # Perform the merge operation only for the master node (rank 0)\n        if node_rank == 0:\n            # Load the index files and merge them\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            index_files.sort()\n            merged_index = {}\n            for index_file in index_files:\n                with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                    index = json.load(f)\n                    if \"chunks\" in index:\n                        merged_index[\"chunks\"] = merged_index.get(\"chunks\", []) + index[\"chunks\"]\n                    if \"config\" in index:\n                        merged_index[\"config\"] = index[\"config\"]\n\n            # Write the merged index file\n            with open(os.path.join(self._cache_dir, f\"merged.{_INDEX_FILENAME}\"), \"w\") as f:\n                json.dump(merged_index, f, sort_keys=True)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index parts to be available\n        while not self.filled:\n            sleep(0.1)\n\n        # Only merge if this is the master node\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\")):\n                sleep(0.1)\n            return\n\n        # Merge all index parts into a single index file\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        if len(index_files) != num_workers:\n            raise ValueError(\n                f\"The number of index files {len(index_files)} doesn't match the expected number of workers {num_workers}.\"\n            )\n\n        chunks_info: List[Dict[str, Any]] = []\n        for index_file in index_files:\n            with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                index_info = json.load(f)\n                chunks_info.extend(index_info[\"chunks\"])\n\n        # Sort the chunks by their index\n        chunks_info = sorted(chunks_info, key=lambda x: x[\"filename\"])\n\n        # Write the merged index file\n        with open(os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\"), \"w\") as out:\n            json.dump({\"chunks\": chunks_info, \"config\": self.get_config()}, out, sort_keys=True)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all parts of the index to be available\n        while not self.filled:\n            sleep(0.1)\n\n        # Only the master node should merge the index\n        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank == 0:\n            # Wait until all parts are available\n            while len(os.listdir(self._cache_dir)) != num_workers:\n                sleep(0.1)\n\n            # Merge the index files\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            index_files.sort()\n            with open(os.path.join(self._cache_dir, _INDEX_FILENAME), \"w\") as out:\n                for index_file in index_files:\n                    with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                        json.dump(json.load(f), out)\n\n            # Wait until the merged index file is available\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(0.1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        # Wait for all index parts to be available\n        while not self.filled:\n            sleep(0.1)\n\n        # Only the master node (rank 0) performs the merge\n        if node_rank == 0:\n            # Read all index files and combine them into a single index\n            index_files = os.listdir(self._cache_dir)\n            index_files = [f for f in index_files if f.endswith(_INDEX_FILENAME)]\n            index_files = sorted(index_files, key=lambda x: int(x.split(\".\")[0]))\n            index_parts = [json.load(open(os.path.join(self._cache_dir, f))) for f in index_files]\n            index = {\"chunks\": [], \"config\": index_parts[0][\"config\"]}\n            for index_part in index_parts:\n                index[\"chunks\"].extend(index_part[\"chunks\"])\n\n            # Write the merged index to a file\n            with open(os.path.join(self._cache_dir, f\"index.{_INDEX_FILENAME}\"), \"w\") as out:\n                json.dump(index, out, sort_keys=True)\n\n            # Delete the individual index files\n            for index_file in index_files:\n                os.remove(os.path.join(self._cache_dir, index_file))\n\n        # Wait for the merged index file to be available\n        while not os.path.exists(os.path.join(self._cache_dir, f\"index.{_INDEX_FILENAME}\")):\n            sleep(0.1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            while not self.filled:\n                sleep(0.1)\n\n        if self.filled:\n            return\n\n        index_files = [\n            os.path.join(self._cache_dir, f\"{rank}.{_INDEX_FILENAME}\")\n            for rank in range(num_workers)\n            if os.path.exists(os.path.join(self._cache_dir, f\"{rank}.{_INDEX_FILENAME}\"))\n        ]\n\n        while len(index_files) < num_workers:\n            sleep(0.1)\n\n        if node_rank == 0:\n            merged_index = {\"chunks\": [], \"config\": {}}\n            for index_file in index_files:\n                with open(index_file, \"r\") as f:\n                    index = json.load(f)\n                    merged_index[\"chunks\"].extend(index[\"chunks\"])\n                    merged_index[\"config\"].update(index[\"config\"])\n\n            with open(os.path.join(self._cache_dir, _INDEX_FILENAME), \"w\") as f:\n                json.dump(merged_index, f)\n\n        while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n            sleep(0.1)\n\n        self._is_done = True\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all parts to be available\n        while True:\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        # Only master node (rank 0) should merge\n        if node_rank is None:\n            node_rank = get_worker_rank()\n\n        if node_rank != 0:\n            # Wait for merged index file to be available\n            while True:\n                index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n                if \"0.json\" in index_files:\n                    break\n                sleep(0.1)\n            return\n\n        # Merge index files\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        index_files = sorted(index_files, key=lambda x: int(x.split(\".\")[0]))\n        chunks_info = []\n        for index_file in index_files:\n            with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                index = json.load(f)\n                chunks_info.extend(index[\"chunks\"])\n\n        # Write down the merged index file\n        with open(os.path.join(self._cache_dir, \"0.json\"), \"w\") as out:\n            json.dump({\"chunks\": chunks_info}, out)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if self._distributed_env.world_size == 1:\n            return\n\n        if node_rank is None:\n            node_rank = self._distributed_env.global_rank\n\n        if node_rank != 0:\n            while True:\n                if os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\")):\n                    break\n                sleep(1)\n\n        if node_rank == 0:\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            index_files = sorted(index_files, key=lambda x: int(x.split(\".\")[0]))\n            with open(os.path.join(self._cache_dir, _INDEX_FILENAME), \"w\") as out:\n                for index_file in index_files:\n                    with open(os.path.join(self._cache_dir, index_file), \"r\") as inp:\n                        data = json.load(inp)\n                        json.dump(data, out, sort_keys=True)\n\n            for index_file in index_files:\n                os.remove(os.path.join(self._cache_dir, index_file))\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all parts to be available\n        while True:\n            index_files = os.listdir(self._cache_dir)\n            if len(index_files) == num_workers:\n                break\n            sleep(0.5)\n\n        # Only master node (rank 0) performs the merge\n        if node_rank != 0:\n            # Wait for the merged index file to be available\n            while True:\n                index_files = os.listdir(self._cache_dir)\n                if f\"{node_rank}.{_INDEX_FILENAME}\" in index_files:\n                    break\n                sleep(0.5)\n            return\n\n        # Load the index files\n        index_files = os.listdir(self._cache_dir)\n        index_files = [f for f in index_files if f.endswith(_INDEX_FILENAME)]\n        index_files.sort(key=lambda x: int(x.split(\".\")[0]))\n        index_files = [os.path.join(self._cache_dir, f) for f in index_files]\n\n        # Load the index files\n        chunks_info = []\n        for index_file in index_files:\n            with open(index_file, \"r\") as f:\n                index = json.load(f)\n                chunks_info.extend(index[\"chunks\"])\n\n        # Sort the chunks by index\n        chunks_info.sort(key=lambda x: x[\"chunk_bytes\"])\n\n        # Write the merged index file\n        with open(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\"), \"w\") as out:\n            json.dump({\"chunks\": chunks_info, \"config\": self.get_config()}, out, sort_keys=True)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all parts of the index to be available\n        while True:\n            if self.filled:\n                break\n            sleep(0.1)\n\n        # Only the master node (rank 0) should merge the index files\n        if node_rank is None:\n            node_rank = self.rank\n        if node_rank != 0:\n            # Wait for the merged index file to be available\n            while True:\n                if os.path.exists(os.path.join(self._cache_dir, f\"{0}.{_INDEX_FILENAME}\")):\n                    break\n                sleep(0.1)\n            return\n\n        # Merge the index files\n        chunks_info: List[Dict[str, Any]] = []\n        for rank in range(num_workers):\n            with open(os.path.join(self._cache_dir, f\"{rank}.{_INDEX_FILENAME}\"), \"r\") as f:\n                index_info = json.load(f)\n                chunks_info.extend(index_info[\"chunks\"])\n        chunks_info.sort(key=lambda x: x[\"filename\"])\n        with open(os.path.join(self._cache_dir, f\"{0}.{_INDEX_FILENAME}\"), \"w\") as f:\n            json.dump({\"chunks\": chunks_info}, f, sort_keys=True)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n        if node_rank != 0:\n            while not self.filled:\n                sleep(1)\n            return\n\n        while not self.filled:\n            sleep(1)\n\n        # Read the index files\n        index_files = [\n            os.path.join(self._cache_dir, f\"{rank}.{_INDEX_FILENAME}\") for rank in range(num_workers)\n        ]\n        index_files = [f for f in index_files if os.path.exists(f)]\n\n        if len(index_files) == 0:\n            return\n\n        if len(index_files) != num_workers:\n            raise ValueError(f\"The number of index files doesn't match the expected number of workers: {num_workers}\")\n\n        # Merge the index files\n        chunks_info = []\n        for index_file in index_files:\n            with open(index_file, \"r\") as f:\n                index = json.load(f)\n                chunks_info.extend(index[\"chunks\"])\n\n        # Sort the chunks by their index\n        chunks_info = sorted(chunks_info, key=lambda x: x[\"chunk_index\"])\n\n        # Write the merged index file\n        with open(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\"), \"w\") as out:\n            json.dump({\"chunks\": chunks_info, \"config\": self.get_config()}, out, sort_keys=True)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n        while True:\n            if self.filled:\n                break\n            sleep(0.1)\n\n        if node_rank == 0:\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) != num_workers:\n                raise RuntimeError(\n                    f\"The number of index files {len(index_files)} doesn't match the expected number of workers {num_workers}.\"\n                )\n            index_files.sort()\n            index_files = [os.path.join(self._cache_dir, f) for f in index_files]\n            index_files = [json.load(open(f)) for f in index_files]\n            merged_index = {\"chunks\": [], \"config\": index_files[0][\"config\"]}\n            for index_file in index_files:\n                merged_index[\"chunks\"].extend(index_file[\"chunks\"])\n            merged_index[\"chunks\"].sort(key=lambda x: x[\"filename\"])\n            merged_index_file = os.path.join(self._cache_dir, f\"{_INDEX_FILENAME}\")\n            with open(merged_index_file, \"w\") as out:\n                json.dump(merged_index, out, sort_keys=True)\n\n            # Delete the index files\n            for index_file in index_files:\n                os.remove(index_file)\n        else:\n            while True:\n                merged_index_file = os.path.join(self._cache_dir, f\"{_INDEX_FILENAME}\")\n                if os.path.exists(merged_index_file):\n                    break\n                sleep(0.1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait until all parts are available\n        while True:\n            index_files = os.listdir(self._cache_dir)\n            index_files = [f for f in index_files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        # Only the master node should merge the index\n        if node_rank is None:\n            node_rank = get_worker_rank()\n        if node_rank == 0:\n            # Merge the index\n            self._merge_index_files()\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index files to be available\n        while True:\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(1)\n\n        # If not master node, wait for the merged index file\n        if node_rank is not None and node_rank != 0:\n            while True:\n                index_file = f\"{node_rank}.{_INDEX_FILENAME}\"\n                if index_file in os.listdir(self._cache_dir):\n                    break\n                sleep(1)\n\n        # Merge index files\n        merged_index = {}\n        for index_file in index_files:\n            with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                index = json.load(f)\n                for chunk in index[\"chunks\"]:\n                    merged_index[chunk[\"filename\"]] = chunk\n\n        # Write merged index file\n        with open(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\"), \"w\") as f:\n            json.dump({\"chunks\": list(merged_index.values()), \"config\": merged_index[list(merged_index.keys())[0]][\"config\"]}, f)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning` package is required in order to use this feature. Please install it with `pip install lightning`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, gpu_type=\"A100\", ram=\"16Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run --app {os.getcwd()} --env {os.environ}\"\n\n    # Get the ids from env variables\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    # Get the ids from env variables\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    # Get the ids from env variables\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ValueError(\"The `lightning-sdk` is not installed. Please install it to use this function.\")\n\n    if machine is None:\n        machine = Machine(cpu=1, memory=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run {os.getcwd()} --env-vars\"\n\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    # Get the cluster\n    cluster = client.cluster_service_get_cluster(project_id=project_id, cluster_id=cluster_id)\n\n    # Get the cloud space\n    cloud_space = client.cloud_space_service_get_cloud_space(\n        project_id=project_id,\n        cloud_space_id=cloud_space_id,\n    )\n\n    # Get the bucket name\n    bucket_name = cluster.spec.aws_v1.bucket_name\n\n    # Create the job\n    job = client.job_service_create_job(\n        project_id=project_id,\n        job_request={\n            \"name\": name,\n            \"job_type\": \"MACHINE\",\n            \"machine"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\n            \"The Lightning SDK is not installed. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise ModuleNotFoundError(\n            \"The boto3 library is not installed. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            cpu=1,\n            gpu=0,\n            gpu_type=\"any\",\n            cloud_compute_type=\"GPU\",\n            cloud_region=\"us-east-1\",\n        )\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} --blocking --env-vars {os.environ}\"\n\n    # Get the ids from env variables\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    client = LightningClient(max_tries=2)\n\n    # Create a job\n    job = client.job_service_create_job(\n        project_id=project_id,\n        job_name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    # Wait for the job to start\n    job_status = client.job_service_get_job(project_id=project_id, job_id=job.job_id).status\n    while job_status not in [\"PENDING\", \"RUNNING\"]:\n        sleep(5)\n        job_status = client.job_service_get_job(project_id=project_id, job_id=job.job_id).status\n\n    # Print the job URL\n    print(f\"Job URL: {_get_lightning_cloud_url()"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is required to execute this function. \"\n            \"Please install it with `pip install lightning`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, memory=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run {os.getcwd()} --env-vars\"\n\n    job = Studio.execute(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    print(f\"Job URL: {job.url}\")\n\n    while True:\n        job = Studio.get_job(job.job_id)\n        if job.status == \"running\":\n            print(f\"Job URL: {job.url}\")\n        elif job.status == \"failed\":\n            raise RuntimeError(f\"Job failed with error: {job.error}\")\n        elif job.status == \"succeeded\":\n            break\n        sleep(1)\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\n            \"The Lightning SDK is not installed. Please install it to use this function: `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, mem=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run {os.getcwd()} --env {os.environ}\"\n\n    # Get the ids from env variables\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    client = LightningClient(max_tries=2)\n\n    job = client.job_service_create_job(\n        project_id=project_id,\n        cloud_space_id=cloud_space_id,\n        job_request={\n            \"name\": name,\n            \"machine\": machine._to_api_object(),\n            \"command\": command,\n            \"num_nodes\": num_nodes,\n        },\n    )\n\n    job_id = job.id\n\n    # Wait for the job to start\n    while True:\n        job_info = client.job_service_get_job(project_id=project_id, job_id=job_id)\n        if job_info.status == \"PENDING\":\n            print(\"Job is pending. Waiting for it to start...\")\n            sleep(5)\n        elif job_info.status == \"RUNNING\":\n            print(\"Job is running. Job URL:\", job_info.url)\n           "}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\"The Lightning SDK is required to execute this function.\")\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, ram=\"4Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app -p {os.getcwd()} -e {os.environ}\"\n\n    if not isinstance(machine, Machine):\n        raise ValueError(\"The provided machine isn't a Machine Object.\")\n\n    if not isinstance(command, str):\n        raise ValueError(\"The provided command isn't a string.\")\n\n    if not isinstance(num_nodes, int):\n        raise ValueError(\"The provided num_nodes isn't an integer.\")\n\n    if num_nodes < 1:\n        raise ValueError(\"The provided num_nodes must be greater than 0.\")\n\n    if num_nodes > 1:\n        command = f\"{command} --parallel\"\n\n    studio = Studio(cloud=True)\n\n    job = studio.execute(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    job.start()\n\n    while job.status != \"succeeded\" and job.status != \"failed\":\n        sleep(1)\n        job.refresh()\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"The job failed with the following error: {job.error}\")\n\n    print(f\"Job URL: {_get_lightning_cloud_url()}/studio/jobs/{job.id}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"The `lightning` package is required in order to run this command. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, gpu_type=\"A100\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} --env-vars {os.environ}\"\n\n    studio = Studio(cloud=True)\n    job = studio.launch(\n        name=name,\n        cloud_compute=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    # Print the job URL\n    print(f\"Job URL: {_get_lightning_cloud_url()}/studio/jobs/{job.id}\")\n\n    # Wait for the job to finish\n    while job.status not in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n        sleep(1)\n        job = studio.get_job(job.id)\n\n    # Raise an exception if the job failed\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"Job failed with the following error: {job.failure_message}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\n            \"The Lightning SDK is not installed. Please install it with `pip install lightning` or `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=4, gpu=0, machine_type=\"cpu-medium\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} -v\"\n\n    job = Studio.execute(\n        name=name,\n        command=command,\n        machine=machine,\n        num_nodes=num_nodes,\n    )\n\n    job_url = f\"{_get_lightning_cloud_url()}/studio/projects/{job.project_id}/jobs/{job.job_id}\"\n\n    print(f\"Job URL: {job_url}\")\n\n    while job.status not in [\"FAILED\", \"COMPLETED\"]:\n        sleep(5)\n        job = Studio.get_job(job_id=job.job_id)\n        print(f\"Job status: {job.status}\")\n\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"The job `{job.name}` failed. Please check the job URL: {job_url}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDKs are installed\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is not installed. Please install it to use this function. HINT: `pip install lightning-sdk`\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The boto3 package is not installed. Please install it to use this function. HINT: `pip install boto3`\"\n        )\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env = os.environ.copy()\n\n    # Get the Lightning Cloud URL\n    lightning_cloud_url = _get_lightning_cloud_url()\n\n    # Create a Studio client\n    studio = Studio(lightning_cloud_url=lightning_cloud_url)\n\n    # Get the current machine\n    if machine is None:\n        machine = Machine(cpu=4, gpu=0, gpu_type=\"A100\", memory=16)\n\n    # Get the command to be executed\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {cwd} --env-vars {','.join([f'{k}={v}' for k, v in env.items()])}\"\n\n    # Create a data preparation machine job\n    job = studio.create_data_preparation_machine_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Wait for the job to start\n    while job.status == \"pending\":\n        sleep(5)\n        job.refresh()\n\n    # Check if the job has started\n    if job.status != \"running\":\n        raise RuntimeError(f\"The job failed with status: {job.status}\")\n\n    # Print the"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\n            \"The `lightning` package is required in order to run this command. Please install it with `pip install lightning`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise ModuleNotFoundError(\n            \"The `boto3` package is required in order to run this command. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=4, gpu=0, gpu_type=\"A100\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.run app {os.getcwd()} --env-file {os.environ}\"\n\n    studio = Studio(\n        cloud_compute=machine,\n        num_nodes=num_nodes,\n        cloud_build_config={\"build_steps\": [{\"name\": \"python\", \"args\": [\"setup.py\", \"bdist_wheel\"]}]},\n    )\n\n    job_id = studio.start_job(name=name, command=command)\n\n    # Wait for the job to start\n    while True:\n        job_status = studio.get_job_status(job_id)\n        if job_status.status == \"running\":\n            print(f\"Job URL: {_get_lightning_cloud_url()}/studio/jobs/{job_id}\")\n            break\n        elif job_status.status == \"failed\":\n            raise RuntimeError(f\"The job failed with the following error: {job_status.error_message}\")\n        sleep(1)\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\n            \"The `lightning` package is required in order to run this command. Please install it with `pip install lightning`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=4, memory=\"16Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run {os.getcwd()}\"\n\n    if num_nodes > 1:\n        raise ValueError(\"We currently don't support distributed training.\")\n\n    studio = Studio(cloud=_get_lightning_cloud_url())\n\n    job = studio.launch(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    job.wait()\n\n    if job.status != \"succeeded\":\n        raise RuntimeError(f\"The job failed with status: {job.status}. Please check the job logs for more information.\")\n\n    print(f\"Job URL: {job.url}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"The Lightning SDK is required to run this function. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\n            \"The boto3 library is required to run this function. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, memory=\"2Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run --app {sys.argv[0]} --blocking\"\n\n    job = Studio.execute(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    # Wait for the job to start\n    while job.status != \"running\":\n        sleep(1)\n        job.refresh()\n\n    print(f\"Job URL: {_get_lightning_cloud_url()}/studio/job/{job.id}\")\n\n    # Wait for the job to finish\n    while job.status != \"succeeded\" and job.status != \"failed\":\n        sleep(1)\n        job.refresh()\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"The job `{job.name}` failed. Please check the job URL for more information.\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is not installed. Please install it by running `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, ram=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning_cli data prepare --name={name} --num_nodes={num_nodes}\"\n\n    studio = Studio(\n        cloud_compute=machine,\n        cloud_build_config={\"build_steps\": [{\"name\": \"lightning-data-prep\", \"command\": command}]},\n    )\n\n    job = studio.run()\n\n    while job.status not in [\"succeeded\", \"failed\"]:\n        sleep(5)\n        job = studio.get_job(job.job_id)\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"The job failed with the following error: {job.error}\")\n\n    print(f\"Job URL: {job.url}\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"To execute a job, please install the Lightning SDK with `pip install lightning-sdk` or `pip install lightning-sdk[extra]`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, gpu_type=\"A100\", interactive=False)\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} --block\"\n\n    studio = Studio(cloud=True)\n    job = studio.execute(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    job_url = f\"{_get_lightning_cloud_url()}/studio/jobs/{job.id}\"\n\n    print(f\"Job URL: {job_url}\")\n\n    while True:\n        job = studio.get_job(job.id)\n        if job.status == \"failed\":\n            raise RuntimeError(f\"Job failed with message: {job.message}\")\n        elif job.status == \"completed\":\n            print(\"Job completed successfully.\")\n            break\n        else:\n            print(f\"Job status: {job.status}\")\n            sleep(10)\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is required to execute the job. Please install it by running `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, memory=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app --app_path={os.getcwd()} --env={os.environ}\"\n\n    studio = Studio(\n        cloud_compute=machine,\n        cloud_build_config=None,\n        cloud_run_config=None,\n        project_name=name,\n        project_root=os.getcwd(),\n    )\n\n    # Create the job\n    job = studio.create_job(command=command)\n\n    # Wait for the job to start\n    while job.status not in [\"Pending\", \"Starting\", \"Running\"]:\n        sleep(1)\n        job.refresh()\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Wait for the job to finish\n    while job.status not in [\"Succeeded\", \"Failed\", \"Cancelled\"]:\n        sleep(1)\n        job.refresh()\n\n    # Check if the job failed\n    if job.status == \"Failed\":\n        raise RuntimeError(f\"Job failed with error: {job.error_message}\")\n\n    # Check if the job was cancelled\n    if job.status == \"Cancelled\":\n        raise RuntimeError(\"Job was cancelled\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # check if the required SDK is available\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The required SDK is not available. Please install the Lightning SDK to use this function.\"\n        )\n\n    # get the current working directory\n    cwd = os.getcwd()\n\n    # get the environment variables\n    env_vars = os.environ\n\n    # create a default machine configuration if not provided\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, ram=\"4Gi\")\n\n    # create a default command if not provided\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {cwd} --env-vars {env_vars}\"\n\n    # create a Studio object\n    studio = Studio(\n        cloud_compute=machine,\n        cloud_build_config=command,\n        cloud_run_config={\n            \"max_num_workers\": num_nodes,\n        },\n    )\n\n    # create a data preparation machine job\n    job = studio.create_job(name)\n\n    # continuously check the job status\n    while job.status not in [\"PENDING\", \"RUNNING\", \"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n        sleep(1)\n        job.refresh()\n\n    # print the job URL when it starts\n    if job.status == \"RUNNING\":\n        print(f\"Job URL: {job.url}\")\n\n    # raise an exception if the job fails\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"Job failed with error: {job.error}\")\n\n    # raise an exception if the job is cancelled\n    if job.status == \"CANCELLED\":\n        raise RuntimeError(\"Job was cancelled\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is not installed. Please install it to use this function. \"\n            \"HINT: `pip install lightning-sdk`\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, ram=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} --env-vars={os.getenv('LIGHTNING_ENV_VARS', '')}\"\n\n    studio = Studio(\n        cloud_compute=machine,\n        cloud_build_config=None,\n        cloud_run_config=None,\n        project_name=name,\n        project_root=os.getcwd(),\n        project_config_name=\"lightning.yaml\",\n        project_config_type=\"lightning\",\n        project_config_path=None,\n        project_id=None,\n        project_cloud_compute=machine,\n        project_cloud_build_config=None,\n        project_cloud_run_config=None,\n        project_description=None,\n        project_tags=None,\n        project_conda_name=None,\n        project_conda_channels=None,\n        project_conda_dependencies=None,\n        project_git_remote_url=None,\n        project_git_branch=None,\n        project_git_commit=None,\n        project_git_dir=None,\n        project_git_submodules=False,\n        project_git_lfs_fetch=False,\n        project_git_lfs_checkout=False,\n        project_git_lfs_pull=False,\n        project_git_lfs_skip_smudge=False,\n        project_git_lfs_skip_fetch=False,\n        project_git_lfs_skip_clone=False,\n        project_git_lfs_skip_checkout=False,\n        project_git_lfs"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning` Python SDK is not installed. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=2, gpu=0)\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} --blocking\"\n\n    studio = Studio(cloud_compute=machine)\n\n    # create a job\n    job = studio.create_job(name, command=command, num_nodes=num_nodes)\n\n    # check job status\n    while True:\n        job.refresh()\n        if job.status == \"running\":\n            print(f\"Job URL: {job.url}\")\n            break\n        elif job.status == \"failed\":\n            raise RuntimeError(f\"Job failed with error: {job.error}\")\n        elif job.status == \"succeeded\":\n            break\n        sleep(1)\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\n            \"The required Lightning SDK is not installed. Please install the SDK to use this function.\"\n        )\n\n    # Fetch the default machine configuration if not provided\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, ram=\"1Gi\")\n\n    # Fetch the current working directory and environment variables\n    current_dir = os.getcwd()\n    env = os.environ.copy()\n\n    # Construct the command to be executed\n    if command is None:\n        command = f\"cd {current_dir} && lightning run app run {current_dir} --env-vars\"\n        for key, value in env.items():\n            command += f\" {key}={value}\"\n\n    # Create a Studio object\n    studio = Studio(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n        cloud_compute=None,\n        cloud_build=None,\n        cloud_run=None,\n        cloud_log=None,\n    )\n\n    # Submit the job to the Studio\n    job = studio.submit(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n        cloud_compute=None,\n        cloud_build=None,\n        cloud_run=None,\n        cloud_log=None,\n    )\n\n    # Print the job URL\n    print(f\"Job URL: {_get_lightning_cloud_url()}/studio/jobs/{job.job_id}\")\n\n    # Continuously check the job status and print the job URL\n    while True:\n        job_status = job.status\n        if job_status == \"running\":\n            print(f\"Job URL: {_get_lightning_cloud_url()}/studio/jobs/{job.job_id}\")\n        elif job_status == \"failed\":\n            raise RuntimeError(\"The"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ValueError(\"The Lightning SDK is not available. Please install it to use this function.\")\n\n    if machine is None:\n        machine = Machine(\n            cpu=1,\n            memory=\"4Gi\",\n            gpu=0,\n            gpu_type=\"\",\n            cloud_compute_type=\"\",\n        )\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.run app {os.getcwd()} --env-vars\"\n\n    job_id = Studio.execute(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    print(f\"Job URL: {_get_lightning_cloud_url()}/job/{job_id}\")\n\n    while True:\n        job_status = Studio.get_job_status(job_id)\n        if job_status == \"running\":\n            print(\".\", end=\"\", flush=True)\n            sleep(1)\n        elif job_status == \"failed\":\n            raise RuntimeError(f\"Job failed with status {job_status}.\")\n        elif job_status == \"succeeded\":\n            print(\"\")\n            break\n        else:\n            raise RuntimeError(f\"Job failed with status {job_status}.\")\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is None:\n            self._config = ChunksConfig(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._item_loader,\n                self._compression,\n                self._distributed_env,\n            )\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            self._config = ChunksConfig.load(\n                cache_dir=self._cache_dir,\n                remote_input_dir=self._remote_input_dir,\n                compression=self._compression,\n                serializers=self._serializers,\n                item_loader=self._item_loader,\n            )\n            return self._config\n        except FileNotFoundError:\n            return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check whether the index files are available\n        if not os.path.exists(os.path.join(self._cache_dir, \"index.json\")):\n            return None\n\n        # Load the configuration\n        self._config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n        )\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Load the config containing the index\n        config = ChunksConfig.load(\n            self._cache_dir,\n            self._serializers,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            item_loader=self._item_loader,\n        )\n\n        if config is None:\n            return None\n\n        self._config = config\n        return config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the index file exists\n        if not os.path.exists(os.path.join(self._cache_dir, \"index.json\")):\n            return None\n\n        # Load the config\n        self._config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n        )\n\n        # Load the index file\n        self._config.load_index_file()\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the index files are available\n        index_files = [file for file in os.listdir(self._cache_dir) if file.endswith(\".index\")]\n        if not index_files:\n            return None\n\n        # Load the configuration\n        self._config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n        )\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            serializers=self._serializers,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            item_loader=self._item_loader,\n        )\n\n        # Check if the index file exists\n        if not config.index_file_exists():\n            return None\n\n        # Load the config\n        config.load()\n\n        # Update the instance's configuration\n        self._config = config\n\n        return config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the config has already been loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config file exists\n        config_file = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(config_file):\n            return None\n\n        # Load the config file\n        config = ChunksConfig.load(config_file)\n\n        # Update the instance's configuration\n        self._config = config\n\n        return config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is not None:\n            return self._config\n\n        if self._distributed_env.is_master_node:\n            config_path = os.path.join(self._cache_dir, \"chunks_config.json\")\n            if os.path.exists(config_path):\n                self._config = ChunksConfig.load(config_path, self._serializers, self._remote_input_dir)\n                return self._config\n\n        return None\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check whether the index files are available\n        index_filepath = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_filepath):\n            return None\n\n        # Load the config\n        config = ChunksConfig.load(\n            self._cache_dir,\n            serializers=self._serializers,\n            remote_input_dir=self._remote_input_dir,\n            item_loader=self._item_loader,\n            compression=self._compression,\n        )\n\n        # Update the instance's configuration\n        self._config = config\n\n        return config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the index file exists\n        index_file = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_file):\n            return None\n\n        # Load the config containing the index\n        self._config = ChunksConfig.load(index_file, self._serializers, self._cache_dir, self._remote_input_dir)\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check whether the index files are available\n        index_files = os.listdir(self._cache_dir)\n        if not index_files:\n            return None\n\n        # Load the index files\n        config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n        )\n        config.load_indexes(index_files)\n\n        # Update the instance's configuration\n        self._config = config\n        return config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        config = ChunksConfig.from_cache_dir(\n            self._cache_dir,\n            serializers=self._serializers,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            item_loader=self._item_loader,\n            max_cache_size=self._max_cache_size,\n        )\n\n        if config is None:\n            return None\n\n        self._config = config\n        return config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check whether the index files are available\n        if not os.path.exists(os.path.join(self._cache_dir, \"indexes.json\")):\n            return None\n\n        # Load the index files\n        config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n            distributed_env=self._distributed_env,\n        )\n\n        # Update the instance's configuration\n        self._config = config\n\n        return config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check whether the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Load the config\n        try:\n            self._config = ChunksConfig(\n                cache_dir=self._cache_dir,\n                remote_input_dir=self._remote_input_dir,\n                compression=self._compression,\n                item_loader=self._item_loader,\n                serializers=self._serializers,\n                distributed_env=self._distributed_env,\n            )\n        except FileNotFoundError as e:\n            logger.warning(f\"{e}. The reader index isn't defined.\")\n            return None\n\n        # Update the rank\n        self._rank = self._distributed_env.rank\n\n        # Initialize the prepare thread\n        self._prepare_thread = PrepareChunksThread(\n            config=self._config,\n            item_loader=self._item_loader,\n            distributed_env=self._distributed_env,\n            max_cache_size=self._max_cache_size,\n        )\n        self._prepare_thread.start()\n\n        # Return the config\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is None:\n            self._config = ChunksConfig(\n                cache_dir=self._cache_dir,\n                serializers=self._serializers,\n                remote_input_dir=self._remote_input_dir,\n                item_loader=self._item_loader,\n                compression=self._compression,\n            )\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is not None:\n            return self._config\n\n        index_path = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_path):\n            raise FileNotFoundError(f\"The provided index file `{index_path}` doesn't exist.\")\n\n        self._config = ChunksConfig.from_index_file(\n            index_path,\n            cache_dir=self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n        )\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        if self._distributed_env.is_distributed:\n            self._rank = self._distributed_env.rank\n        else:\n            self._rank = 0\n\n        index_file = os.path.join(self._cache_dir, f\"{self._rank}.index\")\n        if not os.path.exists(index_file):\n            return None\n\n        self._config = ChunksConfig.load(\n            index_file,\n            self._cache_dir,\n            self._remote_input_dir,\n            self._compression,\n            self._serializers,\n            self._item_loader,\n        )\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is None:\n            try:\n                self._config = ChunksConfig(\n                    cache_dir=self._cache_dir,\n                    serializers=self._serializers,\n                    remote_input_dir=self._remote_input_dir,\n                    item_loader=self._item_loader,\n                )\n            except FileNotFoundError:\n                logger.warning(\n                    f\"No index file found in {self._cache_dir}. \"\n                    f\"You need to create the index file first. \"\n                    f\"You can do it with `litdata.streaming.index.create_index`.\"\n                )\n                return None\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the index files are available\n        if not os.path.exists(self._cache_dir):\n            raise FileNotFoundError(f\"The provided cache_dir `{self._cache_dir}` doesn't exist.\")\n\n        index_files = [\n            f\n            for f in os.listdir(self._cache_dir)\n            if f.endswith(\".json\") and f.startswith(\"index_\") and not f.startswith(\"index_intervals\")\n        ]\n\n        if not index_files:\n            return None\n\n        # Load the configuration\n        config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n        )\n        self._config = config\n        return config\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.put(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        # If the queue is full, we wait for the queue to be empty\n        while self._to_download_queue.qsize() >= self._max_pre_download:\n            self._pre_download_counter += 1\n            if self._pre_download_counter >= self._max_pre_download:\n                self._pre_download_counter = 0\n                self._to_download_queue.get()\n\n        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        # Enqueue the chunk indexes to be downloaded\n        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.put(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        self._to_download_queue.put(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        if not self._has_exited:\n            for chunk_index in chunk_indexes:\n                self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise RuntimeError(\"The reader config isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise RuntimeError(\"The reader config isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config is not defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not set.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not defined. Please define it before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not set.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config is not defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise RuntimeError(\"The configuration is not set.\")\n        return self._config  # type: ignore\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not set. Please set it before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config  # type: ignore\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config is not defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise RuntimeError(\"The reader index isn't defined.\")\n        return self._config  # type: ignore\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config  # type: ignore\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not defined. Please call the load_config method first.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index provided is not a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check if the chunk is available\n        chunk_index = index.chunk_index\n        chunk_filepath, _, _ = self.config[index]\n        if not os.path.exists(chunk_filepath):\n            # Download the chunk\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        return self._item_loader.load(index, chunk_filepath)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        # Check the input\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Check the config\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index\n        chunk_index = index.chunk_index\n\n        # Check if the chunk is already loaded\n        if chunk_index in self._item_loader.loaded_chunks:\n            return self._item_loader.get(chunk_index, index.index)\n\n        # Check if the chunk is already in the queue to be downloaded\n        if chunk_index in self._item_loader.to_download_chunks:\n            return self._item_loader.get(chunk_index, index.index)\n\n        # Check if the chunk is already in the queue to be deleted\n        if chunk_index in self._item_loader.to_delete_chunks:\n            return self._item_loader.get(chunk_index, index.index)\n\n        # Check if the chunk is already in the queue to be pre-loaded\n        if chunk_index in self._item_loader.to_pre_load_chunks:\n            return self._item_loader.get(chunk_index, index.index)\n\n        # Check if the chunk is already in the queue to be downloaded\n        if chunk_index in self._item_loader.to_download_chunks:\n            return self._item_loader.get(chunk_index, index.index)\n\n        # Check if the chunk is already in the queue to be deleted\n        if chunk_index in self._item_loader.to_delete_chunks:\n            return self._item_loader.get(chunk_index, index.index)\n\n        # Check if the chunk is already in the queue to be pre-loaded\n        if chunk_index in self._item_loader.to_pre_load_chunks:\n            return self._item_loader.get(chunk_index, index.index)\n\n        # Check if the chunk"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        if self._prepare_thread is None:\n            raise RuntimeError(\"The prepare thread should be defined.\")\n\n        # If the chunk is not already loaded, we need to load it\n        if index.chunk_index not in self.config.loaded_chunks:\n            self._prepare_thread.download([index.chunk_index])\n\n        # Load the chunk\n        chunk_filepath, _, _ = self.config[index]\n        chunk = self._item_loader.load_chunk(index.chunk_index, chunk_filepath)\n\n        # If the chunk is not in memory, we need to load it\n        if index.chunk_index not in self.config.loaded_chunks:\n            self._prepare_thread.download([index.chunk_index])\n\n        # Load the chunk\n        chunk_filepath, _, _ = self.config[index]\n        chunk = self._item_loader.load_chunk(index.chunk_index, chunk_filepath)\n\n        # If the chunk is not in memory, we need to load it\n        if index.chunk_index not in self.config.loaded_chunks:\n            self._prepare_thread.download([index.chunk_index])\n\n        # Load the chunk\n        chunk_filepath, _, _ = self.config[index]\n        chunk = self._item_loader.load_chunk(index.chunk_index, chunk_filepath)\n\n        # If the chunk is not in memory, we need to load it\n        if index.chunk_index not in self.config.loaded_chunks:\n            self._prepare_thread.download([index.chunk_index])\n\n        # Load the chunk\n        chunk_filepath, _, _ = self.config[index]\n        chunk = self._item_loader.load_chunk(index.chunk_"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check whether the chunk is available\n        chunk_index = index.chunk_index\n        chunk_filepath, _, _ = self._config[index]\n\n        # If the chunk is not available, download it\n        if not os.path.exists(chunk_filepath):\n            self._config.download_chunk_from_index(chunk_index)\n\n        # Load the chunk\n        with open(chunk_filepath, \"rb\") as f:\n            chunk = f.read()\n\n        # Load the item\n        item = self._item_loader.load(chunk, index)\n\n        # Delete the chunk if it is the last one\n        if chunk_index == self._last_chunk_index:\n            self._item_loader.delete(chunk_index, chunk_filepath)\n\n        # Update the last chunk index\n        self._last_chunk_index = chunk_index\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = index.chunk_index\n        if chunk_index is None:\n            chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Load the chunk if not available\n        if chunk_index not in self.config:\n            self._download_chunk(chunk_index)\n\n        # Check if the chunk is available\n        if chunk_index not in self.config:\n            raise Exception(f\"The chunk {chunk_index} is not available.\")\n\n        # Get the chunk and the item index\n        chunk_filepath, _, item_index = self.config[chunk_index]\n\n        # Load the chunk if not available\n        if chunk_index not in self.config:\n            self._download_chunk(chunk_index)\n\n        # Load the chunk if not available\n        if chunk_index not in self.config:\n            raise Exception(f\"The chunk {chunk_index} is not available.\")\n\n        # Load the chunk if not available\n        if chunk_index not in self.config:\n            self._download_chunk(chunk_index)\n\n        # Load the chunk if not available\n        if chunk_index not in self.config:\n            raise Exception(f\"The chunk {chunk_index} is not available.\")\n\n        # Load the chunk if not available\n        if chunk_index not in self.config:\n            self._download_chunk(chunk_index)\n\n        # Load the chunk if not available\n        if chunk_index not in self.config:\n            raise Exception(f\"The chunk {chunk_index} is not available.\")\n\n        # Load the chunk if not available\n        if chunk_index not in self.config:\n            self._download_chunk(chunk_index)\n\n        # Load the chunk if not available\n        if chunk_"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check if the chunk is already loaded\n        if self._config._is_chunk_loaded(index.chunk_index):\n            return self._config._get_item_from_chunk(index)\n\n        # Check if the chunk is already being downloaded\n        if self._config._is_chunk_downloading(index.chunk_index):\n            # Wait until the chunk is fully downloaded\n            while not self._config._is_chunk_loaded(index.chunk_index):\n                pass\n            return self._config._get_item_from_chunk(index)\n\n        # Check if the chunk is already being downloaded\n        if self._config._is_chunk_downloading(index.chunk_index):\n            # Wait until the chunk is fully downloaded\n            while not self._config._is_chunk_loaded(index.chunk_index):\n                pass\n            return self._config._get_item_from_chunk(index)\n\n        # Download the chunk\n        self._config.download_chunk_from_index(index.chunk_index)\n\n        # Wait until the chunk is fully downloaded\n        while not self._config._is_chunk_loaded(index.chunk_index):\n            pass\n\n        return self._config._get_item_from_chunk(index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index is not an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Get the chunk filepath\n        chunk_filepath, _, _ = self._config[index]\n\n        # Load the chunk\n        chunk = self._item_loader.load_chunk(chunk_index, chunk_filepath)\n\n        # Get the item\n        item = self._item_loader.get_item(chunk, index.index)\n\n        # Prefetch the next chunk\n        if self._last_chunk_index != chunk_index:\n            self._last_chunk_index = chunk_index\n            self._prepare_thread.download([chunk_index + 1])\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index should be an instance of ChunkedIndex, not {type(index)}.\")\n\n        # Check whether the chunk is already loaded\n        if not self._item_loader.is_chunk_loaded(index.chunk_index):\n            # Load the chunk\n            chunk_filepath, _, _ = self.config[index]\n\n            # Check whether the chunk is already downloaded\n            if not os.path.exists(chunk_filepath):\n                # Download the chunk\n                self._prepare_thread.download([index.chunk_index])\n\n            # Load the chunk\n            self._item_loader.load_chunk(index.chunk_index, chunk_filepath)\n\n        # Return the item\n        return self._item_loader.get_item(index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = index.chunk_index\n\n        # Check if the chunk is already loaded\n        if chunk_index in self.config._loaded_chunks:\n            return self._item_loader.load(index)\n\n        # Check if the chunk is already downloading\n        if chunk_index in self.config._downloading_chunks:\n            # Wait for the chunk to be downloaded\n            while chunk_index not in self.config._loaded_chunks:\n                pass\n            return self._item_loader.load(index)\n\n        # Check if the chunk is already pre-loaded\n        if chunk_index in self.config._pre_loaded_chunks:\n            return self._item_loader.load(index)\n\n        # Check if the chunk is already deleted\n        if chunk_index in self.config._deleted_chunks:\n            raise Exception(f\"The chunk {chunk_index} is already deleted.\")\n\n        # Download the chunk\n        self._prepare_thread.download([chunk_index])  # type: ignore\n\n        # Wait for the chunk to be downloaded\n        while chunk_index not in self.config._loaded_chunks:\n            pass\n\n        return self._item_loader.load(index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index {index} is not a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = index.chunk_index\n        chunk_filepath, _, _ = self._config[index]\n\n        # Check if the chunk is available in the cache\n        if not os.path.exists(chunk_filepath):\n            # If the chunk is not available, we need to download it\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        return self._item_loader.load(chunk_filepath, index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be an instance of ChunkedIndex. Got {type(index)}.\")\n\n        if self._prepare_thread is None:\n            raise Exception(\"The prepare thread should be defined.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index\n        chunk_index = index.chunk_index\n\n        # Get the chunk filepath\n        chunk_filepath, _, _ = self.config[index]\n\n        # If the chunk is not available locally, download it\n        if not os.path.exists(chunk_filepath):\n            self._prepare_thread.download([chunk_index])\n\n        # Get the item\n        return self._item_loader.load_item(index, chunk_filepath)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = index.chunk_index\n\n        # Check if the chunk is available in the cache\n        if chunk_index not in self._config:\n            # Download the chunk\n            self._prepare_thread.download([chunk_index])\n\n        # Get the chunk path\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n\n        # Load the chunk\n        item = self._item_loader.load(chunk_index, chunk_filepath)\n\n        # Delete the chunk if it's not needed anymore\n        if self._prepare_thread._delete_chunks_when_processed:\n            self._prepare_thread.delete([chunk_index])\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = index.chunk_index\n\n        # Check if the chunk is already loaded\n        if chunk_index not in self._item_loader.loaded_chunks:\n            # If the chunk is not loaded, load it\n            self._item_loader.load_chunk(chunk_index, self._config[index])\n\n        # Get the item from the loaded chunk\n        item = self._item_loader.get_item(index)\n\n        # Delete the chunk if the last item has been consumed\n        if self._last_chunk_index != chunk_index:\n            self._last_chunk_index = chunk_index\n            self._prepare_thread.delete([chunk_index])\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        # Check the index type\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = index.chunk_index\n\n        # Check whether the chunk is available locally\n        if chunk_index not in self._config._chunks_indexes:\n            # Check whether the chunk is available in memory\n            if chunk_index not in self._item_loader.loaded_chunks:\n                # Download the chunk\n                self._prepare_thread.download([chunk_index])\n\n        # Get the chunk from the item loader\n        chunk = self._item_loader.get_chunk(chunk_index)\n\n        # Load the item from the chunk\n        return self._item_loader.load_item(chunk, index.index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index provided is not of type ChunkedIndex. Got: {type(index)}\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the provided index\n        chunk_index = index.chunk_index\n\n        # Load the chunk\n        chunk_filepath, chunk_start_index, chunk_end_index = self.config[index]\n\n        # If the chunk is not available, download it\n        if chunk_index not in self.config.chunk_indexes:\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        chunk = self._item_loader.load_chunk(chunk_index, chunk_filepath)\n\n        # Get the item from the chunk\n        item = self._item_loader.get_item(chunk, index.index - chunk_start_index)\n\n        # If the chunk is fully consumed, delete it\n        if chunk_index == self._last_chunk_index:\n            self._prepare_thread.delete([chunk_index])\n\n        # Update the last chunk index\n        self._last_chunk_index = chunk_index\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index must be an instance of ChunkedIndex, got {index} instead.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check whether the chunk is available in the cache\n        chunk_filepath, chunk_index, chunk_offset = self._config[index]\n        if not os.path.exists(chunk_filepath):\n            # Download the chunk\n            self._prepare_thread.download([chunk_index])\n\n        # Load the item\n        item = self._item_loader.load(chunk_filepath, chunk_offset, index)\n\n        # Delete the chunk if the reader is not the master\n        if self.rank != 0:\n            self._prepare_thread.delete([chunk_index])\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        # Check if the index is a valid ChunkedIndex instance\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index is not a valid ChunkedIndex instance.\")\n\n        # Check if the reader's index configuration is defined\n        if self._config is None:\n            raise Exception(\"The reader's index configuration is not defined.\")\n\n        # Check if the chunk is available either locally or in memory\n        if not self._config.is_chunk_available(index.chunk_index):\n            # Initiate the download of the chunk\n            self._prepare_thread.download([index.chunk_index])\n\n        # Wait for the chunk to be available\n        while not self._config.is_chunk_available(index.chunk_index):\n            pass\n\n        # Load the item from the chunk\n        item = self._item_loader.load(index, self._config)\n\n        # Update the last chunk index\n        self._last_chunk_index = index.chunk_index\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        if self._prepare_thread is None:\n            raise Exception(\"The prepare thread should be defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Get the chunk filepath\n        chunk_filepath, chunk_size, chunk_offset = self.config[index]\n\n        # Check whether the chunk is already in memory\n        if chunk_index not in self._item_loader.chunks_in_memory:\n            # Check whether the chunk is already downloaded\n            if chunk_index not in self._item_loader.chunks_in_memory.chunk_indexes:\n                # Download the chunk\n                self._prepare_thread.download([chunk_index])\n\n            # Wait for the chunk to be downloaded\n            self._item_loader.wait_for_chunk(chunk_index)\n\n        # Get the item from the chunk\n        item = self._item_loader.get_item(index)\n\n        # Check whether the chunk has been fully consumed\n        if chunk_index != self._last_chunk_index:\n            # Delete the chunk\n            self._prepare_thread.delete([chunk_index])\n            self._last_chunk_index = chunk_index\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        # Check the index type\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index `{index}` is not of type ChunkedIndex.\")\n\n        # Check the config\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check the prepare thread\n        if self._prepare_thread is None:\n            raise Exception(\"The prepare thread isn't defined.\")\n\n        # Check the item loader\n        if self._item_loader is None:\n            raise Exception(\"The item loader isn't defined.\")\n\n        # Check the last chunk index\n        if self._last_chunk_index is None:\n            self._last_chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Get the chunk index\n        chunk_index = index.chunk_index\n\n        # Check if the chunk is available\n        if chunk_index not in self._item_loader.available_chunks:\n            # If not, download it\n            self._prepare_thread.download([chunk_index])\n\n        # Get the chunk from the item loader\n        chunk = self._item_loader.get_chunk(chunk_index)\n\n        # Get the item from the chunk\n        item = self._item_loader.get_item(chunk, index.index - index.chunk_index * self._item_loader.chunk_size)\n\n        # Check if the chunk is fully consumed\n        if self._item_loader.is_fully_consumed(chunk_index):\n            # If so, delete it\n            self._prepare_thread.delete([chunk_index])\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check whether the chunk is available either locally or in memory\n        chunk_index = index.chunk_index\n        chunk_filepath, chunk_size, chunk_offset = self.config[index]\n\n        if chunk_index != self._last_chunk_index:\n            # If the chunk is not available, initiate its download\n            if not os.path.exists(chunk_filepath):\n                if self._prepare_thread is None:\n                    raise Exception(\"The prepare thread should be defined.\")\n                self._prepare_thread.download([chunk_index])\n\n            # Check whether the chunk is available either locally or in memory\n            if not os.path.exists(chunk_filepath):\n                raise Exception(\"The chunk should be available either locally or in memory.\")\n\n            # If the chunk is available, load it\n            self._item_loader.load_chunk(chunk_index, chunk_filepath)\n\n            # Delete the chunk once it is fully consumed\n            if self._prepare_thread is None:\n                raise Exception(\"The prepare thread should be defined.\")\n            self._prepare_thread.delete([chunk_index])\n\n            # Update the last chunk index\n            self._last_chunk_index = chunk_index\n\n        # Load the item from the chunk\n        item = self._item_loader.load_item(index)\n\n        return item\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    if not hasattr(_broadcast_object, \"immutable_distributed_map\"):\n        _broadcast_object.immutable_distributed_map = _ImmutableDistributedMap()\n\n    return _broadcast_object.immutable_distributed_map.set_and_get(key, obj)\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    # If the application is running in a distributed setting, use the immutable distributed map to share the object\n    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None and os.getenv(\"LIGHTNING_APP_STATE_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n\n    # If not in a distributed setting, return the object as is\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    # Check if the application is running in a distributed environment\n    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        # If so, use an immutable distributed map to share the object\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    else:\n        # If not, return the original object\n        return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    # check if the application is running in an environment with an external URL\n    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    # create an instance of the _ImmutableDistributedMap\n    distributed_map = _ImmutableDistributedMap()\n\n    # broadcast the object using the distributed map\n    return distributed_map.set_and_get(key, obj)\n\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    try:\n        distributed_map = _ImmutableDistributedMap()\n        return distributed_map.set_and_get(key, obj)\n    except RuntimeError:\n        return obj\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    np.random.seed(seed + current_epoch)\n\n    shuffled_chunks = []\n    for rank_chunks in chunks_per_ranks:\n        np.random.shuffle(rank_chunks)\n        shuffled_chunks.extend(rank_chunks)\n\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the total number of chunks across all nodes\n    total_chunks = sum(chunks_per_ranks[0])\n\n    # Calculate the number of chunks per node\n    chunks_per_node = total_chunks // distributed_env.world_size\n\n    # Calculate the number of chunks per node for the current epoch\n    chunks_per_node_epoch = chunks_per_node // distributed_env.epoch_divisor\n\n    # Calculate the number of chunks per node for the current epoch\n    chunks_per_node_epoch = chunks_per_node // distributed_env.epoch_divisor\n\n    # Calculate the number of chunks per node for the current epoch\n    chunks_per_node_epoch = chunks_per_node // distributed_env.epoch_divisor\n\n    # Calculate the number of chunks per node for the current epoch\n    chunks_per_node_epoch = chunks_per_node // distributed_env.epoch_divisor\n\n    # Calculate the number of chunks per node for the current epoch\n    chunks_per_node_epoch = chunks_per_node // distributed_env.epoch_divisor\n\n    # Calculate the number of chunks per node for the current epoch\n    chunks_per_node_epoch = chunks_per_node // distributed_env.epoch_divisor\n\n    # Calculate the number of chunks per node for the current epoch\n    chunks_per_node_epoch = chunks_per_node // distributed_env.epoch_divisor\n\n    # Calculate the number of chunks per node for the current epoch\n    chunks_per_node_epoch = chunks_per_node // distributed_env.epoch_divisor\n\n    # Calculate the number of chunks per node for the current epoch\n    chunks_per_node_epoch = chunks_per_node // distributed_env.epoch_divisor\n\n    # Calculate the number of chunks per node for the current epoch\n    chunks_per_node_ep"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    np.random.seed(seed + current_epoch)\n\n    # Create a list of shuffled chunk indexes for each rank\n    shuffled_chunk_indexes_per_rank = [\n        np.random.permutation(chunk_indexes)\n        for chunk_indexes in chunks_per_ranks\n    ]\n\n    # Flatten the list of shuffled chunk indexes\n    shuffled_chunk_indexes = [\n        chunk_index\n        for rank_chunk_indexes in shuffled_chunk_indexes_per_rank\n        for chunk_index in rank_chunk_indexes\n    ]\n\n    return shuffled_chunk_indexes\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the chunk indexes for each rank\n    chunk_indexes = []\n    for rank, chunks in enumerate(chunks_per_ranks):\n        chunk_indexes.extend([(rank, chunk) for chunk in chunks])\n\n    # Shuffle the chunk indexes based on the seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunk_indexes)\n\n    # Flatten the list of shuffled chunk indexes\n    shuffled_chunk_indexes = [chunk_index[1] for chunk_index in chunk_indexes]\n\n    return shuffled_chunk_indexes\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Compute the number of chunks assigned to each rank\n    chunks_per_ranks = np.array(chunks_per_ranks)\n    chunks_per_ranks = chunks_per_ranks.sum(axis=1)\n\n    # Compute the number of chunks assigned to each node\n    chunks_per_node = np.array([chunks_per_ranks[i] for i in range(0, distributed_env.world_size, distributed_env.world_size // distributed_env.world_size)])\n\n    # Compute the number of chunks assigned to each node\n    chunks_per_node = np.array([chunks_per_ranks[i] for i in range(0, distributed_env.world_size, distributed_env.world_size // distributed_env.world_size)])\n\n    # Compute the number of chunks assigned to each node\n    chunks_per_node = np.array([chunks_per_ranks[i] for i in range(0, distributed_env.world_size, distributed_env.world_size // distributed_env.world_size)])\n\n    # Compute the number of chunks assigned to each node\n    chunks_per_node = np.array([chunks_per_ranks[i] for i in range(0, distributed_env.world_size, distributed_env.world_size // distributed_env.world_size)])\n\n    # Compute the number of chunks assigned to each node\n    chunks_per_node = np.array([chunks_per_ranks[i] for i in range(0, distributed_env.world_size, distributed_env.world_size // distributed_env.world_size)])\n\n    # Compute the number of chunks assigned to each node\n    chunks_per_node = np.array([chunks_per_ranks[i] for i in range(0, distributed_env.world_size, distributed_env.world_size // distributed_env.world_size)])\n\n    # Compute the number of chunks assigned to each node\n    chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the number of chunks assigned to each rank\n    num_chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the total number of chunks across all ranks\n    total_num_chunks = sum(num_chunks_per_rank)\n\n    # Calculate the chunk indexes for each rank\n    chunk_idxs_per_rank = []\n    start_idx = 0\n    for num_chunks in num_chunks_per_rank:\n        chunk_idxs_per_rank.append(list(range(start_idx, start_idx + num_chunks)))\n        start_idx += num_chunks\n\n    # Shuffle the chunk indexes for each rank\n    for rank in range(distributed_env.world_size):\n        np.random.seed(seed + current_epoch + rank)\n        np.random.shuffle(chunk_idxs_per_rank[rank])\n\n    # Flatten the chunk indexes for each rank into a single list\n    flattened_chunk_idxs = [\n        chunk_idx for chunk_idxs in chunk_idxs_per_rank for chunk_idx in chunk_idxs\n    ]\n\n    return flattened_chunk_idxs\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    if distributed_env.world_size == 1:\n        return [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    chunk_shuffle_list = []\n    for rank, chunks in enumerate(chunks_per_ranks):\n        np.random.seed(seed + rank + current_epoch)\n        np.random.shuffle(chunks)\n        chunk_shuffle_list.extend(chunks)\n\n    return chunk_shuffle_list\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Create a random number generator with the provided seed and current epoch\n    rng = np.random.default_rng(seed + current_epoch)\n\n    # Shuffle the chunk indexes assigned to each rank using the random number generator\n    shuffled_chunks_per_ranks = [rng.permutation(chunks) for chunks in chunks_per_ranks]\n\n    # Flatten the shuffled chunk indexes into a single list\n    shuffled_chunks = [chunk for chunks in shuffled_chunks_per_ranks for chunk in chunks]\n\n    # Return the flattened shuffled chunk indexes\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Generate a random seed for each rank\n    random_seed = seed + distributed_env.rank\n\n    # Shuffle the chunk indexes for each rank\n    np.random.seed(random_seed)\n    np.random.shuffle(chunks_per_ranks[distributed_env.rank])\n\n    # Flatten the list of chunk indexes\n    flattened_chunks_per_ranks = [\n        chunk for chunks in chunks_per_ranks for chunk in chunks\n    ]\n\n    # Shuffle the flattened list of chunk indexes\n    np.random.seed(random_seed + current_epoch)\n    np.random.shuffle(flattened_chunks_per_ranks)\n\n    return flattened_chunks_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Generate a random permutation of chunk indexes for each rank\n    np.random.seed(seed + current_epoch)\n    shuffled_chunk_idxs = [\n        np.random.permutation(chunk_idxs) for chunk_idxs in chunks_per_ranks\n    ]\n\n    # Flatten the list of lists into a single list\n    shuffled_chunk_idxs = [\n        chunk_idx for chunk_idxs in shuffled_chunk_idxs for chunk_idx in chunk_idxs\n    ]\n\n    # Shuffle the flattened list of chunk indexes based on the seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(shuffled_chunk_idxs)\n\n    # Return the flattened list of shuffled chunk indexes\n    return shuffled_chunk_idxs\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes and the world size\n    num_nodes = distributed_env.num_nodes\n    world_size = distributed_env.world_size\n\n    # Calculate the chunk indexes for each node\n    chunk_indexes = []\n    for i in range(num_nodes):\n        chunk_indexes.extend(chunks_per_ranks[i * world_size : (i + 1) * world_size])\n\n    # Shuffle the chunk indexes for each node\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunk_indexes)\n\n    # Flatten the chunk indexes\n    flattened_chunk_indexes = [item for sublist in chunk_indexes for item in sublist]\n\n    return flattened_chunk_indexes\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    np.random.seed(seed + current_epoch)\n\n    shuffled_chunks_per_ranks = []\n    for rank_chunks in chunks_per_ranks:\n        np.random.shuffle(rank_chunks)\n        shuffled_chunks_per_ranks.append(rank_chunks)\n\n    # Flatten the shuffled chunks\n    flattened_shuffled_chunks = [\n        chunk for rank_chunks in shuffled_chunks_per_ranks for chunk in rank_chunks\n    ]\n\n    # Shuffle the flattened list of shuffled chunks\n    np.random.shuffle(flattened_shuffled_chunks)\n\n    return flattened_shuffled_chunks\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Create a random number generator with a modified seed based on the current epoch\n    rng = np.random.default_rng(seed + current_epoch)\n\n    # Shuffle the chunk indexes assigned to each rank using the random number generator\n    shuffled_chunks_per_ranks = [\n        rng.permutation(chunks) for chunks in chunks_per_ranks\n    ]\n\n    # Flatten the shuffled chunk indexes list to obtain a single list of shuffled chunk indexes\n    shuffled_chunks = [chunk for chunks in shuffled_chunks_per_ranks for chunk in chunks]\n\n    # Shuffle the flattened list of shuffled chunk indexes using the random number generator\n    shuffled_chunks = rng.permutation(shuffled_chunks)\n\n    return shuffled_chunks\n\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Generate a random permutation of chunk indexes using the seed and current epoch\n    chunk_indexes = np.arange(len(chunks_per_ranks))\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunk_indexes)\n\n    # Flatten the list of chunk indexes per rank\n    chunk_indexes = [chunk for rank_chunks in chunks_per_ranks for chunk in rank_chunks]\n\n    # Shuffle the flattened list of chunk indexes using the random permutation\n    shuffled_chunk_indexes = [chunk_indexes[i] for i in chunk_indexes]\n\n    # Distribute the shuffled chunk indexes across all nodes\n    shuffled_chunk_indexes = np.array_split(shuffled_chunk_indexes, distributed_env.world_size)\n    shuffled_chunk_indexes = [chunk_indexes.tolist() for chunk_indexes in shuffled_chunk_indexes]\n\n    # Flatten the list of shuffled chunk indexes\n    shuffled_chunk_indexes = [chunk for node_chunks in shuffled_chunk_indexes for chunk in node_chunks]\n\n    return shuffled_chunk_indexes\n\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the number of chunks per node\n    chunks_per_node = [\n        sum(chunks_per_ranks[i]) for i in range(distributed_env.world_size)\n    ]\n\n    # Calculate the chunk indexes for each node\n    node_chunk_idxs = [\n        [\n            sum(chunks_per_ranks[i][:j])\n            for j in range(1, len(chunks_per_ranks[i]) + 1)\n        ]\n        for i in range(distributed_env.world_size)\n    ]\n\n    # Calculate the seed for each node\n    node_seeds = [seed + i for i in range(distributed_env.world_size)]\n\n    # Shuffle the chunk indexes for each node using the seed and current epoch\n    for i in range(distributed_env.world_size):\n        np.random.seed(node_seeds[i] + current_epoch)\n        np.random.shuffle(node_chunk_idxs[i])\n\n    # Flatten the shuffled chunk indexes\n    flattened_node_chunk_idxs = [\n        node_chunk_idxs[i][j] for i in range(distributed_env.world_size) for j in range(len(node_chunk_idxs[i]))\n    ]\n\n    return flattened_node_chunk_idxs\n\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the chunk indexes for each rank\n    chunk_indexes = []\n    for rank, chunks in enumerate(chunks_per_ranks):\n        chunk_indexes.extend([(rank, chunk) for chunk in chunks])\n\n    # Shuffle the chunk indexes based on the seed and epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunk_indexes)\n\n    # Flatten the list of shuffled chunk indexes\n    flattened_chunk_indexes = [chunk_index for rank, chunk_index in chunk_indexes]\n\n    return flattened_chunk_indexes\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes in the distributed environment\n    num_nodes = distributed_env.num_nodes\n\n    # Compute the number of chunks per node\n    num_chunks_per_node = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Compute the number of chunks per rank\n    num_chunks_per_rank = [\n        sum(num_chunks_per_node[i * num_nodes : (i + 1) * num_nodes])\n        for i in range(distributed_env.world_size // num_nodes)\n    ]\n\n    # Compute the chunk indexes for each rank\n    chunk_indexes_per_rank = [\n        np.arange(num_chunks_per_rank[i], dtype=np.int64)\n        for i in range(distributed_env.world_size // num_nodes)\n    ]\n\n    # Shuffle the chunk indexes for each rank\n    np.random.seed(seed + current_epoch)\n    for i in range(distributed_env.world_size // num_nodes):\n        np.random.shuffle(chunk_indexes_per_rank[i])\n\n    # Flatten the chunk indexes for each rank into a single list\n    chunk_indexes_per_rank_flat = [\n        chunk_indexes_per_rank[i][j]\n        for i in range(distributed_env.world_size // num_nodes)\n        for j in range(num_chunks_per_rank[i])\n    ]\n\n    # Return the flattened list of chunk indexes\n    return chunk_indexes_per_rank_flat\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the number of chunks assigned to each rank\n    num_chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the total number of chunks across all ranks\n    total_num_chunks = sum(num_chunks_per_rank)\n\n    # Calculate the number of chunks per node\n    num_chunks_per_node = [\n        num_chunks_per_rank[i] for i in distributed_env.chunk_ranks\n    ]\n\n    # Calculate the total number of chunks per node\n    total_num_chunks_per_node = sum(num_chunks_per_node)\n\n    # Generate a random permutation of chunk indexes for each node\n    chunk_indexes_per_node = []\n    for i in range(distributed_env.world_size):\n        if i in distributed_env.chunk_ranks:\n            chunk_indexes_per_node.append(\n                np.random.default_rng(seed + current_epoch).permutation(\n                    num_chunks_per_node[i]\n                )\n            )\n        else:\n            chunk_indexes_per_node.append(np.zeros(num_chunks_per_node[i], dtype=int))\n\n    # Flatten the chunk indexes per node into a single list\n    flattened_chunk_indexes_per_node = [\n        chunk_indexes_per_node[i][j]\n        for i in range(distributed_env.world_size)\n        for j in range(num_chunks_per_node[i])\n    ]\n\n    # Shuffle the flattened chunk indexes per node based on the seed and current epoch\n    shuffled_flattened_chunk_indexes_per_node = np.random.default_rng(\n        seed + current_epoch\n    ).permutation(flattened_chunk_indexes_per_node)\n\n    # Reshape the shuffled flattened chunk indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the chunk index range for each node\n    chunk_index_range_per_ranks = [\n        (0, sum(chunks_per_ranks[i])) for i in range(distributed_env.world_size)\n    ]\n\n    # Generate a random permutation of chunk indexes for each node\n    random_permutation_per_ranks = [\n        np.random.RandomState(seed + current_epoch + i).permutation(\n            chunk_index_range_per_ranks[i][1]\n        )\n        for i in range(distributed_env.world_size)\n    ]\n\n    # Flatten the random permutation and return\n    return [\n        chunk_index\n        for rank_index, rank_random_permutation in enumerate(random_permutation_per_ranks)\n        for chunk_index in rank_random_permutation\n        + chunk_index_range_per_ranks[rank_index][0]\n    ]\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes and ranks\n    num_nodes = distributed_env.num_nodes\n    num_ranks = distributed_env.world_size\n\n    # Calculate the number of chunks per node\n    chunks_per_node = [len(chunks_per_ranks[i]) for i in range(num_ranks)]\n\n    # Calculate the chunk index offsets for each node\n    chunk_index_offsets = np.cumsum([0] + chunks_per_node[:-1])\n\n    # Calculate the total number of chunks\n    num_chunks = np.sum(chunks_per_node)\n\n    # Calculate the seed for the current epoch\n    seed = seed + current_epoch\n\n    # Calculate the chunk indexes for each node\n    chunk_indexes = np.arange(num_chunks)\n\n    # Shuffle the chunk indexes for each node\n    np.random.seed(seed)\n    np.random.shuffle(chunk_indexes)\n\n    # Map the shuffled chunk indexes to their original positions on each node\n    chunk_indexes_per_node = np.split(chunk_indexes, chunk_index_offsets[1:])\n\n    # Flatten the chunk indexes per node into a single list\n    chunk_indexes_per_node = [chunk_indexes_per_node[i][chunks_per_ranks[i]] for i in range(num_ranks)]\n    chunk_indexes = [item for sublist in chunk_indexes_per_node for item in sublist]\n\n    return chunk_indexes\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return _resolve_dir(indexed_paths[0])\n\n    if len(indexed_paths) == 2:\n        if indexed_paths[0] == indexed_paths[1]:\n            return _resolve_dir(indexed_paths[0])\n        else:\n            raise ValueError(\n                \"Inconsistent file paths found in inputs. Please provide a single file path or a directory path.\"\n            )\n\n    raise ValueError(\"Invalid inputs. Please provide a single file path or a directory path.\")\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            f\"Expected at most two inputs, got {len(indexed_paths)}: {indexed_paths}\"\n        )\n\n    if len(indexed_paths) == 1:\n        return indexed_paths[0]\n\n    first_path, second_path = list(indexed_paths.values())\n\n    if first_path != second_path:\n        raise ValueError(f\"Expected two identical inputs, got {first_path} and {second_path}\")\n\n    return first_path\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if not inputs:\n        return None\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\"Only two paths are allowed\")\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    first_path = list(indexed_paths.values())[0]\n    second_path = list(indexed_paths.values())[1]\n\n    if not os.path.samefile(first_path, second_path):\n        raise ValueError(f\"Paths are not the same: {first_path} != {second_path}\")\n\n    return os.path.dirname(first_path)\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            f\"Found {len(indexed_paths)} valid file paths, expected at most 2.\"\n        )\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return indexed_paths[0]\n\n    if len(indexed_paths) == 2:\n        if indexed_paths[0] != indexed_paths[1]:\n            raise ValueError(\n                f\"Found inconsistent file paths: {indexed_paths[0]} and {indexed_paths[1]}.\"\n            )\n        return indexed_paths[0]\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return indexed_paths[0]\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            f\"Found {len(indexed_paths)} paths in inputs. Expected 1 or 2 paths.\"\n        )\n\n    path_0 = indexed_paths[0]\n    path_1 = indexed_paths[1]\n\n    if path_0 == path_1:\n        return path_0\n\n    if path_0.startswith(path_1):\n        return path_1\n\n    if path_1.startswith(path_0):\n        return path_0\n\n    raise ValueError(\n        f\"Found inconsistent paths in inputs: {path_0} and {path_1}. Expected consistent paths.\"\n    )\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            \"DataProcessor can only handle a maximum of 2 inputs. Please check your inputs.\"\n        )\n\n    if len(indexed_paths) == 1:\n        return indexed_paths[0]\n\n    path_1 = indexed_paths[0]\n    path_2 = indexed_paths[1]\n\n    if path_1 != path_2:\n        raise ValueError(\n            \"DataProcessor can only handle a maximum of 2 inputs. Please check your inputs.\"\n        )\n\n    return path_1\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if not indexed_paths:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            \"Found more than two paths in the input sequence. \"\n            \"Only two paths are supported. \"\n            \"Please provide a single path or a sequence of paths.\"\n        )\n\n    if len(indexed_paths) == 1:\n        return indexed_paths[0]\n\n    first_path, second_path = indexed_paths.values()\n    if first_path != second_path:\n        raise ValueError(\n            \"Found inconsistent file paths in the input sequence. \"\n            \"Please provide a single path or a sequence of paths.\"\n        )\n\n    return first_path\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if not indexed_paths:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\"Too many inputs.\")\n\n    first_path = indexed_paths.get(0)\n    second_path = indexed_paths.get(1)\n\n    if first_path and second_path and first_path != second_path:\n        raise ValueError(\"Inputs are not consistent.\")\n\n    if first_path:\n        return os.path.dirname(first_path)\n\n    if second_path:\n        return os.path.dirname(second_path)\n\n    return None\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return indexed_paths[0]\n\n    paths = list(indexed_paths.values())\n\n    if paths[0] == paths[1]:\n        return paths[0]\n\n    raise ValueError(f\"Inconsistent paths found in the inputs: {paths}\")\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            \"The `inputs` argument can only have up to 2 file paths. \"\n            \"Please check your `inputs` argument.\"\n        )\n\n    if len(indexed_paths) == 1:\n        return indexed_paths[0]\n\n    first_path = indexed_paths[0]\n    second_path = indexed_paths[1]\n\n    if not first_path.startswith(second_path):\n        raise ValueError(\n            \"The `inputs` argument must have consistent file paths. \"\n            \"Please check your `inputs` argument.\"\n        )\n\n    return first_path\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Get indexed paths from the first two elements of the input sequence\n    indexed_paths = _get_indexed_paths(inputs[:2])\n\n    # If there are no indexed paths, return None\n    if not indexed_paths:\n        return None\n\n    # If there is only one indexed path, return the absolute path of the file\n    if len(indexed_paths) == 1:\n        return os.path.abspath(indexed_paths[0])\n\n    # If there are multiple indexed paths, check if they are consistent\n    first_index, first_path = next(iter(indexed_paths.items()))\n    for index, path in indexed_paths.items():\n        if index != first_index or not os.path.exists(path) or not os.path.isfile(path):\n            raise ValueError(\n                \"Inconsistent file paths found in the input sequence. Please provide a single file path or a directory containing multiple files.\"\n            )\n\n    # If the paths are consistent, return the absolute path of the directory containing the files\n    return os.path.dirname(first_path)\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(inputs) > 1:\n        indexed_paths_2 = _get_indexed_paths(inputs[1])\n\n        if indexed_paths != indexed_paths_2:\n            raise ValueError(\n                \"Inconsistent file paths in the input sequence. Please provide a single sequence of inputs with consistent file paths.\"\n            )\n\n    if not indexed_paths:\n        return None\n\n    first_path = list(indexed_paths.values())[0]\n    first_path = _resolve_dir(first_path)\n\n    return first_path\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if not isinstance(inputs, Sequence):\n        raise ValueError(\"inputs must be a sequence of inputs\")\n\n    if len(inputs) == 0:\n        raise ValueError(\"inputs must be a sequence of inputs\")\n\n    if not isinstance(inputs[0], (str, Path)):\n        raise ValueError(\"inputs must be a sequence of inputs\")\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        raise ValueError(\"inputs must be a sequence of inputs\")\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    if len(indexed_paths) > 1:\n        first_path = list(indexed_paths.values())[0]\n        second_path = list(indexed_paths.values())[1]\n\n        if not os.path.commonpath([first_path, second_path]):\n            raise ValueError(\"inputs must be a sequence of inputs\")\n\n        return os.path.dirname(first_path)\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Get indexed paths from the first two elements of the inputs\n    indexed_paths_1 = _get_indexed_paths(inputs[0])\n    indexed_paths_2 = _get_indexed_paths(inputs[1])\n\n    # Check if there are any inconsistent indexed paths\n    if indexed_paths_1 != indexed_paths_2:\n        raise ValueError(\n            \"Inconsistent indexed paths found in the input sequence. \"\n            \"Make sure that all elements in the input sequence have the same indexed paths.\"\n        )\n\n    # Get the first indexed path from the first element\n    indexed_path_1 = list(indexed_paths_1.values())[0]\n\n    # Check if the indexed path is a directory\n    if not os.path.isdir(indexed_path_1):\n        raise ValueError(\n            f\"The indexed path '{indexed_path_1}' is not a directory. \"\n            \"Make sure that the first element of the input sequence contains a valid directory path.\"\n        )\n\n    # Return the absolute path to the input directory\n    return os.path.abspath(indexed_path_1)\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Get indexed paths from inputs\n    indexed_paths = _get_indexed_paths(inputs)\n\n    # Check if there are at least two indexed paths\n    if len(indexed_paths) < 2:\n        return None\n\n    # Extract the file paths from the indexed paths\n    file_paths = list(indexed_paths.values())\n\n    # Check if all file paths are consistent\n    if not all(file_paths[0] == file_path for file_path in file_paths):\n        raise ValueError(\"Inputs must have consistent file paths.\")\n\n    # Get the file path from the first indexed path\n    file_path = file_paths[0]\n\n    # Get the directory path from the file path\n    dir_path = os.path.dirname(file_path)\n\n    return dir_path\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return _resolve_dir(list(indexed_paths.values())[0])\n\n    first_index, first_path = list(indexed_paths.items())[0]\n    second_index, second_path = list(indexed_paths.items())[1]\n\n    if first_index > second_index:\n        first_index, second_index = second_index, first_index\n        first_path, second_path = second_path, first_path\n\n    if first_index != 0:\n        raise ValueError(\n            f\"Indexed paths must start at index 0. Got {first_index} and {second_index}.\"\n        )\n\n    if first_path != second_path:\n        raise ValueError(\n            f\"Indexed paths must be the same. Got {first_path} and {second_path}.\"\n        )\n\n    return _resolve_dir(first_path)\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    indexed_paths = _get_indexed_paths(inputs[0])\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(inputs) > 1:\n        indexed_paths_2 = _get_indexed_paths(inputs[1])\n        if len(indexed_paths_2) > 0:\n            if indexed_paths.keys() != indexed_paths_2.keys():\n                raise ValueError(\n                    \"Inconsistent file paths found in the inputs. \"\n                    \"Please provide only one file path for each input.\"\n                )\n\n    return os.path.dirname(list(indexed_paths.values())[0])\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Determine the input directory from the first two elements of the input sequence\n    input_dir = None\n    if len(inputs) >= 2:\n        # Extract indexed paths from the first two elements of the input sequence\n        indexed_paths = _get_indexed_paths(inputs[:2])\n\n        # If there is only one indexed path, it is the input directory\n        if len(indexed_paths) == 1:\n            input_dir = indexed_paths[0]\n\n        # If there are two indexed paths, they should be the same\n        elif len(indexed_paths) == 2:\n            # Get the absolute paths of the indexed paths\n            input_dirs = [os.path.abspath(path) for path in indexed_paths.values()]\n\n            # If the paths are the same, it is the input directory\n            if input_dirs[0] == input_dirs[1]:\n                input_dir = input_dirs[0]\n\n            # If the paths are different, raise an error\n            else:\n                raise ValueError(\n                    f\"Inconsistent input directories found: {input_dirs[0]} and {input_dirs[1]}\"\n                )\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    if len(indexed_paths) == 2:\n        file_path_1 = list(indexed_paths.values())[0]\n        file_path_2 = list(indexed_paths.values())[1]\n\n        if os.path.dirname(file_path_1) == os.path.dirname(file_path_2):\n            return os.path.dirname(file_path_1)\n\n        raise ValueError(\n            f\"The inputs {file_path_1} and {file_path_2} are inconsistent with respect to the project root or depth in the file system.\"\n        )\n\n    raise ValueError(\n        f\"The inputs {list(indexed_paths.values())} are inconsistent with respect to the project root or depth in the file system.\"\n    )\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if not indexed_paths:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            \"Only two inputs can be provided, but received {}\".format(len(indexed_paths))\n        )\n\n    if len(indexed_paths) == 2:\n        path1, path2 = indexed_paths.values()\n        if not os.path.isdir(path1):\n            path1, path2 = path2, path1\n\n        if not os.path.isdir(path2):\n            raise ValueError(\n                \"The second input must be a directory, but received {}\".format(path2)\n            )\n\n        if path1 != path2:\n            raise ValueError(\n                \"The two inputs must be the same directory, but received {} and {}\".format(\n                    path1, path2\n                )\n            )\n\n    return indexed_paths[0]\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=1\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL)\n\n    try:\n        yield\n    finally:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        try:\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=1\"], stdout=DEVNULL, stderr=DEVNULL)\n            yield\n        finally:\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        yield\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            # Enable DNS optimization\n            Popen([\"networksetup\", \"-setdnsservers\", \"Wi-Fi\", \"Empty\"], stdout=DEVNULL, stderr=DEVNULL)\n        yield\n    finally:\n        # Disable DNS optimization\n        Popen([\"networksetup\", \"-setdnsservers\", \"Wi-Fi\", \"Empty\"], stdout=DEVNULL, stderr=DEVNULL)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_challenge_ack_limit=99999999\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_retries2=99999999\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_challenge_ack_limit=18\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_retries2=5\"], stdout=DEVNULL, stderr=DEVNULL)\n    try:\n        yield\n    finally:\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_challenge_ack_limit=18\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_retries2=5\"], stdout=DEVNULL, stderr=DEVNULL)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        raise ValueError(\"This function is only supported in Lightning Studio.\")\n\n    if enable:\n        # Enable DNS optimization\n        Popen([\"lightning\", \"dns\", \"optimize\", \"enable\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        # Disable DNS optimization\n        Popen([\"lightning\", \"dns\", \"optimize\", \"disable\"], stdout=DEVNULL, stderr=DEVNULL)\n\n    try:\n        yield\n    finally:\n        # Disable DNS optimization\n        Popen([\"lightning\", \"dns\", \"optimize\", \"disable\"], stdout=DEVNULL, stderr=DEVNULL)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        # Enable DNS optimization\n        Popen([\"sudo\", \"resolvconf\", \"-a\", \"resolv.conf.head\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        # Disable DNS optimization\n        Popen([\"sudo\", \"resolvconf\", \"-d\", \"resolv.conf.head\"], stdout=DEVNULL, stderr=DEVNULL)\n\n    try:\n        yield\n    finally:\n        # Disable DNS optimization\n        Popen([\"sudo\", \"resolvconf\", \"-d\", \"resolv.conf.head\"], stdout=DEVNULL, stderr=DEVNULL)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.ip_unprivileged_port_start=0\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_tw_reuse=1\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_fin_timeout=15\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.ip_unprivileged_port_start=1024\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_tw_reuse=0\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_fin_timeout=60\"], stdout=DEVNULL, stderr=DEVNULL)\n    try:\n        yield\n    finally:\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.ip_unprivileged_port_start=1024\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_tw_reuse=0\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"net.ipv4.tcp_fin_timeout=60\"], stdout=DEVNULL, stderr=DEVNULL)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            Popen([\"resolvconf\", \"-u\"], stdout=DEVNULL, stderr=DEVNULL)\n        yield\n    finally:\n        if enable:\n            Popen([\"resolvconf\", \"-d\", \"wlan0\"], stdout=DEVNULL, stderr=DEVNULL)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        try:\n            # Enable DNS optimization\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch=1\"], stdout=DEVNULL, stderr=DEVNULL)\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch_doh=1\"], stdout=DEVNULL, stderr=DEVNULL)\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch_doh_fallback=1\"], stdout=DEVNULL, stderr=DEVNULL)\n        except Exception:\n            pass\n    else:\n        try:\n            # Disable DNS optimization\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch=0\"], stdout=DEVNULL, stderr=DEVNULL)\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch_doh=0\"], stdout=DEVNULL, stderr=DEVNULL)\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch_doh_fallback=0\"], stdout=DEVNULL, stderr=DEVNULL)\n        except Exception:\n            pass\n    try:\n        yield\n    finally:\n        try:\n            # Disable DNS optimization\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch=0\"], stdout=DEVNULL, stderr=DEVNULL)\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch_doh=0\"], stdout=DEVNULL, stderr=DEVNULL)\n            Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch_doh_fallback=0\"], stdout=DE"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = [\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=64\"]\n    else:\n        cmd = [\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=128\"]\n\n    try:\n        with Popen(cmd, stdout=DEVNULL, stderr=DEVNULL) as process:\n            process.wait()\n        yield\n    finally:\n        with Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=128\"], stdout=DEVNULL, stderr=DEVNULL) as process:\n            process.wait()\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            # Enable DNS optimization\n            with Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch=1\"], stdout=DEVNULL, stderr=DEVNULL) as process:\n                process.wait()\n\n        yield\n    finally:\n        # Disable DNS optimization\n        with Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch=0\"], stdout=DEVNULL, stderr=DEVNULL) as process:\n            process.wait()\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        with Popen(\n            [\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.redirect_preferred_source=0\"],\n            stdout=DEVNULL,\n            stderr=DEVNULL,\n        ):\n            yield\n    else:\n        yield\n\n    with Popen(\n        [\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.redirect_preferred_source=1\"],\n        stdout=DEVNULL,\n        stderr=DEVNULL,\n    ):\n        pass"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    # Enable DNS optimization\n    if enable:\n        try:\n            # Disable DNS optimization\n            Popen([\"resolvconf\", \"-d\", \"lightning\"], stdout=DEVNULL, stderr=DEVNULL)\n            yield\n        finally:\n            # Disable DNS optimization\n            Popen([\"resolvconf\", \"-e\", \"lightning\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        yield\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        # Enable DNS optimization\n        with Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=60\"], stdout=DEVNULL, stderr=DEVNULL) as p:\n            p.wait()\n    try:\n        yield\n    finally:\n        # Disable DNS optimization\n        with Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL) as p:\n            p.wait()\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            Popen([\"netsh\", \"interface\", \"ipv4\", \"set\", \"dns\", \"dhcp\", \"register=primary\"], stdout=DEVNULL, stderr=DEVNULL)\n        yield\n    finally:\n        if enable:\n            Popen([\"netsh\", \"interface\", \"ipv4\", \"set\", \"dns\", \"dhcp\", \"register=primary\"], stdout=DEVNULL, stderr=DEVNULL)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet6.ip6.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL)\n    try:\n        yield\n    finally:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet6.ip6.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch=0\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch=1\"], stdout=DEVNULL, stderr=DEVNULL)\n\n    try:\n        yield\n    finally:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.enable_prefetch=0\"], stdout=DEVNULL, stderr=DEVNULL)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        try:\n            _run_command([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.use_temp_buf=1\"])\n        except Exception:\n            pass\n    try:\n        yield\n    finally:\n        try:\n            _run_command([\"sudo\", \"sysctl\", \"-w\", \"net.inet.dns.use_temp_buf=0\"])\n        except Exception:\n            pass\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Check if DNS optimization is already enabled or disabled\n    current_state = _IS_IN_STUDIO\n    if current_state == enable:\n        yield\n        return\n\n    # Enable DNS optimization if it's currently disabled\n    if not current_state:\n        print(\"Enabling DNS optimization...\")\n        Popen([\"sudo\", \"resolvconf\", \"-u\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        # Disable DNS optimization if it's currently enabled\n        print(\"Disabling DNS optimization...\")\n        Popen([\"sudo\", \"resolvconf\", \"-d\", \"wlan0\"], stdout=DEVNULL, stderr=DEVNULL)\n\n    # Execute the desired operations within the context\n    yield\n\n    # Disable DNS optimization regardless of the initial state\n    if not current_state:\n        print(\"Disabling DNS optimization...\")\n        Popen([\"sudo\", \"resolvconf\", \"-d\", \"wlan0\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        print(\"Enabling DNS optimization...\")\n        Popen([\"sudo\", \"resolvconf\", \"-u\"], stdout=DEVNULL, stderr=DEVNULL)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = [\"sudo\", \"sysctl\", \"net.ipv4.tcp_ecn=0\"]\n    else:\n        cmd = [\"sudo\", \"sysctl\", \"net.ipv4.tcp_ecn=1\"]\n\n    # Enable DNS optimization\n    with Popen(cmd, stdout=DEVNULL, stderr=DEVNULL) as p:\n        try:\n            yield\n        finally:\n            # Disable DNS optimization\n            p.terminate()\n            p.wait()\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items per rank\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n\n    # distribute the items across the ranks\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(indexes)):\n        chunks_per_ranks[i % distributed_env.world_size].append(indexes[i])\n\n    # distribute the intervals across the ranks\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(chunk_intervals)):\n        chunk_intervals_per_ranks[i % distributed_env.world_size].append(chunk_intervals[i])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n\n    if drop_last:\n        num_items_per_rank = (len(indexes) + distributed_env.world_size - 1) // distributed_env.world_size\n        for rank in range(distributed_env.world_size):\n            start_index = rank * num_items_per_rank\n            end_index = min((rank + 1) * num_items_per_rank, len(indexes))\n            chunks_per_ranks[rank] = indexes[start_index:end_index]\n            chunk_intervals_per_ranks[rank] = chunk_intervals[start_index:end_index]\n    else:\n        num_items_per_rank = len(indexes) // distributed_env.world_size\n        for rank in range(distributed_env.world_size):\n            start_index = rank * num_items_per_rank\n            end_index = min((rank + 1) * num_items_per_rank, len(indexes))\n            chunks_per_ranks[rank] = indexes[start_index:end_index]\n            chunk_intervals_per_ranks[rank] = chunk_intervals[start_index:end_index]\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n    num_items_per_rank = [num_items_per_rank for _ in range(distributed_env.world_size)]\n\n    # distribute the items across the ranks\n    indexes_per_rank = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_rank = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(indexes)):\n        rank = i % distributed_env.world_size\n        indexes_per_rank[rank].append(indexes[i])\n        intervals_per_rank[rank].append(chunk_intervals[i])\n\n    return indexes_per_rank, intervals_per_rank"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n\n    # distribute the indexes and intervals across the ranks\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank, (start, end) in enumerate(chunk_intervals):\n        chunks_per_ranks[rank % distributed_env.world_size].extend(indexes[start:end])\n        chunk_intervals_per_ranks[rank % distributed_env.world_size].append((start, end))\n\n    return chunks_per_ranks, chunk_intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = (rank + 1) * num_items_per_rank\n        chunks_per_ranks[rank].extend(indexes[start:end])\n        chunk_intervals_per_ranks[rank].extend(chunk_intervals[start:end])\n\n    # if there are remaining items, distribute them to the first ranks\n    if len(indexes) % distributed_env.world_size != 0:\n        remaining_items = len(indexes) % distributed_env.world_size\n        for rank in range(remaining_items):\n            chunks_per_ranks[rank].append(indexes[-remaining_items + rank])\n            chunk_intervals_per_ranks[rank].append(chunk_intervals[-remaining_items + rank])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n\n    # associate chunks and intervals to ranks\n    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks: List[List[Any]] = [[] for _ in range(distributed_env.world_size)]\n    for rank, (start, end) in enumerate(chunk_intervals):\n        chunks_per_ranks[rank % distributed_env.world_size].extend(indexes[start:end])\n        intervals_per_ranks[rank % distributed_env.world_size].extend(\n            chunk_intervals[start:end]\n        )\n\n    return chunks_per_ranks, intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n\n    # assign chunks to each rank\n    chunks_per_ranks = [\n        indexes[i * num_items_per_rank : (i + 1) * num_items_per_rank]\n        for i in range(distributed_env.world_size)\n    ]\n\n    # assign intervals to each rank\n    intervals_per_ranks = [\n        chunk_intervals[i * num_items_per_rank : (i + 1) * num_items_per_rank]\n        for i in range(distributed_env.world_size)\n    ]\n\n    return chunks_per_ranks, intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n    if num_items_per_rank <= 0:\n        raise ValueError(\"Not enough data to distribute over all ranks\")\n\n    # distribute the items across the ranks\n    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(indexes)):\n        chunks_per_ranks[i % distributed_env.world_size].append(indexes[i])\n\n    # distribute the intervals across the ranks\n    chunk_intervals_per_ranks: List[List[Any]] = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(chunk_intervals)):\n        chunk_intervals_per_ranks[i % distributed_env.world_size].append(chunk_intervals[i])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    num_items_remaining = len(indexes) % distributed_env.world_size\n    num_items_per_rank_list = [num_items_per_rank] * distributed_env.world_size\n    for i in range(num_items_remaining):\n        num_items_per_rank_list[i] += 1\n\n    # associate chunks and their intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank, num_items_per_rank in enumerate(num_items_per_rank_list):\n        chunks_per_ranks[rank].extend(indexes[:num_items_per_rank])\n        intervals_per_ranks[rank].extend(chunk_intervals[:num_items_per_rank])\n        indexes = indexes[num_items_per_rank:]\n        chunk_intervals = chunk_intervals[num_items_per_rank:]\n\n    # drop the last items if needed\n    if drop_last:\n        chunks_per_ranks = [chunks[:-1] for chunks in chunks_per_ranks]\n        intervals_per_ranks = [intervals[:-1] for intervals in intervals_per_ranks]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    num_items_last_rank = len(indexes) % distributed_env.world_size\n    num_items_per_rank = (\n        num_items_per_rank + 1 if drop_last and num_items_last_rank > 0 else num_items_per_rank\n    )\n\n    # Distribute the chunks and intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = start + num_items_per_rank\n        chunks_per_ranks[rank] = indexes[start:end]\n        chunk_intervals_per_ranks[rank] = chunk_intervals[start:end]\n\n    # Adjust the number of items in the last rank if necessary\n    if drop_last and num_items_last_rank > 0:\n        chunks_per_ranks[-1] = indexes[-num_items_last_rank:]\n        chunk_intervals_per_ranks[-1] = chunk_intervals[-num_items_last_rank:]\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = (\n        len(indexes) // distributed_env.world_size\n        if not drop_last\n        else len(indexes) // distributed_env.world_size + 1\n    )\n\n    # distribute the chunks and their intervals across different ranks\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunks_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(indexes)):\n        rank = i // num_items_per_rank\n        chunks_per_ranks[rank].append(indexes[i])\n        chunks_intervals_per_ranks[rank].append(chunk_intervals[i])\n\n    return chunks_per_ranks, chunks_intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items to process per rank\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    num_items_remainder = len(indexes) % distributed_env.world_size\n    num_items_per_rank_list = [num_items_per_rank] * distributed_env.world_size\n    if drop_last:\n        num_items_per_rank_list[-1] -= num_items_remainder\n    else:\n        num_items_per_rank_list[-1] += num_items_remainder\n\n    # assign chunks and intervals to ranks\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunks_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    start_index = 0\n    for rank, num_items in enumerate(num_items_per_rank_list):\n        end_index = start_index + num_items\n        chunks_per_ranks[rank] = indexes[start_index:end_index]\n        chunks_intervals_per_ranks[rank] = chunk_intervals[start_index:end_index]\n        start_index = end_index\n\n    return chunks_per_ranks, chunks_intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items per rank\n    items_per_rank = (len(indexes) + distributed_env.world_size - 1) // distributed_env.world_size\n    if drop_last and distributed_env.rank == distributed_env.world_size - 1:\n        items_per_rank = len(indexes) % distributed_env.world_size\n\n    # distribute the chunks and their intervals to the ranks\n    chunks_per_ranks = [\n        indexes[i * items_per_rank : (i + 1) * items_per_rank] for i in range(distributed_env.world_size)\n    ]\n    chunk_intervals_per_ranks = [\n        chunk_intervals[i * items_per_rank : (i + 1) * items_per_rank] for i in range(distributed_env.world_size)\n    ]\n\n    return chunks_per_ranks, chunk_intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items per rank\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n\n    # distribute the items and intervals across the ranks\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(indexes)):\n        rank = i % distributed_env.world_size\n        chunks_per_ranks[rank].append(indexes[i])\n        intervals_per_ranks[rank].append(chunk_intervals[i])\n\n    return chunks_per_ranks, intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # get the number of items per rank\n    num_items_per_rank = _get_num_items_per_rank(distributed_env, len(indexes), drop_last)\n\n    # associate the chunks and intervals to the ranks\n    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks: List[List[Any]] = [[] for _ in range(distributed_env.world_size)]\n    for i, (index, interval) in enumerate(zip(indexes, chunk_intervals)):\n        rank = i // num_items_per_rank\n        chunks_per_ranks[rank].append(index)\n        intervals_per_ranks[rank].append(interval)\n\n    return chunks_per_ranks, intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n\n    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n\n    # distribute the items to the ranks\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = (rank + 1) * num_items_per_rank\n\n        # add the indexes to the corresponding rank\n        chunks_per_ranks[rank].extend(indexes[start_index:end_index])\n\n        # add the intervals to the corresponding rank\n        chunk_intervals_per_ranks[rank].extend(chunk_intervals[start_index:end_index])\n\n    # add the last items to the last rank\n    if drop_last:\n        start_index = distributed_env.world_size * num_items_per_rank\n        chunks_per_ranks[distributed_env.world_size - 1].extend(indexes[start_index:])\n        chunk_intervals_per_ranks[distributed_env.world_size - 1].extend(chunk_intervals[start_index:])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n    num_items_per_rank = int(num_items_per_rank)\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks: Any = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks: Any = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = start + num_items_per_rank\n        if rank == distributed_env.world_size - 1:\n            end += len(indexes) % distributed_env.world_size\n        chunks_per_ranks[rank] = indexes[start:end]\n        chunk_intervals_per_ranks[rank] = chunk_intervals[start:end]\n\n    return chunks_per_ranks, chunk_intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items per rank\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    num_items_per_rank += int(drop_last)\n\n    # calculate the number of items to drop from the end of the list\n    num_items_to_drop = len(indexes) % distributed_env.world_size\n\n    # distribute the items to each rank\n    chunks_per_ranks = []\n    chunk_intervals_per_ranks = []\n    for i in range(distributed_env.world_size):\n        start = i * num_items_per_rank\n        end = start + num_items_per_rank\n        if i == distributed_env.world_size - 1:\n            end -= num_items_to_drop\n        chunks_per_ranks.append(indexes[start:end])\n        chunk_intervals_per_ranks.append(chunk_intervals[start:end])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    chunks_per_ranks_intervals: List[List[Any]] = [[] for _ in range(distributed_env.world_size)]\n    num_items_per_rank: List[int] = [0] * distributed_env.world_size\n\n    num_items = len(indexes)\n    num_items_per_rank = [num_items // distributed_env.world_size] * distributed_env.world_size\n    num_items_per_rank[0] += num_items % distributed_env.world_size\n\n    # associate chunks to ranks\n    for i, chunk_index in enumerate(indexes):\n        rank = i % distributed_env.world_size\n        chunks_per_ranks[rank].append(chunk_index)\n        chunks_per_ranks_intervals[rank].append(chunk_intervals[i])\n\n    if drop_last:\n        # drop the last items to make the distribution even across all ranks\n        for i in range(distributed_env.world_size):\n            num_items_per_rank[i] -= 1\n\n    return chunks_per_ranks, chunks_per_ranks_intervals\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # determine the number of items per rank\n    items_per_rank = len(indexes) // distributed_env.world_size\n\n    # determine the number of items to drop if needed\n    if drop_last:\n        items_to_drop = len(indexes) % distributed_env.world_size\n    else:\n        items_to_drop = 0\n\n    # determine the number of items per rank taking into account the items to drop\n    items_per_rank_per_node = items_per_rank - items_to_drop // distributed_env.num_nodes\n\n    # determine the number of items to drop if needed\n    if drop_last:\n        items_to_drop_per_node = items_to_drop // distributed_env.num_nodes\n    else:\n        items_to_drop_per_node = 0\n\n    # determine the number of items per rank taking into account the items to drop\n    items_per_rank_per_node_per_rank = items_per_rank_per_node - items_to_drop_per_node\n\n    # determine the number of items per rank taking into account the items to drop\n    items_per_rank_per_node_per_rank = items_per_rank_per_node - items_to_drop_per_node\n\n    # determine the number of items per rank taking into account the items to drop\n    items_per_rank_per_node_per_rank = items_per_rank_per_node - items_to_drop_per_node\n\n    # determine the number of items per rank taking into account the items to drop\n    items_per_rank_per_node_per_rank = items_per_rank_per_node - items_to_drop_per_node\n\n    # determine the number of items per rank taking into account the items to drop\n    items_per_rank_per_node_per_rank = items_per_rank_per_node - items_to_drop_per_node\n\n    # determine the number of items per rank taking into account the items to drop\n    items_per_rank_per_node_per_rank = items_per_"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            kwargs = {\"device\": self._device}\n        else:\n            kwargs = {}\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs: Dict[str, Any] = {}\n\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            kwargs = {\"is_last\": is_last}\n        else:\n            kwargs = {}\n\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = self._device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            item_metadata = item_metadata.to(device)\n\n        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(output_dir, item_metadata, **kwargs)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            kwargs = {\"device\": self._device}\n        else:\n            kwargs = {}\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = self._device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            item_metadata = item_metadata.to(device)\n\n        if self._contains_is_last:\n            kwargs = {\"is_last\": is_last}\n        else:\n            kwargs = {}\n\n        self._fn(item_metadata, output_dir, **kwargs)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = self._device or \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            item_metadata = item_metadata.to(device)\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = self._device\n            if device is None:\n                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            self._device = device\n            item_metadata = (item_metadata, device)\n\n        if self._contains_is_last:\n            item_metadata = (item_metadata, is_last)\n\n        self._fn(*item_metadata, output_dir=output_dir)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = item_metadata.device\n        else:\n            device = self._device\n\n        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = self._device or torch.device(\"cpu\")\n            item_metadata = item_metadata.to(device)\n\n        if self._contains_is_last:\n            kwargs = {\"is_last\": is_last}\n        else:\n            kwargs = {}\n\n        self._fn(item_metadata, output_dir, **kwargs)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = self._device or \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            item_metadata = item_metadata.to(device)\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        # We need to pass the device to the function\n        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        # TODO: add support for passing a device to the transformation function\n        # if self._contains_device:\n        #     device = self._device if self._device is not None else \"cpu\"\n        #     kwargs[\"device\"] = device\n\n        kwargs = {}\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = self._device or torch.device(\"cpu\")\n            item_metadata = item_metadata.to(device)\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        # If the function contains the 'device' argument, we need to pass it as a keyword argument.\n        if self._contains_device:\n            kwargs = {\"device\": self._device}\n        else:\n            kwargs = {}\n\n        # If the function contains the 'is_last' argument, we need to pass it as a keyword argument.\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        # Call the transformation function with the provided arguments and keyword arguments.\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        # TODO: add support for multiple inputs\n        if self._contains_device:\n            device = item_metadata.get(\"device\")\n            if device is not None:\n                self._device = device\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            kwargs = {\"device\": device}\n        else:\n            kwargs = {}\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        if self._device is None:\n            self._device = kwargs.get(\"device\", \"cpu\")\n\n        self._fn(item_metadata, output_dir, **kwargs)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File {obj.key} not found. Waiting for {sleep_time} seconds.\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(obj.bucket_name, obj.key)\n            return response\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File {obj.key} not found. Waiting for {sleep_time} seconds...\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File {obj.key} not found. Waiting for {sleep_time} seconds.\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return response\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File {obj.key} not found, waiting {sleep_time} seconds\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.head_object(obj.bucket_name, obj.key)\n            return True\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.object_name)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File not found, waiting for {sleep_time} seconds...\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File {obj.key} not found, waiting for {sleep_time} seconds...\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.warning(f\"File {obj.key} not found. Waiting {sleep_time} seconds before retrying.\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.head_object(obj.bucket_name, obj.key)\n            return True\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(obj.bucket_name, obj.key)\n            return response\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File {obj.key} not found. Waiting for {sleep_time} seconds...\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"Waiting for {obj.geturl()} to exist...\")\n                sleep(sleep_time)\n                continue\n            raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.netloc, Key=obj.path)\n            return response\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File {obj} does not exist yet. Sleeping for {sleep_time} seconds.\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if isinstance(inputs, StreamingDataLoader):\n            input_dir = Dir()\n        else:\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n       "}
{"namespace": "litdata.processing.functions.map", "completion": "    if _IS_IN_STUDIO:\n        from litdata.streaming.studio_data_processor import StudioDataProcessor\n\n        data_processor = StudioDataProcessor(\n            fn=fn,\n            inputs=inputs,\n            output_dir=output_dir,\n            weights=weights,\n            num_workers=num_workers,\n            fast_dev_run=fast_dev_run,\n            num_nodes=num_nodes,\n            machine=machine,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            error_when_not_empty=error_when_not_empty,\n            reader=reader,\n            batch_size=batch_size,\n        )\n    else:\n        data_processor = DataProcessor(\n            fn=fn,\n            inputs=inputs,\n            output_dir=output_dir,\n            weights=weights,\n            num_workers=num_workers,\n            fast_dev_run=fast_dev_run,\n            num_nodes=num_nodes,\n            machine=machine,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            error_when_not_empty=error_when_not_empty,\n            reader=reader,\n            batch_size=batch_size,\n        )\n\n    data_processor.process()\n\n"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not _IS_IN_STUDIO and num_nodes is not None:\n        raise ValueError(\"num_nodes is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and machine is not None:\n        raise ValueError(\"machine is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_downloaders is not None:\n        raise ValueError(\"num_downloaders is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_uploaders is not None:\n        raise ValueError(\"num_uploaders is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_nodes is not None and num_nodes > 1:\n        raise ValueError(\"num_nodes is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and machine is not None and machine != \"cpu\":\n        raise ValueError(\"machine is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_downloaders is not None and num_downloaders > 1:\n        raise ValueError(\"num_downloaders is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_uploaders is not None and num_uploaders > 1:\n        raise ValueError(\"num_uploaders is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_workers is not None and num_workers > 1:\n        raise ValueError(\"num_workers is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_workers is not None and num_nodes is not None:\n        raise ValueError(\"num_workers is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_workers is not None and machine is not None:\n        raise ValueError(\"num_work"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, Dir):\n        output_dir = output_dir.path\n\n    if not isinstance(output_dir, str):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't supported.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_nodes is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"num_nodes is only supported on https://lightning.ai/\")\n\n    if machine is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"machine is only supported on https://lightning.ai/\")\n\n    if num_downloaders is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"num_downloaders is only supported on https://lightning.ai/\")\n\n    if num_uploaders is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"num_uploaders is only supported on https://lightning.ai/\")\n\n    if num_workers < 1:\n        raise ValueError(f\"The provided num_workers {num_workers} isn't supported.\")\n\n    if num_nodes is not None and num_nodes < 1:\n        raise ValueError(f\"The provided num_nodes {num_nodes} isn't supported.\")\n\n    if machine is not None and machine not in [\"cpu\", \"gpu\"]:\n        raise ValueError(f\"The provided machine {machine} isn't supported.\")\n\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(f\"The provided num_downloaders {num_downloaders} isn't supported.\")\n\n    if num_uploaders is not None and num_uploaders < 1:\n        raise ValueError(f\"The provided num_uploaders {num_uploaders} isn't supported.\")\n\n    if fast_dev_run:\n        if isinstance(fast_dev_run, bool):\n            fast_dev_run = 1\n        if fast_dev_run"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not _IS_IN_STUDIO and num_nodes is not None:\n        raise ValueError(\"The `num_nodes` argument is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and machine is not None:\n        raise ValueError(\"The `machine` argument is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_downloaders is not None:\n        raise ValueError(\"The `num_downloaders` argument is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_uploaders is not None:\n        raise ValueError(\"The `num_uploaders` argument is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and reorder_files:\n        raise ValueError(\"The `reorder_files` argument is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and error_when_not_empty:\n        raise ValueError(\"The `error_when_not_empty` argument is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and reader is not None:\n        raise ValueError(\"The `reader` argument is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and batch_size is not None:\n        raise ValueError(\"The `batch_size` argument is only supported on https://lightning.ai/\")\n\n    if not _IS_IN_STUDIO and num_workers is not None and num_workers < 1:\n        raise ValueError(\"The `num_workers` argument must be greater than 0.\")\n\n    if not _IS_IN_STUDIO and fast_dev_run and fast_dev_run < 1:\n        raise ValueError(\"The `fast_dev_run` argument must be greater than 0.\")\n\n    if not _IS_IN_STUDIO and fast_dev_run and isinstance(fast_dev_run, int):\n       "}
{"namespace": "litdata.processing.functions.map", "completion": "    if not isinstance(output_dir, str):\n        output_dir = str(output_dir)\n\n    if not isinstance(output_dir, str):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't supported.\")\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    if error_when_not_empty and os.listdir(output_dir):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't empty.\")\n\n    if weights is not None and len(weights) != len(inputs):\n        raise ValueError(f\"The provided weights {weights} aren't the same length as the inputs {inputs}.\")\n\n    if weights is not None and not all(isinstance(weight, int) for weight in weights):\n        raise ValueError(f\"The provided weights {weights} aren't all integers.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_workers < 1:\n        raise ValueError(f\"The provided num_workers {num_workers} isn't supported.\")\n\n    if fast_dev_run is True:\n        fast_dev_run = 1\n\n    if fast_dev_run and fast_dev_run < 1:\n        raise ValueError(f\"The provided fast_dev_run {fast_dev_run} isn't supported.\")\n\n    if num_nodes is not None and num_nodes < 1:\n        raise ValueError(f\"The provided num_nodes {num_nodes} isn't supported.\")\n\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(f\"The provided num_downloaders {num_downloaders} isn't supported.\")\n\n    if num_uploaders is not None and num_uploaders < 1:\n        raise ValueError(f\"The provided num_uploaders {num_uploaders} isn't supported.\")\n\n    if batch_size is not None and batch_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if isinstance(output_dir, str):\n        output_dir = _resolve_dir(output_dir)\n\n    if num_nodes is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"num_nodes is only supported on https://lightning.ai/\")\n\n    if machine is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"machine is only supported on https://lightning.ai/\")\n\n    if num_downloaders is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"num_downloaders is only supported on https://lightning.ai/\")\n\n    if num_uploaders is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"num_uploaders is only supported on https://lightning.ai/\")\n\n    if error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    input_dir = _get_input_dir(inputs)\n\n    if input_dir is None:\n        raise ValueError(\"The provided item doesn't contain any filepaths.\")\n\n    if isinstance(reader, BaseReader):\n        reader = reader\n    else:\n        reader = BaseReader()\n\n    data_processor = DataProcessor(\n        data_transform_recipe=LambdaDataTransformRecipe(fn=fn, inputs=inputs),\n        data_chunk_recipe=LambdaDataChunkRecipe(\n            fn=fn,\n            inputs=inputs,\n            chunk_size=batch_size,\n            chunk_bytes=batch_size,\n            compression=None,\n        ),\n        input_dir=input_dir,\n        output_dir=output_dir,\n        weights=weights,\n        num_workers=num_workers,\n        fast_dev_run=fast_dev_run,\n        num_nodes=num_nodes,\n        machine=machine,\n        num_downloaders=num_downloaders,\n        num"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't supported.\")\n\n    if not _IS_IN_STUDIO:\n        num_workers = num_workers or _get_default_num_workers()\n\n    if num_workers <= 0:\n        raise ValueError(f\"The provided num_workers {num_workers} isn't supported.\")\n\n    if isinstance(fast_dev_run, bool):\n        fast_dev_run = 1 if fast_dev_run else None\n\n    if fast_dev_run is not None and fast_dev_run <= 0:\n        raise ValueError(f\"The provided fast_dev_run {fast_dev_run} isn't supported.\")\n\n    if num_nodes is not None and num_nodes <= 0:\n        raise ValueError(f\"The provided num_nodes {num_nodes} isn't supported.\")\n\n    if num_downloaders is not None and num_downloaders <= 0:\n        raise ValueError(f\"The provided num_downloaders {num_downloaders} isn't supported.\")\n\n    if num_uploaders is not None and num_uploaders <= 0:\n        raise ValueError(f\"The provided num_uploaders {num_uploaders} isn't supported.\")\n\n    if batch_size is not None and batch_size <= 0:\n        raise ValueError(f\"The provided batch_size {batch_size} isn't supported.\")\n\n    if error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    if isinstance(output_dir, Dir):\n        _assert_dir_has_index_file(output_dir)\n\n    if isinstance(weights, list) and len(weights) > 0 and len(weights) != len(inputs):\n        raise ValueError(f\"The provided weights {weights} and inputs {inputs} aren't supported.\")"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not isinstance(output_dir, Dir):\n        output_dir = _resolve_dir(output_dir)\n\n    if _IS_IN_STUDIO:\n        if num_workers is None:\n            num_workers = _get_default_num_workers()\n        if num_nodes is None:\n            num_nodes = int(os.environ.get(\"DATA_OPTIMIZER_NUM_NODES\", 1))\n        if machine is None:\n            machine = os.environ.get(\"DATA_OPTIMIZER_MACHINE\", \"cpu\")\n        if num_downloaders is None:\n            num_downloaders = int(os.environ.get(\"DATA_OPTIMIZER_NUM_DOWNLOADERS\", 1))\n        if num_uploaders is None:\n            num_uploaders = int(os.environ.get(\"DATA_OPTIMIZER_NUM_UPLOADERS\", 1))\n    else:\n        if num_workers is None:\n            num_workers = _get_default_num_workers()\n        if num_downloaders is None:\n            num_downloaders = 1\n        if num_uploaders is None:\n            num_uploaders = 1\n\n    if not isinstance(output_dir, Dir):\n        output_dir = _resolve_dir(output_dir)\n\n    if error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    if isinstance(fast_dev_run, bool) and fast_dev_run:\n        fast_dev_run = 1\n\n    if isinstance(fast_dev_run, int):\n        fast_dev_run = min(fast_dev_run, len(inputs))\n\n    if batch_size is not None:\n        inputs = [inputs[i : i + batch_size] for i in range(0, len(inputs), batch_size)]\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(\n                f\"The provided weights {weights} must"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(f\"The provided weights {weights} don't match the number of inputs {len(inputs)}.\")\n\n    if num_nodes is not None and machine is None:\n        raise ValueError(\"The machine argument is required when num_nodes is specified.\")\n\n    if num_nodes is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"num_nodes is only supported on https://lightning.ai/.\")\n\n    if num_downloaders is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"num_downloaders is only supported on https://lightning.ai/.\")\n\n    if num_uploaders is not None and not _IS_IN_STUDIO:\n        raise ValueError(\"num_uploaders is only supported on https://lightning.ai/.\")\n\n    if fast_dev_run is True:\n        fast_dev_run = 1\n\n    if fast_dev_run and not isinstance(fast_dev_run, int):\n        raise ValueError(\"fast_dev_run should be a positive integer.\")\n\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(\"num_downloaders should be a positive integer.\")\n\n    if num_uploaders is not None and num_uploaders < 1:\n        raise ValueError(\"num_uploaders should be a positive integer.\")\n\n    if batch_size is not None and batch_size < 1:\n        raise ValueError(\"batch_size should be a positive integer.\")\n\n    if reorder_files and not _IS_IN_STUDIO:\n        raise ValueError(\"reorder_files is only supported on https://lightning.ai/.\")\n\n    if error_when_not_empty and not _IS_IN_STUDIO:\n        raise ValueError"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't supported.\")\n\n    if error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(f\"The provided weights {weights} doesn't match the number of inputs {len(inputs)}.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_nodes is not None and machine is None:\n        machine = \"cpu\"\n\n    if num_nodes is not None and machine is not None:\n        if machine == \"cpu\":\n            raise ValueError(\"CPU is not supported for remote execution.\")\n\n    if num_downloaders is not None and num_uploaders is not None:\n        if num_downloaders + num_uploaders > num_workers:\n            raise ValueError(\n                f\"The provided num_downloaders {num_downloaders} and num_uploaders {num_uploaders} exceed the num_workers {num_workers}.\"\n            )\n\n    if num_downloaders is None and num_uploaders is None:\n        num_downloaders = num_workers\n        num_uploaders = 0\n\n    if num_downloaders is not None and num_uploaders is None:\n        num_uploaders = num_workers - num_downloaders\n\n    if num_downloaders is None and num_uploaders is not None:\n        num_downloaders = num_workers - num_uploaders\n\n    if num_downloaders < 0 or num_uploaders < 0:\n        raise ValueError(\n            f\"The provided num_downloaders {num_downloaders} and num_uploaders {num_uploaders"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not _IS_IN_STUDIO:\n        raise ValueError(\n            \"`map` is only supported in https://lightning.ai/. \"\n            \"Please install `litdata` with `pip install litdata[studio]`.\"\n        )\n\n    if not isinstance(output_dir, Dir):\n        output_dir = Dir(output_dir)\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(f\"The length of weights {len(weights)} doesn't match the length of inputs {len(inputs)}.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_workers == 0:\n        raise ValueError(\"The number of workers must be greater than 0.\")\n\n    if fast_dev_run:\n        if isinstance(fast_dev_run, bool):\n            fast_dev_run = 1\n\n        if fast_dev_run < 1:\n            raise ValueError(\"The fast_dev_run parameter must be a positive integer or True.\")\n\n        if len(inputs) < fast_dev_run:\n            raise ValueError(f\"The number of inputs {len(inputs)} is less than the fast_dev_run value {fast_dev_run}.\")\n\n        inputs = inputs[:fast_dev_run]\n\n    if num_nodes is not None and num_nodes < 1:\n        raise ValueError(\"The number of nodes must be greater than 0.\")\n\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(\"The number of downloaders per worker must be greater than 0.\")\n\n    if num_uploaders is not None and num_uploaders < 1:\n        raise ValueError(\"The number of uploaders per worker must be greater than 0.\")\n\n    if batch_size is not None and batch_size < 1:\n        raise ValueError(\"The batch size must be greater than 0.\")\n\n    if isinstance(fn, partial):\n        fn = fn.func  # type: ignore"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, Dir):\n        output_dir = output_dir.path\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    if error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    if not _IS_IN_STUDIO:\n        if num_nodes is not None or machine is not None:\n            raise ValueError(\n                \"Only supported on https://lightning.ai/. Please use `map_studio` instead.\"\n            )\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(f\"The provided weights {weights} don't match the inputs {inputs}.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_workers <= 0:\n        raise ValueError(f\"The provided num_workers {num_workers} is invalid.\")\n\n    if fast_dev_run:\n        if isinstance(fast_dev_run, bool):\n            fast_dev_run = 1\n\n        if fast_dev_run <= 0:\n            raise ValueError(f\"The provided fast_dev_run {fast_dev_run} is invalid.\")\n\n        inputs = inputs[:fast_dev_run]\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if batch_size is not None and batch_size <= 0:\n        raise ValueError(f\"The provided batch_size {batch_size} is invalid.\")\n\n    if reader is not None and not isinstance(reader, BaseReader):\n        raise ValueError(f\"The provided reader {reader} isn't supported.\")\n\n    _assert_dir_is_empty(output_dir)\n\n    data_processor = DataProcessor(\n        recipe=LambdaDataTransformRecipe(fn=fn, inputs=inputs),\n       "}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The provided output_dir {output_dir} is not supported.\")\n\n    if _IS_IN_STUDIO:\n        if num_nodes is not None:\n            raise ValueError(\"num_nodes is not supported in https://lightning.ai/.\")\n\n        if machine is not None:\n            raise ValueError(\"machine is not supported in https://lightning.ai/.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_downloaders is None:\n        num_downloaders = 1\n\n    if num_uploaders is None:\n        num_uploaders = 1\n\n    if error_when_not_empty and output_dir.is_not_empty():\n        raise ValueError(f\"The output_dir {output_dir} is not empty.\")\n\n    if fast_dev_run:\n        if isinstance(fast_dev_run, int):\n            inputs = inputs[:fast_dev_run]\n        else:\n            inputs = inputs[:1]\n\n    if num_workers < 1:\n        raise ValueError(\"num_workers must be at least 1.\")\n\n    if num_downloaders < 1:\n        raise ValueError(\"num_downloaders must be at least 1.\")\n\n    if num_uploaders < 1:\n        raise ValueError(\"num_uploaders must be at least 1.\")\n\n    if batch_size is not None and batch_size < 1:\n        raise ValueError(\"batch_size must be at least 1.\")\n\n    if reader is not None and not isinstance(reader, BaseReader):\n        raise ValueError(f\"The provided reader {reader} is not supported.\")\n\n    if not isinstance(inputs, Sequence):\n        raise ValueError(f\"The provided inputs {inputs} is not supported.\")\n\n    if not isinstance(fn, (FunctionType, partial)):\n        raise ValueError(f"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not _IS_IN_STUDIO:\n        raise RuntimeError(\"This function is only supported on https://lightning.ai/\")\n\n    if not isinstance(output_dir, Dir):\n        output_dir = Dir(output_dir)\n\n    if not output_dir.is_empty and error_when_not_empty:\n        raise ValueError(f\"The output directory {output_dir.path} is not empty.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_nodes is not None:\n        if num_nodes <= 0:\n            raise ValueError(\"num_nodes should be greater than 0.\")\n\n    if machine is not None:\n        if machine not in [\"cpu\", \"gpu\"]:\n            raise ValueError(\"machine should be 'cpu' or 'gpu'.\")\n\n    if num_downloaders is not None:\n        if num_downloaders <= 0:\n            raise ValueError(\"num_downloaders should be greater than 0.\")\n\n    if num_uploaders is not None:\n        if num_uploaders <= 0:\n            raise ValueError(\"num_uploaders should be greater than 0.\")\n\n    if batch_size is not None:\n        if batch_size <= 0:\n            raise ValueError(\"batch_size should be greater than 0.\")\n\n    if fast_dev_run:\n        if isinstance(fast_dev_run, bool):\n            fast_dev_run = 1\n\n        if fast_dev_run <= 0:\n            raise ValueError(\"fast_dev_run should be greater than 0.\")\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(\n                f\"The provided weights {weights} should have the same length as the inputs {inputs}\"\n            )\n\n    if reorder_files:\n        inputs = sorted(inputs, key=lambda x: os.path.getsize(x))\n\n    if num_nodes is not None:\n        if machine == \"gpu\":\n            if num_nodes > num_work"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The provided {output_dir} isn't supported.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if num_nodes is not None and machine is None:\n        machine = \"cpu\"\n\n    if num_nodes is not None and machine is None:\n        machine = \"cpu\"\n\n    if isinstance(fast_dev_run, int):\n        fast_dev_run = True\n\n    if isinstance(fast_dev_run, bool) and fast_dev_run:\n        if len(inputs) == 0:\n            return\n        inputs = inputs[:1]\n\n    if _IS_IN_STUDIO:\n        if num_nodes is not None and machine is not None:\n            raise ValueError(\n                \"The num_nodes and machine arguments are not supported in https://lightning.ai/. Please remove them.\"\n            )\n\n    if num_workers > len(inputs):\n        num_workers = len(inputs)\n\n    if num_workers > num_downloaders:\n        raise ValueError(\n            f\"The provided num_workers {num_workers} is greater than num_downloaders {num_downloaders}.\"\n        )\n\n    if num_workers > num_uploaders:\n        raise ValueError(\n            f\"The provided num_workers {num_workers} is greater than num_uploaders {num_uploaders}.\"\n        )\n\n    if weights is not None and len(weights) != len(inputs):\n        raise ValueError(\n            f\"The provided weights {weights} doesn't match the number of inputs {len(inputs)}.\"\n        )\n\n    if weights is not"}
{"namespace": "litdata.processing.functions.map", "completion": "    if _IS_IN_STUDIO:\n        if num_nodes is not None or machine is not None:\n            raise ValueError(\"num_nodes and machine are not supported in https://lightning.ai/.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_downloaders is None:\n        num_downloaders = 1\n\n    if num_uploaders is None:\n        num_uploaders = 1\n\n    if fast_dev_run:\n        if isinstance(fast_dev_run, int):\n            fast_dev_run = min(fast_dev_run, len(inputs))\n        else:\n            fast_dev_run = len(inputs)\n\n    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't supported.\")\n\n    if not os.path.exists(output_dir.path):\n        os.makedirs(output_dir.path, exist_ok=True)\n\n    if reorder_files:\n        inputs = sorted(inputs, key=lambda x: os.path.getsize(x))\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(\"The length of weights doesn't match the length of inputs.\")\n\n    if error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    if fast_dev_run:\n        inputs = inputs[:fast_dev_run]\n\n    if reader is None:\n        reader = BaseReader()\n\n    if batch_size is None:\n        batch_size = len(inputs)\n\n    recipe = LambdaDataTransformRecipe(fn, inputs)\n    processor = DataProcessor(\n        recipe=recipe,\n        reader=reader,\n        num_workers=num_workers,\n        num_downloaders=num_downloaders,\n       "}
{"namespace": "litdata.processing.functions.map", "completion": "    if _IS_IN_STUDIO:\n        from lightning_studio.utils.studio_globals import StudioGlobals\n\n        if num_nodes is None:\n            num_nodes = StudioGlobals.get_current_run().num_nodes\n\n        if machine is None:\n            machine = StudioGlobals.get_current_run().machine\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if not isinstance(output_dir, Dir):\n        output_dir = Dir(output_dir)\n\n    if error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    if isinstance(fast_dev_run, bool):\n        if fast_dev_run:\n            fast_dev_run = 1\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if isinstance(reader, BaseReader):\n        reader = reader.to_recipe()\n\n    if weights is None:\n        weights = [1] * len(inputs)\n\n    if len(inputs) != len(weights):\n        raise ValueError(\n            f\"The number of inputs {len(inputs)} doesn't match the number of weights {len(weights)}.\"\n        )\n\n    if len(inputs) == 0:\n        raise ValueError(\"The number of inputs is 0.\")\n\n    if batch_size is not None and batch_size > len(inputs):\n        raise ValueError(f\"The batch size {batch_size} is larger than the number of inputs {len(inputs)}.\")\n\n    if batch_size is None:\n        batch_size = len(inputs)\n\n    if batch_size == 0:\n        raise ValueError(\"The batch size is 0.\")\n\n    if fast_dev_run is not None:\n        inputs = inputs[:fast_dev_run]\n        weights = weights[:fast_dev_run]"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not isinstance(output_dir, Dir):\n        output_dir = Dir(output_dir)\n\n    if not _IS_IN_STUDIO:\n        if num_nodes is not None or machine is not None:\n            raise ValueError(\"Remote execution is not supported outside of https://lightning.ai/.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_nodes is not None and num_nodes > 1:\n        num_workers = num_workers * num_nodes\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if fast_dev_run:\n        if isinstance(fast_dev_run, bool):\n            fast_dev_run = 1\n        inputs = inputs[:fast_dev_run]\n\n    if weights is None:\n        weights = [1 for _ in inputs]\n\n    if len(inputs) != len(weights):\n        raise ValueError(\"The provided inputs and weights have different lengths.\")\n\n    if num_workers > len(inputs):\n        num_workers = len(inputs)\n\n    if num_workers == 0:\n        raise ValueError(\"The provided num_workers is invalid.\")\n\n    if batch_size is not None and batch_size > 1:\n        if batch_size > len(inputs):\n            raise ValueError(\"The provided batch_size is invalid.\")\n\n        inputs = [inputs[i : i + batch_size] for i in range(0, len(inputs), batch_size)]\n\n    if len(inputs) == 0:\n        raise ValueError(\"The provided inputs are empty.\")\n\n    if error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    if reorder_files:\n        inputs = sorted(inputs, key=lambda x: os.path.getsize(x))\n\n    input_dir = _get_input_dir("}
{"namespace": "litdata.processing.functions.map", "completion": "    if not isinstance(output_dir, Dir):\n        output_dir = _resolve_dir(output_dir)\n\n    if not _IS_IN_STUDIO and not isinstance(output_dir, Dir):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't supported.\")\n\n    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't supported.\")\n\n    if _IS_IN_STUDIO and error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    if _IS_IN_STUDIO:\n        _assert_dir_has_index_file(output_dir)\n\n    if isinstance(inputs, Sequence):\n        inputs = tuple(inputs)\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(f\"The provided weights {weights} must have the same length as the inputs {inputs}.\")\n\n    if batch_size is not None and batch_size > 1:\n        inputs = _batch(inputs, batch_size)\n\n    if fast_dev_run:\n        if isinstance(fast_dev_run, bool):\n            inputs = inputs[:1]\n        else:\n            inputs = inputs[:fast_dev_run]\n\n    if reorder_files:\n        inputs = _reorder_files(inputs)\n\n    recipe = LambdaDataTransformRecipe(fn, inputs)\n    data_processor = DataProcessor(\n        recipe,\n        output_dir=output_dir,\n        num_workers=num_workers,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders"}
{"namespace": "litdata.processing.functions.map", "completion": "    if _IS_IN_STUDIO:\n        from lightning.app.utilities.state import AppState\n\n        if AppState.current_app() is None:\n            raise RuntimeError(\"You are not running in an app.\")\n\n    if isinstance(output_dir, str):\n        output_dir = _resolve_dir(output_dir)\n        _assert_dir_is_empty(output_dir)\n        if error_when_not_empty:\n            _assert_dir_has_index_file(output_dir)\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_downloaders is None:\n        num_downloaders = num_workers\n\n    if num_uploaders is None:\n        num_uploaders = num_workers\n\n    if reader is not None and batch_size is not None:\n        raise ValueError(\"You can't use a reader and a batch_size at the same time.\")\n\n    if batch_size is not None:\n        if batch_size < 1:\n            raise ValueError(f\"The provided batch_size {batch_size} is invalid.\")\n\n        if reader is None:\n            reader = BaseReader()\n\n        if isinstance(reader, BaseReader):\n            recipe = LambdaDataChunkRecipe(fn, inputs, None, None, None)\n            processor = DataProcessor(\n                recipe,\n                output_dir,\n                weights,\n                num_workers,\n                fast_dev_run,\n                num_nodes,\n                machine,\n                num_downloaders,\n                num_uploaders,\n                reorder_files,\n                reader,\n                batch_size,\n            )\n            processor.process()\n        else:\n            raise ValueError(f\"The provided reader {reader} isn't supported.\")\n\n    else:\n        recipe = LambdaDataTransformRecipe(fn, inputs)\n        processor = DataProcessor(\n            recipe,\n            output_dir,\n            weights,\n            num_workers,\n            fast_dev_run,\n            num_nodes,\n            machine"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n    while True:\n        try:\n            task_index, file_paths = queue_in.get(timeout=1)\n        except Empty:\n            break\n        for file_path in file_paths:\n            obj = input_dir.get_object(file_path)\n            local_path = os.path.join(cache_dir, file_path)\n            if not os.path.exists(local_path):\n                _wait_for_file_to_exist(s3, obj)\n                s3.download_file(obj, local_path)\n        queue_out.put(task_index)\n    return\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir.s3_endpoint_url, input_dir.s3_access_key_id, input_dir.s3_secret_access_key)\n\n    while True:\n        try:\n            task = queue_in.get(timeout=1)\n            task_index = task[\"index\"]\n            files = task[\"files\"]\n\n            for file in files:\n                obj = parse.urlparse(file)\n                local_path = os.path.join(cache_dir, obj.path.lstrip(\"/\"))\n                if not os.path.exists(local_path):\n                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n\n                    _wait_for_file_to_exist(s3, obj)\n\n                    with open(local_path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n            queue_out.put(task_index)\n        except Empty:\n            pass\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir.url)\n\n    while True:\n        task_index, file_paths = queue_in.get()\n\n        if file_paths is None:\n            break\n\n        for file_path in file_paths:\n            obj = parse.urlparse(file_path)\n\n            if obj.scheme == \"s3\":\n                # check if the file exists in the cache\n                cache_file_path = os.path.join(cache_dir, obj.path.lstrip(\"/\"))\n                if os.path.exists(cache_file_path):\n                    continue\n\n                # check if the file exists in the remote directory\n                _wait_for_file_to_exist(s3, obj)\n\n                # download the file\n                s3.download_file(obj, cache_file_path)\n\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        try:\n            task_index, files = queue_in.get(block=True, timeout=10)\n            for file in files:\n                if not os.path.exists(os.path.join(cache_dir, file)):\n                    if input_dir.is_s3:\n                        s3.download_file(input_dir.path, file, os.path.join(cache_dir, file))\n                    else:\n                        shutil.copyfile(os.path.join(input_dir.path, file), os.path.join(cache_dir, file))\n\n            queue_out.put(task_index)\n        except Empty:\n            continue\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir)\n\n    while True:\n        try:\n            task_index, file_paths = queue_in.get(timeout=1)\n        except Empty:\n            continue\n\n        for file_path in file_paths:\n            obj = parse.urlparse(file_path)\n\n            if obj.scheme == \"s3\":\n                # Download the file from S3 to the cache directory\n                s3.download(obj, os.path.join(cache_dir, obj.path.lstrip(\"/\")))\n            else:\n                # Download the file from the local file system to the cache directory\n                shutil.copy(file_path, os.path.join(cache_dir, os.path.basename(file_path)))\n\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        try:\n            task = queue_in.get(block=True, timeout=1)\n            index, file_paths = task\n\n            for file_path in file_paths:\n                # Check if the file is already downloaded\n                if not os.path.exists(os.path.join(cache_dir, file_path)):\n                    # Download the file\n                    obj = parse.urlparse(os.path.join(input_dir.path, file_path))\n                    if input_dir.type == \"s3\":\n                        s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), os.path.join(cache_dir, file_path))\n                    elif input_dir.type == \"local\":\n                        shutil.copyfile(os.path.join(input_dir.path, file_path), os.path.join(cache_dir, file_path))\n                    else:\n                        raise ValueError(f\"Unsupported input_dir.type: {input_dir.type}\")\n\n            # Signal completion\n            queue_out.put(index)\n        except Empty:\n            pass\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        try:\n            task = queue_in.get(block=True, timeout=None)\n            index, files = task\n            for file in files:\n                obj = parse.urlparse(file)\n                if obj.scheme == \"s3\":\n                    local_path = s3.get_file(obj, cache_dir)\n                else:\n                    local_path = file\n\n                if not os.path.exists(local_path):\n                    if obj.scheme == \"s3\":\n                        _wait_for_file_to_exist(s3, obj)\n                    sleep(1)\n\n            queue_out.put(index)\n        except Empty:\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir.path, input_dir.endpoint, input_dir.access_key, input_dir.secret_key)\n\n    while True:\n        task = queue_in.get()\n        index = task[\"index\"]\n        paths = task[\"paths\"]\n\n        for path in paths:\n            obj = parse.urlparse(path)\n            local_path = os.path.join(cache_dir, path.lstrip(\"/\"))\n\n            if os.path.exists(local_path):\n                continue\n\n            if input_dir.path.startswith(\"s3://\"):\n                _wait_for_file_to_exist(s3, obj)\n                s3.client.download_file(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"), Filename=local_path)\n            else:\n                shutil.copy(path, local_path)\n\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        task = queue_in.get()\n        if task is None:\n            break\n        index, file_paths = task\n        for file_path in file_paths:\n            if not s3.exists(input_dir.url(file_path)):\n                raise ValueError(f\"File {file_path} does not exist in {input_dir.url(file_path)}\")\n\n            # check if file exists in cache\n            if not s3.exists(cache_dir, file_path):\n                # download file\n                s3.download(input_dir.url(file_path), cache_dir, file_path)\n\n        # signal completion\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Create an S3 client to interact with the remote directory\n    s3 = S3Client(input_dir)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        try:\n            # Get the next download task from the input queue\n            task_index, file_paths = queue_in.get()\n\n            # Iterate over the file paths for the current task\n            for file_path in file_paths:\n                # Construct the local file path for the downloaded file\n                local_file_path = os.path.join(cache_dir, file_path)\n\n                # Check if the file is already downloaded\n                if os.path.exists(local_file_path):\n                    # If the file is already downloaded, skip it\n                    continue\n\n                # Construct the remote file path for the file to download\n                remote_file_path = os.path.join(input_dir.path, file_path)\n\n                # Download the file to the local cache directory\n                s3.download_file(remote_file_path, local_file_path)\n\n            # Signal completion by putting the task index into the output queue\n            queue_out.put(task_index)\n        except Exception as e:\n            # Log any exceptions that occur during the download process\n            logger.error(f\"Error downloading file: {e}\")\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        task_index, file_paths = queue_in.get()\n\n        for file_path in file_paths:\n            obj = parse.urlparse(f\"{input_dir.path}/{file_path}\")\n            local_path = os.path.join(cache_dir, file_path)\n            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n\n            if not os.path.exists(local_path):\n                _wait_for_file_to_exist(s3, obj)\n                s3.download_file(obj, local_path)\n\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir.scheme)\n\n    while True:\n        try:\n            task = queue_in.get(timeout=1)\n            if task is None:\n                break\n            index, file_paths = task\n            for file_path in file_paths:\n                obj = parse.urlparse(file_path)\n                if obj.scheme == \"file\":\n                    local_path = os.path.join(cache_dir, obj.path.lstrip(\"/\"))\n                    if not os.path.exists(local_path):\n                        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                        shutil.copy(obj.path, local_path)\n                else:\n                    local_path = os.path.join(cache_dir, obj.path.lstrip(\"/\"))\n                    if not os.path.exists(local_path):\n                        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.download(obj, local_path)\n            queue_out.put(index)\n        except Empty:\n            pass\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Initialize the S3 client\n    s3 = S3Client(input_dir)\n\n    # Initialize the cache\n    cache = Cache(cache_dir)\n\n    # Initialize the download progress bar\n    pbar = _tqdm(total=100, desc=\"Downloading data\", unit=\"%\", leave=False)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        # Fetch a download task from the input queue\n        task_index, file_paths = queue_in.get()\n\n        # Check if the files are already downloaded\n        downloaded_file_paths = []\n        for file_path in file_paths:\n            if cache.exists(file_path):\n                downloaded_file_paths.append(file_path)\n\n        # If all files are already downloaded, skip downloading\n        if len(downloaded_file_paths) == len(file_paths):\n            pbar.update(100 / len(file_paths))\n            pbar.refresh()\n            continue\n\n        # Download the missing files\n        for file_path in file_paths:\n            if file_path not in downloaded_file_paths:\n                cache.download(file_path)\n                pbar.update(100 / len(file_paths))\n                pbar.refresh()\n\n        # Signal completion by putting the task index into the output queue\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n    s3.set_base_path(input_dir.path)\n\n    while True:\n        try:\n            task = queue_in.get(timeout=30)\n            index = task[\"index\"]\n            files = task[\"files\"]\n            for file in files:\n                obj = parse.urlparse(file)\n                if not s3.exists(obj):\n                    raise FileNotFoundError(f\"File {file} does not exist\")\n                if not os.path.exists(os.path.join(cache_dir, obj.path.lstrip(\"/\"))):\n                    logger.info(f\"Downloading {file} to {cache_dir}\")\n                    s3.download(obj, cache_dir)\n                else:\n                    logger.info(f\"File {file} already exists in cache\")\n            queue_out.put(index)\n        except Empty:\n            logger.info(\"No more tasks in queue\")\n            break\n        except Exception as e:\n            logger.error(f\"Error while downloading data: {e}\")\n            logger.error(traceback.format_exc())\n            queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir)\n\n    while True:\n        try:\n            task = queue_in.get()\n            task_index = task[\"index\"]\n            task_files = task[\"files\"]\n            task_dir = os.path.join(cache_dir, str(task_index))\n            os.makedirs(task_dir, exist_ok=True)\n\n            for file in task_files:\n                obj = parse.urlparse(file)\n                if os.path.exists(os.path.join(task_dir, obj.path.lstrip(\"/\"))):\n                    continue\n                _wait_for_file_to_exist(s3, obj)\n                s3.download_file(obj, os.path.join(task_dir, obj.path.lstrip(\"/\")))\n\n            queue_out.put(task_index)\n        except Exception as e:\n            logger.error(f\"Error in _download_data_target: {e}\")\n            logger.error(traceback.format_exc())\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create an S3 client instance\n    s3 = S3Client(input_dir)\n\n    # Create a cache instance to manage the downloaded files\n    cache = Cache(cache_dir)\n\n    while True:\n        try:\n            # Fetch a download task from the input queue\n            task = queue_in.get(timeout=10)\n            task_index = task[\"index\"]\n            file_paths = task[\"file_paths\"]\n\n            # Check if the files are already downloaded\n            if all(cache.exists(file_path) for file_path in file_paths):\n                # If all files are already downloaded, put the task index into the output queue\n                queue_out.put(task_index)\n                continue\n\n            # Download the missing files\n            for file_path in file_paths:\n                if not cache.exists(file_path):\n                    # Download the file to the cache directory\n                    cache.download(file_path)\n\n            # Put the task index into the output queue\n            queue_out.put(task_index)\n        except Empty:\n            # If the input queue is empty, the function will continue to wait for new tasks\n            continue\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir.client_kwargs)\n    while True:\n        try:\n            task_index, file_paths = queue_in.get(timeout=1)\n        except Empty:\n            continue\n        for file_path in file_paths:\n            obj = parse.urlparse(file_path)\n            if obj.scheme == \"s3\":\n                # Download the file from S3\n                local_path = os.path.join(cache_dir, obj.path.lstrip(\"/\"))\n                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                if not os.path.exists(local_path):\n                    _wait_for_file_to_exist(s3, obj)\n                    s3.download(obj.path.lstrip(\"/\"), local_path)\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir.url)\n\n    while True:\n        try:\n            task = queue_in.get(block=True)\n            index = task[0]\n            file_paths = task[1]\n\n            for file_path in file_paths:\n                obj = parse.urlparse(file_path)\n\n                if not os.path.exists(os.path.join(cache_dir, obj.path.lstrip(\"/\"))):\n                    _wait_for_file_to_exist(s3, obj)\n\n                    s3.download_file(\n                        input_dir.url,\n                        obj.path.lstrip(\"/\"),\n                        os.path.join(cache_dir, obj.path.lstrip(\"/\")),\n                    )\n\n            queue_out.put(index)\n        except Exception as e:\n            logger.error(f\"Error while downloading data: {e}\")\n            queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir.bucket, input_dir.prefix)\n\n    while True:\n        try:\n            task_index, file_paths = queue_in.get(timeout=10)\n        except Empty:\n            continue\n\n        if file_paths is None:\n            queue_out.put(task_index)\n            continue\n\n        for file_path in file_paths:\n            file_path = file_path.lstrip(\"/\")\n            file_name = os.path.basename(file_path)\n            file_dir = os.path.dirname(file_path)\n\n            local_path = os.path.join(cache_dir, file_dir)\n            if not os.path.exists(local_path):\n                os.makedirs(local_path)\n\n            local_path = os.path.join(local_path, file_name)\n            if os.path.exists(local_path):\n                continue\n\n            if input_dir.is_s3:\n                s3.download(file_path, local_path)\n            else:\n                shutil.copy(os.path.join(input_dir.path, file_path), local_path)\n\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n    while True:\n        try:\n            task_index, file_paths = queue_in.get(timeout=30)\n        except Empty:\n            continue\n        if task_index is None:\n            break\n        for file_path in file_paths:\n            if file_path is None:\n                continue\n            obj = input_dir.resolve(file_path)\n            if obj is None:\n                continue\n            local_file_path = os.path.join(cache_dir, file_path)\n            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n            if os.path.exists(local_file_path):\n                continue\n            _wait_for_file_to_exist(s3, obj)\n            s3.download_file(obj, local_file_path)\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[Tuple[int, List[str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        index, paths = r\n\n        # 4. Upload all the required paths to unblock the current index\n        for path in paths:\n            if output_dir.path:\n                local_path = path.replace(cache_dir, output_dir.path)\n\n            if output_dir.url is not None or output_dir.path is not None:\n                if output_dir.url:\n                    path = path.replace(cache_dir, output_dir.url)\n\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    dirpath = os.path.dirname(local_path)\n\n                    os.makedirs(dirpath, exist_ok=True)\n\n                    with open(local_path, \"rb\") as f:\n                        s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n                elif os.path.isfile(path):\n                    if not path.startswith(\"/teamspace/studios/this_studio\"):\n                        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                        shutil.copyfile(path, local_path)\n                else:\n                    raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 5. Inform the worker the current files are available\n        remove_queue.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[Tuple[int, List[str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        index, paths = r\n\n        # 4. Iterate through the paths and upload them sequentially.\n        for path in paths:\n            if output_dir.path:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    dirpath = os.path.dirname(path.replace(cache_dir, output_dir.path))\n                    os.makedirs(dirpath, exist_ok=True)\n                    shutil.move(path, path.replace(cache_dir, output_dir.path))\n\n            elif output_dir.url is not None:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    obj = parse.urlparse(path)\n\n                    if obj.scheme == \"s3\":\n                        dirpath = os.path.dirname(path.replace(cache_dir, output_dir.url))\n                        os.makedirs(dirpath, exist_ok=True)\n\n                        with open(path, \"rb\") as f:\n                            s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n                    elif os.path.isfile(path):\n                        if not path.startswith(\"/teamspace/studios/this_studio\"):\n                            dirpath = os.path.dirname(path.replace(cache_dir, output_dir.url))\n                            os.makedirs(dirpath, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        index, path = r\n\n        # 4. Upload the file\n        if output_dir.url is not None:\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n            elif os.path.isfile(path):\n                if not path.startswith(\"/teamspace/studios/this_studio\"):\n                    s3.client.upload_file(path, output_dir.url, path.replace(cache_dir, \"\"))\n\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        elif output_dir.path is not None:\n            if not path.startswith(cache_dir) and output_dir.path is not None:\n                path = path.replace(cache_dir, output_dir.path)\n\n            if os.path.isfile(path):\n                if not path.startswith(\"/teamspace/studios/this_studio\"):\n                    shutil.copyfile(path, path.replace(cache_dir, output_dir.path))\n\n        # 5. Send the file path to the remove queue\n        remove_queue.put(path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        index, paths = r\n\n        # 4. Upload all the required paths to unblock the current index\n        for path in paths:\n            if path.startswith(cache_dir):\n                path = path.replace(cache_dir, \"\")\n\n            if output_dir.url:\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    dirpath = os.path.dirname(path)\n\n                    if dirpath:\n                        s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n                    else:\n                        s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n                elif obj.scheme == \"file\":\n                    dirpath = os.path.dirname(path)\n\n                    if dirpath:\n                        s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n                    else:\n                        s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n                elif obj.scheme == \"s3+file\":\n                    dirpath = os.path.dirname(path)\n\n                    if dirpath:\n                        s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n                    else:\n                        s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n                else:\n                    raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n            elif output_dir.path:\n                dirpath = os.path.dirname(path)\n\n                if dirpath:\n                    s3.client.upload_file(path, output_dir.path, path."}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        paths = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and upload them sequentially.\n        for path in paths:\n            if isinstance(path, tuple):\n                dirpath, path = path\n                path = os.path.join(dirpath, path)\n\n            if output_dir.url:\n                obj = parse.urlparse(path)\n                s3.client.upload_file(path, output_dir.url, obj.path.lstrip(\"/\"))\n            elif output_dir.path:\n                if not path.startswith(cache_dir):\n                    path = path.replace(cache_dir, output_dir.path)\n\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                shutil.move(path, output_dir.path)\n\n        # 4. Send the paths to the remove queue\n        remove_queue.put(paths)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        index, path = r\n\n        # 4. Check whether the file is already uploaded\n        if output_dir.url is not None or output_dir.path is not None:\n            if output_dir.url:\n                # 5. Wait for the removers to catch up when we are uploading data.\n                _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n            if output_dir.url:\n                # 6. Upload the file to the remote directory\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    if output_dir.path:\n                        path = path.replace(cache_dir, output_dir.path)\n\n                    if output_dir.url:\n                        path = path.replace(cache_dir, output_dir.url)\n\n                    dirpath = os.path.dirname(path)\n\n                    s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n                elif output_dir.path:\n                    dirpath = os.path.dirname(path.replace(cache_dir, output_dir.path))\n\n                    os.makedirs(dirpath, exist_ok=True)\n                    shutil.move(path, path.replace(cache_dir, output_dir.path))\n\n                elif output_dir.url:\n                    dirpath = os.path.dirname(path.replace(cache_dir, output_dir.url))\n\n                    os.makedirs(dirpath, exist_ok=True)\n                    shutil.move(path, path.replace(cache_dir, output_dir.url))\n\n            elif output_dir.path:\n                dirpath = os.path.dirname(path.replace(cache_dir, output_dir"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # 1. Initialise the S3 client\n    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        r: Optional[Tuple[str, str]] = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 4. Unpack\n        local_path, remote_path = r\n\n        # 5. Upload the file to the remote directory\n        if output_dir.url:\n            obj = parse.urlparse(remote_path)\n\n            if obj.scheme == \"s3\":\n                with open(local_path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n            elif obj.scheme == \"file\":\n                dirpath = os.path.dirname(remote_path)\n\n                os.makedirs(dirpath, exist_ok=True)\n\n                shutil.copyfile(local_path, remote_path)\n\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 6. Remove the local file\n        if not local_path.startswith(cache_dir):\n            local_path = local_path.replace(cache_dir, output_dir.path)\n\n        if os.path.exists(local_path):\n            os.remove(local_path)\n\n        # 7. Inform the worker the current files are available\n        remove_queue.put(remote_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        paths = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and upload them sequentially.\n        for path in paths:\n            if isinstance(path, tuple):\n                # 3.1. If the path is a tuple, it contains a temporary directory and a file path.\n                # 3.1.1. Upload the file to the remote directory.\n                if output_dir.url is not None:\n                    s3.client.upload_file(\n                        os.path.join(path[0], path[1]),\n                        output_dir.url,\n                        path[1],\n                    )\n                else:\n                    # 3.1.2. Move the file to the remote directory.\n                    shutil.move(os.path.join(path[0], path[1]), os.path.join(output_dir.path, path[1]))\n\n                # 3.2. Remove the temporary directory.\n                shutil.rmtree(path[0])\n\n            else:\n                # 3.3. If the path is not a tuple, it is a file path.\n                if output_dir.url is not None:\n                    # 3.3.1. Upload the file to the remote directory.\n                    s3.client.upload_file(\n                        path,\n                        output_dir.url,\n                        os.path.basename(path),\n                    )\n                else:\n                    # 3.3.2. Move the file to the remote directory.\n                    shutil.move(path, os.path.join(output_dir.path, os.path.basename(path)))\n\n            # 3.4. Send the file path to the remove queue.\n            remove_queue.put(path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        index, path = r\n\n        # 4. Check whether the file already exists\n        if output_dir.url:\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                if not s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\")):\n                    s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n            elif obj.scheme == \"file\":\n                if not os.path.exists(path):\n                    os.makedirs(os.path.dirname(path), exist_ok=True)\n                    shutil.copyfile(path.replace(cache_dir, output_dir.path), path)\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 5. Inform the worker the current files are available\n        remove_queue.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        paths = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and upload them sequentially.\n        for path in paths:\n            if isinstance(path, tuple):\n                local_path = path[1]\n                dirpath = path[0]\n            else:\n                local_path = path\n                dirpath = cache_dir\n\n            if output_dir.url:\n                if output_dir.url.startswith(\"s3://\"):\n                    s3.client.upload_file(local_path, output_dir.url.replace(\"s3://\", \"\"), local_path.replace(dirpath, \"\"))\n                else:\n                    raise ValueError(\"Only s3:// is supported.\")\n\n            elif output_dir.path:\n                if not path.startswith(output_dir.path):\n                    path = path.replace(dirpath, output_dir.path)\n\n                if os.path.exists(path):\n                    os.remove(path)\n\n                shutil.copyfile(local_path, path)\n\n            else:\n                raise ValueError(\"The provided output directory isn't supported.\")\n\n            # 4. Send the path for removal\n            remove_queue.put(local_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # 1. Create an S3 client\n    s3 = S3Client()\n\n    # 2. Continuously process items from the upload queue\n    while True:\n        # 3. Fetch an item from the queue\n        r = upload_queue.get()\n\n        # 4. If the item is None, terminate the process\n        if r is None:\n            return\n\n        # 5. If the item is a file path, upload it to the output directory\n        if isinstance(r, str):\n            # 6. If the output directory is an S3 bucket, upload the file to it\n            if output_dir.url:\n                obj = parse.urlparse(r)\n\n                with open(r, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n            # 7. If the output directory is a local directory, move the file to it\n            elif output_dir.path:\n                os.makedirs(os.path.dirname(r), exist_ok=True)\n                shutil.move(r, r.replace(cache_dir, output_dir.path))\n\n        # 8. If the item is a tuple, upload the file to the output directory and remove the temporary directory\n        elif isinstance(r, tuple):\n            # 9. If the output directory is an S3 bucket, upload the file to it\n            if output_dir.url:\n                obj = parse.urlparse(r[1])\n\n                with open(r[1], \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n            # 10. If the output directory is a local directory, move the file to it\n            elif output_dir.path:\n                os.makedirs(os.path.dirname(r[1]), exist_ok=True)\n                shutil.move(r[1], r[1].replace(cache_dir, output_dir.path))\n\n            # 11. Send the temporary directory to the"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[Tuple[int, str, str]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        index, local_path, remote_path = r\n\n        # 4. Upload the file to the remote location\n        if output_dir.url:\n            obj = parse.urlparse(remote_path)\n            s3.client.upload_file(local_path, obj.netloc, obj.path.lstrip(\"/\"))\n        elif output_dir.path:\n            remote_path = remote_path.replace(cache_dir, output_dir.path)\n            shutil.copyfile(local_path, remote_path)\n        else:\n            raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 5. Inform the worker the current files are available\n        remove_queue.put(remote_path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        paths = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and upload them sequentially.\n        for path in paths:\n            if isinstance(path, tuple):\n                dirpath, path = path\n            else:\n                dirpath = None\n\n            if output_dir.url:\n                if output_dir.url.startswith(\"s3://\"):\n                    s3.client.upload_file(path, output_dir.url[5:].split(\"/\")[0], path.replace(cache_dir, \"\"))\n                elif output_dir.url.startswith(\"s3://\"):\n                    s3.client.upload_file(path, output_dir.url[5:].split(\"/\")[0], path.replace(cache_dir, \"\"))\n                else:\n                    raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n            elif output_dir.path:\n                if dirpath:\n                    os.makedirs(os.path.join(output_dir.path, dirpath), exist_ok=True)\n\n                shutil.move(path, os.path.join(output_dir.path, path.replace(cache_dir, \"\")))\n\n        # 4. Send the paths for removal\n        remove_queue.put(paths)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # 1. Create an S3 client if the output_dir is an S3 bucket\n    s3 = S3Client() if output_dir.url else None\n\n    while True:\n        # 2. Collect paths\n        paths = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 4. Iterate through the paths and upload them sequentially.\n        for path in paths:\n            if isinstance(path, tuple):\n                dirpath, path = path\n\n            if output_dir.path:\n                if not path.startswith(cache_dir):\n                    path = path.replace(cache_dir, output_dir.path)\n\n                if not path.startswith(output_dir.path):\n                    path = os.path.join(output_dir.path, path)\n\n            if output_dir.url:\n                if not path.startswith(cache_dir):\n                    path = path.replace(cache_dir, output_dir.url)\n\n                if not path.startswith(output_dir.url):\n                    path = os.path.join(output_dir.url, path)\n\n            if output_dir.url and s3:\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n                elif os.path.isfile(path):\n                    dirpath = os.path.dirname(path)\n                    os.makedirs(dirpath, exist_ok=True)\n                    shutil.copyfile(path, path.replace(output_dir.url, output_dir.path))\n\n            elif os.path.isfile(path):\n                dirpath = os.path.dirname(path)\n                os.makedirs(dirpath, exist_ok=True)\n                shutil.copyfile(path, path.replace(output_dir.url, output_dir.path))\n\n        # 5. Send the paths to the"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        paths = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and upload them sequentially.\n        for path in paths:\n            if isinstance(path, tuple):\n                dirpath, path = path\n            else:\n                dirpath = None\n\n            if output_dir.url:\n                if dirpath:\n                    path = path.replace(dirpath, cache_dir)\n\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n                elif obj.scheme == \"file\":\n                    os.makedirs(os.path.dirname(path), exist_ok=True)\n                    shutil.copyfile(path, path)\n\n            elif output_dir.path:\n                if dirpath:\n                    path = path.replace(dirpath, cache_dir)\n\n                if os.path.isfile(path):\n                    os.makedirs(os.path.dirname(path), exist_ok=True)\n                    shutil.copyfile(path, path)\n\n            # 4. Remove the file if it is a temporary directory\n            if dirpath:\n                shutil.rmtree(dirpath)\n\n            # 5. Send the path to the remove queue\n            remove_queue.put(path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        paths = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and upload them sequentially.\n        for path in paths:\n            if isinstance(path, tuple):\n                dirpath, path = path\n            else:\n                dirpath = None\n\n            if output_dir.path:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if dirpath:\n                    path = path.replace(dirpath, cache_dir)\n\n                if os.path.exists(path):\n                    if output_dir.path:\n                        s3.client.upload_file(path, output_dir.netloc, output_dir.path.lstrip(\"/\") + path.lstrip(cache_dir))\n                    else:\n                        shutil.copyfile(path, path.replace(cache_dir, output_dir.path))\n\n            elif output_dir.url is not None:\n                if dirpath:\n                    path = path.replace(dirpath, output_dir.url)\n\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n                elif obj.scheme == \"file\":\n                    shutil.copyfile(path, obj.path)\n                else:\n                    raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n            # 4. Remove the paths from the local cache\n            if dirpath:\n                shutil.rmtree(dirpath)\n            else:\n                os.remove(path)\n\n            # 5. Send the path to the remove queue\n            remove_queue.put(path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect the path from the queue\n        path = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if path is None:\n            return\n\n        # 3. If the path is a tuple, unpack it\n        if isinstance(path, tuple):\n            temp_dir, path = path\n\n        # 4. If the path is a file path, upload it\n        if os.path.isfile(path):\n            if output_dir.path:\n                path = path.replace(cache_dir, output_dir.path)\n\n            if output_dir.url:\n                path = path.replace(cache_dir, output_dir.url)\n\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                with open(path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n            elif output_dir.path and output_dir.path.startswith(\"/teamspace/studios/this_studio\"):\n                shutil.copyfile(path, path.replace(cache_dir, output_dir.path))\n\n            else:\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                shutil.copyfile(path, path.replace(cache_dir, output_dir.path))\n\n        # 5. Send the path to the remove queue\n        remove_queue.put(path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        if isinstance(r, tuple):\n            tmp_dir, path = r\n        else:\n            tmp_dir = None\n            path = r\n\n        # 4. Upload the file to the remote directory\n        if output_dir.url:\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                # 4.1 Upload the file to S3\n                s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n                # 4.2 Remove the local file\n                if tmp_dir:\n                    os.remove(path)\n\n            elif obj.scheme == \"file\":\n                # 4.3 Move the file to the local filesystem\n                if tmp_dir:\n                    shutil.move(path, os.path.join(output_dir.path, obj.path.lstrip(\"/\")))\n\n        elif output_dir.path:\n            # 5. Move the file to the local filesystem\n            if tmp_dir:\n                shutil.move(path, os.path.join(output_dir.path, path.replace(cache_dir, \"\")))\n\n        # 6. Send the file path to the remove queue\n        remove_queue.put(path)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect the item from the queue\n        item = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if item is None:\n            return\n\n        # 3. Check if the item is a tuple\n        if isinstance(item, tuple):\n            # 4. If it is a tuple, unpack it\n            dirpath, path = item\n        else:\n            # 5. If it's not a tuple, set the directory path to None\n            dirpath = None\n            path = item\n\n        # 6. Check if the path starts with the cache directory\n        if dirpath and not path.startswith(cache_dir):\n            path = path.replace(dirpath, cache_dir)\n\n        # 7. Check if the output directory is an S3 bucket\n        if output_dir.url:\n            # 8. If it is, upload the file to the S3 bucket\n            obj = parse.urlparse(path)\n\n            with open(path, \"rb\") as f:\n                s3.client.upload_fileobj(f, output_dir.url, obj.path.lstrip(\"/\"))\n\n            # 9. Send the file path to the remove queue for removal\n            remove_queue.put(path)\n\n        # 10. If the output directory is a local directory, copy the file to the target location\n        elif output_dir.path:\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            shutil.copyfile(path, path.replace(cache_dir, output_dir.path))\n\n            # 11. Send the file path to the remove queue for removal\n            remove_queue.put(path)\n\n        # 12. If the output directory is not an S3 bucket or a local directory, raise an error\n        else:\n            raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # 1. Create the S3 client if the output directory is an S3 bucket\n    s3 = S3Client() if output_dir.url is not None else None\n\n    # 2. Initialize the upload queue\n    upload_queue.put(None)\n\n    # 3. Initialize the remove queue\n    remove_queue.put(None)\n\n    # 4. Initialize the upload queue\n    upload_queue.put(None)\n\n    # 5. Initialize the remove queue\n    remove_queue.put(None)\n\n    while True:\n        # 6. Collect paths\n        paths = upload_queue.get()\n\n        # 7. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 8. Iterate through the paths and upload them sequentially.\n        for path in paths:\n            if output_dir.path:\n                if not path.startswith(cache_dir) and output_dir.path is not None:\n                    path = path.replace(cache_dir, output_dir.path)\n\n                if os.path.exists(path):\n                    if output_dir.url is not None:\n                        if output_dir.url.startswith(\"s3\"):\n                            dirpath = os.path.dirname(path)\n\n                            if not dirpath.startswith(cache_dir):\n                                dirpath = dirpath.replace(output_dir.path, cache_dir)\n\n                            s3.client.upload_file(path, output_dir.url, dirpath.lstrip(\"/\"))\n\n                        elif output_dir.url.startswith(\"file\"):\n                            shutil.copyfile(path, path.replace(cache_dir, output_dir.path))\n\n                    elif output_dir.path.startswith(\"file\"):\n                        shutil.copyfile(path, path.replace(cache_dir, output_dir.path))\n\n            elif output_dir.url is not None:\n                if output_dir.url.startswith(\"s3\"):\n                    dirpath = os.path.dirname(path)"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes.\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to the workers based on provided weights.\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=total_workers)\n\n    # Print the distribution details for workers on the current node.\n    print(f\"Distributing items to workers on node {node_rank} ({len(worker_items)} workers):\")\n    for worker_id, items, weight in zip(worker_ids_this_node, worker_items, worker_weights):\n        if file_size:\n            size_mb = sum(os.path.getsize(item) for item in items) / 1024 / 1024\n            print(f\"  Worker {worker_id}: {len(items)} items ({size_mb:.2f} MB)\")\n        else:\n            print(f\"  Worker {worker_id}: {len(items)} items ({weight} weight)\")\n\n    # Shuffle the items assigned to each worker.\n    for worker_id, items in zip(worker_ids_this_node, worker_items):\n        random.shuffle(items)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes.\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to the workers based on weights.\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=total_workers)\n\n    # Print the distribution details for workers on the current node.\n    print(f\"Worker distribution for node {node_rank}:\")\n    for i in worker_ids_this_node:\n        items_str = \", \".join(str(item) for item in worker_items[i])\n        weight_str = f\"{sum(worker_weights[i])} MB\" if file_size else f\"{sum(worker_weights[i])}\"\n        print(f\"Worker {i}: {items_str} ({weight_str})\")\n\n    # Shuffle the items for each worker, and return the result.\n    for worker_id in worker_ids_this_node:\n        random.shuffle(worker_items[worker_id])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Print the distribution details for workers on the current node\n    print(f\"Distributing {len(user_items)} items to {world_size} workers on node {node_rank}\")\n    if file_size:\n        print(f\"Total file size: {sum(worker_weights) / 1000 / 1000:.2f} MB\")\n    else:\n        print(f\"Total weight: {sum(worker_weights)}\")\n\n    for worker_id in worker_ids_this_node:\n        print(f\"Worker {worker_id} has {len(worker_items[worker_id])} items\")\n        if file_size:\n            print(f\"Total file size: {sum(worker_weights[worker_id]) / 1000 / 1000:.2f} MB\")\n        else:\n            print(f\"Total weight: {sum(worker_weights[worker_id])}\")\n\n    # Shuffle the items assigned to each worker\n    for worker_id in worker_ids_this_node:\n        random.shuffle(worker_items[worker_id])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    world_size = num_nodes * num_workers\n\n    # Distribute items to workers based on provided weights\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n\n    # Print the distribution details for workers on the current node\n    print(f\"Worker distribution on node {node_rank}:\")\n    for worker_id in worker_ids_this_node:\n        items = worker_items[worker_id]\n        if file_size:\n            size = sum(weights[i] for i in items) / 1000000  # Convert to MB\n            print(f\"Worker {worker_id}: {len(items)} items ({size:.2f} MB)\")\n        else:\n            print(f\"Worker {worker_id}: {len(items)} items ({sum(weights[i] for i in items)} total weight)\")\n\n    # Shuffle the items for each worker\n    for worker_id in worker_ids_this_node:\n        random.shuffle(worker_items[worker_id])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        # Calculate the total size of items in megabytes.\n        total_size = sum(worker_weights) / 1000000\n    else:\n        # Calculate the total weight of items.\n        total_size = sum(worker_weights)\n\n    # Print the distribution details for workers on the current node.\n    print(f\"Distributing {len(user_items)} items to {num_workers} workers on node {node_rank}.\")\n    print(f\"Total size: {total_size:.2f} MB\")\n    for worker_id, worker_id_this_node in enumerate(worker_ids_this_node):\n        worker_items_this_node = worker_items[worker_id_this_node]\n        worker_weights_this_node = worker_weights[worker_id_this_node]\n        if file_size:\n            # Print the size of items assigned to the worker in megabytes.\n            print(\n                f\"Worker {worker_id_this_node} ({worker_id}): {len(worker_items_this_node)} items, {worker_weights_this_node / 1000000:.2f} MB\"\n            )\n        else:\n            # Print the weight of items assigned to the worker.\n            print(f\"Worker {worker_id_this_node} ({worker_id}): {len(worker_items_this_node)} items, {worker_weights_this_node}\")\n\n    # Shuffle the items assigned to each worker.\n    result = [[] for _ in range(num_workers)]\n    for worker_id, worker_id_this_node in enumerate(worker_ids_this_node):\n        worker_items_this_node = worker_items[worker_id_this_node]\n        random.shuffle(worker_items_this_node)\n        result[worker_id] = worker_items_this_node\n\n    return result\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes.\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to the workers based on weights.\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=total_workers)\n\n    # Print the distribution details for workers on the current node.\n    if file_size:\n        total_size = sum(worker_weights)\n        print(f\"Distributing {len(user_items)} items to {total_workers} workers on node {node_rank}\")\n        print(f\"Total size: {total_size / 1000000:.2f} MB\")\n        print(f\"Distribution:\")\n        for i in range(total_workers):\n            if i in worker_ids_this_node:\n                print(f\"Worker {i}: {worker_weights[i] / 1000000:.2f} MB ({worker_weights[i] / total_size:.2%})\")\n            else:\n                print(f\"Worker {i}: {worker_weights[i] / 1000000:.2f} MB ({worker_weights[i] / total_size:.2%})\")\n    else:\n        total_weight = sum(worker_weights)\n        print(f\"Distributing {len(user_items)} items to {total_workers} workers on node {node_rank}\")\n        print(f\"Total weight: {total_weight:.2f}\")\n        print(f\"Distribution:\")\n        for i in range(total_workers):\n            if i in worker_ids_this_node:\n                print(f\"Worker {i}: {worker_weights[i]:.2f} ({worker_weights[i] / total_weight:.2%})\")\n            else:\n                print(f\"Worker {i}: {worker_weights[i]:.2f} ({worker_weights[i] /"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to the workers based on weights\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=total_workers)\n\n    # Print the distribution details for workers on the current node\n    if file_size:\n        total_size = sum(worker_weights)\n        print(f\"Worker {node_rank} of {num_nodes} nodes:\")\n        for worker_id, items, weight in zip(worker_ids_this_node, worker_items, worker_weights):\n            print(f\"Worker {worker_id}: {len(items)} items ({weight / 1000 / 1000:.2f} MB)\")\n        print(f\"Total size: {total_size / 1000 / 1000:.2f} MB\")\n    else:\n        total_weight = sum(worker_weights)\n        print(f\"Worker {node_rank} of {num_nodes} nodes:\")\n        for worker_id, items, weight in zip(worker_ids_this_node, worker_items, worker_weights):\n            print(f\"Worker {worker_id}: {len(items)} items ({weight} weight)\")\n        print(f\"Total weight: {total_weight}\")\n\n    # Shuffle the items for each worker\n    for worker_items in worker_items:\n        random.shuffle(worker_items)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to workers based on weights\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=total_workers)\n\n    # Print the distribution details for workers on the current node\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n    for worker_id in worker_ids_this_node:\n        worker_items_this_node = worker_items[worker_id]\n        worker_weights_this_node = worker_weights[worker_id]\n        total_weight_this_node = sum(worker_weights_this_node)\n        if file_size:\n            total_size_this_node = sum(os.path.getsize(item) for item in worker_items_this_node) / 1000000\n            print(\n                f\"Worker {worker_id} on node {node_rank} has {len(worker_items_this_node)} items and {total_size_this_node:.2f} MB total size.\"\n            )\n        else:\n            print(\n                f\"Worker {worker_id} on node {node_rank} has {len(worker_items_this_node)} items and {total_weight_this_node:.2f} total weight.\"\n            )\n\n    # Shuffle items for each worker\n    for worker_id in worker_ids_this_node:\n        random.shuffle(worker_items[worker_id])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        # Calculate the total size of items in megabytes\n        total_size = sum(weights) / 1e6\n        print(f\"Total size of items: {total_size:.2f} MB\")\n    else:\n        # Calculate the total weight of items\n        total_weight = sum(weights)\n        print(f\"Total weight of items: {total_weight}\")\n\n    # Print the distribution details for workers on the current node\n    print(f\"Workers on node {node_rank}:\")\n    for worker_id in worker_ids_this_node:\n        worker_items_this_node = worker_items[worker_id]\n        worker_weights_this_node = worker_weights[worker_id]\n        print(f\"Worker {worker_id}: {len(worker_items_this_node)} items, weight: {worker_weights_this_node}\")\n\n    # Shuffle the items assigned to each worker\n    for worker_id in worker_ids_this_node:\n        random.shuffle(worker_items[worker_id])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    world_size = num_nodes * num_workers\n\n    # Distribute items to the workers based on weights\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n\n    # Calculate the IDs of the workers on the current node\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for workers on the current node\n    print(f\"Distribution of items to workers on node {node_rank}\")\n    for worker_id in worker_ids_this_node:\n        worker_items_this_node = worker_items[worker_id]\n        worker_weights_this_node = worker_weights[worker_id]\n        if file_size:\n            total_size_in_mb = sum(worker_weights_this_node) / 1000 / 1000\n            print(f\"Worker {worker_id}: {len(worker_items_this_node)} items ({total_size_in_mb:.2f} MB)\")\n        else:\n            total_weight = sum(worker_weights_this_node)\n            print(f\"Worker {worker_id}: {len(worker_items_this_node)} items ({total_weight} weight)\")\n\n    # Shuffle the items assigned to each worker\n    result = []\n    for worker_id in worker_ids_this_node:\n        worker_items_this_node = worker_items[worker_id]\n        random.shuffle(worker_items_this_node)\n        result.append(worker_items_this_node)\n\n    return result\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        # Calculate the total size of items in megabytes.\n        total_size = sum(weights) / 1000000\n    else:\n        # Calculate the total weight of items.\n        total_size = sum(weights)\n\n    # Print the distribution details for workers on the current node.\n    print(\n        f\"Distributing {len(user_items)} items among {num_workers} workers on node {node_rank} of {num_nodes} nodes. \"\n        f\"Total size: {total_size:.2f} MB.\"\n    )\n\n    # Create a list to store the items for each worker.\n    worker_items_list = [[] for _ in range(num_workers)]\n\n    # Iterate through the items and assign them to workers based on their weights.\n    for item, weight in zip(worker_items, worker_weights):\n        worker_id = random.choices(worker_ids_this_node, weights=[weight] * len(worker_ids_this_node))[0]\n        worker_items_list[worker_id].append(item)\n\n    # Shuffle the items for each worker.\n    for worker_items in worker_items_list:\n        random.shuffle(worker_items)\n\n    # Return the list of items for each worker.\n    return worker_items_list\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Distribute the items to the workers based on weights.\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n\n    # Shuffle the items for each worker.\n    for worker_id in worker_ids_this_node:\n        random.shuffle(worker_items[worker_id])\n\n    # Print the distribution details for workers on the current node.\n    if file_size:\n        total_weight_in_mb = sum(worker_weights[worker_id] for worker_id in worker_ids_this_node) / 1000 / 1000\n        print(f\"Worker IDs on this node: {worker_ids_this_node}\")\n        print(f\"Total weight in MB: {total_weight_in_mb:.2f}\")\n        for worker_id in worker_ids_this_node:\n            worker_weight_in_mb = worker_weights[worker_id] / 1000 / 1000\n            print(f\"Worker {worker_id}: {worker_weight_in_mb:.2f} MB\")\n    else:\n        total_weight = sum(worker_weights[worker_id] for worker_id in worker_ids_this_node)\n        print(f\"Worker IDs on this node: {worker_ids_this_node}\")\n        print(f\"Total weight: {total_weight}\")\n        for worker_id in worker_ids_this_node:\n            print(f\"Worker {worker_id}: {worker_weights[worker_id]}\")\n\n    # Return the list of items for each worker.\n    return [worker_items[worker_id] for worker_id in worker_ids_this_node]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to workers based on weights\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=total_workers)\n\n    # Print distribution details for workers on the current node\n    worker_items_this_node = [worker_items[i] for i in worker_ids_this_node]\n    worker_weights_this_node = [worker_weights[i] for i in worker_ids_this_node]\n\n    if file_size:\n        total_weight = sum(worker_weights_this_node)\n        total_size = total_weight / 1000000\n        print(f\"Worker {node_rank} on node {node_rank // num_workers} has {len(worker_items_this_node)} items\")\n        print(f\"Worker {node_rank} on node {node_rank // num_workers} has {total_size:.2f} MB of data\")\n    else:\n        print(f\"Worker {node_rank} on node {node_rank // num_workers} has {len(worker_items_this_node)} items\")\n        print(f\"Worker {node_rank} on node {node_rank // num_workers} has {sum(worker_weights_this_node)} weight\")\n\n    # Shuffle items within each worker and return the distribution\n    random.shuffle(worker_items_this_node)\n    return worker_items_this_node\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes.\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to the workers based on the weights.\n    worker_ids, worker_items, worker_weights = _pack_greedily(\n        items=user_items, weights=weights, num_bins=total_workers\n    )\n\n    # Print the distribution details for workers on the current node.\n    if file_size:\n        print(f\"Worker distribution for node {node_rank}:\")\n        for worker_id in worker_ids_this_node:\n            worker_item_sizes = [os.path.getsize(item) / 1024 / 1024 for item in worker_items[worker_id]]\n            print(f\"Worker {worker_id}: {sum(worker_item_sizes):.2f} MB ({len(worker_item_sizes)} items)\")\n    else:\n        print(f\"Worker distribution for node {node_rank}:\")\n        for worker_id in worker_ids_this_node:\n            print(f\"Worker {worker_id}: {sum(worker_weights[worker_id]):.2f} ({len(worker_weights[worker_id])} items)\")\n\n    # Shuffle the items for each worker and return the list of shuffled items.\n    shuffled_worker_items = [random.sample(worker_items[worker_id], len(worker_items[worker_id])) for worker_id in worker_ids_this_node]\n    return shuffled_worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to the workers based on provided weights\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=total_workers)\n\n    # Get the worker IDs for the current node\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for workers on the current node\n    print(f\"Worker IDs on this node: {worker_ids_this_node}\")\n    print(f\"Worker IDs on all nodes: {range(total_workers)}\")\n\n    if file_size:\n        # Calculate the total size of items assigned to the workers on the current node\n        total_size = sum(worker_weights[worker_id] for worker_id in worker_ids_this_node)\n        print(f\"Total size of items assigned to workers on this node: {total_size / 1000 / 1000:.2f} MB\")\n    else:\n        # Calculate the total weight of items assigned to the workers on the current node\n        total_weight = sum(worker_weights[worker_id] for worker_id in worker_ids_this_node)\n        print(f\"Total weight of items assigned to workers on this node: {total_weight}\")\n\n    # Shuffle the items assigned to the workers on the current node\n    for worker_id in worker_ids_this_node:\n        random.shuffle(worker_items[worker_id])\n\n    # Return the shuffled items assigned to the workers on the current node\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Print the distribution details for workers on the current node.\n    total_weight = sum(weights)\n    print(f\"Distributing {len(user_items)} items to {world_size} workers on node {node_rank}\")\n\n    if file_size:\n        total_size = sum(worker_weights) / 1000000\n        print(f\"Total size: {total_size:.2f} MB\")\n    else:\n        print(f\"Total weight: {total_weight}\")\n\n    for i, items in enumerate(worker_items):\n        if i in worker_ids_this_node:\n            if file_size:\n                size = sum(weights[j] for j in items) / 1000000\n                print(f\"Worker {i} (node {node_rank}): {len(items)} items, size {size:.2f} MB\")\n            else:\n                weight = sum(weights[j] for j in items)\n                print(f\"Worker {i} (node {node_rank}): {len(items)} items, weight {weight}\")\n\n    # Shuffle the items for each worker.\n    worker_items_shuffled = [random.sample(items, len(items)) for items in worker_items]\n\n    return worker_items_shuffled\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to workers based on weights\n    worker_ids, worker_item_ids = _pack_greedily(items=list(range(len(user_items))), weights=weights, num_bins=total_workers)\n    worker_item_ids = [worker_item_ids[i] for i in worker_ids]\n\n    # Get the items assigned to the current worker\n    worker_items = [user_items[i] for i in worker_item_ids]\n\n    # Shuffle the items assigned to the current worker\n    random.shuffle(worker_items)\n\n    # Print the distribution details for workers on the current node\n    if file_size:\n        print(f\"Worker {node_rank * num_workers} - {(node_rank + 1) * num_workers - 1}\")\n        print(f\"Total weight: {sum(worker_weights) / 1000 / 1000} MB\")\n        for i, item in enumerate(worker_items):\n            print(f\"  {i + 1}. {item} - {worker_weights[i] / 1000 / 1000} MB\")\n    else:\n        print(f\"Worker {node_rank * num_workers} - {(node_rank + 1) * num_workers - 1}\")\n        print(f\"Total weight: {sum(worker_weights)}\")\n        for i, item in enumerate(worker_items):\n            print(f\"  {i + 1}. {item} - {worker_weights[i]}\")\n\n    # Return the list of items for each worker, shuffled randomly\n    return [worker_items]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Associate the items to the workers based on number of nodes and node rank.\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for the workers on the current node.\n    total_weight = sum(worker_weights)\n    if file_size:\n        total_size = sum([os.path.getsize(item) for item in worker_items])\n        print(\n            f\"Worker IDs on this node ({node_rank + 1}/{num_nodes}): {worker_ids_this_node}\\n\"\n            f\"Total weight: {total_weight} ({total_size / 1000 / 1000:.2f} MB)\\n\"\n            f\"Distribution: {[len(worker_items[i]) for i in worker_ids_this_node]}\"\n        )\n    else:\n        print(\n            f\"Worker IDs on this node ({node_rank + 1}/{num_nodes}): {worker_ids_this_node}\\n\"\n            f\"Total weight: {total_weight}\\n\"\n            f\"Distribution: {[len(worker_items[i]) for i in worker_ids_this_node]}\"\n        )\n\n    # Shuffle the items assigned to each worker.\n    for worker_items_this_node in worker_items:\n        random.shuffle(worker_items_this_node)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Initialize a list to store the items for each worker\n    worker_items = [[] for _ in range(num_workers)]\n\n    # Iterate over the items and assign them to workers based on their weights\n    for item, weight in zip(user_items, weights):\n        # Calculate the index of the worker to assign the item to\n        worker_index = sum(worker_weights[:weight]) % num_workers\n\n        # Add the item to the worker's list\n        worker_items[worker_index].append(item)\n\n    # Print the distribution details for workers on the current node\n    total_weight = sum(weights)\n    total_size = sum([os.path.getsize(item) for item in user_items]) if file_size else total_weight\n    print(\n        f\"Distributing {len(user_items)} items to {num_workers} workers on node {node_rank} of {num_nodes} nodes, \"\n        f\"with total weight {total_weight} and total size {total_size / 1000000:.2f} MB.\"\n    )\n\n    # Shuffle the items for each worker\n    for worker_id in worker_ids_this_node:\n        random.shuffle(worker_items[worker_id])\n\n    # Return the list of items for each worker\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes.\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to workers based on weights.\n    worker_ids, worker_items, worker_weights = _pack_greedily(\n        items=user_items, weights=weights, num_bins=total_workers\n    )\n\n    # Shuffle items for each worker.\n    for worker_id in worker_ids:\n        random.shuffle(worker_items[worker_id])\n\n    # Print distribution details for workers on the current node.\n    if _get_node_rank() == 0:\n        total_weight = sum(weights)\n        print(f\"Distributing {len(user_items)} items among {total_workers} workers on {num_nodes} nodes:\")\n        for worker_id in worker_ids_this_node:\n            if file_size:\n                item_sizes_mb = [os.path.getsize(item) / 1024 / 1024 for item in worker_items[worker_id]]\n                total_size_mb = sum(item_sizes_mb)\n                print(\n                    f\"Worker {worker_id}: {len(worker_items[worker_id])} items, {total_size_mb:.2f} MB ({total_weight / total_size_mb:.2f}x weight)\"\n                )\n            else:\n                print(\n                    f\"Worker {worker_id}: {len(worker_items[worker_id])} items, {sum(worker_weights[worker_id])} weight\"\n                )\n\n    # Return a list of items for each worker, with items shuffled.\n    return [worker_items[worker_id] for worker_id in worker_ids_this_node]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_per_node = num_workers // num_nodes\n    num_workers_per_node_remainder = num_workers % num_nodes\n\n    if num_workers_per_node_remainder:\n        num_workers_per_node += 1\n        num_workers_per_node_remainder -= 1\n\n    num_workers_per_node_remainder_per_node = num_workers_per_node_remainder // num_nodes\n    num_workers_per_node_remainder_per_node_remainder = num_workers_per_node_remainder % num_nodes\n\n    if num_workers_per_node_remainder_per_node_remainder:\n        num_workers_per_node_remainder_per_node += 1\n        num_workers_per_node_remainder_per_node_remainder -= 1\n\n    num_workers_per_node_remainder_per_node_remainder_per_node = (\n        num_workers_per_node_remainder_per_node_remainder // num_nodes\n    )\n\n    num_workers_per_node_remainder_per_node_remainder_per_node_remainder = (\n        num_workers_per_node_remainder_per_node_remainder % num_nodes\n    )\n\n    if num_workers_per_node_remainder_per_node_remainder_per_node_remainder:\n        num_workers_per_node_remainder_per_node_remainder_per_node += 1\n        num_workers_per_node_remainder_per_node_remainder_per_node_remainder -= 1\n\n    num_workers_per_node_remainder_per_node_remainder_per_"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    total_workers = num_workers * num_nodes\n\n    num_items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    worker_items = []\n    start_index = 0\n\n    for i in range(total_workers):\n        end_index = start_index + num_items_per_worker\n\n        if i < remainder:\n            end_index += 1\n\n        worker_items.append(user_items[start_index:end_index])\n        start_index = end_index\n\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items[node_rank::num_nodes]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * _get_num_nodes()\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    for i in range(total_workers - 1, total_workers - 1 - remainder, -1):\n        num_items_per_worker += 1\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + [num_items_per_worker] * (total_workers - 1))\n    end_indices = np.cumsum([num_items_per_worker] * total_workers)\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * _get_num_nodes()\n\n    # Calculate how many items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([num_items_per_worker] * total_workers)\n    worker_end_indices = np.cumsum([num_items_per_worker + 1] * remainder) + num_items_per_worker * (\n        total_workers - remainder\n    )\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [\n        user_items[worker_start_indices[i] : worker_end_indices[i]]\n        for i in range(_get_node_rank() * num_workers, (_get_node_rank() + 1) * num_workers)\n    ]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = (len(user_items) + num_nodes - 1) // num_nodes\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = [node_rank * num_items_per_worker]\n    end_indices = [min((node_rank + 1) * num_items_per_worker, len(user_items))]\n    for i in range(node_rank + 1, num_nodes):\n        start_indices.append(i * num_items_per_worker)\n        end_indices.append(min((i + 1) * num_items_per_worker, len(user_items)))\n\n    # Distribute the items to the workers\n    worker_items = []\n    for i in range(num_nodes):\n        worker_items.append(user_items[start_indices[i] : end_indices[i]])\n\n    # Check if the output list has the correct length\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_workers_total = num_workers * _get_num_nodes()\n\n    # Calculate how many items each worker should process\n    num_items_per_worker = len(user_items) // num_workers_total\n    remainder = len(user_items) % num_workers_total\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([num_items_per_worker] * num_workers_total)\n    worker_end_indices = np.cumsum([num_items_per_worker + 1] * remainder) + num_items_per_worker * _get_node_rank()\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    if remainder > 0:\n        worker_start_indices[-remainder:] += num_items_per_worker + 1\n        worker_end_indices[-remainder:] += num_items_per_worker\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [user_items[start:end] for start, end in zip(worker_start_indices, worker_end_indices)]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    total_workers = num_workers * num_nodes\n    items_per_worker = (len(user_items) + total_workers - 1) // total_workers\n    worker_start_indices = np.arange(0, len(user_items), items_per_worker)\n    worker_end_indices = np.minimum(worker_start_indices + items_per_worker, len(user_items))\n    worker_indices = [\n        list(range(start, end)) for start, end in zip(worker_start_indices, worker_end_indices)\n    ]\n\n    if len(worker_indices) != total_workers:\n        raise RuntimeError(\n            f\"The number of workers ({len(worker_indices)}) is not equal to the number of nodes * workers per node ({total_workers}).\"\n        )\n\n    return worker_indices[node_rank::num_nodes]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = _get_num_nodes() * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    adjusted_items = [num_items + 1 if i < remainder else num_items for i in range(total_workers)]\n\n    # Calculate the cumulative sum of adjusted items to determine the start and end indices for each worker's items\n    cumulative_sum = np.cumsum(adjusted_items)\n    start_indices = cumulative_sum[:-1]\n    end_indices = cumulative_sum[1:]\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [\n        user_items[start:end] for start, end in zip(start_indices, end_indices)\n    ]\n\n    # Ensure that the output list has a length equal to the number of workers; otherwise, raise a RuntimeError\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # 1. Calculate the total number of workers across all nodes\n    total_num_workers = _get_num_nodes() * num_workers\n\n    # 2. Calculate how many items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n    num_items_remainder = len(user_items) % total_num_workers\n\n    # 3. Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    num_items_per_worker_with_remainder = num_items_per_worker + 1\n    num_items_per_worker_with_remainder_list = [num_items_per_worker_with_remainder] * num_items_remainder\n    num_items_per_worker_list = [num_items_per_worker] * (total_num_workers - num_items_remainder)\n    num_items_per_worker_list.extend(num_items_per_worker_with_remainder_list)\n\n    # 4. Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + num_items_per_worker_list)\n    end_indices = np.cumsum(num_items_per_worker_list)\n\n    # 5. Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items_list = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # 6. Ensure the output list has a length equal to the number of workers\n    if len(worker_items_list) != total_num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items_list\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of workers across all nodes\n    total_workers = _get_num_nodes() * num_workers\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of items that will be left over after distributing evenly\n    remainder = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([items_per_worker] * total_workers)\n    end_indices = np.cumsum([items_per_worker + 1] * (total_workers - remainder) + [items_per_worker] * remainder)\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Check if the output list has the same length as the number of workers\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_workers = num_workers * num_nodes\n\n    items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    start_indices = np.arange(0, total_workers, dtype=np.int64) * items_per_worker\n    start_indices += np.minimum(start_indices, remainder)\n    start_indices = np.cumsum(start_indices)\n\n    end_indices = np.arange(1, total_workers + 1, dtype=np.int64) * items_per_worker\n    end_indices += np.minimum(end_indices, remainder)\n    end_indices = np.cumsum(end_indices)\n\n    start_index = start_indices[node_rank]\n    end_index = end_indices[node_rank]\n\n    worker_items = user_items[start_index:end_index]\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            f\"The number of items assigned to the worker {node_rank} is not equal to the number of workers {num_workers}.\"\n        )\n\n    return [worker_items]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    total_workers = num_workers * num_nodes\n    num_items = len(user_items)\n\n    items_per_worker = num_items // total_workers\n    remainder = num_items % total_workers\n\n    worker_starts = np.arange(total_workers) * items_per_worker\n    worker_starts = np.append(worker_starts, worker_starts[-1] + remainder)\n    worker_ends = np.append(worker_starts[1:], num_items)\n\n    worker_items = []\n    for start, end in zip(worker_starts, worker_ends):\n        worker_items.append(user_items[start:end])\n\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"Improper assignment\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_per_node = num_workers // num_nodes\n    num_workers_per_node_remainder = num_workers % num_nodes\n\n    if num_workers_per_node_remainder != 0:\n        if node_rank < num_workers_per_node_remainder:\n            num_workers_per_node += 1\n\n    num_items_per_worker = len(user_items) // num_workers_per_node\n    num_items_per_worker_remainder = len(user_items) % num_workers_per_node\n\n    if num_items_per_worker_remainder != 0:\n        if node_rank < num_items_per_worker_remainder:\n            num_items_per_worker += 1\n\n    start_indices = np.arange(num_workers_per_node) * num_items_per_worker\n    end_indices = np.arange(1, num_workers_per_node + 1) * num_items_per_worker\n    end_indices[-1] += num_items_per_worker_remainder\n\n    worker_items = []\n\n    for i in range(num_workers_per_node):\n        start_index = start_indices[i]\n        end_index = end_indices[i]\n        worker_items.append(user_items[start_index:end_index])\n\n    if len(worker_items) != num_workers_per_node:\n        raise RuntimeError(f\"Improper assignment. Expected {num_workers_per_node} workers, got {len(worker_items)}.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_num_workers = _get_num_nodes() * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n\n    # Calculate the number of items to assign to the last worker\n    remainder = len(user_items) % total_num_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.arange(0, total_num_workers, dtype=int) * num_items_per_worker\n    end_indices = np.arange(1, total_num_workers + 1, dtype=int) * num_items_per_worker\n    end_indices[-1] += remainder\n\n    # Calculate the start and end indices for the current worker's items\n    worker_rank = _get_node_rank()\n    start_index = start_indices[worker_rank]\n    end_index = end_indices[worker_rank]\n\n    # Slice the user_items list to get the items assigned to the current worker\n    worker_items = user_items[start_index:end_index]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            f\"The number of worker items ({len(worker_items)}) does not match the number of workers ({num_workers})\"\n        )\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_num_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n\n    # Calculate the number of items that will be assigned to the last workers\n    remainder = len(user_items) % total_num_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = [node_rank * num_items_per_worker + i * num_items_per_worker for i in range(num_nodes)]\n    end_indices = [(node_rank + 1) * num_items_per_worker + i * num_items_per_worker for i in range(num_nodes)]\n\n    # Adjust the start and end indices for the last workers\n    start_indices[-1] += remainder\n    end_indices[-1] += remainder\n\n    # Create a list of lists to store the items assigned to each worker\n    worker_items = [[] for _ in range(num_workers)]\n\n    # Iterate through the user items and assign them to the appropriate worker\n    for i, item in enumerate(user_items):\n        worker_index = i // num_items_per_worker\n        worker_items[worker_index].append(item)\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    # Return the list of lists containing the items assigned to each worker\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_per_node = num_workers // num_nodes\n    num_workers_remainder = num_workers % num_nodes\n\n    num_workers_per_node_per_node = [num_workers_per_node] * num_nodes\n\n    for i in range(num_workers_remainder):\n        num_workers_per_node_per_node[i] += 1\n\n    num_workers_per_node_per_node_cumsum = np.cumsum(num_workers_per_node_per_node)\n\n    start_index = num_workers_per_node_per_node_cumsum[node_rank]\n    end_index = num_workers_per_node_per_node_cumsum[node_rank + 1]\n\n    user_items_per_worker = [user_items[start_index:end_index]]\n\n    for i in range(num_workers_remainder):\n        if i == node_rank:\n            user_items_per_worker.append(user_items[end_index:end_index + num_workers_per_node_per_node[i]])\n        else:\n            user_items_per_worker.append(user_items[end_index:end_index + num_workers_per_node_per_node[i] - 1])\n\n    if len(user_items_per_worker) != num_workers:\n        raise RuntimeError(\n            f\"The number of workers ({num_workers}) doesn't match the number of items ({len(user_items)})\"\n        )\n\n    return user_items_per_worker\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // (num_workers * num_nodes)\n    num_items_per_worker_remainder = len(user_items) % (num_workers * num_nodes)\n\n    # Determine the start and end indices for each worker's items\n    start_indices = [\n        sum(num_items_per_worker + (1 if i < num_items_per_worker_remainder else 0) for i in range(num_workers * node_rank))\n    ]\n    end_indices = [start_indices[0] + num_items_per_worker + (1 if node_rank < num_items_per_worker_remainder else 0)]\n    for i in range(1, num_workers):\n        start_indices.append(start_indices[i - 1] + num_items_per_worker + (1 if i <= node_rank else 0))\n        end_indices.append(end_indices[i - 1] + num_items_per_worker + (1 if i < node_rank else 0))\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    items_per_worker = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Check that the output list has the expected length\n    if len(items_per_worker) != num_workers:\n        raise RuntimeError(\n            f\"The number of workers ({num_workers}) does not match the number of items per worker ({len(items_per_worker)})\"\n        )\n\n    return items_per_worker\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Calculate how many items each worker should process\n    items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    items_per_worker += remainder // num_workers\n    remainder = remainder % num_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.arange(0, len(user_items), items_per_worker)\n    end_indices = np.append(start_indices[1:] + remainder, len(user_items))\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    items_per_worker_per_node = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(items_per_worker_per_node) != num_workers:\n        raise RuntimeError(\n            f\"The number of workers per node ({num_workers}) does not match the number of workers across all nodes ({len(items_per_worker_per_node)})\"\n        )\n\n    # Adjust the items per worker for the current node\n    items_per_worker_per_node = items_per_worker_per_node[node_rank::num_nodes]\n\n    return items_per_worker_per_node\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_workers = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    num_items_per_worker += min(remainder, num_workers)\n    remainder -= min(remainder, num_workers)\n\n    # Calculate the start and end indices for each worker's items\n    indices = np.cumsum([num_items_per_worker] * num_workers + [remainder])\n    start_indices = indices[:-1]\n    end_indices = indices[1:]\n\n    # Distribute the items among the workers\n    items_per_worker = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(items_per_worker) != num_workers:\n        raise RuntimeError(\n            f\"Improper assignment of items to workers. Expected {num_workers} workers, got {len(items_per_worker)}.\"\n        )\n\n    # Determine the worker's items based on the current node's rank\n    items_per_worker = items_per_worker[node_rank::num_nodes]\n\n    return items_per_worker\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_per_node = num_workers // num_nodes\n    num_workers_per_node_remainder = num_workers % num_nodes\n\n    num_workers_per_node_remainder_per_node = [\n        num_workers_per_node_remainder // num_nodes for _ in range(num_nodes)\n    ]\n\n    num_workers_per_node_remainder_per_node[0] += num_workers_per_node_remainder_per_node[0]\n\n    num_workers_per_node_remainder_per_node = np.cumsum(num_workers_per_node_remainder_per_node)\n\n    start_idx = num_workers_per_node_remainder_per_node[node_rank]\n    end_idx = num_workers_per_node_remainder_per_node[node_rank + 1]\n\n    if end_idx - start_idx != num_workers_per_node:\n        raise RuntimeError(\n            f\"The number of workers assigned to a node ({num_workers_per_node}) is not equal to the number of workers per node \"\n            f\"remainder ({num_workers_per_node_remainder}) + the number of workers per node remainder per node ({num_workers_per_node_remainder_per_node}).\"\n        )\n\n    if len(user_items) < num_workers:\n        raise RuntimeError(\n            f\"The number of items ({len(user_items)}) is less than the number of workers ({num_workers}).\"\n        )\n\n    user_items = np.array(user_items)\n\n    worker_items = [\n        user_items[start_idx + num_workers_per_node * i : start_idx + num_workers_per_node * (i + 1)].tol"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/processing/__init__.py\nfrom .base import DataChunkRecipe, DataProcessor, DataRecipe, DataTransformRecipe\nfrom .readers import BaseReader, StreamingDataLoaderReader\n\n__all__ = [\n    \"DataChunkRecipe\",\n    \"DataProcessor\",\n    \"DataRecipe\",\n    \"DataTransformRecipe\",\n    \"BaseReader\",\n    \"StreamingDataLoaderReader\",\n]"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/processing/__init__.py\nfrom .data_processor import DataChunkRecipe, DataProcessor, DataTransformRecipe\nfrom .readers import BaseReader, StreamingDataLoaderReader\n\n__all__ = [\n    \"DataChunkRecipe\",\n    \"DataProcessor\",\n    \"DataTransformRecipe\",\n    \"BaseReader\",\n    \"StreamingDataLoaderReader\",\n]\n\n#litdata/streaming/__init__.py\nfrom .cache import Cache, Dir\nfrom .client import S3Client\nfrom .dataloader import StreamingDataLoader\nfrom .resolver import _resolve_dir\n\n__all__ = [\"Cache\", \"Dir\", \"S3Client\", \"StreamingDataLoader\", \"_resolve_dir\"]"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n        cache_data_dir = _get_cache_data_dir()\n        # Cleanup the cache data dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n\n\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n        cache_data_dir = _get_cache_data_dir()\n        # Cleanup the cache data dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n\n\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/dataloader.py\nimport os\nfrom typing import Any, List, Optional, Union\n\nfrom litdata.constants import _LIGHTNING_CLOUD_LATEST\nfrom litdata.processing.readers import BaseReader\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.broadcast import broadcast_object\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1DatasetType\n\n\nclass StreamingDataLoader(BaseReader):\n    def __init__(\n        self,\n        input_dir: Union[str, \"Dir\"],\n        dataset_type: Optional[V1DatasetType] = None,\n        dataset_id: Optional[str] = None,\n        storage_dir: Optional[str] = None,\n        empty: bool = False,\n        size: Optional[int] = None,\n        num_bytes: Optional[int] = None,\n        data_format: Optional[Any] = None,\n        compression: Optional[str] = None,\n        num_chunks: Optional[int] = None,\n        num_bytes_per_chunk: Optional[List[int]] = None,\n    ) -> None:\n        \"\"\"The `StreamingDataLoader` is a reader that can be used to read data from a streaming dataset.\n\n        Arguments:\n            input_dir: The path to where the input data are stored.\n            dataset_type: The type of the dataset.\n            dataset_id: The dataset id.\n            storage_dir: The path to where the data are stored.\n            empty: Whether the dataset is empty.\n            size: The size of the dataset.\n            num_bytes: The number of bytes of the dataset.\n            data_format: The data format of the dataset.\n            compression: The compression of the dataset.\n            num_chunks: The number of chunks of the dataset.\n            num_bytes_per_chunk: The number of bytes per chunk of the dataset.\n\n        \"\"\"\n        self.input_dir = _resolve_dir(input_dir)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/__init__.py\nfrom .cache import Cache\nfrom .client import S3Client\nfrom .dataloader import StreamingDataLoader\nfrom .resolver import resolve_dir\nfrom .processor import DataProcessor, DataRecipe, DataChunkRecipe, DataTransformRecipe\n\n#litdata/streaming/processor.py\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Dict, List, Optional, TypeVar\n\nfrom litdata.processing.readers import BaseReader\nfrom litdata.streaming import Cache, StreamingDataLoader\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.packing import _pack_greedily\n\nT = TypeVar(\"T\")\n\n\nclass DataRecipe:\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        pass\n\n    @abstractmethod\n    def prepare_item(self, *args: Any, **kwargs: Any) -> Any:\n        pass\n\n    def __init__(self) -> None:\n        self._name: Optional[str] = None\n\n    def _done(self, size: int, delete_cached_files: bool, output_dir: str) -> None:\n        pass\n\n\nclass DataChunkRecipe(DataRecipe):\n    def __init__(\n        self,\n        chunk_size: Optional[int] = None,\n        chunk_bytes: Optional[Union[int, str]] = None,\n        compression: Optional[str] = None,\n    ):\n        super().__init__()\n        if chunk_size is not None and chunk_bytes is not None:\n            raise ValueError(\"Either one of the `chunk_size` or the `chunk_bytes` need to be provided.\")\n\n        self.chunk_size = chunk_size\n        self.chunk_bytes = 1 << 26 if chunk_size is None else chunk_bytes\n        self.compression = compression\n\n    @abstractmethod\n    def prepare_"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/processing/utilities.py\nimport json\nimport logging\nimport os\nimport shutil\nfrom typing import Any, Dict, List, Optional\n\nimport numpy as np\nimport torch\nfrom lightning_cloud.openapi import V1DatasetType\n\nfrom litdata.constants import _LIGHTNING_CLOUD_LATEST\nfrom litdata.streaming.cache import Dir\nfrom litdata.streaming.client import S3Client\nfrom litdata.streaming.dataloader import StreamingDataLoader\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.packing import _pack_greedily\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1DatasetType\n\nlogger = logging.Logger(__name__)\n\n\ndef _create_dataset(\n    input_dir: str,\n    storage_dir: str,\n    dataset_type: V1DatasetType,\n    empty: bool,\n    size: Optional[int] = None,\n    num_bytes: Optional[int] = None,\n    data_format: Optional[Any] = None,\n    compression: Optional[str] = None,\n    num_chunks: Optional[int] = None,\n    num_bytes_per_chunk: Optional[List[int]] = None,\n) -> None:\n    \"\"\"This function is used to create a dataset in the Lightning Cloud.\"\"\"\n    s3 = S3Client()\n\n    if dataset_type == V1DatasetType.CHUNKED:\n        assert size\n        assert num_bytes\n        assert data_format\n        assert compression\n        assert num_chunks\n        assert num_bytes_per_chunk\n\n        with open(os.path.join(storage_dir, \"index.json\"), \"w\") as f:\n            json.dump(\n                {\n                    \"size\": size,\n                    \"num_bytes\": num_bytes,\n                    \"config\": {\n                        \"data_format\": data_format,\n                        \"data_spec\": torch.utils._pytree.trees"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/processing/utilities.py\nimport os\nimport shutil\nimport tempfile\nfrom typing import Any, List, Optional, Tuple\n\nfrom litdata.constants import _LIGHTNING_CLOUD_LATEST\nfrom litdata.utilities.packing import _pack_greedily\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1DatasetType\n\n\ndef _create_dataset(\n    input_dir: str,\n    storage_dir: str,\n    dataset_type: V1DatasetType,\n    empty: bool,\n    size: Optional[int] = None,\n    num_bytes: Optional[str] = None,\n    data_format: Optional[Any] = None,\n    compression: Optional[str] = None,\n    num_chunks: Optional[int] = None,\n    num_bytes_per_chunk: Optional[List[int]] = None,\n) -> None:\n    \"\"\"This function creates a dataset on the platform.\"\"\"\n    from lightning_cloud import LightningClient\n\n    client = LightningClient()\n\n    dataset_name = os.path.basename(input_dir)\n    dataset_id = client.create_dataset(\n        dataset_name,\n        dataset_type=dataset_type,\n        empty=empty,\n        size=size,\n        num_bytes=num_bytes,\n        data_format=data_format,\n        compression=compression,\n        num_chunks=num_chunks,\n        num_bytes_per_chunk=num_bytes_per_chunk,\n    )\n\n    client.upload_dataset_files(\n        dataset_id,\n        input_dir,\n        storage_dir,\n    )\n\n\ndef _create_dataset_from_chunks(\n    input_dir: str,\n    storage_dir: str,\n    dataset_type: V1DatasetType,\n    num_chunks: Optional[int] = None,\n    num_bytes_per_chunk: Optional[List[int]] = None,\n) ->"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/__init__.py\nfrom .cache import Cache, Dir\nfrom .client import S3Client\nfrom .dataloader import StreamingDataLoader\nfrom .processor import DataProcessor\nfrom .resolver import _resolve_dir\n\n__all__ = [\n    \"Cache\",\n    \"Dir\",\n    \"S3Client\",\n    \"StreamingDataLoader\",\n    \"DataProcessor\",\n    \"Dir\",\n    \"_resolve_dir\",\n]\n\n#litdata/streaming/dataloader.py\nfrom typing import Any, List, Optional, Union\n\nfrom litdata.constants import _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.streaming.processor import DataRecipe\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils.data import DataLoader\n\n\nclass StreamingDataLoader(DataRecipe):\n    \"\"\"The StreamingDataLoader class is used to process data from a DataLoader.\"\"\"\n\n    def __init__(self, dataloader: Union[DataLoader, Any], *args: Any, **kwargs: Any) -> None:\n        \"\"\"The StreamingDataLoader class is used to process data from a DataLoader.\n\n        Arguments:\n            dataloader: The DataLoader to use.\n\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.dataloader = dataloader\n\n    def prepare_structure(self, input_dir: Optional[str] = None) -> List[Any]:\n        \"\"\"Return the structure of your data.\n\n        Each element should contain at least a filepath.\n\n        \"\"\"\n        return list(self.dataloader)\n\n    def prepare_item(self, item_metadata: Any) -> Any:\n        \"\"\"The return of this `prepare_item` method is persisted in chunked binary files.\"\"\"\n        return item_metadata\n\n    def remap_items(self, items: List[Any], num_workers: int) -> List[List[Any]]:\n        \"\"\"Remap"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/dataloader.py\nimport os\nfrom abc import abstractmethod\nfrom typing import Any, Dict, List, Optional, TypeVar\n\nfrom litdata.constants import _IS_IN_STUDIO\nfrom litdata.streaming.cache import Dir\nfrom litdata.streaming.client import S3Client\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.packing import _pack_greedily\n\nif _IS_IN_STUDIO:\n    from lightning_cloud.openapi import V1DatasetType\n\nT = TypeVar(\"T\")\n\n\nclass StreamingDataLoader:\n    @abstractmethod\n    def __getitem__(self, index: int) -> Any:\n        pass\n\n    @abstractmethod\n    def __len__(self) -> int:\n        pass\n\n    @abstractmethod\n    def __iter__(self) -> Any:\n        pass\n\n    @abstractmethod\n    def __del__(self) -> None:\n        pass\n\n    def remap_items(self, items: List[T], num_workers: int) -> List[List[T]]:\n        \"\"\"This function is used to remap the items to the workers.\"\"\"\n        return _pack_greedily(items=items, num_bins=num_workers)\n\n\nclass S3DataLoader(StreamingDataLoader):\n    def __init__(\n        self,\n        input_dir: Union[str, Dir],\n        num_workers: Optional[int] = None,\n        num_downloaders: Optional[int] = None,\n        num_uploaders: Optional[int] = None,\n        delete_cached_files: bool = True,\n        fast_dev_run: Optional[Union[bool, int]] = None,\n        random_seed: Optional[int] = 42,\n        reorder_files: bool = True,\n        weights: Optional[List[int]] = None,\n    ):\n        \"\"\"The `S3DataLoader` provides an efficient way to process data across multiple machine into chunks to"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/dataloader.py\nfrom typing import Any, Callable, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\n\nfrom litdata.streaming import BaseStreamingDataset\nfrom litdata.streaming.utilities import _get_chunk_size\n\n\nclass StreamingDataLoader(torch.utils.data.DataLoader):\n    def __init__(\n        self,\n        dataset: BaseStreamingDataset,\n        batch_size: int = 1,\n        shuffle: bool = False,\n        sampler: Optional[torch.utils.data.Sampler] = None,\n        batch_sampler: Optional[torch.utils.data.Sampler] = None,\n        num_workers: int = 0,\n        collate_fn: Optional[Callable[[List[Any]], Any]] = None,\n        pin_memory: bool = False,\n        drop_last: bool = False,\n        timeout: float = 0,\n        worker_init_fn: Optional[Callable[[int], None]] = None,\n        multiprocessing_context: Optional[Any] = None,\n        generator: Optional[torch.Generator] = None,\n        *,\n        prefetch_factor: int = 2,\n        persistent_workers: bool = False,\n    ) -> None:\n        \"\"\"The `StreamingDataLoader` is a wrapper around the `torch.utils.data.DataLoader` that allows for streaming\n        datasets.\n\n        Arguments:\n            dataset: The dataset to load.\n            batch_size: How many samples per batch to load.\n            shuffle: Whether to shuffle the data.\n            sampler: The sampler to use.\n            batch_sampler: The batch sampler to use.\n            num_workers: How many subprocesses to use for data loading.\n            collate_fn: The function to use for collating the data.\n            pin_memory: Whether to pin memory.\n            drop_last: Whether to drop the last batch.\n            timeout: The timeout for the data loader.\n            worker_init_fn: The function to"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n        # Cleanup the cache data dir folder to avoid corrupted files from previous run to be there.\n        cache_data_dir = _get_cache_data_dir()\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/processing/__init__.py\nfrom .optimizer import DataProcessor, DataTransformRecipe, DataChunkRecipe\n\n#litdata/processing/utilities.py\nimport json\nimport os\nimport shutil\nimport uuid\nfrom typing import Any, List, Optional, Union\n\nimport numpy as np\nimport torch\nfrom lightning_cloud.openapi import V1DatasetType\n\nfrom litdata.constants import _LIGHTNING_CLOUD_LATEST, _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.utilities.packing import _pack_greedily\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import tree_flatten, tree_unflatten, treespec_dumps\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1DatasetType\n\n\ndef _create_dataset(\n    input_dir: str,\n    storage_dir: str,\n    dataset_type: V1DatasetType,\n    empty: bool = False,\n    size: Optional[int] = None,\n    num_bytes: Optional[int] = None,\n    data_format: Optional[Any] = None,\n    compression: Optional[str] = None,\n    num_chunks: Optional[int] = None,\n    num_bytes_per_chunk: Optional[List[int]] = None,\n) -> None:\n    \"\"\"This function is used to create a dataset on the Lightning Cloud platform.\"\"\"\n    from lightning_cloud import LightningClient\n\n    client = LightningClient()\n    dataset_id = str(uuid.uuid4())\n    client.datasets.create(\n        dataset_id=dataset_id,\n        input_dir=input_dir,\n        storage_dir=storage_dir,\n        dataset_type=dataset_type,\n        empty=empty,\n        size=size,\n        num_bytes=num_bytes,\n        data_format=data_format,"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/__init__.py\nfrom .cache import Cache, Dir\nfrom .client import S3Client\nfrom .dataloader import StreamingDataLoader\nfrom .optimizer import DataProcessor\nfrom .resolver import _resolve_dir\n\n#litdata/streaming/dataloader.py\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom litdata.streaming import _get_node_rank\n\n\nclass StreamingDataLoader(DataLoader):\n    \"\"\"The StreamingDataLoader is a wrapper around the PyTorch DataLoader to support distributed streaming.\n\n    Arguments:\n        dataset: The dataset to stream.\n        batch_size: The batch size to use.\n        num_workers: The number of workers to use.\n        pin_memory: Whether to pin the memory.\n        persistent_workers: Whether to keep the workers alive.\n        prefetch_factor: The prefetch factor to use.\n        persistent_workers: Whether to keep the workers alive.\n        collate_fn: The collate function to use.\n        drop_last: Whether to drop the last batch.\n        timeout: The timeout to use.\n        worker_init_fn: The worker init function to use.\n        multiprocessing_context: The multiprocessing context to use.\n        generator: The generator to use.\n        *args: The arguments to pass to the DataLoader.\n        **kwargs: The keyword arguments to pass to the DataLoader.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: Any,\n        batch_size: Optional[int] = None,\n        num_workers: Optional[int] = None,\n        pin_memory: bool = False,\n        persistent_workers: bool = False,\n        prefetch_factor: int = 2,\n        collate_fn: Optional[Any] = None,\n        drop_last: bool = False,\n        timeout: float = 0,\n        worker_init_fn: Optional[Any] = None,\n        multiprocessing_"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/dataloader.py\nimport copy\nimport logging\nimport os\nimport random\nfrom typing import Any, Callable, Dict, Generator, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.dataloader import _collate_fn_t, _worker_init_fn_t\n\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.broadcast import broadcast_object\nfrom litdata.utilities.packing import _pack_greedily\n\nlogger = logging.Logger(__name__)\n\n\nclass StreamingDataLoader(DataLoader):\n    def __init__(\n        self,\n        dataset: Union[Dataset, List[Dataset]],\n        batch_size: int = 1,\n        shuffle: bool = False,\n        sampler: Optional[Any] = None,\n        batch_sampler: Optional[Any] = None,\n        num_workers: int = 0,\n        collate_fn: Optional[Callable[[List[Any]], Any]] = None,\n        pin_memory: bool = False,\n        drop_last: bool = False,\n        timeout: float = 0,\n        worker_init_fn: Optional[_worker_init_fn_t] = None,\n        multiprocessing_context: Optional[Any] = None,\n        generator: Optional[Any] = None,\n        *,\n        prefetch_factor: int = 2,\n        persistent_workers: bool = False,\n        pin_memory_device: Optional[str] = None,\n        src_resolver: Optional[Callable[[str], str]] = None,\n        weights: Optional[List[int]] = None,\n    ) -> None:\n        \"\"\"This class is a wrapper around the PyTorch DataLoader to allow for streaming data.\n\n        Arguments:\n            dataset: The dataset to be used.\n            batch_size: The batch size to use.\n            shuffle: Whether to shuffle the data.\n            sampler"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/__init__.py\nfrom .cache import Cache, Dir\nfrom .client import S3Client\nfrom .dataloader import StreamingDataLoader\nfrom .optimizer import DataProcessor, DataChunkRecipe, DataTransformRecipe\n\n#litdata/streaming/dataloader.py\nfrom typing import Any, List, Optional, Tuple, Union\n\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.streaming.client import S3Client\nfrom litdata.streaming.optimizer import DataRecipe\nfrom litdata.constants import _LIGHTNING_CLOUD_LATEST\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1DatasetType\n\n\nclass StreamingDataLoader(DataRecipe):\n    def __init__(self, items: List[Any], input_dir: Optional[Union[str, \"Dir\"]] = None) -> None:\n        \"\"\"The StreamingDataLoader is a wrapper around a list of items to be used by the DataProcessor.\n\n        Arguments:\n            items: The list of items to be processed.\n            input_dir: The path to where the input data are stored.\n\n        \"\"\"\n        super().__init__()\n        self.items = items\n        self.input_dir = _resolve_dir(input_dir)\n        self.s3 = S3Client()\n\n    def prepare_structure(self, input_dir: Optional[str]) -> List[Any]:\n        return self.items\n\n    def prepare_item(self, item_metadata: Any) -> Any:\n        return item_metadata\n\n    def remap_items(self, items: List[Any], num_workers: int) -> List[List[Any]]:\n        \"\"\"Remaps the items to the workers based on the number of items and number of workers.\"\"\"\n        items_per_worker = len(items) // num_workers\n        extra_items = len(items) % num_workers\n\n        start = 0\n        result = []\n        for i in range("}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/dataloader.py\nimport os\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.packing import _pack_greedily\n\n\nclass StreamingDataLoader:\n    \"\"\"\n    The StreamingDataLoader class is a wrapper around a PyTorch DataLoader that allows for streaming data from a remote source.\n\n    This class is designed to work with the DataProcessor class, which is responsible for optimizing the data processing pipeline.\n    It provides methods to map the items to the workers and to read a slice of the data.\n\n    Attributes:\n        data_loader: A PyTorch DataLoader object.\n        num_workers: An integer representing the number of workers to use.\n        weights: A list of weights associated to the inputs. This is used to evenly split the work among the workers.\n        src_resolver: A function that takes a path and returns a path to the remote source.\n        input_dir: A string representing the path to the input directory.\n        output_dir: A string representing the path to the output directory.\n        random_seed: An integer representing the random seed to be set before shuffling the data.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_loader: Any,\n        num_workers: Optional[int] = None,\n        weights: Optional[List[int]] = None,\n        src_resolver: Optional[Any] = None,\n        input_dir: Optional[Union[str, Dict[str, str]]] = None,\n        output_dir: Optional[Union[str, Dict[str, str]]] = None,\n        random_seed: Optional[int] = 42,\n    ):\n        \"\"\"\n        Initializes a new instance of the StreamingDataLoader class.\n\n        Arguments:\n            data_loader: A PyTorch DataLoader object.\n            num_workers: An integer representing the number of workers to use.\n            weights: A list of weights associated to the inputs. This is used to evenly split the work among the workers"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/processing/__init__.py\nfrom .optimizer import DataProcessor\nfrom .readers import (\n    BaseReader,\n    CSVReader,\n    ImageReader,\n    JsonReader,\n    StreamingDataLoaderReader,\n    TextReader,\n    TFRecordReader,\n    TFRecordReaderV2,\n)\nfrom .utilities import (\n    _create_dataset,\n    _get_dataset_type,\n    _get_dataset_type_from_storage_dir,\n    _get_storage_dir,\n    _get_storage_dir_from_dataset_type,\n)\n\n#litdata/streaming/__init__.py\nfrom .cache import Cache\nfrom .client import S3Client\nfrom .dataloader import StreamingDataLoader\nfrom .resolver import _resolve_dir\n\n#litdata/streaming/resolver.py\nimport os\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom litdata.constants import _LIGHTNING_CLOUD_LATEST\nfrom litdata.streaming.cache import Dir\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1DatasetType\n\n\ndef _resolve_dir(dir: Optional[Union[str, Dir]]) -> Optional[Dir]:\n    if dir is None:\n        return None\n    if isinstance(dir, str):\n        return Dir(path=dir)\n    return dir\n\n\ndef _resolve_dir_from_storage_dir(storage_dir: Optional[str]) -> Optional[Dir]:\n    if storage_dir is None:\n        return None\n    return Dir(path=storage_dir)\n\n\ndef _resolve_dir_from_dataset_type(dataset_type: Optional[Union[str, V1DatasetType]]) -> Optional[Dir]:\n    if dataset_type is None:\n        return None\n    if isinstance(dataset_type, str):\n        if dataset_type == \"CHUNKED\":\n            dataset_type = V1DatasetType.CHUNKED\n        elif dataset"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/dataloader.py\nfrom typing import Any, Callable, Dict, Iterator, List, Optional, Tuple, Union\n\nfrom litdata.constants import _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.packing import _pack_greedily\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils.data import DataLoader\n\n\nclass StreamingDataLoader:\n    \"\"\"\n    The StreamingDataLoader class is a wrapper around the torch.utils.data.DataLoader class that provides a streaming interface for loading data.\n\n    This class is used to load data from a dataset in a streaming manner, where each batch is loaded on demand.\n    This is useful for large datasets that cannot be fit into memory, as it allows the data to be loaded in small chunks,\n    rather than all at once.\n\n    Arguments:\n        data_loader: The torch.utils.data.DataLoader object that will be used to load the data.\n        num_workers: The number of worker processes to use for loading the data.\n        batch_size: The batch size to use for loading the data.\n        shuffle: Whether to shuffle the data before loading.\n        drop_last: Whether to drop the last incomplete batch.\n        pin_memory: Whether to pin the data in memory.\n        persistent_workers: Whether to keep the workers alive after loading the data.\n        prefetch_factor: The number of batches to prefetch.\n        num_nodes: The number of nodes to use for loading the data.\n        node_rank: The rank of the node to use for loading the data.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        data_loader: DataLoader,\n        num_workers: Optional[int] = None,\n        batch_size: Optional[int] = None,\n        shuffle: Optional[bool] = None,\n        drop_last: Optional[bool] = None,\n        pin_memory: Optional"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/__init__.py\nfrom .cache import Cache, Dir\nfrom .client import S3Client\nfrom .dataloader import StreamingDataLoader\nfrom .resolver import _resolve_dir\nfrom .utilities import _get_s3_credentials\nfrom .dataset import Dataset\n\n#litdata/streaming/client.py\nimport logging\nimport os\nfrom typing import Any, Dict, List, Optional\n\nfrom litdata.constants import _BOTO3_AVAILABLE, _LIGHTNING_CLOUD_LATEST\nfrom litdata.streaming import _get_s3_credentials\nfrom litdata.streaming.utilities import _get_bucket_region\n\nif _BOTO3_AVAILABLE:\n    import boto3\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1S3Connection\n\nlogger = logging.Logger(__name__)\n\n\nclass S3Client:\n    def __init__(self, s3_connection: Optional[V1S3Connection] = None) -> None:\n        self.s3_connection = s3_connection\n        self.client = None\n\n        if self.s3_connection is not None:\n            self._setup_boto3_client()\n\n    def _setup_boto3_client(self) -> None:\n        \"\"\"This function is used to setup the boto3 client.\"\"\"\n        if self.s3_connection is None:\n            raise ValueError(\"The S3 connection is not defined.\")\n\n        if self.client is not None:\n            return\n\n        if self.s3_connection.access_key_id is None or self.s3_connection.secret_access_key is None:\n            raise ValueError(\"The S3 connection doesn't have the credentials.\")\n\n        credentials = _get_s3_credentials(self.s3_connection)\n\n        self.client = boto3.client(\n            \"s3\",\n            aws_access_key_id=credentials[\"access_key_id\"],"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    # Collect the results from the futures\n    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        try:\n            item_sizes.append(future.result())\n        except Exception as exc:\n            print(f\"Exception occurred while retrieving file size: {exc}\")\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        try:\n            item_sizes.append(future.result())\n        except Exception as e:\n            print(f\"Exception occurred: {e}\")\n            raise e\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        try:\n            item_sizes.append(future.result())\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    # Wait for all futures to complete and retrieve the results\n    for future in concurrent.futures.as_completed(futures):\n        try:\n            item_sizes.append(future.result())\n        except Exception as exc:\n            print(f\"An exception occurred while retrieving the file size: {exc}\")\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    # Retrieve the file sizes from the futures and append them to the list\n    for future in concurrent.futures.as_completed(futures):\n        try:\n            item_sizes.append(future.result())\n        except Exception as e:\n            logger.exception(e)\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        try:\n            item_sizes.append(future.result())\n        except Exception as e:\n            raise RuntimeError(f\"Failed to compute file sizes for item: {e}\")\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n    element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n    element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return os.path.exists(element) or (input_dir is not None and element.startswith(input_dir))\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if input_dir is not None and not element.startswith(input_dir):\n        return False\n\n    if not os.path.exists(element):\n        return False\n\n    return True\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if input_dir is not None and not element.startswith(input_dir):\n        return False\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is None:\n        return os.path.exists(element)\n\n    if not isinstance(element, str):\n        return False\n\n    if element.startswith(input_dir):\n        return True\n\n    element = str(Path(element).absolute())\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is None:\n        return os.path.exists(element)\n\n    if element.startswith(input_dir):\n        return True\n\n    element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        if input_dir is not None and element.startswith(input_dir):\n            return True\n        return os.path.exists(element)\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is None:\n        return os.path.exists(element)\n\n    if not isinstance(element, str):\n        return False\n\n    if element.startswith(input_dir):\n        return True\n\n    element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return (\n        os.path.exists(element)\n        or (input_dir is not None and element.startswith(input_dir))\n        or (input_dir is None and element.startswith(\"/\"))\n    )\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if _IS_IN_STUDIO and input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n        element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return os.path.exists(element) or (input_dir is not None and element.startswith(input_dir))\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n        element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return (input_dir is None or element.startswith(input_dir)) and os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is None:\n        return os.path.exists(element)\n\n    return os.path.exists(element) or element.startswith(input_dir)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n    element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if element.startswith(\"s3://\"):\n        return True\n\n    return os.path.exists(element)\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            from tinycudann.layers import (\n                Linear,\n                ReLU,\n                Sigmoid,\n                Tanh,\n                Sequential,\n                Dropout,\n                BatchNorm1d,\n            )\n\n            if n_neurons > 2048:\n                n_neurons = 2048\n\n            if n_neurons > 1024:\n                n_neurons = 1024\n\n            if n_neurons > 512:\n                n_neurons = 512\n\n            if n_neurons > 256:\n                n_neurons = 256\n\n            if n_neurons > 128:\n                n_neurons = 128\n\n            if n_neurons > 64:\n                n_neurons = 64\n\n            if n_neurons > 32:\n                n_neurons = 32\n\n            if n_neurons > 16:\n                n_neurons = 16\n\n            if n_neurons > 8:\n                n_neurons = 8\n\n            if n_neurons > 4:\n                n_neurons = 4\n\n            if n_neurons > 2:\n                n_neurons = 2\n\n            if n_neurons > 1:\n                n_neurons = 1\n\n            if n_neurons == 1:\n                n_neurons = 2\n\n            if n_neurons == 2:\n                n_neurons = 4\n\n            if n_neurons == 4:\n                n_neurons = 8\n\n            if n_neurons == 8:\n                n_neurons = 16\n\n            if n_neurons == 16:\n                n_neurons = 32\n\n            if n_neurons == 32:\n                n_neurons = 64\n\n            if"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            if n_neurons < 1024:\n                network_type = tcnn.NetworkWithInputEncoding\n            else:\n                network_type = tcnn.Network\n\n            return network_type(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_layers=n_layers,\n                activation=activation,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n        else:\n            layers = []\n            for _ in range(n_layers - 1):\n                layers.append(nn.Linear(n_input_dims, n_neurons))\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n                elif activation == \"None\":\n                    pass\n                else:\n                    raise ValueError(f\"Unknown activation function: {activation}\")\n                n_input_dims = n_neurons\n            layers.append(nn.Linear(n_input_dims, n_output_dims))\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n            elif output_activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Unknown activation function: {output_activation}\")\n            return nn.Sequential(*layers)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            from tinycudann.layers import Dense\n            from tinycudann.activation import ReLU, Sigmoid\n\n            if n_neurons > 1024:\n                return self.get_network(n_input_dims, n_output_dims, n_layers, 1024, activation, output_activation) + self.get_network(n_input_dims, n_output_dims, n_layers, n_neurons - 1024, activation, output_activation)\n\n            if n_neurons > 512:\n                return self.get_network(n_input_dims, n_output_dims, n_layers, 512, activation, output_activation) + self.get_network(n_input_dims, n_output_dims, n_layers, n_neurons - 512, activation, output_activation)\n\n            if n_neurons > 256:\n                return self.get_network(n_input_dims, n_output_dims, n_layers, 256, activation, output_activation) + self.get_network(n_input_dims, n_output_dims, n_layers, n_neurons - 256, activation, output_activation)\n\n            if n_neurons > 128:\n                return self.get_network(n_input_dims, n_output_dims, n_layers, 128, activation, output_activation) + self.get_network(n_input_dims, n_output_dims, n_layers, n_neurons - 128, activation, output_activation)\n\n            if n_neurons > 64:\n                return self.get_network(n_input_dims, n_output_dims, n_layers, 64, activation, output_activation) + self.get"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            from tinycudann.lib import Network\n\n            if n_neurons == 1:\n                return Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_hidden_layers=n_layers - 1,\n                    n_neurons_per_layer=1,\n                    activation=activation,\n                    output_activation=output_activation,\n                    seed=self._get_seed(),\n                )\n            else:\n                return Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_hidden_layers=n_layers - 1,\n                    n_neurons_per_layer=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                    seed=self._get_seed(),\n                )\n        else:\n            layers = []\n            for i in range(n_layers - 1):\n                layers.append(nn.Linear(n_input_dims, n_neurons))\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n                n_input_dims = n_neurons\n\n            layers.append(nn.Linear(n_input_dims, n_output_dims))\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n\n            return nn.Sequential(*layers)\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            from tinycudann.torch import TCN\n\n            if n_neurons > 10000:\n                n_neurons = 10000\n\n            if n_neurons > 1000:\n                n_neurons = 1000\n\n            if n_neurons > 100:\n                n_neurons = 100\n\n            if n_neurons > 10:\n                n_neurons = 10\n\n            if n_neurons > 2:\n                n_neurons = 2\n\n            if n_neurons > 1:\n                n_neurons = 1\n\n            if n_neurons > 0:\n                n_neurons = 1\n\n            if n_neurons == 0:\n                n_neurons = 1\n\n            if n_neurons < 0:\n                n_neurons = 1\n\n            if n_neurons < 1:\n                n_neurons = 1\n\n            if n_neurons < 2:\n                n_neurons = 2\n\n            if n_neurons < 10:\n                n_neurons = 10\n\n            if n_neurons < 100:\n                n_neurons = 100\n\n            if n_neurons < 1000:\n                n_neurons = 1000\n\n            if n_neurons < 10000:\n                n_neurons = 10000\n\n            if n_neurons < 100000:\n                n_neurons = 100000\n\n            if n_neurons < 1000000:\n                n_neurons = 1000000\n\n            if n_neurons < 1000"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            from tinycudann.layers import (\n                InputLayer,\n                DenseLayer,\n                ActivationLayer,\n                OutputLayer,\n            )\n\n            layers = [InputLayer(n_input_dims)]\n\n            for _ in range(n_layers - 1):\n                layers.append(DenseLayer(n_neurons))\n                layers.append(ActivationLayer(activation))\n\n            layers.append(OutputLayer(n_output_dims))\n            layers.append(ActivationLayer(output_activation))\n\n            return NetworkWithSkipLayers(layers[:-1], layers[-1])\n\n        else:\n            layers = []\n\n            for _ in range(n_layers - 1):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                layers.append(nn.ReLU())\n\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n\n            return nn.Sequential(*layers)\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            from tinycudann.layers import Dense\n            from tinycudann.activations import ReLU, Sigmoid\n\n            if n_neurons > 1024:\n                n_neurons = 1024\n\n            layers = [Dense(n_input_dims, n_neurons, self._get_seed())]\n            for _ in range(n_layers - 1):\n                layers.append(Dense(n_neurons, n_neurons, self._get_seed()))\n                layers.append(ReLU())\n\n            layers.append(Dense(n_neurons, n_output_dims, self._get_seed()))\n            if output_activation == \"ReLU\":\n                layers.append(ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(Sigmoid())\n\n            return layers\n\n        else:\n            layers = []\n\n            if activation == \"ReLU\":\n                activation = nn.ReLU()\n            elif activation == \"None\":\n                activation = nn.Identity()\n\n            if output_activation == \"ReLU\":\n                output_activation = nn.ReLU()\n            elif output_activation == \"Sigmoid\":\n                output_activation = nn.Sigmoid()\n            elif output_activation == \"None\":\n                output_activation = nn.Identity()\n\n            layers.append(nn.Linear(n_input_dims, n_neurons))\n            layers.append(activation)\n            for _ in range(n_layers - 2):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                layers.append(activation)\n\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            layers.append(output_activation)\n\n            return nn.Sequential(*layers)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            if n_neurons > 1024:\n                raise ValueError(\n                    \"TinyCudaNN does not support more than 1024 neurons in a single layer.\"\n                )\n\n            activation_function = {\n                \"ReLU\": tcnn.Activation.RELU,\n                \"None\": tcnn.Activation.NONE,\n            }[activation]\n\n            output_activation_function = {\n                \"ReLU\": tcnn.Activation.RELU,\n                \"Sigmoid\": tcnn.Activation.SIGMOID,\n                \"None\": tcnn.Activation.NONE,\n            }[output_activation]\n\n            return tcnn.NetworkWithInputEncoding(\n                n_input_dims=n_input_dims,\n                encoding_config={\n                    \"otype\": \"Hash\",\n                    \"n_levels\": 10,\n                    \"n_features_per_level\": 2,\n                    \"permutation\": \"random\",\n                    \"random_seed\": self._get_seed(),\n                },\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_layers=n_layers,\n                activation=activation_function,\n                output_activation=output_activation_function,\n            )\n        else:\n            layers = []\n            for _ in range(n_layers - 1):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                layers.append(nn.ReLU())\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigm"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            from tinycudann.layers import Linear, ReLU, Sigmoid\n\n            if n_neurons < 1024:\n                activation = \"ReLU\"\n            else:\n                activation = \"None\"\n\n            if output_activation == \"ReLU\":\n                output_activation = ReLU()\n            elif output_activation == \"Sigmoid\":\n                output_activation = Sigmoid()\n            else:\n                output_activation = None\n\n            if activation == \"ReLU\":\n                activation = ReLU()\n            else:\n                activation = None\n\n            layers = [Linear(n_input_dims, n_neurons, bias=True, seed=self._get_seed())]\n            for _ in range(n_layers - 2):\n                layers.append(Linear(n_neurons, n_neurons, bias=True, seed=self._get_seed()))\n                layers.append(activation)\n            layers.append(Linear(n_neurons, n_output_dims, bias=True, seed=self._get_seed()))\n            if output_activation is not None:\n                layers.append(output_activation)\n            return NetworkWithSkipLayers(skip_layers=layers[:-1], output_layers=layers[-1])\n        else:\n            layers = [nn.Linear(n_input_dims, n_neurons, bias=True)]\n            if activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            for _ in range(n_layers - 2):\n                layers.append(nn.Linear(n_neurons, n_neurons, bias=True))\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n            layers.append(nn.Linear(n_neurons, n_output_dims, bias=True))\n            if output_activation == \"ReLU\":\n                layers."}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n            from tinycudann.activation import get_activation\n\n            if n_neurons > 1024:\n                n_neurons = 1024\n\n            activation_fn = get_activation(activation)\n            output_activation_fn = get_activation(output_activation)\n\n            if n_layers == 1:\n                return tcnn.NetworkWithInputEncoding(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    encoding_config=dict(otype=\"HashGrid\", n_levels=1, n_features_per_level=2),\n                    network_config=dict(otype=\"FullyFusedMLP\", n_neurons=n_neurons,\n                                        output_activation=output_activation_fn),\n                )\n\n            layers = []\n            for _ in range(n_layers - 2):\n                layers.append(\n                    tcnn.NetworkWithInputEncoding(\n                        n_input_dims=n_input_dims,\n                        n_output_dims=n_neurons,\n                        encoding_config=dict(otype=\"HashGrid\", n_levels=1, n_features_per_level=2),\n                        network_config=dict(otype=\"FullyFusedMLP\", n_neurons=n_neurons,\n                                            output_activation=activation_fn),\n                    )\n                )\n\n            layers.append(\n                tcnn.NetworkWithInputEncoding(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    encoding_config=dict(otype=\"HashGrid\", n_levels=1, n_features_per_level=2),\n                    network_config=dict(otype=\"FullyFusedMLP\","}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            from tinycudann.lib import TCNN\n\n            if n_neurons > 1024:\n                raise ValueError(\n                    f\"n_neurons must be less than 1024 for TCNN, got {n_neurons}\"\n                )\n            if n_layers > 16:\n                raise ValueError(\n                    f\"n_layers must be less than 16 for TCNN, got {n_layers}\"\n                )\n\n            if activation == \"ReLU\":\n                activation = TCNN.Activation.ReLU\n            elif activation == \"None\":\n                activation = TCNN.Activation.None_\n            else:\n                raise ValueError(f\"Unknown activation function: {activation}\")\n\n            if output_activation == \"ReLU\":\n                output_activation = TCNN.Activation.ReLU\n            elif output_activation == \"Sigmoid\":\n                output_activation = TCNN.Activation.Sigmoid\n            elif output_activation == \"None\":\n                output_activation = TCNN.Activation.None_\n            else:\n                raise ValueError(f\"Unknown activation function: {output_activation}\")\n\n            return TCNN(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n        else:\n            layers = []\n            for _ in range(n_layers - 1):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                layers.append(nn.ReLU())\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            if output_"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            from tinycudann.torch import TCNN\n\n            if n_neurons > 1024:\n                raise NotImplementedError(\"Tinycudann only supports up to 1024 neurons\")\n\n            if n_layers == 1:\n                return TCNN(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_neurons=n_neurons,\n                    n_layers=1,\n                    activation=activation,\n                    output_activation=output_activation,\n                    seed=self._get_seed(),\n                )\n\n            skip_layers = [\n                TCNN(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_neurons,\n                    n_neurons=n_neurons,\n                    n_layers=1,\n                    activation=activation,\n                    output_activation=\"None\",\n                    seed=self._get_seed(),\n                )\n            ]\n\n            for _ in range(n_layers - 2):\n                skip_layers.append(\n                    TCNN(\n                        n_input_dims=n_neurons,\n                        n_output_dims=n_neurons,\n                        n_neurons=n_neurons,\n                        n_layers=1,\n                        activation=activation,\n                        output_activation=\"None\",\n                        seed=self._get_seed(),\n                    )\n                )\n\n            return NetworkWithSkipLayers(\n                skip_layers=skip_layers,\n                output_layers=TCNN(\n                    n_input_dims=n_neurons,\n                    n_output_dims=n_output_dims,\n                    n_neurons=n_neurons,\n                    n_layers=1,\n                    activation=activation,\n                    output_activation=output_activation"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            n_neurons_per_layer = [n_neurons] * n_layers\n            network_config = tcnn.NetworkConfig(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons_per_layer,\n                activation=activation,\n                output_activation=output_activation,\n            )\n            network = tcnn.TinyCudann(network_config, self._get_seed())\n\n        else:\n            layers = []\n            for i in range(n_layers):\n                if i == 0:\n                    layers.append(nn.Linear(n_input_dims, n_neurons))\n                else:\n                    layers.append(nn.Linear(n_neurons, n_neurons))\n\n                if i < n_layers - 1:\n                    if activation == \"ReLU\":\n                        layers.append(nn.ReLU())\n                    elif activation == \"None\":\n                        pass\n                    else:\n                        raise ValueError(f\"Unknown activation: {activation}\")\n\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n            elif output_activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Unknown activation: {output_activation}\")\n\n            network = nn.Sequential(*layers)\n\n        return network"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            from tinycudann.layers import Dense\n            from tinycudann.activation import ReLU, Sigmoid\n\n            if n_neurons > 1024:\n                n_neurons = 1024\n\n            layers = [\n                Dense(\n                    n_input_dims,\n                    n_neurons,\n                    activation=ReLU(self._get_seed()),\n                    seed=self._get_seed(),\n                )\n            ]\n\n            for _ in range(n_layers - 2):\n                layers.append(\n                    Dense(\n                        n_neurons,\n                        n_neurons,\n                        activation=ReLU(self._get_seed()),\n                        seed=self._get_seed(),\n                    )\n                )\n\n            if n_layers > 1:\n                layers.append(\n                    Dense(\n                        n_neurons,\n                        n_output_dims,\n                        activation=Sigmoid(self._get_seed())\n                        if output_activation == \"Sigmoid\"\n                        else None,\n                        seed=self._get_seed(),\n                    )\n                )\n\n            return NetworkWithSkipLayers(layers[:-1], layers[-1])\n\n        else:\n            activation_dict = {\n                \"ReLU\": nn.ReLU,\n                \"Sigmoid\": nn.Sigmoid,\n                \"None\": nn.Identity,\n            }\n\n            layers = []\n            if n_layers > 1:\n                layers.append(nn.Linear(n_input_dims, n_neurons))\n                layers.append(activation_dict[activation]())\n\n            for _ in range(n_layers - 2):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                layers.append(activation_dict[activation]())\n\n            if n_layers > 1:\n                layers.append(nn.Linear(n_ne"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n            from tinycudann.activation import Activation\n\n            # Construct the network using tinycudann\n            model = tcnn.NetworkWithInputEncoding(\n                n_input_dims=n_input_dims,\n                encoding_config={\n                    \"otype\": \"Grid\",\n                    \"n_levels\": 1,\n                    \"n_features_per_level\": 2,\n                    \"periodic_input_dims\": [],\n                },\n            )\n\n            for _ in range(n_layers):\n                model.add_layer(\n                    tcnn.Layer(\n                        n_neurons=n_neurons,\n                        activation=Activation(activation),\n                        otype=\"None\",\n                    )\n                )\n\n            model.add_layer(\n                tcnn.Layer(\n                    n_neurons=n_output_dims,\n                    activation=Activation(output_activation),\n                    otype=\"None\",\n                )\n            )\n\n            return model\n        else:\n            # Construct the network using PyTorch\n            layers = []\n            for _ in range(n_layers):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                layers.append(nn.ReLU())\n\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n\n            if output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n\n            return nn.Sequential(*layers)\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            from tinycudann.torch import TinyCudann\n            from tinycudann.torch.layers import Linear, ReLU\n\n            if n_neurons > 1024:\n                return TinyCudann.Linear(n_input_dims, n_output_dims, bias=True)\n\n            layers = [Linear(n_input_dims, n_neurons, bias=True)]\n            for _ in range(n_layers - 1):\n                layers.append(ReLU())\n                layers.append(Linear(n_neurons, n_neurons, bias=True))\n\n            layers.append(ReLU())\n            layers.append(Linear(n_neurons, n_output_dims, bias=True))\n\n            if activation == \"ReLU\":\n                layers.append(ReLU())\n\n            if output_activation == \"Sigmoid\":\n                layers.append(TinyCudann.Sigmoid())\n\n            return NetworkWithSkipLayers(layers[:-1], layers[-1])\n\n        else:\n            if activation == \"ReLU\":\n                activation = nn.ReLU()\n            elif activation == \"None\":\n                activation = nn.Identity()\n            else:\n                raise ValueError(f\"Unknown activation {activation}\")\n\n            if output_activation == \"ReLU\":\n                output_activation = nn.ReLU()\n            elif output_activation == \"Sigmoid\":\n                output_activation = nn.Sigmoid()\n            elif output_activation == \"None\":\n                output_activation = nn.Identity()\n            else:\n                raise ValueError(f\"Unknown output activation {output_activation}\")\n\n            layers = [nn.Linear(n_input_dims, n_neurons, bias=True)]"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            from tinycudann.torch import TinyShuffleNet\n            from tinycudann.torch import TinyShuffleNetV2\n\n            if n_neurons < 2048:\n                return TinyShuffleNet(\n                    num_classes=n_output_dims,\n                    num_input_channels=n_input_dims,\n                    num_layers=n_layers,\n                    num_filters=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                    seed=self._get_seed(),\n                )\n            else:\n                return TinyShuffleNetV2(\n                    num_classes=n_output_dims,\n                    num_input_channels=n_input_dims,\n                    num_layers=n_layers,\n                    num_filters=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                    seed=self._get_seed(),\n                )\n        else:\n            layers = []\n            activation_fn = nn.ReLU if activation == \"ReLU\" else nn.Identity\n            output_activation_fn = (\n                nn.ReLU if output_activation == \"ReLU\" else nn.Identity\n            )\n\n            for i in range(n_layers - 1):\n                layers.append(nn.Linear(n_input_dims, n_neurons))\n                layers.append(activation_fn())\n                n_input_dims = n_neurons\n\n            layers.append(nn.Linear(n_input_dims, n_output_dims))\n            layers.append(output_activation_fn())\n\n            return nn.Sequential(*layers)\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            from tinycudann.layers import Linear, ReLU\n\n            if n_neurons > 1000:\n                from tinycudann.layers import GELU\n                activation = \"GELU\"\n\n            layers = []\n            if activation == \"ReLU\":\n                layers.append(ReLU(n_input_dims, n_neurons, seed=self._get_seed()))\n            elif activation == \"GELU\":\n                layers.append(GELU(n_input_dims, n_neurons, seed=self._get_seed()))\n            elif activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Unknown activation function: {activation}\")\n\n            for _ in range(n_layers - 2):\n                layers.append(ReLU(n_neurons, n_neurons, seed=self._get_seed()))\n\n            if n_layers > 1:\n                layers.append(Linear(n_neurons, n_output_dims, seed=self._get_seed()))\n\n            if output_activation == \"ReLU\":\n                layers.append(ReLU(n_output_dims, n_output_dims, seed=self._get_seed()))\n            elif output_activation == \"Sigmoid\":\n                layers.append(Linear(n_output_dims, n_output_dims, seed=self._get_seed()))\n            elif output_activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Unknown output activation function: {output_activation}\")\n\n            return NetworkWithSkipLayers(layers[:-1], layers[-1])\n\n        else:\n            layers = []\n            if activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif activation == \"None\":\n                pass"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 1024:\n                return self.get_tinycudann_network(\n                    n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation\n                )\n            else:\n                return self.get_pytorch_network(\n                    n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation\n                )\n        else:\n            return self.get_pytorch_network(\n                n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation\n            )\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            if n_neurons > 2048:\n                n_neurons = 2048\n\n            if n_neurons > 1024:\n                n_layers = 2\n            elif n_neurons > 512:\n                n_layers = 3\n            elif n_neurons > 256:\n                n_layers = 4\n\n            activation_function = getattr(tcnn, activation)\n            output_activation_function = getattr(tcnn, output_activation)\n\n            model = tcnn.NetworkWithInputEncoding(\n                n_input_dims=n_input_dims,\n                encoding_config={\n                    \"otype\": \"Grid\",\n                    \"n_steps\": n_neurons,\n                    \"ndim_coding\": n_neurons,\n                    \"periodic_input_dims\": [],\n                    \"input_dim\": n_input_dims,\n                    \"n_neurons\": n_neurons,\n                    \"n_layers\": n_layers,\n                    \"kernel_size\": 3,\n                    \"stride\": 1,\n                    \"padding\": 1,\n                    \"use_bias\": True,\n                    \"activation\": activation_function,\n                    \"output_activation\": output_activation_function,\n                },\n            )\n        else:\n            layers = []\n\n            for i in range(n_layers):\n                if i == 0:\n                    layers.append(nn.Linear(n_input_dims, n_neurons))\n                else:\n                    layers.append(nn.Linear(n_neurons, n_neurons))\n\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n\n            if output_activation == \"ReLU\":\n               "}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        kernel_size = 2 * kernel_offset + 1\n        signal_length = len(signal)\n        signal_padded = np.concatenate((signal[-kernel_offset:], signal, signal[:kernel_offset]))\n        signal_median = np.zeros(signal_length)\n\n        for i in range(signal_length):\n            signal_median[i] = np.median(signal_padded[i : i + kernel_size])\n\n        return signal_median[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Shift the signal by a range defined by the kernel offset\n        shifted_signals = [signal[i:] for i in range(kernel_offset)] + [signal] + [signal[:-i] for i in range(1, kernel_offset + 1)]\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the resulting median array to account for edge effects\n        trimmed_median = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        signal_len = len(signal)\n        signal_shifted = np.roll(signal, np.arange(-kernel_offset, kernel_offset + 1))\n        signal_median = np.median(signal_shifted, axis=1)\n\n        signal_median_trimmed = signal_median[kernel_offset:-kernel_offset]\n\n        return signal_median_trimmed"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal array\n        signal_length = len(signal)\n\n        # Calculate the length of the trimmed signal array\n        trimmed_signal_length = signal_length - 2 * kernel_offset\n\n        # Initialize an empty array to store the trimmed rolling median\n        trimmed_rolling_median = np.zeros(trimmed_signal_length)\n\n        # Loop over the trimmed signal array\n        for i in range(trimmed_signal_length):\n            # Calculate the start and end indices for the shifted signal\n            start_index = i\n            end_index = i + 2 * kernel_offset + 1\n\n            # Extract the shifted signal from the original signal array\n            shifted_signal = signal[start_index:end_index]\n\n            # Compute the median of the shifted signal\n            trimmed_rolling_median[i] = np.median(shifted_signal)\n\n        # Return the trimmed rolling median\n        return trimmed_rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        kernel_size = 2 * kernel_offset + 1\n\n        # Shift the signal by the kernel offset to the left and right\n        signal_left_shifted = np.roll(signal, kernel_offset)\n        signal_right_shifted = np.roll(signal, -kernel_offset)\n\n        # Compute the rolling median by taking the median of the shifted signals\n        rolling_median = np.median(np.vstack((signal_left_shifted, signal_right_shifted)), axis=0)\n\n        # Trim the rolling median to account for edge effects introduced by the shifting\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Pad the signal array with zeros on both ends to account for edge effects\n        padded_signal = np.pad(signal, (kernel_offset, kernel_offset), mode=\"constant\", constant_values=0)\n\n        # Compute the shifted versions of the signal array\n        shifted_signals = np.array([padded_signal[i : i + 2 * kernel_offset + 1] for i in range(len(signal))])\n\n        # Compute the rolling median of the shifted signals\n        median_signals = np.median(shifted_signals, axis=1)\n\n        # Trim the median signals to remove edge effects\n        trimmed_median_signals = median_signals[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signals"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        kernel_size = 2 * kernel_offset + 1\n        signal_length = len(signal)\n        median_length = signal_length - 2 * kernel_offset\n        signal_padded = np.pad(signal, (kernel_offset, kernel_offset), mode='reflect')\n        rolling_median = np.zeros(signal_length)\n\n        for i in range(median_length):\n            rolling_median[i] = np.median(signal_padded[i:i + kernel_size])\n\n        for i in range(median_length, signal_length):\n            rolling_median[i] = np.median(signal_padded[i - kernel_offset:i + kernel_offset + 1])\n\n        return rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Generate shifted versions of the signal\n        shifted_signals = [signal[i:i + len(signal)] for i in range(kernel_offset + 1)]\n        shifted_signals += [signal[i - kernel_offset:i] for i in range(kernel_offset + 1, len(signal))]\n\n        # Compute the median of each shifted signal\n        median_signals = [np.median(shifted_signal) for shifted_signal in shifted_signals]\n\n        # Trim the median signals to remove edge effects\n        trimmed_median_signals = median_signals[kernel_offset:-kernel_offset]\n\n        return np.array(trimmed_median_signals)"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements in the signal\n        n = len(signal)\n\n        # Create a new array to store the shifted signals\n        shifted_signals = np.zeros((2 * kernel_offset + 1, n))\n\n        # Shift the signal by kernel_offset elements to the left and right\n        shifted_signals[kernel_offset] = signal\n        shifted_signals[:kernel_offset] = np.roll(signal, kernel_offset)\n        shifted_signals[kernel_offset + 1 :] = np.roll(signal, -kernel_offset)\n\n        # Compute the rolling median of the shifted signals\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to remove edge effects\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the total number of elements in the signal array\n        num_elements = len(signal)\n\n        # Calculate the total number of elements in the kernel\n        num_kernel_elements = 2 * kernel_offset + 1\n\n        # Initialize an empty array to store the rolling median\n        rolling_median = np.zeros(num_elements)\n\n        # Iterate over the signal array, shifting the kernel by kernel_offset elements at each iteration\n        for i in range(num_elements):\n            # Calculate the start and end indices for the current kernel shift\n            start_index = max(0, i - kernel_offset)\n            end_index = min(num_elements, i + kernel_offset + 1)\n\n            # Extract the shifted signal from the original signal array\n            shifted_signal = signal[start_index:end_index]\n\n            # Calculate the median of the shifted signal\n            rolling_median[i] = np.median(shifted_signal)\n\n        # Trim the rolling median array to remove edge effects introduced by the shifting process\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the length of the signal and the number of shifts required\n        signal_length = len(signal)\n        num_shifts = signal_length - 2 * kernel_offset\n\n        # Create an empty array to store the shifted signals\n        shifted_signals = np.zeros((num_shifts, 2 * kernel_offset + 1))\n\n        # Shift the signal by a range defined by the kernel offset and store the shifted signals in the shifted_signals array\n        for i in range(num_shifts):\n            shifted_signals[i] = np.concatenate((signal[i:i + kernel_offset], signal[i + kernel_offset:], signal[:kernel_offset]))\n\n        # Compute the rolling median of the shifted signals along the first axis (axis=0)\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to account for edge effects introduced by the shifting process\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Create a copy of the input signal\n        signal_copy = signal.copy()\n\n        # Compute the number of elements to shift the signal by\n        shift_length = kernel_offset * 2\n\n        # Create a list to store the shifted signals\n        shifted_signals = []\n\n        # Shift the signal by the specified offset and append the shifted signal to the list\n        for i in range(shift_length):\n            shifted_signal = np.roll(signal_copy, i + 1)\n            shifted_signals.append(shifted_signal)\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median signal to remove edge effects\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements in the signal array\n        n = len(signal)\n\n        # Calculate the total number of elements in the kernel\n        kernel_size = 2 * kernel_offset + 1\n\n        # Initialize the output array with zeros\n        result = np.zeros(n)\n\n        # Loop over the input signal array\n        for i in range(n):\n            # Calculate the start and end indices for the kernel\n            start = max(0, i - kernel_offset)\n            end = min(n, i + kernel_offset + 1)\n\n            # Extract the kernel from the input signal\n            kernel = signal[start:end]\n\n            # Calculate the median of the kernel\n            result[i] = np.median(kernel)\n\n        # Trim the result array to remove edge effects\n        result = result[kernel_offset:-kernel_offset]\n\n        return result"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the kernel size\n        signal_length = len(signal)\n        kernel_size = 2 * kernel_offset + 1\n\n        # Initialize the output array with zeros\n        output = np.zeros_like(signal)\n\n        # Calculate the trimmed length of the output array\n        trimmed_length = signal_length - 2 * kernel_offset\n\n        # Iterate over the signal and compute the rolling median\n        for i in range(trimmed_length):\n            # Get the shifted signals\n            shifted_signals = signal[i : i + kernel_size]\n            # Calculate the median of the shifted signals\n            output[i + kernel_offset] = np.median(shifted_signals)\n\n        # Trim the output array to remove edge effects\n        output = output[kernel_offset : -kernel_offset]\n\n        return output"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the total number of elements in the signal\n        n = len(signal)\n\n        # Compute the total number of shifted signals that will be generated\n        n_shifts = n - 2 * kernel_offset\n\n        # Initialize an empty array to store the shifted signals\n        shifted_signals = np.zeros((n_shifts, 2 * kernel_offset + 1))\n\n        # Generate the shifted signals by shifting the input signal by the kernel offset\n        for i in range(n_shifts):\n            shifted_signals[i] = np.roll(signal, i + kernel_offset)\n\n        # Calculate the rolling median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=1)\n\n        # Trim the median signal to account for edge effects\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the number of elements to shift\n        signal_length = len(signal)\n        shift_length = 2 * kernel_offset\n\n        # Initialize the output array with NaN values\n        median_signal = np.empty(signal_length)\n        median_signal[:] = np.nan\n\n        # Loop through the signal and calculate the rolling median for each element\n        for i in range(shift_length, signal_length - shift_length):\n            # Get the shifted signal\n            shifted_signal = np.concatenate((signal[i - shift_length : i], signal[i + 1 : i + shift_length + 1]))\n\n            # Calculate the rolling median\n            median_signal[i] = np.median(shifted_signal)\n\n        # Trim the median signal to remove edge effects\n        median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return median_signal"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements in the signal array\n        n = len(signal)\n\n        # Calculate the number of elements in the kernel (2 * kernel_offset + 1)\n        k = 2 * kernel_offset + 1\n\n        # Initialize an empty array to store the rolling median values\n        median = np.zeros(n)\n\n        # Calculate the rolling median for each element in the signal\n        for i in range(n):\n            # Calculate the start and end indices for the kernel window\n            start = max(0, i - kernel_offset)\n            end = min(n, i + kernel_offset + 1)\n\n            # Extract the signal values within the kernel window\n            kernel = signal[start:end]\n\n            # Calculate the median of the kernel window\n            median[i] = np.median(kernel)\n\n        # Trim the median array to remove edge effects\n        median = median[kernel_offset:-kernel_offset]\n\n        return median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements in the signal\n        n = len(signal)\n\n        # Calculate the number of elements to be trimmed from the beginning and end of the median array\n        trim_start = kernel_offset\n        trim_end = kernel_offset\n\n        # Calculate the total number of elements to be trimmed from the median array\n        trim_total = trim_start + trim_end\n\n        # Calculate the number of elements to be shifted by the kernel offset\n        shift_length = n - 2 * kernel_offset\n\n        # Initialize an empty array to store the shifted signals\n        shifted_signals = np.zeros((shift_length, n))\n\n        # Shift the signal by the kernel offset and store the shifted signals in the shifted_signals array\n        for i in range(shift_length):\n            shifted_signals[i] = np.roll(signal, i - kernel_offset)\n\n        # Calculate the rolling median of the shifted signals\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to remove edge effects introduced by the shifting process\n        rolling_median = rolling_median[trim_start:-trim_end]\n\n        return rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the number of elements to shift\n        signal_length = len(signal)\n        shift_length = 2 * kernel_offset\n\n        # Generate a shifted version of the signal by concatenating the input signal with itself shifted by kernel_offset elements to the left and kernel_offset elements to the right\n        shifted_signal = np.concatenate((signal[-kernel_offset:], signal, signal[:kernel_offset]))\n\n        # Initialize an empty array to store the rolling median\n        rolling_median = np.zeros(signal_length)\n\n        # Loop over the signal and compute the rolling median at each point\n        for i in range(signal_length):\n            # Extract the shifted signal window centered at the current point\n            window = shifted_signal[i : i + shift_length + 1]\n            # Compute the median of the window and assign it to the corresponding element in the rolling median array\n            rolling_median[i] = np.median(window)\n\n        # Trim the rolling median array to remove edge effects introduced by the shifting process\n        trimmed_rolling_median = rolling_median[kernel_offset : -kernel_offset]\n\n        return trimmed_rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the number of elements in the signal array\n        num_elements = len(signal)\n\n        # Calculate the number of elements in the shifted signal array\n        num_shifted_elements = num_elements + 2 * kernel_offset\n\n        # Initialize an empty array to store the shifted signal\n        shifted_signal = np.empty(num_shifted_elements)\n\n        # Shift the signal to the left by kernel_offset elements\n        shifted_signal[kernel_offset:-kernel_offset] = signal\n\n        # Fill the left and right ends with the first and last elements of the signal, respectively\n        shifted_signal[:kernel_offset] = signal[0]\n        shifted_signal[-kernel_offset:] = signal[-1]\n\n        # Initialize an empty array to store the rolling median\n        rolling_median = np.empty(num_elements)\n\n        # Loop through each element of the signal\n        for i in range(num_elements):\n            # Get the shifted signal for the current element\n            shifted_signal_slice = shifted_signal[i:i + 2 * kernel_offset + 1]\n\n            # Compute the median of the shifted signal\n            rolling_median[i] = np.median(shifted_signal_slice)\n\n        # Return the trimmed rolling median array\n        return rolling_median[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Template codesize mismatch\")\n\n    if template_probe.codes"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the templates have the same iris code size\n    if template_probe.iris_code_size != template_gallery.iris_code_size:\n        raise MatcherError(\"Templates have different iris code sizes\")\n\n    # Check if the templates have the same iris code size\n    if template_probe.iris_code_size != template_gallery.iris_code_size:\n        raise MatcherError(\"Templates have different iris code sizes\")\n\n    # Check if the templates have the same iris code size\n    if template_probe.iris_code_size != template_gallery.iris_code_size:\n        raise MatcherError(\"Templates have different iris code sizes\")\n\n    # Check if the templates have the same iris code size\n    if template_probe.iris_code_size != template_gallery.iris_code_size:\n        raise MatcherError(\"Templates have different iris code sizes\")\n\n    # Check if the templates have the same iris code size\n    if template_probe.iris_code_size != template_gallery.iris_code_size:\n        raise MatcherError(\"Templates have different iris code sizes\")\n\n    # Check if the templates have the same iris code size\n    if template_probe.iris_code_size != template_gallery.iris_code_size:\n        raise MatcherError(\"Templates have different iris code sizes\")\n\n    # Check if the templates have the same iris code size\n    if template_probe.iris_code_size != template_gallery.iris_code_size:\n        raise MatcherError(\"Templates have different iris code sizes\")\n\n    # Check if the templates have the same iris code size\n    if template_probe.iris_code_size != template_gallery.iris_code_size:\n        raise MatcherError(\"Templates have different iris code sizes\")\n\n    # Check if the templates have the same iris code size\n    if template_probe.iris_code_size != template_gallery.ir"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iris_code.shape[0] != template_gallery.iris_code.shape[0]:\n        raise MatcherError(\"Iris codes must have the same number of rows\")\n\n    if template_probe.iris_code.shape[1] != template_gallery.iris_code.shape[1]:\n        raise MatcherError(\"Iris codes must have the same number of columns\")\n\n    if template_probe.iris_code.shape[2] != template_gallery.iris_code.shape[2]:\n        raise MatcherError(\"Iris codes must have the same number of channels\")\n\n    if template_probe.mask_code.shape[0] != template_gallery.mask_code.shape[0]:\n        raise MatcherError(\"Mask codes must have the same number of rows\")\n\n    if template_probe.mask_code.shape[1] != template_gallery.mask_code.shape[1]:\n        raise MatcherError(\"Mask codes must have the same number of columns\")\n\n    if template_probe.mask_code.shape[2] != template_gallery.mask_code.shape[2]:\n        raise MatcherError(\"Mask codes must have the same number of channels\")\n\n    if template_probe.iris_code.shape[0] != template_gallery.iris_code.shape[0]:\n        raise MatcherError(\"Iris codes must have the same number of rows\")\n\n    if template_probe.iris_code.shape[1] != template_gallery.iris_code.shape[1]:\n        raise MatcherError(\"Iris codes must have the same number of columns\")\n\n    if template_probe.iris_code.shape[2] != template_gallery.iris_code.shape[2]:\n        raise MatcherError(\"Iris codes must have the same number of channels\")\n\n    if template_probe.mask_code.shape[0] != template_gallery.mask_code.shape[0]:\n        raise MatcherError(\"Mask codes must have the same number of rows\")"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iriscodes.shape[0] != template_gallery.iriscodes.shape[0]:\n        raise MatcherError(\n            \"template_probe.iriscodes.shape[0] != template_gallery.iriscodes.shape[0]\"\n        )\n\n    if template_probe.iriscodes.shape[1] != template_gallery.iriscodes.shape[1]:\n        raise MatcherError(\n            \"template_probe.iriscodes.shape[1] != template_gallery.iriscodes.shape[1]\"\n        )\n\n    if template_probe.iriscodes.shape[2] != template_gallery.iriscodes.shape[2]:\n        raise MatcherError(\n            \"template_probe.iriscodes.shape[2] != template_gallery.iriscodes.shape[2]\"\n        )\n\n    if template_probe.iriscodes.shape[3] != template_gallery.iriscodes.shape[3]:\n        raise MatcherError(\n            \"template_probe.iriscodes.shape[3] != template_gallery.iriscodes.shape[3]\"\n        )\n\n    if template_probe.iriscodes.shape[0] != template_gallery.iriscodes.shape[0]:\n        raise MatcherError(\n            \"template_probe.iriscodes.shape[0] != template_gallery.iriscodes.shape[0]\"\n        )\n\n    if template_probe.iriscodes.shape[1] != template_gallery.iriscodes.shape[1]:\n        raise MatcherError(\n            \"template_probe.iriscodes.shape[1] != template_gallery.iriscodes.shape[1]\"\n        )\n\n    if template_probe.iriscodes.shape[2] != template_gallery.iriscodes.shape[2]:\n        raise MatcherError(\n            \"template_probe.iriscodes.shape[2] != template_gallery.iriscodes.shape[2]\"\n        )\n\n   "}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iriscodes.shape != template_gallery.iriscodes.shape:\n        raise MatcherError(\"iriscode shapes do not match\")\n\n    if template_probe.maskcodes.shape != template_gallery.maskcodes.shape:\n        raise MatcherError(\"maskcode shapes do not match\")\n\n    if template_probe.iriscodes.shape[1] % 2 != 0:\n        raise MatcherError(\"iriscode width must be even\")\n\n    if template_probe.maskcodes.shape[1] % 2 != 0:\n        raise MatcherError(\"maskcode width must be even\")\n\n    if template_probe.iriscodes.shape[1] != template_gallery.iriscodes.shape[1]:\n        raise MatcherError(\"iriscode widths do not match\")\n\n    if template_probe.maskcodes.shape[1] != template_gallery.maskcodes.shape[1]:\n        raise MatcherError(\"maskcode widths do not match\")\n\n    if template_probe.iriscodes.shape[2] != template_gallery.iriscodes.shape[2]:\n        raise MatcherError(\"iriscode heights do not match\")\n\n    if template_probe.maskcodes.shape[2] != template_gallery.maskcodes.shape[2]:\n        raise MatcherError(\"maskcode heights do not match\")\n\n    if template_probe.iriscodes.shape[0] != template_gallery.iriscodes.shape[0]:\n        raise MatcherError(\"iriscode depths do not match\")\n\n    if template_probe.maskcodes.shape[0] != template_gallery.maskcodes.shape[0]:\n        raise MatcherError(\"maskcode depths do not match\")\n\n    if template_probe.iriscodes.dtype != template_gallery.iriscodes.dtype:\n        raise MatcherError(\"iriscode dtypes do not match\")\n\n    if template_probe.maskcodes.dtype != template_gallery.maskcodes."}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iris_code.shape != template_gallery.iris_code.shape:\n        raise MatcherError(\"Iris codes are not the same size.\")\n\n    if template_probe.mask_code.shape != template_gallery.mask_code.shape:\n        raise MatcherError(\"Mask codes are not the same size.\")\n\n    if template_probe.iris_code.shape[0] != template_gallery.iris_code.shape[0]:\n        raise MatcherError(\"Iris codes are not the same number of rows.\")\n\n    if template_probe.iris_code.shape[1] != template_gallery.iris_code.shape[1]:\n        raise MatcherError(\"Iris codes are not the same number of columns.\")\n\n    if template_probe.mask_code.shape[0] != template_gallery.mask_code.shape[0]:\n        raise MatcherError(\"Mask codes are not the same number of rows.\")\n\n    if template_probe.mask_code.shape[1] != template_gallery.mask_code.shape[1]:\n        raise MatcherError(\"Mask codes are not the same number of columns.\")\n\n    if template_probe.mask_code.shape[2] != template_gallery.mask_code.shape[2]:\n        raise MatcherError(\"Mask codes are not the same number of channels.\")\n\n    if template_probe.iris_code.shape[2] != template_gallery.iris_code.shape[2]:\n        raise MatcherError(\"Iris codes are not the same number of channels.\")\n\n    if template_probe.iris_code.shape[2] != weights[0].shape[2]:\n        raise MatcherError(\"Iris codes are not the same number of channels.\")\n\n    if template_probe.mask_code.shape[2] != weights[0].shape[2]:\n        raise MatcherError(\"Mask codes are not the same number of channels.\")\n\n    if template_probe.iris_code.shape[0] != weights[0].shape[0]:\n        raise"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the templates are the same\n    if template_probe.iris_id == template_gallery.iris_id:\n        raise MatcherError(\"Template probe and template gallery are the same\")\n\n    # Check if the templates have the same size\n    if template_probe.iris_size != template_gallery.iris_size:\n        raise MatcherError(\"Template probe and template gallery have different sizes\")\n\n    # Check if the templates have the same number of irisbits\n    if template_probe.iris_bits != template_gallery.iris_bits:\n        raise MatcherError(\"Template probe and template gallery have different number of irisbits\")\n\n    # Check if the templates have the same number of maskbits\n    if template_probe.mask_bits != template_gallery.mask_bits:\n        raise MatcherError(\"Template probe and template gallery have different number of maskbits\")\n\n    # Check if the templates have the same number of irisbits\n    if template_probe.iris_bits != template_gallery.iris_bits:\n        raise MatcherError(\"Template probe and template gallery have different number of irisbits\")\n\n    # Check if the templates have the same number of maskbits\n    if template_probe.mask_bits != template_gallery.mask_bits:\n        raise MatcherError(\"Template probe and template gallery have different number of maskbits\")\n\n    # Check if the templates have the same number of iriscodes\n    if template_probe.iris_codes != template_gallery.iris_codes:\n        raise MatcherError(\"Template probe and template gallery have different number of iriscodes\")\n\n    # Check if the templates have the same number of maskcodes\n    if template_probe.mask_codes != template_gallery.mask_codes:\n        raise MatcherError(\"Template probe and template gallery have different number of maskcodes\")\n\n    # Check if the templates have the same number of iriscodes\n    if template_probe.iris_codes != template_gallery.iris_codes:\n        raise MatcherError(\"Template probe and template gallery"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights:\n        if len(weights) != 2:\n            raise MatcherError(\"weights must be a list of two weights tables.\")\n        if len(weights[0]) != len(weights[1]):\n            raise MatcherError(\"weights must be a list of two weights tables with the same number of rows.\")\n        if len(weights[0][0]) != len(weights[1][0]):\n            raise MatcherError(\"weights must be a list of two weights tables with the same number of columns.\")\n        if len(weights[0][0][0]) != len(weights[1][0][0]):\n            raise MatcherError(\"weights must be a list of two weights tables with the same number of channels.\")\n\n    if template_probe.irisbits.shape != template_gallery.irisbits.shape:\n        raise MatcherError(\"irisbits must have the same shape.\")\n\n    if template_probe.maskbits.shape != template_gallery.maskbits.shape:\n        raise MatcherError(\"maskbits must have the same shape.\")\n\n    if template_probe.irisbits.shape[0] != template_gallery.irisbits.shape[0]:\n        raise MatcherError(\"irisbits must have the same number of channels.\")\n\n    if template_probe.maskbits.shape[0] != template_gallery.maskbits.shape[0]:\n        raise MatcherError(\"maskbits must have the same number of channels.\")\n\n    if template_probe.irisbits.shape[1] != template_gallery.irisbits.shape[1]:\n        raise MatcherError(\"irisbits must have the same number of rows.\")\n\n    if template_probe.maskbits.shape[1] != template_gallery.maskbits.shape[1]:\n        raise MatcherError(\"maskbits must have the same number of rows.\")\n\n    if template_probe.irisbits.shape[2] != template_gallery.irisbits.shape[2]:\n        raise MatcherError(\"irisbits must have the same number of columns.\")"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.shape != template_gallery.shape:\n        raise MatcherError(\"Templates must have the same shape.\")\n\n    if weights:\n        if len(weights) != template_probe.shape[0]:\n            raise MatcherError(\"Number of weights must match the number of iris codes.\")\n        for weight in weights:\n            if weight.shape != template_probe.shape:\n                raise MatcherError(\"Weights must have the same shape as the iris templates.\")\n\n    if nm_dist:\n        if nm_dist < 0:\n            raise MatcherError(\"Nonmatch distance must be positive.\")\n\n    if rotation_shift < 0:\n        raise MatcherError(\"Rotation shift must be positive.\")\n\n    if weights:\n        sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n            template_probe.shape[1], template_probe.half_width, weights\n        )\n    else:\n        sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n            template_probe.shape[1], template_probe.half_width\n        )\n\n    irisbits_top = np.roll(template_probe.irisbits, rotation_shift, axis=1)\n    irisbits_bot = np.roll(template_probe.irisbits, -rotation_shift, axis=1)\n\n    if weights:\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits_top, template_gallery.maskbits, template_gallery.half_width, weights\n        )\n    else:\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits_top, template_gallery.maskbits, template_gallery.half_width\n        )\n\n    if nm_dist:\n        norm_HD_top"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.code_size != template_gallery.code_size:\n        raise MatcherError(\n            f\"Template code sizes don't match. Probe: {template_probe.code_size} Gallery: {template_gallery.code_size}\"\n        )\n\n    if template_probe.code_width != template_gallery.code_width:\n        raise MatcherError(\n            f\"Template code widths don't match. Probe: {template_probe.code_width} Gallery: {template_gallery.code_width}\"\n        )\n\n    if template_probe.code_height != template_gallery.code_height:\n        raise MatcherError(\n            f\"Template code heights don't match. Probe: {template_probe.code_height} Gallery: {template_gallery.code_height}\"\n        )\n\n    if template_probe.code_size * 3 // 4 != template_gallery.code_size * 3 // 4:\n        raise MatcherError(\n            f\"Template code sizes don't match. Probe: {template_probe.code_size * 3 // 4} Gallery: {template_gallery.code_size * 3 // 4}\"\n        )\n\n    if template_probe.code_width * 3 // 4 != template_gallery.code_width * 3 // 4:\n        raise MatcherError(\n            f\"Template code widths don't match. Probe: {template_probe.code_width * 3 // 4} Gallery: {template_gallery.code_width * 3 // 4}\"\n        )\n\n    if template_probe.code_height * 3 // 4 != template_gallery.code_height * 3 // 4:\n        raise MatcherError(\n            f\"Template code heights don't match. Probe: {template_probe.code_height * 3 // 4} Gallery: {template_gallery.code_height * 3 // 4}\"\n        )\n\n    if weights:\n        for w in weights:\n            if w."}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iriscodes.shape != template_gallery.iriscodes.shape:\n        raise MatcherError(\"Templates must have the same shape\")\n\n    if template_probe.iriscodes.shape[0] != 3:\n        raise MatcherError(\"Templates must have 3 channels\")\n\n    if weights and len(weights) != template_probe.iriscodes.shape[0]:\n        raise MatcherError(\"Weights must have the same number of channels as the templates\")\n\n    if weights and any(w.shape != template_probe.iriscodes.shape for w in weights):\n        raise MatcherError(\"Weights must have the same shape as the templates\")\n\n    if weights and any(np.any(w < 0) or np.any(w > 1) for w in weights):\n        raise MatcherError(\"Weights must be between 0 and 1\")\n\n    if weights and any(np.sum(w) != 1 for w in weights):\n        raise MatcherError(\"Weights must sum to 1\")\n\n    if weights and any(np.sum(w, axis=1) != 1 for w in weights):\n        raise MatcherError(\"Weights must sum to 1 in each row\")\n\n    if weights and any(np.sum(w, axis=0) != 1 for w in weights):\n        raise MatcherError(\"Weights must sum to 1 in each column\")\n\n    if weights and any(np.sum(w, axis=2) != 1 for w in weights):\n        raise MatcherError(\"Weights must sum to 1 in each channel\")\n\n    if weights and any(np.sum(w, axis=3) != 1 for w in weights):\n        raise MatcherError(\"Weights must sum to 1 in each pixel\")\n\n    if weights and any(np.sum(w, axis=4) != 1 for w in weights):\n        raise MatcherError(\"Weights must sum to 1 in each iriscode\")\n\n    if weights and any(np.sum(w, axis=5) != 1 for w in weights):"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    irisbits_top = template_probe.irisbits_top\n    irisbits_bot = template_probe.irisbits_bot\n    maskbits_top = template_gallery.maskbits_top\n    maskbits_bot = template_gallery.maskbits_bot\n    half_width = template_gallery.half_width\n\n    if weights:\n        if len(weights) != 2:\n            raise MatcherError(\"Weighted Hamming distance requires 2 weights tables.\")\n        if len(half_width) != 2:\n            raise MatcherError(\"Weighted Hamming distance requires 2 half widths.\")\n\n    if nm_dist:\n        sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n            template_probe.total_codesize, half_width, weights\n        )\n\n    if weights:\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits_top, maskbits_top, half_width, weights\n        )\n    else:\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits_top, maskbits_top, half_width\n        )\n\n    if weights:\n        norm_HD_top = normalized_HD(\n            irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist\n        )\n        norm_HD_bot = normalized_HD(\n            irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist\n        )\n    else:\n        norm_HD_top = irisbitcount_top / maskbitcount_top\n        norm_HD_bot = irisbitcount_bot / maskbitcount_bot\n\n    min_HD = min(norm_HD_top, norm_HD_bot)\n    if min_HD == norm_HD_top:\n        return min"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights:\n        if not len(weights) == len(template_probe.irisbits):\n            raise MatcherError(\"Number of weights is not equal to number of irisbits\")\n        if not len(weights) == len(template_gallery.irisbits):\n            raise MatcherError(\"Number of weights is not equal to number of irisbits\")\n\n        for weight in weights:\n            if not weight.shape == template_probe.irisbits[0].shape:\n                raise MatcherError(\"Shape of weight is not equal to shape of irisbits\")\n\n        if not len(template_probe.irisbits) == len(template_gallery.irisbits):\n            raise MatcherError(\"Number of irisbits is not equal to number of irisbits\")\n\n    if not len(template_probe.irisbits) == len(template_gallery.irisbits):\n        raise MatcherError(\"Number of irisbits is not equal to number of irisbits\")\n\n    if not len(template_probe.irisbits) == len(template_gallery.maskbits):\n        raise MatcherError(\"Number of maskbits is not equal to number of irisbits\")\n\n    if not len(template_probe.irisbits) == len(template_gallery.half_width):\n        raise MatcherError(\"Number of half_width is not equal to number of irisbits\")\n\n    if not len(template_probe.irisbits) == len(template_gallery.irisbits[0]):\n        raise MatcherError(\"Number of irisbits is not equal to number of irisbits\")\n\n    if not len(template_probe.irisbits) == len(template_gallery.maskbits[0]):\n        raise MatcherError(\"Number of maskbits is not equal to number of irisbits\")\n\n    if not len(template_probe.irisbits) == len(template_gallery.half_width[0]):\n        raise MatcherError(\"Number of half_width is not equal to number of irisbits\")\n\n    if not len(template_probe.irisbits[0])"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if not isinstance(template_probe, IrisTemplate):\n        raise MatcherError(\"template_probe must be an instance of IrisTemplate\")\n    if not isinstance(template_gallery, IrisTemplate):\n        raise MatcherError(\"template_gallery must be an instance of IrisTemplate\")\n    if not isinstance(rotation_shift, int):\n        raise MatcherError(\"rotation_shift must be an integer\")\n\n    if nm_dist is not None and not isinstance(nm_dist, float):\n        raise MatcherError(\"nm_dist must be a float\")\n\n    if weights is not None and not isinstance(weights, list):\n        raise MatcherError(\"weights must be a list\")\n\n    if weights is not None:\n        for weight in weights:\n            if not isinstance(weight, np.ndarray):\n                raise MatcherError(\"weights must be a list of numpy arrays\")\n\n    if rotation_shift < 0 or rotation_shift > template_probe.iriscodes.shape[1]:\n        raise MatcherError(\"rotation_shift must be between 0 and the number of columns of template_probe\")\n\n    if template_probe.iriscodes.shape != template_gallery.iriscodes.shape:\n        raise MatcherError(\"template_probe and template_gallery must have the same shape\")\n\n    if template_probe.iriscodes.ndim != 3:\n        raise MatcherError(\"template_probe must have 3 dimensions\")\n\n    if template_gallery.iriscodes.ndim != 3:\n        raise MatcherError(\"template_gallery must have 3 dimensions\")\n\n    if template_probe.iriscodes.dtype != np.bool_:\n        raise MatcherError(\"template_probe must have dtype bool\")\n\n    if template_gallery.iriscodes.dtype != np.bool_:\n        raise MatcherError(\"template_gallery must have dtype bool\")\n\n    if template_probe.maskcodes.shape != template_gallery.maskcodes.shape:\n        raise MatcherError(\"template_probe and template_gall"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights:\n        if len(weights) != len(template_probe.irisbits):\n            raise MatcherError(\n                f\"The number of weights provided ({len(weights)}) does not match the number of irisbits ({len(template_probe.irisbits)})\"\n            )\n        if any(w.shape != template_probe.irisbits[0].shape for w in weights):\n            raise MatcherError(\n                f\"The shape of the weights provided does not match the shape of the first irisbits ({template_probe.irisbits[0].shape})\"\n            )\n\n    # Calculate the total amount of sqrt bits\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        template_probe.total_codesize,\n        template_probe.half_width,\n        weights,\n    )\n\n    # Calculate the nonmatch bits\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        template_probe.irisbits,\n        template_gallery.maskbits,\n        template_probe.half_width,\n        weights,\n    )\n\n    # Calculate the Hamming distance\n    if nm_dist is not None:\n        hd_top = normalized_HD(\n            irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist\n        )\n        hd_bot = normalized_HD(\n            irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist\n        )\n    else:\n        hd_top = (irisbitcount_top - maskbitcount_top) / sqrt_totalbitcount_top\n        hd_bot = (irisbitcount_bot - maskbitcount_bot) / sqrt_totalbitcount_bot\n\n    # Find the minimum Hamming distance and corresponding rotation shift\n    hd_min = min(hd_top, hd_bot)"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iris_code.shape != template_gallery.iris_code.shape:\n        raise MatcherError(\"Iris codes must have the same shape\")\n\n    if weights:\n        if len(weights) != 2:\n            raise MatcherError(\"Weights must have 2 elements\")\n        if any(w.shape != template_probe.iris_code.shape for w in weights):\n            raise MatcherError(\"Weights must have the same shape as iris codes\")\n\n    if nm_dist is not None and nm_dist < 0:\n        raise MatcherError(\"Nonmatch distance must be positive\")\n\n    if rotation_shift < 0:\n        raise MatcherError(\"Rotation shift must be positive\")\n\n    if rotation_shift >= template_probe.iris_code.shape[1]:\n        raise MatcherError(\"Rotation shift must be less than the width of iris codes\")\n\n    if rotation_shift >= template_probe.iris_code.shape[2]:\n        raise MatcherError(\"Rotation shift must be less than the height of iris codes\")\n\n    if weights:\n        hw_top = template_probe.iris_code.shape[2] // 2\n        hw_bot = template_probe.iris_code.shape[2] - hw_top\n        half_width = [hw_top, hw_bot]\n    else:\n        half_width = [0]\n\n    irisbits = np.roll(template_probe.iris_code, rotation_shift, axis=1)\n    maskbits = template_gallery.iris_code\n\n    if weights:\n        sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n            template_probe.iris_code.size, half_width, weights\n        )\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits, maskbits, half_width, weights\n        )\n    else"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.maskbits.shape != template_gallery.maskbits.shape:\n        raise MatcherError(\"Iris templates have different shape\")\n\n    if weights and len(weights) != template_probe.maskbits.shape[0]:\n        raise MatcherError(\"Wrong number of weights\")\n\n    if nm_dist is not None and nm_dist < 0:\n        raise MatcherError(\"Nonmatch distance can not be negative\")\n\n    if rotation_shift < 0:\n        raise MatcherError(\"Rotation shift can not be negative\")\n\n    if rotation_shift > template_probe.maskbits.shape[1]:\n        raise MatcherError(\"Rotation shift can not be greater than maskbits width\")\n\n    if rotation_shift > template_gallery.maskbits.shape[1]:\n        raise MatcherError(\"Rotation shift can not be greater than maskbits width\")\n\n    if weights:\n        irisbits_probe = np.roll(template_probe.irisbits, rotation_shift, axis=1)\n        irisbits_gallery = np.roll(template_gallery.irisbits, rotation_shift, axis=1)\n    else:\n        irisbits_probe = template_probe.irisbits\n        irisbits_gallery = template_gallery.irisbits\n\n    half_width = [int(hw) for hw in template_probe.maskbits.shape[1] / 2]\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits_probe, template_gallery.maskbits, half_width, weights\n    )\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        template_probe.irisbits.size, half_width, weights\n    )\n\n    if nm_dist is None:\n        nm_dist = np.sqrt(np.sum([np.sum(np.multiply(x, y)) for x, y in zip(irisbits"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iriscodes.shape != template_gallery.iriscodes.shape:\n        raise MatcherError(\"Iris templates have different shape.\")\n\n    if template_probe.iriscodes.shape[0] != 2:\n        raise MatcherError(\"Iris templates have different shape.\")\n\n    if weights and len(weights) != template_probe.iriscodes.shape[0]:\n        raise MatcherError(\"Number of weights tables must match the number of iris codes.\")\n\n    if weights:\n        if not all(w.shape == template_probe.iriscodes.shape for w in weights):\n            raise MatcherError(\"Weights tables must have the same shape as iris codes.\")\n\n    if nm_dist is not None and nm_dist < 0:\n        raise MatcherError(\"Nonmatch distance must be non-negative.\")\n\n    if rotation_shift < 0:\n        raise MatcherError(\"Rotation shift must be non-negative.\")\n\n    half_width = [\n        template_probe.iriscodes.shape[1] // 2,\n        template_gallery.iriscodes.shape[1] // 2,\n    ]\n\n    irisbits = [\n        np.bitwise_xor(template_probe.iriscodes, np.roll(template_gallery.iriscodes, rotation_shift, axis=1))\n        for rotation_shift in range(rotation_shift + 1)\n    ]\n\n    maskbits = [\n        np.bitwise_and(template_probe.iriscodes, template_gallery.iriscodes)\n        for rotation_shift in range(rotation_shift + 1)\n    ]\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        template_probe.iriscodes.size, half_width, weights\n    )\n\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits, maskbits, half_width, weights\n   "}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Convert the rotation shift to columns\n    rotation_shift_columns = rotation_shift\n\n    # Get the iris codes from the templates\n    iris_probe = template_probe.iris_code\n    iris_gallery = template_gallery.iris_code\n\n    # Check if the iris codes have the same shape\n    if iris_probe.shape != iris_gallery.shape:\n        raise MatcherError(\"The shapes of the iris codes do not match.\")\n\n    # Get the iris code width\n    iris_code_width = iris_probe.shape[1]\n\n    # Get the half width of the iris code\n    half_width = iris_code_width // 2\n\n    # Check if the rotation shift is valid\n    if rotation_shift_columns > half_width:\n        raise MatcherError(\"The rotation shift is too large.\")\n\n    # Calculate the Hamming distance for each rotation\n    hamming_distances = []\n    for rotation_shift_columns in range(rotation_shift_columns + 1):\n        # Rotate the iris codes\n        iris_probe_rotated = np.roll(iris_probe, rotation_shift_columns, axis=1)\n        iris_gallery_rotated = np.roll(iris_gallery, rotation_shift_columns, axis=1)\n\n        # Calculate the Hamming distance\n        hamming_distance = np.count_nonzero(iris_probe_rotated != iris_gallery_rotated)\n\n        # Add the Hamming distance to the list\n        hamming_distances.append(hamming_distance)\n\n    # Find the minimum Hamming distance and the corresponding rotation shift\n    min_hamming_distance = min(hamming_distances)\n    min_hamming_distance_rotation_shift = hamming_distances.index(min_hamming_distance)\n\n    # Calculate the normalized Hamming distance\n    if nm_dist is not None:\n        # Count the square root of total bits\n        sqrt_totalbitcount, sqrt_totalbitcount_top,"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iriscodes.shape != template_gallery.iriscodes.shape:\n        raise MatcherError(\"Templates have different shape\")\n\n    if rotation_shift >= template_probe.iriscodes.shape[1]:\n        raise MatcherError(\"Rotation shift is greater than template width\")\n\n    half_width = [int(hw) for hw in np.ceil(template_probe.iriscodes.shape[1] / 2)]\n\n    if weights:\n        if len(weights) != 2:\n            raise MatcherError(\"Weights list must have 2 elements\")\n        if weights[0].shape != weights[1].shape:\n            raise MatcherError(\"Weights shapes are not equal\")\n        if weights[0].shape != template_probe.iriscodes.shape:\n            raise MatcherError(\"Weights shape is not equal to template shape\")\n\n    if nm_dist is not None:\n        if nm_dist < 0:\n            raise MatcherError(\"Nonmatch distance must be positive\")\n        if nm_dist > np.sqrt(template_probe.iriscodes.shape[1] * 3 / 4):\n            raise MatcherError(\"Nonmatch distance must be less than or equal to sqrt(3/4) of template width\")\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        template_probe.iriscodes.shape[1], half_width, weights\n    )\n\n    irisbits_top = np.roll(template_probe.iriscodes, rotation_shift, axis=1) & template_gallery.iriscodes\n    irisbits_bot = np.roll(template_probe.iriscodes, -rotation_shift, axis=1) & template_gallery.iriscodes\n\n    irisbits_top = irisbits_top[:, :half_width[0], ...]\n    irisbits_bot = irisbits_bot[:, half_width[0]:, ...]\n\n    maskbits_top = template_probe.irisc"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                first_point_index = np.random.randint(0, len(polygon))\n                second_point_index = np.random.randint(0, len(polygon))\n\n                first_point = polygon[first_point_index]\n                second_point = polygon[second_point_index]\n\n                if np.linalg.norm(first_point - second_point) > min_distance_between_sector_points_in_px:\n                    first_bisectors_point[i] = first_point\n                    second_bisectors_point[i] = second_point\n                    break\n\n            if np.linalg.norm(first_bisectors_point[i] - second_bisectors_point[i]) <= min_distance_between_sector_points_in_px:\n                raise EyeCentersEstimationError(\n                    f\"Could not find a sufficient number of point pairs that meet the distance criterion within {max_iterations} iterations.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n\n        for i in range(self.params.num_bisectors):\n            for j in range(self.params.max_iterations):\n                first_point = polygon[np.random.randint(0, len(polygon))]\n                second_point = polygon[np.random.randint(0, len(polygon))]\n\n                if np.linalg.norm(first_point - second_point) > min_distance_between_sector_points_in_px:\n                    first_bisectors_point[i] = first_point\n                    second_bisectors_point[i] = second_point\n                    break\n\n            if np.linalg.norm(first_bisectors_point[i] - second_bisectors_point[i]) < min_distance_between_sector_points_in_px:\n                raise EyeCentersEstimationError(\"Could not find a sufficient number of point pairs that meet the distance criterion.\")\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n\n        for i in range(self.params.num_bisectors):\n            for j in range(self.params.max_iterations):\n                first_point_index, second_point_index = np.random.choice(len(polygon), size=2, replace=False)\n                first_point = polygon[first_point_index]\n                second_point = polygon[second_point_index]\n\n                if np.linalg.norm(first_point - second_point) > min_distance_between_sector_points_in_px:\n                    break\n\n            if j == self.params.max_iterations - 1:\n                raise EyeCentersEstimationError(\n                    f\"Could not find a sufficient number of point pairs that meet the distance criterion within {self.params.max_iterations} iterations.\"\n                )\n\n            first_bisectors_point[i] = first_point\n            second_bisectors_point[i] = second_point\n\n        return first_bisectors_point, second_bisectors_point\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.empty((num_bisectors, 2))\n        second_bisectors_point = np.empty((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                first_point_index = np.random.randint(0, len(polygon))\n                second_point_index = np.random.randint(0, len(polygon))\n\n                if first_point_index != second_point_index:\n                    first_point = polygon[first_point_index]\n                    second_point = polygon[second_point_index]\n                    distance = np.linalg.norm(first_point - second_point)\n\n                    if distance > min_distance_between_sector_points_in_px:\n                        midpoint = (first_point + second_point) / 2\n                        perpendicular_vector = np.array([-1 * (second_point[1] - first_point[1]), second_point[0] - first_point[0]])\n                        perpendicular_vector /= np.linalg.norm(perpendicular_vector)\n                        perpendicular_vector *= distance / 2\n\n                        first_bisectors_point[i] = midpoint - perpendicular_vector\n                        second_bisectors_point[i] = midpoint + perpendicular_vector\n\n                        break\n\n            if j == max_iterations - 1:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find {num_bisectors} perpendicular bisectors within {max_iterations} iterations.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                # Choose two random points from the polygon\n                point_1 = polygon[np.random.randint(0, len(polygon))]\n                point_2 = polygon[np.random.randint(0, len(polygon))]\n\n                # Calculate the midpoint between the two points\n                midpoint = (point_1 + point_2) / 2\n\n                # Calculate the perpendicular bisector of the line segment connecting the two points\n                perpendicular_slope = -1 / ((point_2[1] - point_1[1]) / (point_2[0] - point_1[0]))\n                perpendicular_intercept = midpoint[1] - perpendicular_slope * midpoint[0]\n\n                # Calculate the intersection point of the perpendicular bisector with the polygon\n                intersection_point = self._calculate_intersection(\n                    point_1, point_2, perpendicular_slope, perpendicular_intercept\n                )\n\n                # Check if the distance between the intersection point and the midpoint is greater than the minimum distance\n                distance = np.linalg.norm(intersection_point - midpoint)\n                if distance > min_distance_between_sector_points_in_px:\n                    first_bisectors_point[i] = point_1\n                    second_bisectors_point[i] = point_2\n                    break\n\n            if j == max_iterations - 1:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find a sufficient number of point pairs ({num_bisectors}) that meet the distance criterion within {max_iterations} iterations.\"\n                )\n\n        return first_bisectors_point, second_bis"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        first_bisectors_point = []\n        second_bisectors_point = []\n        for _ in range(self.params.num_bisectors):\n            points_pair = self._get_random_points_pair(polygon, min_distance_between_sector_points_in_px)\n            if points_pair is None:\n                raise EyeCentersEstimationError(\n                    \"Could not find a sufficient number of point pairs that meet the distance criterion.\"\n                )\n            first_bisector, second_bisector = self._calculate_bisectors(points_pair)\n            first_bisectors_point.append(first_bisector)\n            second_bisectors_point.append(second_bisector)\n\n        return np.array(first_bisectors_point), np.array(second_bisectors_point)\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        num_vertices = polygon.shape[0]\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                first_point_index = np.random.randint(0, num_vertices)\n                second_point_index = np.random.randint(0, num_vertices)\n\n                first_point = polygon[first_point_index]\n                second_point = polygon[second_point_index]\n\n                if np.linalg.norm(first_point - second_point) >= min_distance_between_sector_points_in_px:\n                    first_bisectors_point[i] = first_point\n                    second_bisectors_point[i] = second_point\n                    break\n\n            if j == max_iterations - 1:\n                raise EyeCentersEstimationError(\"Failed to find a sufficient number of point pairs.\")\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                points_indices = np.random.choice(polygon.shape[0], 2, replace=False)\n                point1 = polygon[points_indices[0]]\n                point2 = polygon[points_indices[1]]\n\n                if np.linalg.norm(point1 - point2) > min_distance_between_sector_points_in_px:\n                    bisector_point = (point1 + point2) / 2\n                    bisector_vector = point2 - point1\n                    bisector_vector = bisector_vector / np.linalg.norm(bisector_vector)\n                    bisector_vector = np.array([-bisector_vector[1], bisector_vector[0]])\n                    first_bisectors_point[i] = bisector_point + bisector_vector * min_distance_between_sector_points_in_px / 2\n                    second_bisectors_point[i] = bisector_point - bisector_vector * min_distance_between_sector_points_in_px / 2\n                    break\n            else:\n                raise EyeCentersEstimationError(\"Could not find enough points for bisectors.\")\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables\n        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n        num_points = polygon.shape[0]\n\n        # Initialize arrays to store bisectors\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        # Iterate over the maximum number of iterations\n        for i in range(max_iterations):\n            # Generate random indices for the points\n            random_indices = np.random.choice(num_points, size=(num_bisectors, 2), replace=False)\n\n            # Extract the points from the polygon\n            points = polygon[random_indices]\n\n            # Calculate the perpendicular bisectors\n            bisectors = self._calculate_bisectors(points)\n\n            # Check if the distance between any two points in a pair is greater than the minimum distance\n            distances = np.linalg.norm(points[:, 0] - points[:, 1], axis=1)\n            if np.all(distances > min_distance_between_sector_points_in_px):\n                # If the distance criterion is satisfied, store the bisectors\n                first_bisectors_point = bisectors[:, 0]\n                second_bisectors_point = bisectors[:, 1]\n                break\n\n        # If the maximum number of iterations is reached and the distance criterion is not satisfied, raise an error\n        if i == max_iterations - 1:\n            raise EyeCentersEstimationError(\n                f\"Failed to find sufficient point pairs with distance greater than {min_distance_between_sector_points_in_px} in {max_iterations} iterations.\"\n            )\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        first_bisectors_point = []\n        second_bisectors_point = []\n\n        for _ in range(self.params.max_iterations):\n            first_point_index, second_point_index = self._get_random_pair_of_points_index(polygon)\n\n            if self._is_distance_between_points_greater_than_min_distance(\n                polygon, first_point_index, second_point_index, min_distance_between_sector_points_in_px\n            ):\n                first_bisectors_point.append(polygon[first_point_index])\n                second_bisectors_point.append(polygon[second_point_index])\n\n            if len(first_bisectors_point) == self.params.num_bisectors:\n                break\n\n        if len(first_bisectors_point) < self.params.num_bisectors:\n            raise EyeCentersEstimationError(\"Could not find enough bisectors\")\n\n        return np.array(first_bisectors_point), np.array(second_bisectors_point)\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables\n        first_bisectors_point = np.array([])\n        second_bisectors_point = np.array([])\n        num_iterations = 0\n\n        # Loop until the desired number of bisectors are found\n        while len(first_bisectors_point) < self.params.num_bisectors:\n            # Select two random points from the polygon\n            first_point_index = np.random.randint(0, len(polygon))\n            second_point_index = np.random.randint(0, len(polygon))\n\n            # Ensure that the selected points are not too close to each other\n            if (\n                np.linalg.norm(polygon[first_point_index] - polygon[second_point_index])\n                > min_distance_between_sector_points_in_px\n            ):\n                # Calculate the perpendicular bisector\n                first_point = polygon[first_point_index]\n                second_point = polygon[second_point_index]\n                midpoint = (first_point + second_point) / 2\n                perpendicular_vector = np.array([-second_point[1] + first_point[1], second_point[0] - first_point[0]])\n                perpendicular_vector /= np.linalg.norm(perpendicular_vector)\n                perpendicular_bisector = np.array([midpoint[0] - perpendicular_vector[1], midpoint[1] + perpendicular_vector[0]])\n\n                # Add the perpendicular bisector to the list of bisectors\n                first_bisectors_point = np.append(first_bisectors_point, perpendicular_bisector[0])\n                second_bisectors_point = np.append(second_bisectors_point, perpendicular_bisector[1])\n\n            # Increment the iteration counter\n            num_iterations += 1\n\n            # Check if the maximum number of iterations has been reached\n            if num_iterations >= self.params.max_iterations:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find {self"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of points in the polygon\n        num_points = len(polygon)\n\n        # Initialize the number of iterations and the number of pairs found\n        num_iterations = 0\n        num_pairs_found = 0\n\n        # Initialize the arrays to store the starting and ending points of the bisectors\n        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n\n        # Iterate until the required number of point pairs is found or the maximum number of iterations is reached\n        while num_pairs_found < self.params.num_bisectors and num_iterations < self.params.max_iterations:\n            # Select two random points from the polygon\n            first_point_index = np.random.randint(num_points)\n            second_point_index = np.random.randint(num_points)\n\n            # Ensure that the two selected points are not the same\n            if first_point_index == second_point_index:\n                continue\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(polygon[first_point_index] - polygon[second_point_index])\n\n            # Check if the distance is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint between the two points\n                midpoint = (polygon[first_point_index] + polygon[second_point_index]) / 2\n\n                # Calculate the normal vector to the line connecting the two points\n                normal_vector = np.array([-polygon[second_point_index][1] + polygon[first_point_index][1], polygon[second_point_index][0] - polygon[first_point_index][0]])\n\n                # Normalize the normal vector\n                normal_vector = normal_vector / np.linalg.norm(normal_vector)\n\n                # Calculate the starting point of the"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables\n        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n        num_points = polygon.shape[0]\n        bisectors_starting_points = np.zeros((num_bisectors, 2))\n        bisectors_ending_points = np.zeros((num_bisectors, 2))\n        num_iterations = 0\n\n        # Select random pairs of points\n        while num_iterations < max_iterations:\n            num_iterations += 1\n            idx1 = np.random.randint(0, num_points)\n            idx2 = np.random.randint(0, num_points)\n            point1 = polygon[idx1]\n            point2 = polygon[idx2]\n\n            # Check if the distance between the points is greater than the minimum distance\n            distance = np.linalg.norm(point1 - point2)\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the perpendicular bisector\n                midpoint = (point1 + point2) / 2\n                perpendicular_vector = np.array([-point2[1] + point1[1], point2[0] - point1[0]])\n                perpendicular_vector /= np.linalg.norm(perpendicular_vector)\n                bisector_start = midpoint - perpendicular_vector * 10000\n                bisector_end = midpoint + perpendicular_vector * 10000\n\n                # Store the bisector\n                bisectors_starting_points[num_iterations - 1] = bisector_start\n                bisectors_ending_points[num_iterations - 1] = bisector_end\n\n                # Check if we have enough bisectors\n                if num_iterations == num_bisectors:\n                    break\n\n        # If we did not find enough bisectors, raise an error\n        if num_iterations < num_bisectors:\n            raise EyeCentersEstimationError(\n                f\"Failed to find enough"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables\n        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        num_bisectors_found = 0\n        iterations = 0\n\n        # Iterate until sufficient number of bisectors are found\n        while num_bisectors_found < self.params.num_bisectors:\n            # Select two random points\n            point_a = polygon[np.random.randint(0, len(polygon))]\n            point_b = polygon[np.random.randint(0, len(polygon))]\n\n            # Check if the distance between the points is greater than the minimum distance\n            if np.linalg.norm(point_a - point_b) > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint of the line segment between the two points\n                midpoint = (point_a + point_b) / 2\n\n                # Calculate the perpendicular bisector of the line segment\n                bisector_direction = np.array([-point_b[1] + point_a[1], point_b[0] - point_a[0]])\n                bisector_direction /= np.linalg.norm(bisector_direction)\n\n                # Calculate the starting and ending points of the bisector\n                bisector_start = midpoint - bisector_direction * min_distance_between_sector_points_in_px / 2\n                bisector_end = midpoint + bisector_direction * min_distance_between_sector_points_in_px / 2\n\n                # Add the bisector to the list of bisectors\n                first_bisectors_point[num_bisectors_found] = bisector_start\n                second_bisectors_point[num_bisectors_found] = bisector_end\n                num_bisectors_found += 1\n\n            # Check if maximum number of iterations has been reached\n            iterations += 1\n            if iterations"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables\n        first_bisectors_point = np.array([])\n        second_bisectors_point = np.array([])\n        iterations = 0\n\n        # Calculate the number of pairs of points to generate\n        num_pairs = self.params.num_bisectors\n\n        # Iterate until the required number of pairs is found\n        while len(first_bisectors_point) < num_pairs:\n            # Select two random points from the polygon\n            point1_idx = np.random.randint(0, len(polygon))\n            point2_idx = np.random.randint(0, len(polygon))\n\n            # Ensure that the points are not the same\n            if point1_idx == point2_idx:\n                continue\n\n            # Calculate the distance between the points\n            distance = np.linalg.norm(polygon[point1_idx] - polygon[point2_idx])\n\n            # Check if the distance is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the perpendicular bisector of the line connecting the points\n                midpoint = (polygon[point1_idx] + polygon[point2_idx]) / 2\n                perpendicular_vector = np.array([-1 * (polygon[point1_idx][1] - polygon[point2_idx][1]), polygon[point1_idx][0] - polygon[point2_idx][0]])\n                perpendicular_vector = perpendicular_vector / np.linalg.norm(perpendicular_vector)\n                bisector_point1 = midpoint + perpendicular_vector * (distance / 2)\n                bisector_point2 = midpoint - perpendicular_vector * (distance / 2)\n\n                # Append the bisector points to the arrays\n                first_bisectors_point = np.append(first_bisectors_point, bisector_point1)\n                second_bisectors_point = np.append(second_bisectors_point, bisector_point2)\n\n            # Increment the iteration counter\n           "}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the variables\n        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n        num_points = len(polygon)\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        # Iterate until the required number of point pairs is found\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                # Choose two random points from the polygon\n                point_1_index = np.random.randint(0, num_points)\n                point_2_index = np.random.randint(0, num_points)\n\n                # Calculate the distance between the chosen points\n                distance = np.linalg.norm(polygon[point_1_index] - polygon[point_2_index])\n\n                # Check if the distance is greater than the minimum distance\n                if distance > min_distance_between_sector_points_in_px:\n                    # Calculate the midpoint between the chosen points\n                    midpoint = (polygon[point_1_index] + polygon[point_2_index]) / 2\n\n                    # Calculate the perpendicular bisector using the midpoint and the chosen points\n                    perpendicular_bisector = np.array([-1, 1]) * midpoint\n\n                    # Calculate the intersection point of the perpendicular bisector with the polygon\n                    intersection_point = self._find_intersection(\n                        polygon, perpendicular_bisector, midpoint\n                    )\n\n                    # Check if the intersection point is within the polygon\n                    if self._is_point_in_polygon(intersection_point, polygon):\n                        # Store the intersection point as the starting point of the bisector\n                        first_bisectors_point[i] = intersection_point\n\n                        # Calculate the vector from the intersection point to the midpoint\n                        vector = midpoint - intersection_point\n\n                        # Calculate the unit vector in the same direction as the vector\n                        unit_vector"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        num_vertices = polygon.shape[0]\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                random_index_1 = np.random.randint(0, num_vertices)\n                random_index_2 = np.random.randint(0, num_vertices)\n\n                while np.linalg.norm(polygon[random_index_1] - polygon[random_index_2]) < min_distance_between_sector_points_in_px:\n                    random_index_2 = np.random.randint(0, num_vertices)\n\n                first_bisectors_point[i] = polygon[random_index_1]\n                second_bisectors_point[i] = polygon[random_index_2]\n\n                if j == max_iterations - 1:\n                    raise EyeCentersEstimationError(\"Failed to find sufficient number of point pairs.\")\n\n        return first_bisectors_point, second_bisectors_point\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables\n        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n\n        # Iterate for a maximum number of iterations\n        for _ in range(self.params.max_iterations):\n            # Choose a random point from the polygon\n            random_point = polygon[np.random.randint(0, len(polygon))]\n\n            # Find the nearest point on the polygon to the random point\n            nearest_point = self._find_nearest_point(polygon, random_point)\n\n            # Calculate the distance between the random point and the nearest point\n            distance = np.linalg.norm(random_point - nearest_point)\n\n            # Check if the distance is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the perpendicular bisector of the line segment connecting the random point and the nearest point\n                bisector_start_point, bisector_end_point = self._calculate_perpendicular_bisector(\n                    random_point, nearest_point\n                )\n\n                # Add the bisector to the list of bisectors\n                first_bisectors_point = np.vstack((first_bisectors_point, bisector_start_point))\n                second_bisectors_point = np.vstack((second_bisectors_point, bisector_end_point))\n\n                # Check if the number of bisectors is sufficient\n                if len(first_bisectors_point) >= self.params.num_bisectors:\n                    # Return the first and second bisectors points\n                    return first_bisectors_point[1:], second_bisectors_point[1:]\n\n        # Raise an exception if the number of bisectors is not sufficient\n        raise EyeCentersEstimationError(\n            f\"Failed to find sufficient number of point pairs ({self.params.num_bisectors}) that meet the distance criterion within {self"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Generate a list of random indices for the polygon vertices\n        random_indices = np.random.choice(polygon.shape[0], size=num_bisectors, replace=False)\n\n        # Initialize variables to store the first and second bisectors\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        # Iterate over the random indices\n        for i, idx in enumerate(random_indices):\n            # Initialize variables to store the best pair of points and their distance\n            best_pair = None\n            best_distance = np.inf\n\n            # Iterate over the maximum number of iterations\n            for j in range(max_iterations):\n                # Choose a random index for the second point\n                idx2 = np.random.choice(polygon.shape[0])\n\n                # Calculate the distance between the two points\n                distance = np.linalg.norm(polygon[idx] - polygon[idx2])\n\n                # If the distance is greater than the minimum distance, update the best pair and break the loop\n                if distance > min_distance_between_sector_points_in_px:\n                    best_pair = (idx, idx2)\n                    best_distance = distance\n                    break\n\n            # If no suitable pair of points was found, raise an error\n            if best_pair is None:\n                raise EyeCentersEstimationError(\n                    f\"Could not find a suitable pair of points after {max_iterations} iterations.\"\n                )\n\n            # Calculate the perpendicular bisector of the best pair of points\n            midpoint = (polygon[best_pair[0]] + polygon[best_pair[1]]) / 2\n            perpendicular_vector = np.array([-best_distance / 2, 0])\n            first_bisectors_point[i] = midpoint + perpendicular_vector\n            second_bis"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables\n        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Initialize lists to store the starting and ending points of the bisectors\n        first_bisectors_point = []\n        second_bisectors_point = []\n\n        # Loop until the required number of bisectors are found or the maximum number of iterations is reached\n        for _ in range(num_bisectors):\n            # Generate random indices for the polygon vertices\n            random_indices = np.random.choice(len(polygon), 2, replace=False)\n\n            # Extract the points corresponding to the random indices\n            first_point = polygon[random_indices[0]]\n            second_point = polygon[random_indices[1]]\n\n            # Check if the distance between the points is greater than the minimum distance\n            distance = np.linalg.norm(first_point - second_point)\n            if distance < min_distance_between_sector_points_in_px:\n                # If the distance is less than the minimum distance, generate new random indices\n                continue\n\n            # Calculate the midpoint of the line segment\n            midpoint = (first_point + second_point) / 2\n\n            # Calculate the normal vector to the line segment\n            normal_vector = np.array([-1 * (second_point[1] - first_point[1]), second_point[0] - first_point[0]])\n\n            # Normalize the normal vector\n            normal_vector /= np.linalg.norm(normal_vector)\n\n            # Calculate the perpendicular bisector\n            perpendicular_bisector = midpoint + normal_vector\n\n            # Add the starting and ending points of the perpendicular bisector to the lists\n            first_bisectors_point.append(first_point)\n            second_bisectors_point.append(perpendicular_bisector)\n\n        # If the required number of bisectors are not found after the maximum number of iterations, raise an exception\n        if len(first_bisectors_point) <"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._pre_execution_hooks(*args, **kwargs)\n        result = self._run(*args, **kwargs)\n        self._post_execution_hooks(result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execution(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execution(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_run(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_run(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execution(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n        result = self.run(*args, **kwargs)\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.on_pre_execute(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self.on_post_execute(*args, **kwargs)\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execution(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execution_hook()\n        result = self.run(*args, **kwargs)\n        self.post_execution_hook()\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execute(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self.post_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execution(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_run(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_run(result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.on_pre_execute(self, *args, **kwargs)\n\n        result = self._execute(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.on_post_execute(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            if self.check_type(deserialized_output, type_definition):\n                return True\n            else:\n                return False\n        except json.JSONDecodeError:\n            return False\n\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            if self.check_type(deserialized_output, type_definition):\n                return True\n            else:\n                return False\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            if not self.check_type(deserialized_output, type_definition):\n                return False\n            return True\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n        return self.check_type(deserialized_output, type_definition)\n\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        if self.check_type(deserialized_output, type_definition):\n            return True\n        return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            if not self.check_type(deserialized_output, type_definition):\n                return False\n        except json.JSONDecodeError:\n            return False\n\n        return True\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            if self.check_type(deserialized_output, type_definition):\n                return True\n            else:\n                return False\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            if not self.check_type(deserialized_output, type_definition):\n                return False\n        except json.JSONDecodeError:\n            return False\n\n        return True\n\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            if type_hint in type_hints:\n                return type_hints[type_hint]\n            elif type_hint in type_hints:\n                return type_hints[type_hint]\n            else:\n                return None\n\n        def get_class_definition_from_type(type_hint):\n            if type_hint in type_hints:\n                return type_hints[type_hint]\n            elif type_hint in type_hints:\n                return type_hints[type_hint]\n            else:\n                return None\n\n        def get_class_definition_from_type_hint(type_hint):\n            if type_hint in type_hints:\n                return type_hints[type_hint]\n            elif type_hint in type_hints:\n                return type_hints[type_hint]\n            else:\n                return None\n\n        def get_class_definition_from_type_hint(type_hint):\n            if type_hint in type_hints:\n                return type_hints[type_hint]\n            elif type_hint in type_hints:\n                return type_hints[type_hint]\n            else:\n                return None\n\n        def get_class_definition_from_type_hint(type_hint):\n            if type_hint in type_hints:\n                return type_hints[type_hint]\n            elif type_hint in type_hints:\n                return type_hints[type_hint]\n            else:\n                return None\n\n        def get_class_definition_from_type_hint(type_hint):\n            if type_hint in type_hints:\n                return type_hints[type_hint]\n            elif type_hint in type_hints:\n                return type_hints[type_hint]\n            else:\n                return None\n\n        def get_class_definition_from_type_hint(type"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param_name == \"self\":\n                continue\n            if param.kind == param.VAR_KEYWORD:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for parameter in signature.parameters.values():\n            if parameter.name in type_hints:\n                input_type_hints[parameter.name] = type_hints[parameter.name]\n                input_class_definitions[parameter.name] = get_class_definition(type_hints[parameter.name])\n\n        if signature.return_annotation in type_hints:\n            output_type_hints = type_hints[signature.return_annotation]\n            output_class_definitions = get_class_definition(type_hints[signature.return_annotation])\n\n        if isinstance(output_type_hints, Union):\n            if issubclass(output_type_hints.__args__[0], Embedding):\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definitions = output_type_hints.__args__[0]\n            else:\n                function_type = FunctionType.SYMBOLIC\n                output_class_definitions = output_type_hints.__args__[0]\n        else:\n            if issubclass(output_type_hints, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definitions = output_type_hints\n            else:\n                function_type = FunctionType.SYMBOLIC\n                output_class_definitions = output_type_hints\n\n        function_description = FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_defin"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_hints = {}\n        output_hints = {}\n\n        for param in signature.parameters.values():\n            if param.name in hints:\n                input_hints[param.name] = hints[param.name]\n\n        output_hint = hints.get(\"return\", None)\n\n        if output_hint:\n            output_hints[\"return\"] = output_hint\n\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for input_name, input_hint in input_hints.items():\n            input_class_definitions[input_name] = get_class_definition(input_hint)\n\n        for output_name, output_hint in output_hints.items():\n            output_class_definitions[output_name] = get_class_definition(output_hint)\n\n        function_type = FunctionType.SYMBOLIC\n        output_class_definition = None\n\n        if output_hint:\n            if issubclass(output_hint, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definition = get_class_definition(output_hint)\n            elif issubclass(output_hint, Union):\n                for sub_type in output_hint.__args__:\n                    if issubclass(sub_type, Embedding):\n                        function_type = FunctionType.EMBEDDABLE\n                        output_class_definition = get_class_definition(sub_type)\n                        break\n\n        return FunctionDescription(func_object.__name__,\n                                   docstring,\n                                   input_hints,\n                                   output_hints,\n                                   input_class_definitions,\n                                   output_class_definitions,\n                                   function_type,\n                                   output_class_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        # Get input and output type hints\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param_type in signature.parameters.items():\n            if param_name == \"self\":\n                continue\n            if param_type.annotation == inspect._empty:\n                input_type_hints[param_name] = None\n            else:\n                input_type_hints[param_name] = param_type.annotation\n\n        output_type_hints = type_hints.get(\"return\", None)\n\n        # Get class definitions for input and output types\n        input_class_definitions = {}\n        output_class_definition = None\n        for param_name, param_type in input_type_hints.items():\n            if param_type is None:\n                input_class_definitions[param_name] = None\n            else:\n                input_class_definitions[param_name] = get_class_definition(param_type)\n\n        if output_type_hints is not None:\n            if issubclass(output_type_hints, Union):\n                output_type_hints = output_type_hints.__args__\n                output_class_definition = get_class_definition(output_type_hints[0])\n            else:\n                output_class_definition = get_class_definition(output_type_hints)\n\n        # Determine function type\n        function_type = FunctionType.SYMBOLIC\n        if issubclass(output_type_hints, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif output_type_hints is not None and issubclass(output_type_hints, Union):\n            for subtype in output_type_hints.__args__:\n                if issubclass(subtype, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    break"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {}\n        output_type_hints = {}\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.kind in (param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY):\n                if param.annotation not in (param.empty, Ellipsis):\n                    input_type_hints[param_name] = param.annotation\n                    input_class_definitions[param_name] = get_class_definition(param.annotation)\n\n        if signature.return_annotation not in (signature.empty, Ellipsis):\n            output_type_hints['output'] = signature.return_annotation\n            output_class_definitions['output'] = get_class_definition(signature.return_annotation)\n\n        if output_type_hints['output'] in (signature.empty, Ellipsis):\n            output_type_hints['output'] = None\n            output_class_definitions['output'] = None\n\n        if output_type_hints['output'] is not None:\n            if issubclass(output_type_hints['output'], Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            elif issubclass(output_type_hints['output'], Union):\n                if any(issubclass(output_type_hints['output'], Embedding) for output_type_hints['output'] in\n                       output_type_hints['output'].__args__):\n                    function_type = FunctionType.EMBEDDABLE\n                else:\n                    function_type = FunctionType.SYMBOLIC\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(func_object.__name__, func_object.__doc__, input_type_hints, output_type_hints,\n                                   input_"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Get input and output type hints\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param_type in signature.parameters.items():\n            if param_name not in type_hints:\n                continue\n            input_type_hints[param_name] = type_hints[param_name]\n\n        if signature.return_annotation not in type_hints:\n            raise ValueError(f\"Return type hint for {func_object.__name__} not found\")\n\n        output_type_hints = type_hints[signature.return_annotation]\n\n        # Get input and output class definitions\n        input_class_definitions = {}\n        output_class_definition = None\n\n        for param_name, param_type in input_type_hints.items():\n            class_definition = Register.get_class_definition(param_type)\n            if class_definition:\n                input_class_definitions[param_name] = class_definition\n\n        output_class_definition = Register.get_class_definition(output_type_hints)\n\n        # Determine function type\n        function_type = FunctionType.SYMBOLIC\n        if issubclass(output_type_hints, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif issubclass(output_type_hints, Union):\n            for subtype in output_type_hints.__args__:\n                if issubclass(subtype, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    output_class_definition = Register.get_class_definition(subtype)\n                    break\n\n        return FunctionDescription(func_object.__name__, func_object.__doc__, input_type_hints,\n                                   output_type_hints, input_class_definitions, output_class_definition,\n                                   function_type)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function signature\n        signature = inspect.signature(func_object)\n\n        # Get the type hints for the input and output parameters\n        type_hints = get_type_hints(func_object)\n\n        # Get the docstring for the function\n        docstring = func_object.__doc__\n\n        # Initialize variables to store the input and output type hints and class definitions\n        input_type_hints = []\n        output_type_hints = []\n        input_class_definitions = []\n        output_class_definitions = []\n\n        # Iterate over the parameters in the function signature\n        for parameter in signature.parameters.values():\n            # Get the type hint for the parameter\n            type_hint = type_hints.get(parameter.name, None)\n\n            # If the type hint is not None, add it to the input type hints and class definitions\n            if type_hint is not None:\n                input_type_hints.append(type_hint)\n                input_class_definitions.append(get_class_definition(type_hint))\n\n        # Get the output type hint\n        output_type_hint = type_hints.get('return', None)\n\n        # If the output type hint is not None, add it to the output type hints and class definitions\n        if output_type_hint is not None:\n            output_type_hints.append(output_type_hint)\n            output_class_definitions.append(get_class_definition(output_type_hint))\n\n            # Check if the output type hint is a subclass of an Embedding class or part of a Union type\n            if issubclass(output_type_hint, Embedding):\n                # If it is, set the function type to EMBEDDABLE\n                function_type = FunctionType.EMBEDDABLE\n            else:\n                # If it is not, check if the output type hint is a class or a subclass of a Union\n                if issubclass(output_type_hint, Union):\n                    # If it is a class or a subclass of a Union, set the function"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            if type_hint is None:\n                return None\n            if isinstance(type_hint, type):\n                return type_hint\n            elif isinstance(type_hint, str):\n                return type_hint\n            elif isinstance(type_hint, type(None)):\n                return None\n            elif isinstance(type_hint, tuple):\n                return type_hint\n            elif get_origin(type_hint) is Union:\n                return get_type_hints(type_hint)\n            elif get_origin(type_hint) is Literal:\n                return get_type_hints(type_hint)\n            else:\n                return get_type_hints(type_hint)\n\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {}\n        output_type_hints = {}\n        input_class_definitions = {}\n        output_class_definitions = {}\n        for parameter in signature.parameters.values():\n            if parameter.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                input_type_hints[parameter.name] = type_hints[parameter.name]\n                input_class_definitions[parameter.name] = get_class_definition(type_hints[parameter.name])\n            elif parameter.kind == inspect.Parameter.VAR_POSITIONAL:\n                input_type_hints[parameter.name] = type_hints[parameter.name]\n                input_class_definitions[parameter.name] = get_class_definition(type_hints[parameter.name])\n            elif parameter.kind == inspect.Parameter.VAR_KEYWORD:\n                input_type_hints[parameter.name] = type_hints[parameter.name]\n                input_class_definitions[parameter.name] = get_class_definition(type_hints[parameter.name])\n\n        output_type_hint = type_hints.get(\"return\")"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        hints = get_type_hints(func_object)\n\n        def get_class_definition(type_hint):\n            if type_hint in hints:\n                return hints[type_hint]\n            elif get_origin(type_hint) is Union:\n                for type_hint_arg in get_args(type_hint):\n                    if type_hint_arg in hints:\n                        return hints[type_hint_arg]\n            elif get_origin(type_hint) is Literal:\n                return type_hint.__origin__\n            elif get_origin(type_hint) is Tuple:\n                return type_hint.__origin__\n            else:\n                return None\n\n        input_hints = {}\n        output_hints = {}\n\n        for key, value in signature.parameters.items():\n            if value.annotation is not inspect.Parameter.empty:\n                input_hints[key] = value.annotation\n\n        if signature.return_annotation is not inspect.Signature.empty:\n            output_hints = signature.return_annotation\n\n        input_class_definitions = {}\n        output_class_definition = None\n\n        for key, value in input_hints.items():\n            input_class_definitions[key] = get_class_definition(value)\n\n        if issubclass(output_hints, Embedding):\n            output_class_definition = get_class_definition(output_hints)\n            function_type = FunctionType.EMBEDDABLE\n        elif issubclass(output_hints, Union):\n            for type_hint in get_args(output_hints):\n                if issubclass(type_hint, Embedding):\n                    output_class_definition = get_class_definition(type_hint)\n                    function_type = FunctionType.EMBEDDABLE\n                    break\n            if not output_class_definition:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            if type_hint is None:\n                return None\n            if type_hint == int or type_hint == float or type_hint == str or type_hint == bool or type_hint == dict or type_hint == list:\n                return None\n            if type_hint.__module__ == 'builtins':\n                return None\n            if get_origin(type_hint) is Union:\n                return None\n            if get_origin(type_hint) is Literal:\n                return None\n            if get_origin(type_hint) is Tuple:\n                return None\n            if get_origin(type_hint) is list:\n                return None\n            if get_origin(type_hint) is dict:\n                return None\n            if get_origin(type_hint) is set:\n                return None\n            if get_origin(type_hint) is int:\n                return None\n            if get_origin(type_hint) is float:\n                return None\n            if get_origin(type_hint) is str:\n                return None\n            if get_origin(type_hint) is bool:\n                return None\n            if get_origin(type_hint) is tuple:\n                return None\n            if get_origin(type_hint) is set:\n                return None\n            if get_origin(type_hint) is list:\n                return None\n            if get_origin(type_hint) is dict:\n                return None\n            return type_hint\n\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {}\n        output_type_hints = {}\n        for param in signature.parameters.values():\n            if param.name in type_hints:\n                input_type_hints[param.name] = type_hints[param.name]\n        if signature.return_annotation:\n            output_type_hints = signature.return_annotation\n        input_class_definitions"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            if type(type_hint) == type:\n                return type_hint\n            elif type_hint.__origin__ == Union:\n                return type_hint.__args__[0]\n            else:\n                return type_hint.__args__[0]\n\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        input_class_definitions = {}\n        output_class_definitions = {}\n        for parameter_name, parameter_type_hint in type_hints.items():\n            if parameter_name in signature.parameters:\n                input_type_hints[parameter_name] = parameter_type_hint\n                input_class_definitions[parameter_name] = get_class_definition(parameter_type_hint)\n            else:\n                output_type_hints[parameter_name] = parameter_type_hint\n                output_class_definitions[parameter_name] = get_class_definition(parameter_type_hint)\n\n        output_class_definition = None\n        output_class_definitions_list = list(output_class_definitions.values())\n        if len(output_class_definitions_list) == 1:\n            output_class_definition = output_class_definitions_list[0]\n        elif len(output_class_definitions_list) > 1:\n            output_class_definition = Union[tuple(output_class_definitions_list)]\n\n        output_type_hint = output_type_hints[list(output_type_hints.keys())[0]]\n        if issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif issubclass(output_type_hint, Union):\n            if any(issubclass(cls, Embedding) for cls in output_type_hint.__args__):\n                function_type = Function"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            \"\"\"\n            Fetch the class definition for a given type hint.\n            :param type_hint: The type hint to fetch the class definition for.\n            :return: The class definition for the given type hint, or None if the type hint is not a class or a subclass of a Union.\n            \"\"\"\n            if type_hint.__class__.__name__ == \"GenericAlias\":\n                type_hint = type_hint.__origin__\n            if type_hint.__class__.__name__ == \"Union\":\n                return None\n            if type_hint.__class__.__name__ == \"GenericMeta\":\n                return None\n            if type_hint.__class__.__name__ == \"ForwardRef\":\n                return None\n            if type_hint.__class__.__name__ == \"type\":\n                return type_hint\n            if type_hint.__class__.__name__ == \"ClassVar\":\n                return type_hint\n            if type_hint.__class__.__name__ == \"typing_extensions.Self\":\n                return type_hint\n            if type_hint.__class__.__name__ == \"typing_extensions.TypeAlias\":\n                return type_hint\n            if type_hint.__class__.__name__ == \"typing_extensions.Literal\":\n                return type_hint\n            if type_hint.__class__.__name__ == \"typing_extensions.Final\":\n                return type_hint\n            if type_hint.__class__.__name__ == \"typing_extensions.TypedDict\":\n                return type_hint\n            if type_hint.__class__.__name__ == \"typing_extensions.Final\":\n                return type_hint\n            if type_hint.__class__.__name__ == \"typing_extensions.Final\":\n                return type_hint\n            if type_hint.__class__.__name__ == \"typing_extensions.Final\":\n                return type_hint\n            if type_hint.__class__.__name__ == \"typing_extensions.Final\":\n                return type_hint"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        output_class_definition = None\n\n        for param_name, param in signature.parameters.items():\n            if param.kind in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n                input_type_hints[param_name] = type_hints[param_name]\n\n        if signature.return_annotation is not signature.empty:\n            output_type_hints = type_hints[signature.return_annotation]\n            if isinstance(output_type_hints, dict):\n                output_type_hints = output_type_hints['type']\n            if isinstance(output_type_hints, Union):\n                if issubclass(output_type_hints.__args__[0], Embedding):\n                    output_class_definition = output_type_hints.__args__[0]\n                    output_type_hints = output_type_hints.__args__[0]\n                else:\n                    output_type_hints = output_type_hints.__args__[0]\n            else:\n                if issubclass(output_type_hints, Embedding):\n                    output_class_definition = output_type_hints\n\n        if output_class_definition is None:\n            output_class_definition = get_class_definition(output_type_hints)\n\n        function_type = FunctionType.SYMBOLIC\n        if issubclass(output_type_hints, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n\n        return FunctionDescription(func_object.__name__,\n                                   func_object.__doc__,\n                                   input_type_hints,\n                                   output_type_hints,\n                                   output_class_definition,\n                                   function_type)\n\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            if type(type_hint) == type:\n                return type_hint\n            else:\n                if get_origin(type_hint) is Union:\n                    return get_class_definition(type_hint.__args__[0])\n                elif get_origin(type_hint) is Literal:\n                    return get_class_definition(type_hint.__args__[0])\n                else:\n                    return type_hint.__origin__\n\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {}\n        output_type_hint = None\n        output_class_definition = None\n        for parameter in signature.parameters.values():\n            if parameter.kind == inspect.Parameter.VAR_KEYWORD:\n                continue\n            if parameter.kind == inspect.Parameter.VAR_POSITIONAL:\n                continue\n            if parameter.kind == inspect.Parameter.POSITIONAL_ONLY:\n                continue\n            if parameter.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                input_type_hints[parameter.name] = type_hints[parameter.name]\n            elif parameter.kind == inspect.Parameter.KEYWORD_ONLY:\n                input_type_hints[parameter.name] = type_hints[parameter.name]\n\n        output_type_hint = signature.return_annotation\n        if output_type_hint is None:\n            raise ValueError(\"Function has no return type hint\")\n\n        if get_origin(output_type_hint) is Union:\n            if issubclass(output_type_hint.__args__[0], Embedding):\n                output_class_definition = get_class_definition(output_type_hint.__args__[0])\n                output_type_hint = output_type_hint.__args__[0]\n            else:\n                output_class_definition = get_class_definition(output_type_hint.__args__[1])\n                output_type_h"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            if type_hint is None:\n                return None\n            if type_hint.__module__ == 'builtins':\n                return None\n            if type_hint.__module__ == 'typing':\n                if type_hint.__name__ == 'Any':\n                    return None\n                if type_hint.__name__ == 'Union':\n                    return None\n            return type_hint\n\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hint = None\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                if param.annotation == inspect.Parameter.empty:\n                    input_type_hints[param_name] = None\n                else:\n                    input_type_hints[param_name] = param.annotation\n            elif param.kind == inspect.Parameter.VAR_POSITIONAL:\n                input_type_hints[param_name] = tuple(param.annotation)\n            elif param.kind == inspect.Parameter.VAR_KEYWORD:\n                input_type_hints[param_name] = dict\n            elif param.kind == inspect.Parameter.KEYWORD_ONLY:\n                input_type_hints[param_name] = param.annotation\n\n        if signature.return_annotation == inspect.Parameter.empty:\n            output_type_hint = None\n        else:\n            output_type_hint = signature.return_annotation\n\n        output_class_definition = get_class_definition(output_type_hint)\n        output_type_hint_is_union = False\n\n        if output_type_hint is not None and get_origin(output_type_hint) is Union:\n            output_type_hint_is_union = True\n            output_type_hint = get_args(output_type_hint)[0]\n\n        if output_type_hint"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for parameter in signature.parameters.values():\n            if parameter.name in hints:\n                type_hint = hints[parameter.name]\n                if get_origin(type_hint) is Union:\n                    if Embedding in get_args(type_hint):\n                        output_type_hints[parameter.name] = type_hint\n                    else:\n                        input_type_hints[parameter.name] = type_hint\n                else:\n                    input_type_hints[parameter.name] = type_hint\n\n        input_class_definitions = {}\n        for input_name, input_type_hint in input_type_hints.items():\n            input_class_definitions[input_name] = get_class_definition(input_type_hint)\n\n        output_class_definitions = {}\n        for output_name, output_type_hint in output_type_hints.items():\n            output_class_definitions[output_name] = get_class_definition(output_type_hint)\n\n        if not output_type_hints:\n            output_type_hint = hints['return']\n            output_class_definition = get_class_definition(output_type_hint)\n            output_class_definitions['return'] = output_class_definition\n            function_type = FunctionType.SYMBOLIC\n        else:\n            if Embedding in get_args(output_type_hints['return']):\n                function_type = FunctionType.EMBEDDABLE\n            else:\n                function_type = FunctionType.SYMBOLIC\n\n            output_class_definitions['return'] = get_class_definition(output_type_hints['return'])\n\n        function_description = FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param_type in signature.parameters.items():\n            if param_type.annotation != inspect._empty:\n                input_type_hints[param_name] = param_type.annotation\n\n        if signature.return_annotation != inspect._empty:\n            output_type_hints = signature.return_annotation\n\n        input_class_definitions = {}\n        for param_name, param_type in input_type_hints.items():\n            if param_type == Literal:\n                input_class_definitions[param_name] = None\n            else:\n                input_class_definitions[param_name] = get_class_definition(param_type)\n\n        output_class_definition = None\n        if isinstance(output_type_hints, Union):\n            for hint in output_type_hints.__args__:\n                if issubclass(hint, Embedding):\n                    output_class_definition = hint\n                    break\n            if output_class_definition is None:\n                output_class_definition = output_type_hints.__args__[0]\n        elif issubclass(output_type_hints, Embedding):\n            output_class_definition = output_type_hints\n        else:\n            output_class_definition = get_class_definition(output_type_hints)\n\n        function_type = FunctionType.SYMBOLIC if output_class_definition is None else FunctionType.EMBEDDABLE\n\n        return FunctionDescription(func_object.__name__, func_object.__doc__, input_type_hints, output_type_hints,\n                                   input_class_definitions, output_class_definition, function_type)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            if type_hint in [int, float, str, bool, list, dict, tuple, set, frozenset, complex, bytes, bytearray,\n                             memoryview]:\n                return None\n            else:\n                return type_hint\n\n        def get_output_class_definition(output_type_hint):\n            if output_type_hint is None:\n                return None\n            elif isinstance(output_type_hint, type):\n                return get_class_definition(output_type_hint)\n            elif get_origin(output_type_hint) == Union:\n                return get_class_definition(output_type_hint.__args__[0])\n            else:\n                return None\n\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {name: type_hints[name] for name in signature.parameters.keys()}\n        output_type_hint = type_hints.get('return', None)\n        output_class_definition = get_output_class_definition(output_type_hint)\n        function_type = FunctionType.SYMBOLIC\n        if issubclass(output_type_hint, Embedding) or output_type_hint in [list, tuple, set, frozenset, dict]:\n            function_type = FunctionType.EMBEDDABLE\n\n        return FunctionDescription(func_object.__name__, func_object.__doc__, input_type_hints, output_type_hint,\n                                   output_class_definition, function_type)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            if type_hint in (int, float, str, bool, bytes, dict, list, tuple, set, frozenset, type(None)):\n                return None\n            elif isinstance(type_hint, type):\n                return type_hint\n            elif isinstance(type_hint, tuple) and type_hint[0] == type(None):\n                return None\n            elif isinstance(type_hint, type):\n                return type_hint\n            elif type_hint.__origin__ in (list, tuple, set, frozenset):\n                return get_class_definition(type_hint.__args__[0])\n            elif type_hint.__origin__ == dict:\n                return get_class_definition(type_hint.__args__[1])\n            elif type_hint.__origin__ == Union:\n                return get_class_definition(type_hint.__args__[0])\n            else:\n                return type_hint.__origin__\n\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {param: type_hints[param] for param in signature.parameters}\n        output_type_hint = type_hints[\"return\"]\n        input_class_definitions = {param: get_class_definition(input_type_hints[param]) for param in\n                                   input_type_hints}\n        output_class_definition = get_class_definition(output_type_hint)\n        if issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif issubclass(output_type_hint, Union) and issubclass(output_type_hint.__args__[0], Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(func_object.__name__, func_object.__doc__, input_type_"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.indices[index] += 1\n            self.bit_array[index] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            #print(f\"Add: Seed={seed}, Digest={index}\")\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n\n            #print(f\"Add: Seed={seed}, Digest={index}, BitValue={self.bit_array[index]}\")\n            self.bit_array[index] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            #print(f\"Add: Seed={seed}, Digest={index}, BitValue={self.bit_array[index]}\")\n            self.bit_array[index] = 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            #print(f\"Add: Seed={seed}, Digest={index}\")\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            #print(f\"Add: Seed={seed}, Digest={index}\")\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n\n            #print(f\"Add: Seed={seed}, Digest={index}, BitValue={self.bit_array[index]}\")\n            if self.bit_array[index] == 0:\n                self.bit_array[index] = 1\n                self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array and len(loaded_bit_array) != self.size:\n            logging.warning(f\"Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({self.size}). Reinitializing bit array and indices.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) != self.size:\n            logging.warning(f\"Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({self.size}). Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array is not None and len(loaded_bit_array) != self.size:\n            logging.warning(\"BloomFilter: Corruption detected in persistence. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is None:\n            logging.warning(\"No bit array found in persistence.\")\n            return\n\n        if len(bit_array) != self.size:\n            logging.warning(\"Bit array length mismatch. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n\n        if loaded_bit_array is None:\n            logging.warning(\"No bit array found in persistence. Initializing new bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"Bit array length mismatch. Initializing new bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"Loaded bit array has a different length than expected. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array is None:\n            logging.warning(\"No bit array found in persistence. Initializing new bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"Loaded bit array length does not match expected length. Initializing new bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array is None:\n            logging.warning(\"BloomFilter: No bit array found in persistence. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"BloomFilter: Bit array length mismatch. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is None:\n            logging.warning(\"No bit array found in persistence\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        if len(bit_array) != self.size:\n            logging.warning(f\"Bit array length mismatch. Expected length: {self.size}, Actual length: {len(bit_array)}\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) != self.size:\n            logging.warning(f\"Corruption detected in BloomFilter. Expected size: {self.size}, Actual size: {len(loaded_bit_array)}\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n\n        if bit_array and len(bit_array) != self.size:\n            logging.warning(f\"BloomFilter: Loaded bit array length ({len(bit_array)}) does not match expected length ({self.size})\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is None:\n            logging.warning(\"BloomFilter.load: No bit array found in persistence\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        if len(bit_array) != self.size:\n            logging.warning(\"BloomFilter.load: Bit array length mismatch. Expected length: %d, Actual length: %d\", self.size, len(bit_array))\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array:\n            if len(loaded_bit_array) != self.size:\n                logging.warning(f\"Loaded bit array length {len(loaded_bit_array)} does not match expected length {self.size}\")\n                self.bit_array, self.indices = self.init_bit_array(self.size)\n                self.save()\n            else:\n                self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n\n        if bit_array is None:\n            logging.warning(\"No bloom filter found. Initializing new bloom filter\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        if len(bit_array) != self.size:\n            logging.warning(\"Bloom filter corruption detected. Reinitializing bloom filter\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array and len(bit_array) != self.size:\n            logging.warning(\"BloomFilter.load: Corruption detected. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            bit_array = self.persistence.load()\n            if len(bit_array) != self.size:\n                logging.warning(\"BloomFilter: Bit array length mismatch. Reinitializing bit array.\")\n                self.bit_array, self.indices = self.init_bit_array(self.size)\n                self.save()\n            else:\n                self.bit_array = bit_array\n        except FileNotFoundError:\n            logging.warning(\"BloomFilter: Persistence file not found. Initializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is not None and len(bit_array) != self.size:\n            logging.warning(f\"Loaded bit array has length {len(bit_array)} which does not match expected length {self.size}. Reinitializing bit array and indices.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        # Load the bit array from persistence\n        loaded_bit_array = self.persistence.load()\n\n        # Calculate the expected length based on the BloomFilter size\n        expected_length = math.ceil(self.size / 8)\n\n        # Check if the loaded bit array's length matches the expected length\n        if len(loaded_bit_array) != expected_length:\n            # Log a warning indicating potential corruption\n            logging.warning(\"BloomFilter: Bit array length mismatch. Reinitializing BloomFilter.\")\n\n            # Reinitialize the bit array and indices\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n\n            # Save the new state\n            self.save()\n\n        # Copy the loaded bit array into the BloomFilter instance's bit array\n        self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            loaded_bit_array = self.persistence.load()\n            if len(loaded_bit_array) != self.size:\n                logging.warning(f\"Loaded bit array has unexpected length of {len(loaded_bit_array)}. Reinitializing bit array and indices.\")\n                self.bit_array, self.indices = self.init_bit_array(self.size)\n                self.save()\n            else:\n                self.bit_array = loaded_bit_array\n        except FileNotFoundError as e:\n            logging.warning(f\"BloomFilter: {e}\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is None:\n            logging.warning(\"BloomFilter.load: No bit array found in persistence\")\n            return\n        if len(bit_array) != self.size:\n            logging.warning(\"BloomFilter.load: Loaded bit array length does not match expected length\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            combined_hash = (hash1 + i * hash2) % self.size\n            if self.bit_array[combined_hash] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        indices = [(hash1 + i * hash2) % self.size for i in range(self.hash_count)]\n        return all(self.bit_array[index] for index in indices)\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        indices = [(hash1 + i * hash2) % self.size for i in range(self.hash_count)]\n        return all(self.bit_array[i] for i in indices)\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if not self.bit_array[index]:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            combined_hash = (hash1 + i * hash2) % self.size\n            if self.bit_array[combined_hash] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        indices = [(hash1 + i * hash2) % self.size for i in range(self.hash_count)]\n        return all(self.bit_array[index] for index in indices)\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if not self.bit_array[index]:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        indices = [(hash1 + i * hash2) % self.size for i in range(self.hash_count)]\n        return all(self.bit_array[i] for i in indices)\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Generate indices using hash functions\n        hash1, hash2 = self.hash_functions(string)\n        indices = [(hash1 + i * hash2) % self.size for i in range(self.hash_count)]\n\n        # Check if all bits at indices are set in the bit array\n        for index in indices:\n            if not self.bit_array[index]:\n                return False\n\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        indices = [(hash1 + i * hash2) % self.size for i in range(self.hash_count)]\n        return all(self.bit_array[index] for index in indices)\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            combined_hash = (hash1 + i * hash2) % self.size\n            if not self.bit_array[combined_hash]:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        indices = []\n        for i in range(self.hash_count):\n            indices.append((hash1 + i * hash2) % self.size)\n\n        for index in indices:\n            if not self.bit_array[index]:\n                return False\n\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if \"distilled_model\" in json_dict:\n            self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"])\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if json_dict.get(DISTILLED_MODEL) is not None:\n            self.distilled_model = config_factory.get_config(json_dict[DISTILLED_MODEL])\n        if json_dict.get(TEACHER_MODEL) is not None:\n            self.teacher_models = [config_factory.get_config(teacher_model) for teacher_model in json_dict[TEACHER_MODEL]]\n        if json_dict.get(\"current_model_stats\") is not None:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if json_dict.get(\"last_training_run\") is not None:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if json_dict.get(\"current_training_run\") is not None:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if json_dict.get(\"nr_of_training_runs\") is not None:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if \"distilled_model\" in json_dict:\n            self.distilled_model = config_factory.create_model_config(json_dict[\"distilled_model\"])\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[DISTILLED_MODEL]\n        self.last_training_run = json_dict[DISTILLED_MODEL]\n        self.current_training_run = json_dict[DISTILLED_MODEL]\n        self.nr_of_training_runs = json_dict[DISTILLED_MODEL]\n\n        if TEACHER_MODEL in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model_dict) for teacher_model_dict in json_dict[TEACHER_MODEL]]\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if json_dict.get(DISTILLED_MODEL) is not None:\n            self.distilled_model = config_factory.create_model_config(json_dict[DISTILLED_MODEL])\n        if json_dict.get(\"current_model_stats\") is not None:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if json_dict.get(\"last_training_run\") is not None:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if json_dict.get(\"current_training_run\") is not None:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if json_dict.get(\"nr_of_training_runs\") is not None:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if json_dict.get(\"teacher_models\") is not None:\n            self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[TEACHER_MODEL]\n        self.last_training_run = json_dict[TEACHER_MODEL]\n        self.current_training_run = json_dict[TEACHER_MODEL]\n        self.nr_of_training_runs = json_dict[TEACHER_MODEL]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if json_dict is not None:\n            if \"distilled_model\" in json_dict:\n                self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"])\n            if \"current_model_stats\" in json_dict:\n                self.current_model_stats = json_dict[\"current_model_stats\"]\n            if \"last_training_run\" in json_dict:\n                self.last_training_run = json_dict[\"last_training_run\"]\n            if \"current_training_run\" in json_dict:\n                self.current_training_run = json_dict[\"current_training_run\"]\n            if \"teacher_models\" in json_dict:\n                self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n            if \"nr_of_training_runs\" in json_dict:\n                self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if \"distilled_model\" in json_dict:\n            self.distilled_model = config_factory.create_model_config(json_dict[\"distilled_model\"])\n\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.create_model_config(teacher_model_json) for teacher_model_json in json_dict[\"teacher_models\"]]\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[TEACHER_MODEL]\n        self.last_training_run = json_dict[TEACHER_MODEL]\n        self.current_training_run = json_dict[TEACHER_MODEL]\n        self.nr_of_training_runs = json_dict[TEACHER_MODEL]\n        self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict[TEACHER_MODEL]]\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if \"distilled_model\" in json_dict:\n            self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"])\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if 'distilled_model' in json_dict:\n            self.distilled_model = config_factory.get_model_config(json_dict['distilled_model'])\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict['teacher_models']]\n        return self\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[TEACHER_MODEL]\n        self.last_training_run = json_dict[TEACHER_MODEL]\n        self.current_training_run = json_dict[TEACHER_MODEL]\n        self.teacher_models = [config_factory.get_model_config(teacher_model_name) for teacher_model_name in json_dict[TEACHER_MODEL]]\n        self.nr_of_training_runs = json_dict[TEACHER_MODEL]\n\n        return self\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[TEACHER_MODEL]\n        self.last_training_run = json_dict[TEACHER_MODEL]\n        self.current_training_run = json_dict[TEACHER_MODEL]\n        self.nr_of_training_runs = json_dict[TEACHER_MODEL]\n        self.teacher_models = [config_factory.create_model_config(teacher_model_dict) for teacher_model_dict in json_dict[\"teacher_models\"]]\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[DISTILLED_MODEL]\n        self.last_training_run = json_dict[DISTILLED_MODEL]\n        self.current_training_run = json_dict[DISTILLED_MODEL]\n        self.nr_of_training_runs = json_dict[DISTILLED_MODEL]\n        if TEACHER_MODEL in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[TEACHER_MODEL]]\n        return self\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[TEACHER_MODEL]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        self.teacher_models = [config_factory.create_model_config(teacher_model_config) for teacher_model_config in json_dict[\"teacher_models\"]]\n        return self\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_config_from_dict(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[DISTILLED_MODEL][\"current_model_stats\"]\n        self.last_training_run = json_dict[DISTILLED_MODEL][\"last_training_run\"]\n        self.current_training_run = json_dict[DISTILLED_MODEL][\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[DISTILLED_MODEL][\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict[DISTILLED_MODEL]:\n            self.teacher_models = [config_factory.get_config_from_dict(teacher_model) for teacher_model in json_dict[DISTILLED_MODEL][\"teacher_models\"]]\n\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if \"distilled_model\" in json_dict:\n            self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"])\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model_config) for teacher_model_config in json_dict[\"teacher_models\"]]\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == \"distilled_model\":\n                self.distilled_model = config_factory.get_model_config(value, DISTILLED_MODEL)\n            elif key == \"current_model_stats\":\n                self.current_model_stats = value\n            elif key == \"last_training_run\":\n                self.last_training_run = value\n            elif key == \"current_training_run\":\n                self.current_training_run = value\n            elif key == \"nr_of_training_runs\":\n                self.nr_of_training_runs = value\n            elif key == \"teacher_models\":\n                self.teacher_models = [config_factory.get_model_config(teacher_model, TEACHER_MODEL) for teacher_model in value]\n            else:\n                raise ValueError(f\"Unknown key: {key}\")\n\n        return self\n\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key in json_dict:\n            if key == DISTILLED_MODEL:\n                self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n            elif key == TEACHER_MODEL:\n                self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[TEACHER_MODEL]]\n            else:\n                setattr(self, key, json_dict[key])\n        return self\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[TEACHER_MODEL]\n        self.last_training_run = json_dict[TEACHER_MODEL]\n        self.current_training_run = json_dict[TEACHER_MODEL]\n        self.nr_of_training_runs = json_dict[TEACHER_MODEL]\n        if TEACHER_MODEL in json_dict:\n            self.teacher_models = [config_factory.create_model_config(teacher_model_dict) for teacher_model_dict in json_dict[TEACHER_MODEL]]\n        return self\n\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is set\n        self.check_api_key()\n\n        # Validate the input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"prompt must be a string\")\n        if not isinstance(kwargs, dict):\n            raise ValueError(\"kwargs must be a dictionary\")\n        for key in kwargs.keys():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # Set the default model if not provided\n        if model.model_name is None:\n            model.model_name = DEFAULT_DISTILLED_MODEL_NAME\n\n        # Set the default temperature if not provided\n        if \"temperature\" not in kwargs:\n            kwargs[\"temperature\"] = 0.7\n\n        # Set the default max_new_tokens if not provided\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 200\n\n        # Set the default frequency_penalty if not provided\n        if \"frequency_penalty\" not in kwargs:\n            kwargs[\"frequency_penalty\"] = 0\n\n        # Set the default presence_penalty if not provided\n        if \"presence_penalty\" not in kwargs:\n            kwargs[\"presence_penalty\"] = 0\n\n        # Set the default top_p if not provided\n        if \"top_p\" not in kwargs:\n            kwargs[\"top_p\"] = 1\n\n        # Set the default system_message if not provided\n        if system_message is None:\n            system_message = \"\"\n\n        # Set the default prompt if not provided\n        if prompt is None:\n            prompt = \"\"\n\n        # Set the default model if not provided\n        if"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Check if the model has parsing helper tokens\n        if model.parsing_helper_tokens:\n            # Add the parsing helper tokens to the system message\n            system_message += f\"\\n\\n{model.parsing_helper_tokens}\"\n\n        # Check if the prompt is empty\n        if not prompt:\n            raise ValueError(\"Prompt cannot be empty\")\n\n        # Check if the model has parsing helper tokens\n        if model.parsing_helper_tokens:\n            # Add the parsing helper tokens to the system message\n            system_message += f\"\\n\\n{model.parsing_helper_tokens}\"\n\n        # Check if the model has parsing helper tokens\n        if model.parsing_helper_tokens:\n            # Add the parsing helper tokens to the system message\n            system_message += f\"\\n\\n{model.parsing_helper_tokens}\"\n\n        # Check if the model has parsing helper tokens\n        if model.parsing_helper_tokens:\n            # Add the parsing helper tokens to the system message\n            system_message += f\"\\n\\n{model.parsing_helper_tokens}\"\n\n        # Check if the model has parsing helper tokens\n        if model.parsing_helper_tokens:\n            # Add the parsing helper tokens to the system message\n            system_message += f\"\\n\\n{model.parsing_helper_tokens}\"\n\n        # Check if the model has parsing helper tokens\n        if model.parsing_helper_tokens:\n            # Add the parsing helper tokens to the system message\n            system_message += f\"\\n\\n{model.parsing_helper_tokens}\"\n\n        # Check if the model has parsing helper tokens\n        if model.parsing_helper_tokens:\n            # Add the parsing helper tokens to the system message\n            system_message += f\"\\n\\n{model.parsing_helper_tokens}\"\n\n        # Check if the model has parsing helper tokens\n        if model.parsing_helper_tokens:\n           "}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify that the API key is set\n        self.check_api_key()\n\n        # Validate the input parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                assert isinstance(kwargs[param], float)\n                assert 0 <= kwargs[param] <= 1\n\n        # Set the default model name if not provided\n        if model.model_name is None:\n            model.model_name = DEFAULT_DISTILLED_MODEL_NAME\n\n        # Set the default temperature if not provided\n        if \"temperature\" not in kwargs:\n            kwargs[\"temperature\"] = 0.7\n\n        # Set the default max_new_tokens if not provided\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 200\n\n        # Set the default frequency_penalty if not provided\n        if \"frequency_penalty\" not in kwargs:\n            kwargs[\"frequency_penalty\"] = 0\n\n        # Set the default presence_penalty if not provided\n        if \"presence_penalty\" not in kwargs:\n            kwargs[\"presence_penalty\"] = 0\n\n        # Set the default top_p if not provided\n        if \"top_p\" not in kwargs:\n            kwargs[\"top_p\"] = 1\n\n        # Set the default stop if not provided\n        if \"stop\" not in kwargs:\n            kwargs[\"stop\"] = model.stop\n\n        # Set the default model if not provided\n        if \"model\" not in kwargs:\n            kwargs[\"model\"] = model.model_name\n\n        # Set the default user if not provided\n        if \"user\" not in kwargs:\n            kwargs[\"user\"] = \"user\"\n\n        # Set the default messages if not provided\n        if \"messages\" not in kwargs:\n            kwargs[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Check if all required parameters are present\n        if not all(param in kwargs for param in LLM_GENERATION_PARAMETERS):\n            raise ValueError(f\"Missing required parameters: {LLM_GENERATION_PARAMETERS}\")\n\n        # Validate the model configuration\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration. Expected an instance of OpenAIConfig.\")\n\n        # Validate the system message\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message. Expected a string.\")\n\n        # Validate the prompt\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt. Expected a string.\")\n\n        # Validate the temperature\n        temperature = kwargs.get(\"temperature\", 0.7)\n        if not isinstance(temperature, float) or not 0 <= temperature <= 1:\n            raise ValueError(\"Invalid temperature. Expected a float between 0 and 1.\")\n\n        # Validate the top_p\n        top_p = kwargs.get(\"top_p\", 1.0)\n        if not isinstance(top_p, float) or not 0 <= top_p <= 1:\n            raise ValueError(\"Invalid top_p. Expected a float between 0 and 1.\")\n\n        # Validate the frequency_penalty\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0.0)\n        if not isinstance(frequency_penalty, float) or not -1 <= frequency_penalty <= 1:\n            raise ValueError(\"Invalid frequency_penalty. Expected a float between -1 and 1.\")\n\n        # Validate the presence_penalty\n        presence_penalty = kwargs.get(\"presence_penalty\", 0.0)\n        if not isinstance(presence_penalty, float) or not -1 <= presence_penalty <= 1:\n            raise ValueError(\"Invalid presence_penalty. Expected a float between"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Check if all required parameters are present\n        if not all(param in kwargs for param in LLM_GENERATION_PARAMETERS):\n            raise ValueError(\"Missing required parameters for generation\")\n\n        # Check if all parameters are valid\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs and not isinstance(kwargs[param], (int, float)):\n                raise ValueError(f\"Invalid value for parameter {param}\")\n\n        # Check if the model has parsing helper tokens\n        if not model.parsing_helper_tokens:\n            raise ValueError(\"Model does not have parsing helper tokens\")\n\n        # Check if the model is the default model\n        if model.model_name == DEFAULT_DISTILLED_MODEL_NAME:\n            raise ValueError(\"Cannot use default model for generation\")\n\n        # Check if the system message is not empty\n        if not system_message:\n            raise ValueError(\"System message cannot be empty\")\n\n        # Check if the prompt is not empty\n        if not prompt:\n            raise ValueError(\"Prompt cannot be empty\")\n\n        # Check if the prompt is not too long\n        if len(prompt) > 1000:\n            raise ValueError(\"Prompt is too long\")\n\n        # Check if the system message is not too long\n        if len(system_message) > 1000:\n            raise ValueError(\"System message is too long\")\n\n        # Check if the prompt and system message are not too long\n        if len(prompt) + len(system_message) > 2000:\n            raise ValueError(\"Prompt and system message are too long\")\n\n        # Check if the prompt and system message are not too long\n        if len(prompt) + len(system_message) > 2000:\n            raise ValueError(\"Prompt and system message are too long\")\n\n        # Check if the prompt and system message are not too long\n        if len(prompt) + len(system_message) > 2000:\n            raise ValueError(\"Prompt and system message are"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise TypeError(\"model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise TypeError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise TypeError(\"prompt must be a string\")\n\n        # Validate kwargs\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # Set up the API request\n        api_url = \"https://api.openai.com/v1/chat/completions\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n        data = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # Send the API request and handle retries\n        for i in range(5):\n            try:\n                response = requests.post(api_url, headers=headers, json=data)\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                if i < 4:\n                    backoff_time = 2 ** i\n                    logging.warning(f\"API request failed. Retrying in {backoff_time} seconds (attempt {i+1}/5)\")\n                    time.sleep(backoff_time)\n                else:\n                    raise e\n\n        # Process the response\n        response_json = response.json()\n        if \"error\" in response_json:\n            raise ValueError(f\"API error: {response_json['error']['message']}\")\n\n        # Extract the generated text from the response\n        generated_text = response_json[\"choices\"]["}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate the input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"prompt must be a string\")\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs and not isinstance(kwargs[param], (int, float)):\n                raise ValueError(f\"{param} must be an integer or float\")\n\n        # Set default values for parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1)\n        kwargs.setdefault(\"frequency_penalty\", 0)\n        kwargs.setdefault(\"presence_penalty\", 0)\n        kwargs.setdefault(\"max_new_tokens\", 100)\n\n        # Define the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            **kwargs,\n        }\n\n        # Define the headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\",\n        }\n\n        # Define the number of retries\n        num_retries = 5\n\n        # Define the exponential backoff factor\n        backoff_factor = 1.5\n\n        # Define the initial retry delay\n        retry_delay = 1\n\n        # Loop until the maximum number of retries is reached\n        for i in range(num_retries):\n            # Make the request\n            response = requests.post(OPENAI_URL, json=request_body, headers=headers)\n\n            # Check the response status code\n            if response.status"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate the parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                assert isinstance(kwargs[param], (int, float)), f\"{param} must be a number\"\n\n        # Create the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        }\n\n        # Add the optional parameters to the request body\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                request_body[param] = kwargs[param]\n\n        # Retry up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                response = requests.post(\n                    OPENAI_URL,\n                    headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                    json=request_body,\n                )\n                response.raise_for_status()\n                response_json = response.json()\n                if \"error\" in response_json:\n                    raise Exception(response_json[\"error\"])\n                elif \"choices\" in response_json:\n                    text = response_json[\"choices\"][0][\"message\"][\"content\"]\n                    # Remove any parsing helper tokens\n                    text = self.remove_parsing_helper_tokens(text, model.parsing_helper_tokens)\n                    return text\n                else:\n                    raise Exception(\"Invalid response from OpenAI API\")\n            except Exception as e:\n                if i < 4:\n                    wait_time = 2 ** i\n                    logging.warning(f\"Error generating text: {e}. Retrying in {wait_time} seconds...\")\n                    time.sleep(wait_time)\n                else:\n                    raise Exception(f\"Error generating text: {e}\")\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Validate input parameters\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter in kwargs:\n                if not isinstance(kwargs[parameter], (int, float)):\n                    raise ValueError(f\"{parameter} must be a number\")\n                if kwargs[parameter] < 0:\n                    raise ValueError(f\"{parameter} must be greater than or equal to 0\")\n\n        # Verify API key\n        self.check_api_key()\n\n        # Set up request parameters\n        request_data = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n        request_data.update(kwargs)\n\n        # Set up retry parameters\n        max_retries = 5\n        retry_delay = 1\n\n        # Send the request and handle retries\n        for i in range(max_retries):\n            try:\n                response = requests.post(OPENAI_URL, json=request_data, headers={\"Authorization\": f\"Bearer {self.api_key}\"})\n                response.raise_for_status()\n                response_data = response.json()\n                if \"error\" in response_data:\n                    raise Exception(response_data[\"error\"][\"message\"])\n                if \"choices\" not in response_data:\n                    raise Exception(\"No choices in response\")\n                if len(response_data[\"choices\"]) == 0:\n                    raise Exception(\"No choices in response\")\n                if \"message\" not in response_data[\"choices\"][0]:\n                    raise Exception(\"No message in response\")\n                if \"content\" not in response_data[\"choices\"][0][\"message\"]:\n                    raise Exception(\"No content in response\")\n                generated_text = response_data[\"choices\"][0][\"message\"][\"content\"]\n                # Remove parsing helper tokens from the generated text\n                generated_text = self.remove_parsing_helper_tokens(generated_text, model)\n                return generated_"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify that the API key is set\n        self.check_api_key()\n\n        # Validate the parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                assert isinstance(kwargs[param], (int, float))\n                assert 0 <= kwargs[param] <= 1\n\n        # Set the model name and parsing helper tokens\n        model_name = model.model_name\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # Create the request body\n        request_body = {\n            \"model\": model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n\n        # Add any additional parameters to the request body\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                request_body[param] = kwargs[param]\n\n        # Set the number of retries and initial backoff time\n        num_retries = 5\n        backoff_time = 1\n\n        # Try to generate the response up to the maximum number of retries\n        for i in range(num_retries):\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(OPENAI_URL, headers={\"Authorization\": f\"Bearer {self.api_key}\"}, json=request_body)\n\n                # Check if the request was successful\n                if response.status_code != 200:\n                    raise Exception(f\"Request failed with status code {response.status_code}\")\n\n                # Parse the response\n                response_json = response.json()\n                choices = response_json[\"choices\"]\n                if len(choices) == 0:\n                    raise Exception(\"No choices returned\")\n                choice = choices[0]\n                if \"message\" not in choice:\n                    raise Exception(\"No message in choice\")\n                message = choice[\"message\"]\n                if \"content\" not in message:\n                    raise Exception(\""}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate input parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                if not isinstance(kwargs[param], (int, float)):\n                    raise ValueError(f\"Invalid value for parameter '{param}': {kwargs[param]}\")\n\n        # Set default values for parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"max_new_tokens\", 100)\n\n        # Create the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n        request_body.update(kwargs)\n\n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(OPENAI_URL, headers={\"Authorization\": f\"Bearer {self.api_key}\"}, json=request_body)\n                response.raise_for_status()\n                response_json = response.json()\n\n                # Check if the request was successful\n                if \"choices\" not in response_json or len(response_json[\"choices\"]) == 0:\n                    raise ValueError(\"No response from OpenAI API\")\n\n                # Extract the generated text from the response\n                generated_text = response_json[\"choices\"][0][\"message\"][\"content\"].strip()\n\n                # Remove any parsing helper tokens from the generated text\n                generated_text = self.remove_parsing_helper_tokens(generated_text, model.parsing_helper_tokens)\n\n                return generated_text\n\n            except requests.exceptions.RequestException as e:\n                # Handle request exceptions\n                logging.error(f\"Request failed: {e}\")\n                if attempt < 4:\n                    # Retry with exponential backoff\n                    time.sleep(2 ** attempt"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # check if the API key is set\n        self.check_api_key()\n\n        # validate the input parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                if not isinstance(kwargs[param], (int, float)):\n                    raise ValueError(f\"Invalid value for parameter '{param}': {kwargs[param]}\")\n\n        # construct the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n\n        # add the optional parameters to the request body\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                request_body[param] = kwargs[param]\n\n        # make the request\n        response = self.client.chat.create(\n            model=model.model_name,\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n\n        # check if the request was successful\n        if response.status_code == 200:\n            # extract the response text\n            response_text = response.json()[\"choices\"][0][\"message\"][\"content\"]\n\n            # remove any parsing helper tokens from the response text\n            response_text = self.remove_parsing_helper_tokens(response_text, model.parsing_helper_tokens)\n\n            # return the response text\n            return response_text\n        else:\n            # handle the error\n            raise ValueError(f\"Request failed with status code {response.status_code}\")\n\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        self.check_api_key()\n\n        # Validate input parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                assert isinstance(kwargs[param], (int, float))\n\n        # Set default values for optional parameters\n        kwargs.setdefault(\"temperature\", 0.5)\n        kwargs.setdefault(\"top_p\", 1.0)\n        kwargs.setdefault(\"frequency_penalty\", 0.0)\n        kwargs.setdefault(\"presence_penalty\", 0.0)\n        kwargs.setdefault(\"max_new_tokens\", 100)\n\n        # Build the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # Set up the request headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        # Retry up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(OPENAI_URL, json=request_body, headers=headers)\n                response.raise_for_status()\n                response_json = response.json()\n\n                # Extract the generated text from the response\n                generated_text = response_json[\"choices\"][0][\"message\"][\"content\"]\n\n                # Remove any parsing helper tokens\n                generated_text = self.remove_parsing_helper_tokens(generated_text, model)\n\n                return generated_text\n            except Exception as e:\n                # If there is an error, wait for an exponentially increasing time before retrying\n                wait_time = 2 ** i\n                logging.error(f\"An error occurred: {e}. Retrying in {"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate parameters\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # Prepare the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                response = requests.post(\n                    OPENAI_URL,\n                    headers={\n                        \"Content-Type\": \"application/json\",\n                        \"Authorization\": f\"Bearer {self.api_key}\"\n                    },\n                    json=request_body\n                )\n                response.raise_for_status()\n                response_json = response.json()\n\n                # Check if the response contains the parsing helper tokens\n                if any(token in model.parsing_helper_tokens for token in response_json[\"choices\"][0][\"message\"][\"content\"].split()):\n                    # Remove the parsing helper tokens from the response\n                    response_json[\"choices\"][0][\"message\"][\"content\"] = self.remove_parsing_helper_tokens(response_json[\"choices\"][0][\"message\"][\"content\"], model.parsing_helper_tokens)\n\n                return response_json[\"choices\"][0][\"message\"][\"content\"]\n            except requests.exceptions.RequestException as e:\n                logging.error(f\"Error during request: {e}\")\n                time.sleep(2 ** attempt)\n\n        raise RuntimeError(\"Failed to generate response after 5 attempts\")\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Check if all required parameters are provided\n        if not system_message:\n            raise ValueError(\"Missing required parameter: system_message\")\n        if not prompt:\n            raise ValueError(\"Missing required parameter: prompt\")\n\n        # Validate the provided parameters\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # Retry up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Create the request payload\n                payload = {\n                    \"model\": model.model_name,\n                    \"messages\": [\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt}\n                    ],\n                    **kwargs\n                }\n\n                # Make the API request\n                response = requests.post(OPENAI_URL, json=payload, headers={\"Authorization\": f\"Bearer {self.api_key}\"})\n\n                # Check the response status code\n                if response.status_code != 200:\n                    raise ValueError(f\"API request failed with status code {response.status_code}\")\n\n                # Parse the response JSON\n                response_json = response.json()\n\n                # Extract the generated text\n                generated_text = response_json[\"choices\"][0][\"message\"][\"content\"].strip()\n\n                # Remove any parsing helper tokens from the generated text\n                if model.parsing_helper_tokens:\n                    for token in model.parsing_helper_tokens:\n                        generated_text = generated_text.replace(token, \"\")\n\n                # Return the generated text\n                return generated_text\n\n            except Exception as e:\n                # Log the error and wait for an exponential backoff period\n                logging.error(f\"Error: {e}\")\n                wait_time = 2 ** i\n                logging.info(f\"Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n\n        # If"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # Validate the input parameters\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter in kwargs:\n                if not isinstance(kwargs[parameter], (int, float)):\n                    raise ValueError(f\"Invalid value for parameter '{parameter}': {kwargs[parameter]}\")\n\n        # Create the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n\n        # Add the optional parameters to the request body\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter in kwargs:\n                request_body[parameter] = kwargs[parameter]\n\n        # Set the number of retries and the initial wait time\n        num_retries = 5\n        wait_time = 1\n\n        # Loop through the retries\n        for i in range(num_retries):\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(OPENAI_URL, headers={\"Authorization\": f\"Bearer {self.api_key}\"}, json=request_body)\n                response.raise_for_status()\n\n                # Process the response to remove any parsing helper tokens\n                response_json = response.json()\n                if \"choices\" in response_json and len(response_json[\"choices\"]) > 0:\n                    message = response_json[\"choices\"][0][\"message\"]\n                    if \"content\" in message:\n                        content = message[\"content\"]\n                        for token in model.parsing_helper_tokens:\n                            if content.startswith(token):\n                                content = content[len(token):]\n                        return content.strip()\n\n                # If the response does not contain a valid content, raise an exception\n                raise ValueError(\"Invalid response from OpenAI API\")\n            except requests.exceptions.RequestException as e:\n                # If there is a request exception, wait for the specified"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        self.check_api_key()\n\n        # Validate parameters\n        if not all(key in LLM_GENERATION_PARAMETERS for key in kwargs.keys()):\n            raise ValueError(f\"Invalid parameter: {kwargs.keys()}\")\n\n        # Set default parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"max_new_tokens\", 256)\n        kwargs.setdefault(\"frequency_penalty\", 0)\n        kwargs.setdefault(\"presence_penalty\", 0)\n        kwargs.setdefault(\"top_p\", 1)\n\n        # Set model and system message\n        kwargs[\"model\"] = model.model_name\n        kwargs[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}]\n\n        # Set API URL\n        api_url = OPENAI_URL\n\n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Send request to OpenAI API\n                response = requests.post(api_url, json=kwargs, headers={\"Authorization\": f\"Bearer {self.api_key}\"})\n                response.raise_for_status()\n                response_json = response.json()\n\n                # Check if the response contains any parsing helper tokens\n                if any(token in response_json[\"choices\"][0][\"message\"][\"content\"] for token in model.parsing_helper_tokens):\n                    # Remove parsing helper tokens from response\n                    response_json[\"choices\"][0][\"message\"][\"content\"] = response_json[\"choices\"][0][\"message\"][\"content\"].split(model.parsing_helper_tokens[0])[1].split(model.parsing_helper_tokens[1])[0]\n\n                # Return final text response\n                return response_json[\"choices\"][0][\"message\"][\"content\"]\n            except Exception as e:\n                #"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify that the API key is set\n        self.check_api_key()\n\n        # Validate the input parameters\n        self.validate_input_parameters(kwargs)\n\n        # Construct the request payload\n        payload = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(OPENAI_URL, json=payload, headers={\"Authorization\": f\"Bearer {self.api_key}\"})\n\n                # Check the response status code\n                if response.status_code != 200:\n                    raise Exception(f\"Request failed with status code {response.status_code}\")\n\n                # Parse the response JSON\n                response_json = response.json()\n\n                # Check if the response is an error\n                if \"error\" in response_json:\n                    raise Exception(f\"OpenAI API returned an error: {response_json['error']['message']}\")\n\n                # Get the generated text from the response\n                generated_text = self.parse_generated_text(response_json, model)\n\n                # Return the generated text\n                return generated_text\n\n            except Exception as e:\n                # Log the error\n                logging.error(f\"Error occurred during request: {e}\")\n\n                # Calculate the exponential backoff delay\n                delay = 2 ** attempt\n\n                # Sleep for the calculated delay\n                time.sleep(delay)\n\n        # If all attempts fail, raise an exception\n        raise Exception(\"Failed to generate text after 5 retries\")\n\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if API key is set\n        self.check_api_key()\n\n        # Validate input parameters\n        self.validate_generate_params(model, system_message, prompt, **kwargs)\n\n        # Set up parameters for the request\n        params = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n\n        # Add additional parameters to the request\n        for key, value in kwargs.items():\n            if key in LLM_GENERATION_PARAMETERS:\n                params[key] = value\n\n        # Set up headers for the request\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        # Retry up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(OPENAI_URL, headers=headers, json=params)\n                response.raise_for_status()\n\n                # Parse the response\n                response_json = response.json()\n                if \"choices\" in response_json:\n                    # Extract the generated text from the response\n                    generated_text = response_json[\"choices\"][0][\"message\"][\"content\"]\n\n                    # Remove any parsing helper tokens from the generated text\n                    generated_text = self.remove_parsing_helper_tokens(generated_text, model)\n\n                    # Return the generated text\n                    return generated_text\n                else:\n                    # Handle other types of responses\n                    raise ValueError(\"Unexpected response format\")\n            except requests.exceptions.RequestException as e:\n                # Handle any request-related exceptions\n                logging.warning(f\"Request failed: {e}\")\n                time.sleep(2 ** i)  # Exponential backoff\n            except ValueError as e:\n                # Handle any parsing-related exceptions\n                logging.warning(f\"Pars"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # validate input parameters\n        if not all(param in LLM_GENERATION_PARAMETERS for param in kwargs):\n            raise ValueError(\"Invalid generation parameter provided\")\n        # validate input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model provided\")\n        # validate input parameters\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message provided\")\n        # validate input parameters\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt provided\")\n\n        # set up the request payload\n        payload = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n\n        # add any additional parameters to the payload\n        for param, value in kwargs.items():\n            payload[param] = value\n\n        # set up the request headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        # set up the request URL\n        url = OPENAI_URL\n\n        # set up the number of retries and backoff time\n        num_retries = 5\n        backoff_time = 1\n\n        # set up the response variable\n        response = None\n\n        # loop through the retries\n        for i in range(num_retries):\n            # send the request\n            response = requests.post(url, headers=headers, json=payload)\n            # check the response status code\n            if response.status_code == 200:\n                # parse the response\n                response_json = response.json()\n                # remove any parsing helper tokens from the response\n                response_json[\"choices\"][0][\"message\"][\"content\"] = self.remove_parsing_helper_tokens(response_json[\"choices\"][0][\"message\"][\"content\"], model.parsing_helper_tokens)\n                #"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    # Check if the matrix is square\n    assert_is_square(x)\n\n    # Check if the matrix is equal to its transpose\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have zeros on its diagonal\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0, atol=1e-8):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0, atol=1e-8):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements must be zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        #"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            self."}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self."}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        model = self.api_provider[function_description.model_type]\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            self.token_counts[func_hash] = 0"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function has been initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is suitable for distillation\n        if function_description.suitable_for_distillation:\n            # if so, use a distilled model\n            model = function_description.distilled_model\n            save_to_finetune = False\n            is_distilled_model = True\n        else:\n            # otherwise, use a teacher model\n            model = function_description.teacher_model\n            save_to_finetune = True\n            is_distilled_model = False\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == \"\":\n            # if not, initialize it\n            self.initialized_functions[func_hash][\"model\"] = model.model_name\n\n        # check if the function is suitable for distillation\n        if function_description.suitable_for_distillation:\n            # if so, use a distilled model\n            model = function_description.distilled_model\n            save_to_finetune = False\n            is_distilled_model = True\n        else:\n            # otherwise, use a teacher model\n            model = function_description.teacher_model\n            save_to_finetune = True\n            is_distilled_model = False\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == \"\":\n            # if not, initialize it\n            self.initialized_functions[func_hash][\"model\"] = model.model_name\n\n        # check if the function is suitable for distillation\n        if function_description.suitable_for_distillation:\n            # if so, use a distilled model\n            model = function_description.distilled_model\n            save_to_finetune = False\n            is_distilled_model = True\n        else:"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # if the function is not initialized, initialize it\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = self.function_modeler.initialize_function(\n                args, kwargs, function_description)\n\n        # get the current function setup\n        current_function_setup = self.initialized_functions[func_hash]\n\n        # get the current model\n        model = current_function_setup[\"model\"]\n\n        # get the current examples\n        examples = current_function_setup[\"examples\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n        # get the current distilled model\n        distilled_model = current_function_setup[\"distilled_model\"]\n\n       "}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # if the function is not initialized, initialize it\n        if func_hash not in self.initialized_functions:\n            self.initialize_function(args, kwargs, function_description)\n\n        # get the current function setup\n        current_function_setup = self.initialized_functions[func_hash]\n        # get the current model\n        model = current_function_setup[\"model\"]\n\n        # get the current examples\n        examples = current_function_setup[\"examples\"]\n\n        # get the current align statements\n        align_statements = current_function_setup[\"align_statements\"]\n\n        # get the current prompt\n        prompt = current_function_setup[\"prompt\"]\n\n        # get the current model type\n        model_type = current_function_setup[\"model_type\"]\n\n        # get the current model provider\n        model_provider = current_function_setup[\"model_provider\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_parameters = current_function_setup[\"model_parameters\"]\n\n        # get the current model parameters\n        model_"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            self.token_counts[func_hash] = 0\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            self.token_counts[func_hash] = 0\n\n        # get the model and prompt\n        model = self.get_model(function_description)\n        prompt = self.get_prompt(args, kwargs, function_description)\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == model.model_name:\n            return prompt, model, False, False\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == \"\":\n            self.initialized_functions[func_hash][\"model\"] = model.model_name\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == model.model_name:\n            return prompt, model, False, False\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == \"\":\n            self.initialized_functions[func_hash][\"model\"] = model.model_name\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == model.model_name:\n            return prompt, model, False, False\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == \"\":\n            self."}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the function modeler\n        function_modeler = self.function_modeler\n        # get the function name\n        function_name = function_description.name\n        # get the function arguments\n        function_args = function_description.args\n        # get the function kwargs\n        function_kwargs = function_description.kwargs\n        # get the function examples\n        function_examples = function_description.examples\n        # get the function output type\n        function_output_type = function_description.output_type_hint\n        # get the function description\n        function_description = function_description.description\n        # get the function model type\n        function_model_type = function_description.model_type\n        # get the function model\n        function_model = function_description.model\n        # get the function api\n        function_api = function_description.api\n        # get the function api parameters\n        function_api_parameters = function_description.api_parameters\n        # get the function api provider\n        function_api_provider = function_description.api_provider\n        # get the function api provider parameters\n        function_api_provider_parameters = function_description.api_provider_parameters\n        # get the function api provider parameters\n        function_api_provider_parameters = function_description.api_provider_parameters\n        # get the function api provider parameters\n        function_api_provider_parameters = function_description.api_provider_parameters\n        # get the function api provider parameters\n        function_api_provider_parameters = function_description.api_provider_parameters\n        # get the function api provider parameters\n        function_api_provider_parameters = function_description.api_provider_parameters\n        # get the function api provider parameters\n        function_api_provider_parameters = function_description.api_provider_parameters\n        # get the function api provider parameters\n        function_api_provider_parameters = function_description.api_provider_parameters\n        # get the function api provider parameters\n        function_api_provider_parameters = function_description.api_provider_parameters\n        # get the function api provider parameters\n        function_api_provider_parameters = function_description.api_provider_parameters\n        #"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        model = self.function_modeler.get_model(function_description)\n\n        # get the prompt\n        prompt = self.function_modeler.get_prompt(function_description, args, kwargs)\n\n        # check if the model is suitable for distillation\n        is_distilled_model = self.function_modeler.is_model_suitable_for_distillation(function_description, model, prompt)\n\n        # check if the model is suitable for finetuning\n        save_to_finetune = self.function_modeler.is_model_suitable_for_finetuning(function_description, model, prompt)\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized and does not require saving examples for fine-tuning\n        if func_hash in self.initialized_functions and not save_to_finetune:\n            return prompt, model, False, is_distilled_model\n\n        # check if the function is already initialized and requires saving examples for fine-tuning\n        if func_hash in self.initialized_functions and save_to_finetune:\n            # update the examples for fine-tuning\n            self.function_modeler.update_examples_for_finetuning(function_description, args, kwargs, model, prompt)\n            return prompt, model, True, is_distilled_model\n\n        # if the function is not initialized, save the examples for fine-tuning\n        if func_hash not in self.initialized_functions and save_to_finetune:\n            # save the examples for fine-tuning\n            self.function_modeler.save_examples_for_finetuning(function_description, args, kwargs, model, prompt)\n            return prompt, model, True, is_distilled_model\n\n        # if the function is not initialized and does not require saving examples for fine-t"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        current_function_setup = self.initialized_functions.get(func_hash, None)\n        if not current_function_setup:\n            # if not initialized, initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n            current_function_setup = self.initialized_functions[func_hash]\n\n        # get the model\n        model = self.function_modeler.get_model(function_description)\n        # get the prompt\n        prompt = self.function_modeler.get_prompt(function_description, args, kwargs)\n\n        # check if the function is suitable for distillation\n        is_distilled_model = self.function_modeler.is_suitable_for_distillation(function_description, args, kwargs)\n        if is_distilled_model:\n            # if it is, get the distilled model\n            model = self.function_modeler.get_distilled_model(function_description)\n\n        # check if the function is already initialized\n        if current_function_setup[\"model\"] == model.model_name:\n            # if it is, check if the function is suitable for distillation\n            if is_distilled_model:\n                # if it is, return the prompt, model, and is_distilled_model\n                return prompt, model, False, True\n            else:\n                # if it is not, return the prompt, model, and is_distilled_model\n                return prompt, model, False, False\n        else:\n            # if it is not, check if the function is suitable for distillation\n            if is_distilled_model:\n                # if it is, return the prompt, model, and is_distilled_model\n                return prompt, model, True, True\n            else:\n                # if it is not, return the prompt, model, and is_distilled_model\n                return prompt, model, True, False\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        model = self.get_model(function_description)\n        # get the prompt\n        prompt = self.get_prompt(args, kwargs, function_description)\n\n        # check if the model is suitable for distillation\n        is_distilled_model = self.is_distilled_model(prompt, model)\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialize_function(function_description, model)\n\n        # check if the function is suitable for saving examples for fine-tuning\n        save_to_finetune = self.is_suitable_for_finetuning(function_description, prompt)\n\n        # update the examples for fine-tuning if necessary\n        if save_to_finetune:\n            self.update_examples_for_finetuning(function_description, prompt)\n\n        return prompt, model, save_to_finetune, is_distilled_model\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # if the function is not initialized, initialize it\n        if func_hash not in self.initialized_functions:\n            self.initialize_function(args, kwargs, function_description)\n\n        # get the function setup\n        function_setup = self.initialized_functions[func_hash]\n\n        # get the model\n        model = function_setup[\"model\"]\n\n        # get the prompt\n        prompt = self.construct_prompt(args, kwargs, function_description, function_setup[\"align_statements\"])\n\n        # get the token count\n        token_count = approximate_token_count(prompt)\n\n        # get the distillation flag\n        distillation_flag = self.token_counts[func_hash][\"distillation_flag\"]\n\n        # get the save to finetune flag\n        save_to_finetune = self.token_counts[func_hash][\"save_to_finetune\"]\n\n        # if the token count is above the threshold, set the distillation flag to true\n        if token_count > self.token_counts[func_hash][\"distillation_threshold\"]:\n            distillation_flag = True\n\n        # if the distillation flag is true, set the save to finetune flag to true\n        if distillation_flag:\n            save_to_finetune = True\n\n        # if the save to finetune flag is true, set the distillation flag to false\n        if save_to_finetune:\n            distillation_flag = False\n\n        # if the function is not initialized, set the distillation flag to false\n        if func_hash not in self.initialized_functions:\n            distillation_flag = False\n\n        # update the token count\n        self.token_counts[func_hash][\"token_count\"] = token_count\n\n        # update the distillation flag\n        self.token_counts[func_hash][\"distillation_flag\"] = distillation_flag\n\n        # update the save to finetune flag\n        self.token_counts"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # initialize function data if not already done\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n\n        # get the function data\n        current_function_setup = self.initialized_functions[func_hash]\n\n        # get the model\n        model = self.function_modeler.get_model(\n            args, kwargs, function_description, current_function_setup[\"model\"])\n\n        # get the prompt\n        prompt = self.function_modeler.get_prompt(\n            args, kwargs, function_description, model)\n\n        # check if the model is suitable for distillation\n        is_distilled_model = self.function_modeler.is_model_suitable_for_distillation(\n            args, kwargs, function_description, model)\n\n        # check if the model is suitable for saving examples for fine-tuning\n        save_to_finetune = self.function_modeler.is_model_suitable_for_finetuning(\n            args, kwargs, function_description, model)\n\n        # update examples for fine-tuning if necessary\n        if save_to_finetune:\n            current_function_setup[\"examples\"].append(\n                (args, kwargs, function_description.output_type_hint))\n\n        return prompt, model, save_to_finetune, is_distilled_model\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        model = self.api_provider[function_description.model_type]\n\n        # get the prompt\n        prompt = self.function_modeler.get_prompt(args, kwargs, function_description)\n\n        # get the token count\n        token_count = approximate_token_count(prompt)\n\n        # check if the model is suitable for distillation\n        is_distilled_model = model.suitable_for_distillation\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\"model\": model.model_name, \"examples\": []}\n\n        # check if the model is suitable for distillation\n        if is_distilled_model:\n            # check if the model is already initialized\n            if self.initialized_functions[func_hash][\"model\"] != model.model_name:\n                # initialize the model\n                self.initialized_functions[func_hash][\"model\"] = model.model_name\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == model.model_name:\n            # check if the function is already initialized\n            if len(self.initialized_functions[func_hash][\"examples\"]) == 0:\n                # save the example\n                self.initialized_functions[func_hash][\"examples\"].append(prompt)\n\n            # check if the example is already saved\n            if prompt not in self.initialized_functions[func_hash][\"examples\"]:\n                # save the example\n                self.initialized_functions[func_hash][\"examples\"].append(prompt)\n\n            # check if the token count is below the limit\n            if token_count > self.default_generation_length:\n                # save the example\n                self.initialized_functions[func_hash][\"examples\"].append(prompt)\n\n        # check if the model is suitable for distillation\n        if is_distilled_model:\n            # check"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        model = self.api_provider[function_description.model_type]\n        # get the prompt\n        prompt = self.function_modeler.get_prompt(args, kwargs, function_description)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is suitable for distillation\n        if self.function_modeler.is_suitable_for_distillation(args, kwargs, function_description):\n            # if it is, use a distilled model\n            model = self.api_provider[function_description.distilled_model_type]\n            save_to_finetune = False\n            is_distilled_model = True\n        else:\n            # if not, use a teacher model\n            save_to_finetune = True\n            is_distilled_model = False\n\n        # check if the function is already initialized and does not require saving examples for fine-tuning\n        if self.initialized_functions[func_hash][\"model\"] == \"\" and not save_to_finetune:\n            # if it is, update examples for fine-tuning\n            self.initialized_functions[func_hash][\"examples\"].append(FunctionExample(args, kwargs, prompt))\n            # check if the token count requirements are met\n            if self.token_counts.get(func_hash, 0) + approximate_token_count(prompt) > model.max_token_count:\n                # if not, use a distilled model\n                model = self.api_provider[function_description.distilled_model_type]\n                save_to_finetune = False\n                is_distilled_model = True\n\n        return prompt, model, save_to_finetune, is_distilled_model\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == \"\":\n            # if the function is not initialized, initialize it\n            self.function_modeler.initialize_function(function_description, llm_parameters)\n\n        # get the model and the prompt\n        model = self.function_modeler.get_model(function_description, llm_parameters)\n        prompt = self.function_modeler.get_prompt(args, kwargs, function_description, llm_parameters)\n\n        # check if the function is suitable for distillation\n        is_distilled_model = self.function_modeler.is_distilled_model(function_description, llm_parameters)\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == \"\":\n            # if the function is not initialized, initialize it\n            self.function_modeler.initialize_function(function_description, llm_parameters)\n\n        # check if the function is suitable for saving examples for fine-tuning\n        save_to_finetune = self.function_modeler.is_suitable_for_finetuning(function_description, llm_parameters)\n\n        # update the examples for fine-tuning if necessary\n        if save_to_finetune:\n            self.function_modeler.update_examples(function_description, args, kwargs)\n\n        return prompt, model, save_to_finetune, is_distilled_model\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            self.token_counts[func_hash] = {}\n\n        # check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # check if the function is already initialized\n            if self.initialized_functions[func_hash][\"model\"] == \"\":\n                # if the function is not initialized, initialize it\n                self.initialized_functions[func_hash][\"model\"] = self.function_modeler.get_model(function_description,\n                                                                                               llm_parameters)\n            # get the model\n            model = self.api_provider[self.initialized_functions[func_hash][\"model\"]]\n\n            # check if the function is suitable for distillation\n            if self.function_modeler.is_suitable_for_distillation(function_description, llm_parameters):\n                # if the function is suitable for distillation, use a distilled model\n                prompt = self.function_modeler.get_distilled_prompt(function_description, args, kwargs)\n                save_to_finetune = False\n                is_distilled_model = True\n                return prompt, model, save_to_finetune, is_distilled_model\n\n            # check if the function is already initialized\n            if func_hash in self.initialized_functions:\n                # get the examples\n                examples = self.initialized_functions[func_hash][\"examples\"]\n                # get the token counts\n                token_counts = self.token_counts[func_hash]\n                # check if the examples are empty\n                if len(examples) == 0:\n                    # if the examples are empty, initialize the examples\n                    examples = self.function_modeler.get_examples(function_description, args, kwargs)\n                    # update the examples\n                    self.initialized_functions[func_hash][\"examples\"] = examples\n                    # update"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is suitable for distillation\n        token_count = approximate_token_count(args, kwargs)\n        if token_count > self.function_modeler.max_distillation_token_count:\n            is_distilled_model = False\n        else:\n            is_distilled_model = True\n\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == \"\":\n            # if not, initialize the function\n            model = self.function_modeler.initialize_function(args, kwargs, function_description, llm_parameters, is_distilled_model)\n            self.initialized_functions[func_hash][\"model\"] = model.model_name\n        else:\n            model = self.function_modeler.get_model(self.initialized_functions[func_hash][\"model\"])\n\n        # check if the function is suitable for distillation\n        if is_distilled_model:\n            # if so, use the distilled model\n            prompt = self.function_modeler.get_distillation_prompt(args, kwargs, function_description)\n            save_to_finetune = False\n        else:\n            # if not, use the teacher model\n            prompt = self.function_modeler.get_prompt(args, kwargs, function_description)\n            save_to_finetune = True\n\n        return prompt, model, save_to_finetune, is_distilled_model\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002) algorithm\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah.edu/~beebe/papers/cholesky.pdf\n        # https://www.math.utah"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return cov_nearest_higham(cov, higham_max_iteration)\n    else:\n        return cov_nearest_clipping(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        # https://www.mathworks.com/matlabcentral/fileexchange/42885-expmat\n        # https://www.mathworks.com/matlabcentral/fileexchange/42886-logmat\n        # https://www.mathworks.com/matlabcentral/fileexchange/42887-expmat2\n        # https://www.mathworks.com/matlabcentral/fileexchange/42888-logmat2\n        # https://www.mathworks.com/matlabcentral/fileexchange/42889-expmat3\n        # https://www.mathworks.com/matlabcentral/fileexchange/42890-logmat3\n        # https://www.mathworks.com/matlabcentral/fileexchange/42891-expmat4\n        # https://www.mathworks.com/matlabcentral/fileexchange/42892-logmat4\n        # https://www.mathworks.com/matlabcentral/fileexchange/42893-expmat5\n        # https://www.mathworks.com/matlabcentral/fileexchange/42894-logmat5\n        # https://www.mathworks.com/matlabcentral/fileexchange/42895-expmat6\n        # https://www.mathworks.com/matlabcentral/fileexchange/42896-logmat6\n        # https://www.mathworks.com/matlabcentral/fileexchange/42897-expmat7\n        # https://www.mathworks.com/matlabcentral/fileexchange/42898-logmat7\n        # https://www.mathworks.com/matlabcentral/fileexchange/42899-expmat8\n        # https://www.mathworks.com/matlabcentral"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        cov_chol = np.linalg.cholesky(cov)\n        cov_chol_inv = np.linalg.inv(cov_chol)\n        cov_chol_inv_sqrt = np.linalg.cholesky(cov_chol_inv)\n        cov_chol_inv_sqrt_t = cov_chol_inv_sqrt.T\n        cov_chol_inv_sqrt_t_cov_chol_inv_sqrt = cov_chol_inv_sqrt_t.dot(\n            cov_chol_inv_sqrt\n        )\n        cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_cov_chol_inv_sqrt_t = cov_chol_inv_sqrt_t_cov_chol_inv_sqrt.dot(\n            cov_chol_inv_sqrt_t\n        )\n        cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_t = cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_cov_chol_inv_sqrt_t.dot(\n            cov_chol_inv_sqrt_t\n        )\n        cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_t = cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_t.dot(\n            cov_chol_inv_sqrt_t\n        )\n        cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_t_cov_chol_inv_sqrt_t"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        # This algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input matrix until it becomes positive definite\n        # The algorithm is implemented as a loop with a maximum number of iterations\n        # The algorithm is based on the idea of iteratively adding small perturbations to the input"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = _cov_nearest_clipped(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use Higham & Nick (2002) algorithm\n        return _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        # Clip eigenvalues\n        return _cov_nearest_clipping(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        cov_nearest = cov.copy()\n        for _ in range(higham_max_iteration):\n            try:\n                np.linalg.cholesky(cov_nearest)\n                return cov_nearest\n            except np.linalg.LinAlgError:\n                eigvals, eigvecs = np.linalg.eigh(cov_nearest)\n                eigvals[eigvals < 0] = 0\n                cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n    else:\n        # Clip eigenvalues to ensure the resulting matrix is positive definite\n        eigvals, eigvecs = np.linalg.eigh(cov)\n        eigvals[eigvals < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm\n        cov_nearest = cov.copy()\n        for _ in range(higham_max_iteration):\n            try:\n                np.linalg.cholesky(cov_nearest)\n                break\n            except np.linalg.LinAlgError:\n                eigvals, _ = np.linalg.eigh(cov_nearest)\n                eigvals[eigvals < 0] = 0\n                cov_nearest = np.linalg.inv(np.diag(eigvals))\n    else:\n        # Clip eigenvalues\n        cov_nearest = cov.copy()\n        eigvals, _ = np.linalg.eigh(cov_nearest)\n        eigvals[eigvals < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov_nearest = np.linalg.inv(np.diag(eigvals))\n\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use Higham & Nick (2002) algorithm\n        return _cov_nearest_higham(cov, higham_max_iteration)\n    else:\n        # Use clipping of eigenvalues\n        return _cov_nearest_clipping(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = cov_nearest_higham(cov, max_iter=higham_max_iteration)\n    else:\n        cov = cov_nearest_clipping(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _cov_nearest_higham(cov, higham_max_iteration)\n    else:\n        return _cov_nearest_clipping(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        return _cov_nearest_clipped(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Check if the input is a 2D array\n    if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"Input matrix must be a square 2D array\")\n\n    # Check if the input is symmetric\n    if not np.allclose(cov, cov.T):\n        raise ValueError(\"Input matrix must be symmetric\")\n\n    # Compute the nearest covariance matrix using the Higham & Nick (2002) algorithm\n    if higham:\n        # Compute the nearest covariance matrix using the Higham & Nick (2002) algorithm\n        cov_nearest = _higham_nearest_cov(cov, max_iteration=higham_max_iteration)\n\n    # Compute the nearest covariance matrix using clipping\n    else:\n        # Compute the eigenvalues and eigenvectors of the input covariance matrix\n        eigvals, eigvecs = np.linalg.eigh(cov)\n\n        # Clip the eigenvalues to ensure they are positive\n        eigvals[eigvals < _CLIPPING_VALUE] = _CLIPPING_VALUE\n\n        # Compute the nearest covariance matrix using the clipped eigenvalues and eigenvectors\n        cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n\n    # Check if the resulting covariance matrix is positive definite\n    if not is_positive_definite(cov_nearest):\n        raise ValueError(\"The resulting covariance matrix is not positive definite\")\n\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Check if the input is a covariance matrix\n    if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"The input must be a square matrix\")\n\n    # If the input is already positive definite, return the input\n    if is_positive_definite(cov):\n        return cov\n\n    # If the input is not positive definite, adjust the eigenvalues to make it positive definite\n    if higham:\n        # Use the Higham & Nick (2002) algorithm to adjust the eigenvalues\n        cov = higham_cov_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        # Clip the eigenvalues to make the matrix positive definite\n        cov = clip_cov_eigenvalues(cov)\n\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002) algorithm\n        cov_nearest = cov\n        for _ in range(higham_max_iteration):\n            w, v = np.linalg.eig(cov_nearest)\n            w_pos = np.maximum(w, 0)\n            cov_nearest = v @ np.diag(w_pos) @ v.T\n            if np.all(w >= 0):\n                break\n        return cov_nearest\n\n    # Clip eigenvalues\n    w, v = np.linalg.eig(cov)\n    w_pos = np.maximum(w, _CLIPPING_VALUE)\n    cov_nearest = v @ np.diag(w_pos) @ v.T\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Compute the nearest positive definite matrix using Higham & Nick (2002) algorithm\n        cov_nearest = cov.copy()\n        cov_nearest = cov_nearest + np.eye(cov.shape[0]) * _CLIPPING_VALUE\n        for _ in range(higham_max_iteration):\n            w, v = np.linalg.eig(cov_nearest)\n            w = np.maximum(w, 0)\n            cov_nearest = v @ np.diag(w) @ v.T\n            if np.allclose(cov_nearest, cov):\n                break\n        cov_nearest = cov_nearest + np.eye(cov.shape[0]) * _CLIPPING_VALUE\n    else:\n        # Compute the nearest positive definite matrix by clipping eigenvalues\n        w, v = np.linalg.eig(cov)\n        w = np.maximum(w, _CLIPPING_VALUE)\n        cov_nearest = v @ np.diag(w) @ v.T\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Ensure input is a square matrix\n    if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"The input matrix must be square\")\n\n    # Check if the input matrix is already positive definite\n    if is_positive_definite(cov):\n        return cov\n\n    # Use Higham & Nick (2002) algorithm if higham is True\n    if higham:\n        # Compute the nearest positive definite matrix using the Higham & Nick (2002) algorithm\n        cov_nearest = _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        # Clip eigenvalues and compute the nearest positive definite matrix\n        cov_nearest = _cov_nearest_clipping(cov)\n\n    # Check if the resulting matrix is positive definite\n    if not is_positive_definite(cov_nearest):\n        raise ValueError(\"The resulting matrix is not positive definite\")\n\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Compute the nearest positive definite matrix using Higham & Nick (2002) algorithm\n    if higham:\n        cov = _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = _cov_nearest_clipping(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002) algorithm\n        cov = np.asarray(cov, dtype=float)\n        n = cov.shape[0]\n        b = np.zeros((n, n))\n        d = np.zeros((n,))\n        for i in range(higham_max_iteration):\n            b = (cov + cov.T) / 2\n            d, _ = np.linalg.eig(b)\n            if np.all(d > 0):\n                break\n            min_d = np.min(d)\n            if min_d > 0:\n                break\n            d[d <= 0] = min_d\n            cov = b - np.diag(d)\n        return cov\n\n    # Clip eigenvalues based on the specified parameters\n    cov = np.asarray(cov, dtype=float)\n    n = cov.shape[0]\n    d, _ = np.linalg.eig(cov)\n    d = np.maximum(d, _CLIPPING_VALUE)\n    return np.dot(np.dot(np.diag(d), np.linalg.inv(np.diag(d))), cov)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home, ignore_errors=True)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home, ignore_errors=True)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home, ignore_errors=True)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home, ignore_errors=True)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return obj, IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}.\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise NotImplementedError(f\"Flattening {type(obj)} is not supported.\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        res, schema = ListSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, collections.Mapping):\n        res, schema = DictSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, Instances):\n        res, schema = InstancesSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        res, schema = TensorWrapSchema.flatten(obj)\n        return res, schema\n    else:\n        raise NotImplementedError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        res, schema = ListSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, tuple):\n        res, schema = TupleSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, dict):\n        res, schema = DictSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, Instances):\n        res, schema = InstancesSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, Boxes):\n        res, schema = TensorWrapSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, ROIMasks):\n        res, schema = TensorWrapSchema.flatten(obj)\n        return res, schema\n    else:\n        raise TypeError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return obj, IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    raise TypeError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise TypeError(f\"Unsupported type {type(obj)}.\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return obj, IdentitySchema()\n    elif isinstance(obj, bytes):\n        return obj, IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        res, schema = ListSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        res, schema = DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        res, schema = InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        res, schema = TensorWrapSchema.flatten(obj)\n    else:\n        raise NotImplementedError(f\"Unsupported type {type(obj)}\")\n    return res, schema\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise TypeError(f\"Unsupported type {type(obj)}.\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        res, schema = ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        res, schema = TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        res, schema = DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        res, schema = InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        res, schema = TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")\n    return res, schema\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, list):\n        res, schema = ListSchema.flatten(obj)\n        return res, schema\n    if isinstance(obj, tuple):\n        res, schema = TupleSchema.flatten(obj)\n        return res, schema\n    if isinstance(obj, dict):\n        res, schema = DictSchema.flatten(obj)\n        return res, schema\n    if isinstance(obj, Instances):\n        res, schema = InstancesSchema.flatten(obj)\n        return res, schema\n    if isinstance(obj, Boxes) or isinstance(obj, ROIMasks):\n        res, schema = TensorWrapSchema.flatten(obj)\n        return res, schema\n    raise TypeError(f\"Unsupported type {type(obj)} for flattening.\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        res = [flatten_to_tuple(k) for k in obj]\n        values, schema = ListSchema.flatten(res)\n        return values, schema\n    elif isinstance(obj, collections.abc.Mapping):\n        res = [flatten_to_tuple(k) for k in obj.values()]\n        values, schema = ListSchema.flatten(res)\n        return values, DictSchema(schema.schemas, schema.sizes, list(obj.keys()))\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}.\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return obj, IdentitySchema()\n    if isinstance(obj, (list, tuple)):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    raise ValueError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return obj, IdentitySchema()\n    elif isinstance(obj, bytes):\n        return obj, IdentitySchema()\n    elif isinstance(obj, int):\n        return obj, IdentitySchema()\n    elif isinstance(obj, float):\n        return obj, IdentitySchema()\n    elif isinstance(obj, torch.Tensor):\n        return obj, IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise TypeError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    raise TypeError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, (list, tuple)):\n        res, schema = ListSchema.flatten(obj)\n        return res, schema\n    if isinstance(obj, collections.abc.Mapping):\n        res, schema = DictSchema.flatten(obj)\n        return res, schema\n    if isinstance(obj, Instances):\n        res, schema = InstancesSchema.flatten(obj)\n        return res, schema\n    if isinstance(obj, Boxes):\n        res, schema = TensorWrapSchema.flatten(obj)\n        return res, schema\n    if isinstance(obj, ROIMasks):\n        res, schema = TensorWrapSchema.flatten(obj)\n        return res, schema\n    raise TypeError(f\"Unsupported type {type(obj)}.\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise TypeError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return obj, IdentitySchema()\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    raise ValueError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    raise ValueError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return obj, IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise TypeError(f\"Unsupported type: {type(obj)}\")\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if the groups and equations are valid\n    if groups is None or equations is None:\n        raise EquationToMatrixError(\n            f\"{names[0]} and {names[1]} cannot be None\"\n        )\n    if not isinstance(groups, np.ndarray):\n        raise EquationToMatrixError(\n            f\"{names[0]} must be a numpy array\"\n        )\n    if not isinstance(equations, np.ndarray):\n        raise EquationToMatrixError(\n            f\"{names[1]} must be a numpy array\"\n        )\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} must be a 2D array\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"{names[1]} must be a 1D array\"\n        )\n\n    # Initialize the left and right matrices\n    n_equations = equations.shape[0]\n    n_assets = groups.shape[1]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    # Iterate over the equations\n    for i, equation in enumerate(equations):\n        # Split the equation into groups and coefficients\n        groups_coefs = re.split(r\"([<>=]+)\", equation)\n\n        # Iterate over the groups and coefficients\n        for j in range(0, len(groups_coefs), 2):\n            # Get the group and coefficient\n            group = groups_coefs[j]\n            coef = groups_coefs[j + 1]\n\n            # Check if the group is in the groups array\n            if group not in groups:\n                if raise_if_group_missing:\n                    raise GroupNotFoundError(\n                        f\"Group {group} not found in {names[0]} array\"\n                    )\n                else:\n                    warnings.warn(\n                        f\"Group {group} not found in {names[0]} array\",\n                        UserWarning,\n                    )\n                    continue"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check input types\n    if not isinstance(groups, np.ndarray):\n        raise TypeError(\n            f\"`{names[0]}` must be a numpy array. Got {type(groups)} instead.\"\n        )\n\n    if not isinstance(equations, np.ndarray):\n        raise TypeError(\n            f\"`{names[1]}` must be a numpy array. Got {type(equations)} instead.\"\n        )\n\n    # Check input shapes\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"`{names[0]}` must be a 2D array. Got {groups.ndim} dimensions instead.\"\n        )\n\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"`{names[1]}` must be a 1D array. Got {equations.ndim} dimensions instead.\"\n        )\n\n    # Check input values\n    if not np.issubdtype(groups.dtype, np.number):\n        raise ValueError(\n            f\"`{names[0]}` must be a numeric array. Got {groups.dtype} instead.\"\n        )\n\n    if not np.issubdtype(equations.dtype, np.str_):\n        raise ValueError(\n            f\"`{names[1]}` must be a string array. Got {equations.dtype} instead.\"\n        )\n\n    # Check if all equations are valid\n    for equation in equations:\n        if not isinstance(equation, str):\n            raise EquationToMatrixError(\n                f\"`{names[1]}` must be a string array. Got {type(equation)} instead.\"\n            )\n        if not re.match(r\"^[+-]?\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?(?:[+-][+-]?\\d+)?[*]?\\w+(?:[*]?\\w+)*(?:[<=]|[>=])[+-]?\\d+(?:\\.\\d+)?(?:[eE][+-"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert to numpy arrays\n    groups = np.array(groups)\n    equations = np.array(equations)\n\n    # Check if all groups are present in the groups array\n    for equation in equations:\n        groups_in_equation = re.findall(r\"[a-zA-Z]+\", equation)\n        if not all(group in groups for group in groups_in_equation):\n            if raise_if_group_missing:\n                raise GroupNotFoundError(\n                    f\"One or more groups mentioned in the {names[1]} array are not found in the {names[0]} array.\"\n                )\n            else:\n                warnings.warn(\n                    f\"One or more groups mentioned in the {names[1]} array are not found in the {names[0]} array.\"\n                )\n\n    # Initialize the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Iterate over each equation and extract the groups and coefficients\n    for i, equation in enumerate(equations):\n        groups_in_equation = re.findall(r\"[a-zA-Z]+\", equation)\n        coefficients = re.findall(r\"[+-]?\\d+\", equation)\n\n        # Convert coefficients to float\n        coefficients = [float(coefficient) for coefficient in coefficients]\n\n        # Check if the number of groups and coefficients match\n        if len(groups_in_equation) != len(coefficients):\n            raise EquationToMatrixError(\n                f\"The number of groups ({len(groups_in_equation)}) and coefficients ({len(coefficients)}) in equation {i} do not match.\"\n            )\n\n        # Iterate over each group and coefficient and update the left and right matrices\n        for j, group in enumerate(groups_in_equation):\n            group_index = np.where(groups == group)[0][0]\n            left[i, group_index] = coefficients[j]\n\n        # Add the right side of the equation to the right matrix\n        right[i] = re"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert input arrays to numpy arrays\n    groups = np.array(groups)\n    equations = np.array(equations)\n\n    # Check if all groups are present in the groups array\n    all_groups_present = True\n    for group in groups:\n        if not np.any(np.all(group == groups, axis=1)):\n            all_groups_present = False\n            break\n\n    if not all_groups_present:\n        if raise_if_group_missing:\n            raise GroupNotFoundError(\n                f\"Not all groups mentioned in the {names[1]} are present in the {names[0]} array.\"\n            )\n        else:\n            warnings.warn(\n                f\"Not all groups mentioned in the {names[1]} are present in the {names[0]} array.\"\n            )\n            return None\n\n    # Initialize the left and right matrices\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    # Loop over each equation\n    for i, equation in enumerate(equations):\n        # Split the equation into groups and coefficients\n        groups_coefs = re.findall(r\"[+-]?\\d*\\.\\d+|[+-]?\\d+\", equation)\n        groups_coefs = [\n            groups_coefs[i] + groups_coefs[i + 1]\n            for i in range(0, len(groups_coefs), 2)\n        ]\n        groups_coefs = [\n            groups_coefs[i] + groups_coefs[i + 1]\n            for i in range(0, len(groups_coefs), 2)\n        ]\n\n        # Loop over each group and coefficient\n        for j, group_coef in enumerate(groups_coefs):\n            # Get the group and coefficient\n            group, coef = group_coef.split"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if isinstance(groups, list):\n        groups = np.array(groups)\n    if isinstance(equations, list):\n        equations = np.array(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"The '{names[0]}' array must be 2-dimensional, but got {groups.ndim} dimensions.\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"The '{names[1]}' array must be 1-dimensional, but got {equations.ndim} dimensions.\"\n        )\n\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        left[i] = np.zeros(n_assets)\n        right[i] = 0\n\n        groups_in_equation = re.findall(r\"[a-zA-Z]+\", equation)\n        if len(groups_in_equation) == 0:\n            raise EquationToMatrixError(\n                f\"The equation '{equation}' does not contain any groups. \"\n                \"Please provide a valid equation with at least one group.\"\n            )\n\n        for group in groups_in_equation:\n            group_index = np.where(groups[:, 0] == group)[0]\n            if len(group_index) == 0:\n                if raise_if_group_missing:\n                    raise GroupNotFoundError(\n                        f\"The group '{group}' mentioned in the equation '{equation}' was not found in the '{names[0]}' array.\"\n                    )\n                else:\n                    warnings.warn(\n                        f\"The group '{group}' mentioned in the equation '{equation}' was not found in the '{names[0]}' array. \"\n                        \"The equation will be ignored.\"\n                    )\n                    continue\n\n            left[i] += groups[group_"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert groups and equations to numpy arrays\n    groups = np.array(groups)\n    equations = np.array(equations)\n\n    # Check if any group is mentioned in the equations\n    if not any(group in equation for equation in equations for group in groups):\n        if raise_if_group_missing:\n            raise GroupNotFoundError(f\"None of the groups in {names[0]} are mentioned in {names[1]}.\")\n        else:\n            warnings.warn(f\"None of the groups in {names[0]} are mentioned in {names[1]}.\")\n            return None\n\n    # Extract the group names from the equations\n    group_names = set(re.findall(r\"\\w+\", \" \".join(equations)))\n\n    # Check if all group names are present in the groups array\n    if not all(group in group_names for group in groups):\n        raise EquationToMatrixError(f\"All groups in {names[0]} must be mentioned in {names[1]}.\")\n\n    # Initialize the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Iterate over the equations and fill the left and right matrices\n    for i, equation in enumerate(equations):\n        # Split the equation into groups and coefficients\n        groups_coefs = re.findall(r\"([+-]?\\d*\\.?\\d*)\\s*(\\w+)\", equation)\n\n        # Iterate over the groups and coefficients and fill the left and right matrices\n        for coef, group in groups_coefs:\n            coef = float(coef)\n            group_idx = np.where(groups == group)[0][0]\n            left[i, group_idx] = coef\n\n        # Add the constant term to the right matrix\n        right[i] = re.findall(r\"([+-]?\\d*\\.?\\d*)$\", equation)[0]\n\n    # Normalize the left matrix if sum_to_one is True\n    if sum_to_one:\n        left"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"Expected groups to be a 2D array, got {groups.ndim}D array instead.\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"Expected equations to be a 1D array, got {equations.ndim}D array instead.\"\n        )\n\n    if groups.shape[1] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"The number of assets in {names[0]} ({groups.shape[1]}) does not match the number of assets in {names[1]} ({equations.shape[0]}).\"\n        )\n\n    # Extract the groups from the equations\n    groups_in_equations = re.findall(r\"\\[[^\\]]+\\]\", \" \".join(equations))\n    groups_in_equations = [\n        group.strip(\"[]\").split(\",\") for group in groups_in_equations\n    ]\n\n    # Check if all groups in the equations are part of the input groups\n    if not all(\n        any(np.all(group == g) for g in groups) for group in groups_in_equations\n    ):\n        if raise_if_group_missing:\n            raise GroupNotFoundError(\n                f\"One or more groups mentioned in {names[1]} are not part of {names[0]}.\"\n            )\n        else:\n            warnings.warn(\n                f\"One or more groups mentioned in {names[1]} are not part of {names[0]}.\"\n            )\n            return None\n\n    # Create the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    for i, equation in enumerate(equations):\n        # Extract the coefficients and groups from the equation\n        coeffs = re.findall(r\"[+-]?\\d+\\.?\\d*\", equation)"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups)\n    equations = np.array(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array\")\n\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array\")\n\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(f\"{names[0]} and {names[1]} must have compatible shapes\")\n\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros((n_equations,))\n\n    for i, equation in enumerate(equations):\n        if equation == \"\":\n            continue\n\n        groups_found = []\n        for group_index in range(n_groups):\n            group_name = f\"G{group_index + 1}\"\n            if group_name in equation:\n                groups_found.append(group_index)\n\n        if len(groups_found) == 0:\n            if raise_if_group_missing:\n                raise GroupNotFoundError(\n                    f\"No groups found in {names[1]} for equation {i+1}\"\n                )\n            else:\n                warnings.warn(\n                    f\"No groups found in {names[1]} for equation {i+1}. Skipping equation.\"\n                )\n                continue\n\n        for group_index in groups_found:\n            group_name = f\"G{group_index + 1}\"\n            equation = equation.replace(group_name, \"\")\n\n        equation = equation.replace(\" \", \"\")\n\n        if \"<=\" in equation:\n            equation = equation.replace(\"<=\", \"\")\n        elif \">=\" in equation:\n            equation = equation.replace(\">=\", \"\")\n            left[i, :] = -left[i, :]\n            right[i] = -right[i"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups)\n    equations = np.array(equations)\n\n    n_groups, n_assets = groups.shape\n    n_equations = equations.shape[0]\n\n    # Initialize the left and right matrices\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    # Compile the regular expression pattern for matching group names\n    pattern = r\"(\\w+)\"\n\n    # Iterate over the equations\n    for i, equation in enumerate(equations):\n\n        # Split the equation into left and right sides\n        left_side, right_side = equation.split(\"<=\")\n\n        # Find all group names in the left side\n        left_groups = re.findall(pattern, left_side)\n\n        # Check if any groups are missing\n        missing_groups = set(left_groups) - set(names[0])\n        if missing_groups and raise_if_group_missing:\n            raise GroupNotFoundError(\n                f\"The following groups are missing in the {names[0]} array: {missing_groups}.\"\n            )\n        elif missing_groups:\n            warnings.warn(\n                f\"The following groups are missing in the {names[0]} array: {missing_groups}. Equation {i} will be ignored.\"\n            )\n            continue\n\n        # Iterate over the groups in the left side\n        for group in left_groups:\n\n            # Find the index of the group in the groups array\n            group_index = np.where(names[0] == group)[0][0]\n\n            # Add the group's assets to the left matrix\n            left[i, :] += groups[group_index, :]\n\n        # Convert the right side to a float\n        right[i] = float(right_side)\n\n    # Check if any groups are missing\n    if np.any(np.isnan(left)):\n        raise EquationToMatrixError(\n            f\"The {names[1]} array contains equations that reference groups that are not present in the {names[0]} array.\"\n        )"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert groups and equations to numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if all groups are present in the groups array\n    missing_groups = []\n    for group in groups:\n        if not np.any(group):\n            missing_groups.append(group)\n    if missing_groups and raise_if_group_missing:\n        raise GroupNotFoundError(\n            f\"The following groups are not present in the {names[0]} array: {missing_groups}\"\n        )\n    elif missing_groups:\n        warnings.warn(\n            f\"The following groups are not present in the {names[0]} array: {missing_groups}\"\n        )\n\n    # Initialize left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Iterate over equations and update left and right matrices\n    for i, equation in enumerate(equations):\n        # Split the equation into left and right sides\n        left_side, right_side = equation.split(\"<=\")\n\n        # Parse the left side of the equation\n        left_side = left_side.strip()\n        left_side_terms = re.split(r\"\\s*[+-]\\s*\", left_side)\n        for term in left_side_terms:\n            if \"*\" in term:\n                # If the term contains a multiplication operator, extract the group and coefficient\n                group_name, coefficient = term.split(\"*\")\n                group_name = group_name.strip()\n                coefficient = float(coefficient.strip())\n            else:\n                # If the term does not contain a multiplication operator, assume the coefficient is 1\n                group_name = term.strip()\n                coefficient = 1.0\n\n            # Find the group index in the groups array\n            group_index = np.where(groups == group_name)[0]\n\n            # If the group is not found, raise an error or issue a warning\n            if len(group_index) == 0:\n                if raise_"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert groups and equations to numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if any groups are mentioned in the equations\n    if not np.any(np.isin(groups, equations)):\n        if raise_if_group_missing:\n            raise GroupNotFoundError(\n                f\"None of the groups in the {names[0]} array are mentioned in the {names[1]} array.\"\n            )\n        else:\n            warnings.warn(\n                f\"None of the groups in the {names[0]} array are mentioned in the {names[1]} array.\",\n                UserWarning,\n            )\n            return None\n\n    # Create a dictionary to store the coefficients for each group\n    group_coefficients = {}\n\n    # Loop through each equation and extract the coefficients for each group\n    for equation in equations:\n        # Split the equation into its components\n        components = re.split(r\"[+-]\", equation)\n\n        # Loop through each component and extract the group and its coefficient\n        for component in components:\n            # Split the component into the group and its coefficient\n            group, coefficient = component.split(\"*\")\n\n            # Convert the coefficient to a float\n            coefficient = float(coefficient)\n\n            # Add the coefficient to the group's total\n            if group in group_coefficients:\n                group_coefficients[group] += coefficient\n            else:\n                group_coefficients[group] = coefficient\n\n    # Create a list to store the coefficients for each group\n    coefficients = []\n\n    # Loop through each group and find its coefficients in the group_coefficients dictionary\n    for group in groups:\n        # Find the coefficients for the group in the group_coefficients dictionary\n        group_coefficients_list = [\n            group_coefficients.get(str(g), 0) for g in group\n        ]\n\n        # Add the coefficients for the group to the list\n        coefficients.append(group_coefficients_list)\n\n    # Convert the coefficients to a numpy array\n    coefficients = np.array"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if groups and equations are arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if groups and equations are 2D arrays\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} must be a 2D array, got {groups.ndim}D array instead\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"{names[1]} must be a 1D array, got {equations.ndim}D array instead\"\n        )\n\n    # Check if groups and equations have the same number of assets\n    n_assets = groups.shape[1]\n    if n_assets != equations.shape[0]:\n        raise ValueError(\n            f\"{names[0]} and {names[1]} must have the same number of assets\"\n        )\n\n    # Check if groups and equations have the same number of assets\n    if n_assets != equations.shape[0]:\n        raise ValueError(\n            f\"{names[0]} and {names[1]} must have the same number of assets\"\n        )\n\n    # Check if groups and equations have the same number of assets\n    if n_assets != equations.shape[0]:\n        raise ValueError(\n            f\"{names[0]} and {names[1]} must have the same number of assets\"\n        )\n\n    # Initialize left and right matrices\n    left = np.zeros((len(equations), n_assets))\n    right = np.zeros(len(equations))\n\n    # Loop through equations and parse them\n    for i, equation in enumerate(equations):\n        # Split equation into left and right sides\n        left_side, right_side = equation.split(\"<=\")\n\n        # Loop through groups and parse them\n        for j, group in enumerate(groups):\n            # Check if group is mentioned in the left side\n            if group.any():\n                if group.any():\n                    if group.any():\n                        # Get group name and coefficient\n                        group_name = group"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if groups and equations are provided as 1D arrays\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"The '{names[0]}' argument must be a 2D array, but it has shape {groups.shape}.\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"The '{names[1]}' argument must be a 1D array, but it has shape {equations.shape}.\"\n        )\n\n    # Check if the number of equations and groups are equal\n    if equations.shape[0] != groups.shape[0]:\n        raise ValueError(\n            f\"The number of equations ({equations.shape[0]}) must be equal to the number of groups ({groups.shape[0]}).\"\n        )\n\n    # Check if the number of assets in each group is equal\n    if not np.all(groups.sum(axis=1) == groups.shape[1]):\n        raise ValueError(\n            f\"The number of assets in each group must be equal to the number of assets in the portfolio ({groups.shape[1]}).\"\n        )\n\n    # Check if all elements in groups are either 0 or 1\n    if not np.all((groups == 0) | (groups == 1)):\n        raise ValueError(\n            f\"All elements in the '{names[0]}' array must be either 0 or 1.\"\n        )\n\n    # Check if the groups are sorted in the same order as the equations\n    if not np.all(groups.sum(axis=1) == groups.sum(axis=1).sort()):\n        raise ValueError(\n            f\"The '{names[0]}' array must be sorted in the same order as the '{names[1]}' array.\"\n        )\n\n    # Check if the equations are valid\n    for equation in equations:\n        if not isinstance(equation, str):\n            raise ValueError(\n                f\"All elements in the '{names[1]}' array must be strings.\"\n            )\n        if not re.match(r\"^\\s*[-"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups)\n    equations = np.array(equations)\n\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The {names[0]} array must have 2 dimensions. \"\n            f\"Got {groups.ndim} dimensions instead.\"\n        )\n\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The {names[1]} array must have 1 dimension. \"\n            f\"Got {equations.ndim} dimensions instead.\"\n        )\n\n    if groups.shape[0] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"The number of groups ({groups.shape[0]}) in the {names[0]} array \"\n            f\"must match the number of equations ({equations.shape[0]}) \"\n            f\"in the {names[1]} array.\"\n        )\n\n    # Initialize the left and right matrices\n    left = np.zeros((equations.shape[0], groups.shape[1]))\n    right = np.zeros(equations.shape[0])\n\n    # Define a regular expression pattern to extract group names and coefficients\n    pattern = r\"(-?\\d*\\.?\\d+)?\\s*(\\w+)\"\n\n    # Iterate over each equation and extract the group names and coefficients\n    for i, equation in enumerate(equations):\n        matches = re.findall(pattern, equation)\n\n        if not matches:\n            raise EquationToMatrixError(\n                f\"Invalid equation: {equation}. \"\n                f\"Could not extract group names and coefficients.\"\n            )\n\n        # Iterate over each match and update the left and right matrices\n        for match in matches:\n            coefficient, group = match\n\n            # If the coefficient is not provided, assume it is 1\n            if coefficient is None:\n                coefficient = 1\n\n            # Convert the coefficient to a float\n            coefficient = float(coefficient)\n\n            # Find the index of the group in the groups array\n            group_index = np.where(groups[i] == group)[0]"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert input to numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Get the number of groups and assets\n    n_groups, n_assets = groups.shape\n\n    # Initialize the left and right matrices\n    left = np.zeros((len(equations), n_assets))\n    right = np.zeros(len(equations))\n\n    # Iterate over the equations\n    for i, equation in enumerate(equations):\n        # Split the equation into its components\n        components = equation.split(\"<=\")\n\n        # Check if the equation is valid\n        if len(components) != 2:\n            raise EquationToMatrixError(\n                f\"Invalid equation: {equation}. Expected format: 'group1 <= group2'.\"\n            )\n\n        # Get the group names from the equation\n        group1, group2 = components[0].strip(), components[1].strip()\n\n        # Find the indices of the groups in the groups array\n        try:\n            group1_idx = np.where(groups == group1)[0][0]\n            group2_idx = np.where(groups == group2)[0][0]\n        except IndexError:\n            if raise_if_group_missing:\n                raise GroupNotFoundError(\n                    f\"Group {group1} or {group2} not found in {names[0]} array.\"\n                )\n            warnings.warn(\n                f\"Group {group1} or {group2} not found in {names[0]} array. Equation {i+1} will be ignored.\"\n            )\n            continue\n\n        # Add the group1 to the left matrix\n        left[i, group1_idx] = 1\n\n        # Add the group2 to the right matrix\n        right[i] = 1\n\n        # Subtract the group2 from the left matrix\n        left[i, group2_idx] = -1\n\n    if sum_to_one:\n        # Add a constraint to ensure that all elements in the group sum to one\n        group_sum_constraint = np.zeros((len(equations), n_assets"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert input to numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if any group is mentioned in the equations\n    groups_in_equations = np.unique(\n        [re.findall(r\"\\b\\w+\\b\", equation) for equation in equations]\n    )\n    if not groups_in_equations.size:\n        warnings.warn(\n            f\"No groups found in the {names[1]} array. Returning None.\"\n        )\n        return None\n\n    # Check if all groups mentioned in the equations are part of the input groups\n    groups_not_found = np.setdiff1d(groups_in_equations, groups[:, 0])\n    if groups_not_found.size:\n        if raise_if_group_missing:\n            raise GroupNotFoundError(\n                f\"The following groups were not found in the {names[0]} array: {groups_not_found}\"\n            )\n        warnings.warn(\n            f\"The following groups were not found in the {names[0]} array: {groups_not_found}. Returning None.\"\n        )\n        return None\n\n    # Extract the names of the groups from the equations\n    groups_names = np.unique(groups_in_equations)\n\n    # Initialize the left and right matrices\n    n_equations = equations.size\n    n_assets = groups.shape[1]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    # Iterate over each equation and populate the left and right matrices\n    for i, equation in enumerate(equations):\n        # Extract the coefficients and group names from the equation\n        coeffs, groups = _extract_coeffs_and_groups(equation)\n\n        # Check if the groups are part of the input groups\n        if not np.all(np.isin(groups, groups_names)):\n            raise EquationToMatrixError(\n                f\"The following groups were not found in the {names[0]} array: {np.setdiff1"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert to numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if all groups are present in the groups array\n    groups_present = np.all(np.isin(groups, groups), axis=1)\n    if not np.any(groups_present):\n        if raise_if_group_missing:\n            raise GroupNotFoundError(f\"No groups found in {names[0]}\")\n        warnings.warn(f\"No groups found in {names[0]}\")\n        return None\n\n    # Initialize the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Iterate through the equations\n    for i, equation in enumerate(equations):\n        # Split the equation into left and right sides\n        left_side, right_side = equation.split(\"<=\")\n\n        # Extract the group and coefficient from the left side\n        group, coefficient = left_side.strip().split(\" \")\n        group = group.strip()\n        coefficient = coefficient.strip()\n\n        # Find the index of the group in the groups array\n        group_index = np.where(groups_present)[0][np.where(groups == group)[0][0]]\n\n        # Convert the coefficient to a float\n        try:\n            coefficient = float(coefficient)\n        except ValueError:\n            raise EquationToMatrixError(\n                f\"Invalid coefficient '{coefficient}' in equation '{equation}'\"\n            )\n\n        # Add the group and coefficient to the left matrix\n        left[i, group_index] = coefficient\n\n        # Add the right side to the right matrix\n        right[i] = float(right_side)\n\n    # Normalize the left matrix if sum_to_one is True\n    if sum_to_one:\n        left /= np.sum(left, axis=1)[:, np.newaxis]\n\n    # Return the left and right matrices\n    return left, right\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert input to numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if any of the groups mentioned in the equations are not present in the groups array\n    missing_groups = set(re.findall(r\"[a-zA-Z0-9]+\", \" \".join(equations))) - set(\n        groups[:, 0]\n    )\n\n    if missing_groups and raise_if_group_missing:\n        raise GroupNotFoundError(\n            f\"The following groups were not found in the {names[0]} array: {missing_groups}\"\n        )\n\n    elif missing_groups:\n        warnings.warn(\n            f\"The following groups were not found in the {names[0]} array: {missing_groups}\"\n        )\n\n    # Initialize left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Iterate over each equation\n    for i, equation in enumerate(equations):\n\n        # Split the equation into left and right sides\n        left_side, right_side = equation.split(\"<=\")\n\n        # Extract the coefficients and group names from the left side\n        left_coeffs, left_groups = [], []\n        for term in left_side.split(\"+\"):\n            coeff, group = term.split(\"*\")\n            left_coeffs.append(float(coeff))\n            left_groups.append(group.strip())\n\n        # Extract the constant from the right side\n        right_side = right_side.strip()\n        if right_side.endswith(\")\"):\n            right_side = right_side[:-1]\n        right_side = right_side.split(\"(\")[0]\n        right_const = float(right_side)\n\n        # Find the indices of the groups in the left side\n        group_indices = [np.where(groups[:, 0] == group)[0][0] for group in left_groups]\n\n        # Ass"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups)\n    equations = np.array(equations)\n\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"`{names[0]}` must be 2D array of shape (n_groups, n_assets)\"\n        )\n\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"`{names[1]}` must be 1D array of shape (n_equations,)\"\n        )\n\n    n_groups, n_assets = groups.shape\n\n    left = np.zeros((equations.shape[0], n_assets))\n    right = np.zeros(equations.shape[0])\n\n    # Compile regex pattern for group name\n    pattern = re.compile(r\"\\b\\w+\\b\")\n\n    for i, equation in enumerate(equations):\n        # Extract group names from equation\n        groups_in_equation = pattern.findall(equation)\n\n        if len(groups_in_equation) == 0:\n            raise EquationToMatrixError(\n                f\"No groups found in equation '{equation}'\"\n            )\n\n        # Check if all groups in equation are part of the input groups\n        groups_not_found = [\n            group\n            for group in groups_in_equation\n            if np.any(np.all(groups == group, axis=1))\n        ]\n\n        if len(groups_not_found) > 0:\n            if raise_if_group_missing:\n                raise GroupNotFoundError(\n                    f\"Group(s) '{groups_not_found}' not found in {names[0]}\"\n                )\n            else:\n                warnings.warn(\n                    f\"Group(s) '{groups_not_found}' not found in {names[0]}\"\n                )\n                continue\n\n        # Find the indices of the groups in the input groups array\n        group_indices = [\n            np.where(np.all(groups == group, axis=1))[0][0]\n            for group in groups_in_equation\n        ]\n\n       "}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert inputs to numpy arrays\n    groups = np.array(groups)\n    equations = np.array(equations)\n\n    # Check if any groups are missing\n    missing_groups = np.unique([g for eq in equations for g in re.findall(r\"[^\\d\\.]+\", eq)])\n    if np.any(missing_groups):\n        if raise_if_group_missing:\n            raise GroupNotFoundError(\n                f\"The following groups are missing from the {names[0]} array: {missing_groups}\"\n            )\n        else:\n            warnings.warn(\n                f\"The following groups are missing from the {names[0]} array: {missing_groups}\"\n            )\n\n    # Create a dictionary mapping groups to their indices in the groups array\n    group_indices = {group: i for i, group in enumerate(np.unique(groups))}\n\n    # Create a list of tuples containing the coefficients and group names for each equation\n    equation_coefficients = []\n    for equation in equations:\n        coeffs = re.findall(r\"[-+]?\\d+\\.?\\d*\", equation)\n        groups = re.findall(r\"[^\\d\\.]+\", equation)\n        equation_coefficients.append(\n            [(float(coeffs[i]), groups[i]) for i in range(len(coeffs))]\n        )\n\n    # Create the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Fill in the left and right matrices\n    for i, equation in enumerate(equation_coefficients):\n        for coeff, group in equation:\n            left[i, group_indices[group]] = coeff\n        right[i] = equation[0][0]\n\n    # Normalize the coefficients in each row of the left matrix to sum to one\n    if sum_to_one:\n        left /= left.sum(axis=1)[:, np.newaxis]"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    # create a new class\n    newInstances = type(\n        f\"Instances{_counter}\",\n        (Instances,),\n        {\n            \"__module__\": \"detectron2.structures\",\n            \"__qualname__\": \"Instances\",\n            \"__annotations__\": {\n                \"image_size\": torch.Size,\n            },\n        },\n    )\n    for name, typ in fields:\n        newInstances.__annotations__[name] = typ\n        setattr(newInstances, f\"_{name}\", nn.Parameter(torch.empty(0)))\n\n    _add_instances_conversion_methods(newInstances)\n\n    # write the new class to a temporary file\n    with tempfile.NamedTemporaryFile(suffix=\".py\") as f:\n        f.write(newInstances.__module__.encode(\"utf-8\"))\n        f.write(b\"\\n\")\n        f.write(newInstances.__qualname__.encode(\"utf-8\"))\n        f.write(b\" = \")\n        f.write(newInstances.__module__.encode(\"utf-8\"))\n        f.write(b\".\\n\")\n        f.write(newInstances.__module__.encode(\"utf-8\"))\n        f.write(b\".\")\n        f.write(newInstances.__qualname__.encode(\"utf-8\"))\n        f.write(b\" = \")\n        f.write(newInstances.__module__.encode(\"utf-8\"))\n        f.write(b\".\\n\")\n        f.write(newInstances.__module__.encode(\"utf-8\"))\n        f.write(b\".\\n\")\n        f.write(newInstances.__module__.encode(\"utf-8\"))\n        f.write(b\".\\n\")\n        f.write(newInstances.__module__.encode(\"utf-8\"))\n        f.write(b\".\\n\")\n        f.write(newInstances.__module__.encode(\"utf-8\"))\n        f.write("}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    module_name = f\"instances_{_counter}\"\n    module_path = os.path.join(tempfile.gettempdir(), f\"{module_name}.py\")\n    with open(module_path, \"w\") as f:\n        f.write(\n            f\"import torch\\n\"\n            f\"from torch import nn\\n\"\n            f\"from detectron2.structures import Boxes, Instances\\n\"\n            f\"\\n\"\n            f\"class {module_name}(Instances):\\n\"\n            f\"    __annotations__ = {{\\n\"\n        )\n        for name, typ in fields:\n            f.write(f\"        '{name}': {typ},\\n\")\n        f.write(\"    }\\n\")\n        for name, typ in fields:\n            f.write(f\"    @torch.jit.unused\\n\")\n            f.write(f\"    def {name}(self) -> {typ}:\\n\")\n            f.write(f\"        return self.__{name}\\n\")\n        f.write(\"    @torch.jit.unused\\n\")\n        f.write(\"    def __setstate__(self, state):\\n\")\n        f.write(\"        super().__setstate__(state)\\n\")\n        f.write(\"        for name, val in state.items():\\n\")\n        f.write(\"            if name.startswith('_'):\\n\")\n        f.write(\"                setattr(self, name, val)\\n\")\n        f.write(\"    @torch.jit.unused\\n\")\n        f.write(\"    def __getstate__(self):\\n\")\n        f.write(\"        state = super().__getstate__()\\n\")\n        for name, typ in fields:\n            f.write(f\"        state['_{name}'] = self.{name}\\n\")\n        f.write(\"        return state\\n\")\n        f.write(\"    def __repr__(self):\\n\")\n        f.write(\"       "}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    new_instances_name = f\"NewInstances{_counter}\"\n    new_instances_module = nn.Module()\n    new_instances_module.__name__ = new_instances_name\n    new_instances_module.__qualname__ = new_instances_name\n    new_instances_module.__doc__ = \"A scriptable version of Instances\"\n    for name, type_ in fields:\n        setattr(new_instances_module, name, torch.empty(0, dtype=type_))\n        setattr(new_instances_module, f\"_{name}\", torch.empty(0, dtype=type_))\n\n    _add_instances_conversion_methods(new_instances_module)\n\n    with ExitStack() as stack:\n        tempdir = stack.enter_context(tempfile.TemporaryDirectory())\n        temp_instances_path = os.path.join(tempdir, f\"{new_instances_name}.py\")\n        with open(temp_instances_path, \"w\") as f:\n            f.write(f\"from torch import nn\\n\")\n            f.write(f\"from detectron2.structures import Instances\\n\")\n            f.write(f\"\\n\")\n            f.write(f\"class {new_instances_name}(Instances):\\n\")\n            f.write(f\"    def __init__(self, image_size):\\n\")\n            f.write(f\"        super().__init__(image_size)\\n\")\n            for name, type_ in fields:\n                f.write(f\"        self.{name} = nn.Parameter(torch.empty(0, dtype={type_.__name__}))\\n\")\n                f.write(f\"        self._{name} = nn.Parameter(torch.empty(0, dtype={type_.__name__}))\\n\")\n            f.write(f\"\\n\")\n\n        # clear jit cache\n        _clear_jit_cache()"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as f:\n        f.write(\n            f\"\"\"# Copyright (c) Facebook, Inc. and its affiliates."}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    with ExitStack() as stack:\n        # create a new class\n        newInstances = type(\"Instances\", (Instances,), {})\n        # add fields to the new class\n        for name, type_ in fields:\n            newInstances.__annotations__[name] = type_\n            setattr(newInstances, f\"_{name}\", nn.Parameter(torch.empty(0)))\n        _add_instances_conversion_methods(newInstances)\n        # create a temporary module\n        mod = nn.Module()\n        mod.Instances = newInstances\n        # write the module to a temporary file\n        with tempfile.NamedTemporaryFile(suffix=f\"{_counter}.py\") as f:\n            f.write(mod.code)\n            f.flush()\n            # import the module\n            sys.path.append(os.path.dirname(f.name))\n            stack.enter_context(\n                mock.patch.dict(\n                    \"sys.modules\",\n                    {\n                        \"detectron2.structures.instances\": _import_file(\n                            f.name, \"Instances\"\n                        ),\n                    },\n                )\n            )\n            # clear jit cache\n            _clear_jit_cache()\n            # yield the new class\n            yield newInstances\n        # clean up\n        stack.pop_all()\n        sys.path.remove(os.path.dirname(f.name))\n        del sys.modules[\"detectron2.structures.instances\"]\n        _clear_jit_cache()"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    name = f\"Instances_{_counter}\"\n    newInstances = type(name, (Instances,), {})\n    for field in fields:\n        field_name, field_type = field\n        setattr(newInstances, f\"_{field_name}\", field_type)\n    _add_instances_conversion_methods(newInstances)\n\n    with ExitStack() as stack:\n        temp_dir = stack.enter_context(tempfile.TemporaryDirectory())\n        temp_file = os.path.join(temp_dir, \"instances.py\")\n        with open(temp_file, \"w\") as f:\n            f.write(f\"from detectron2.structures import Instances\\n\\n\")\n            f.write(f\"class {name}(Instances):\\n\")\n            for field in fields:\n                field_name, field_type = field\n                f.write(f\"    {field_name}: {field_type}\\n\")\n        sys.path.insert(0, temp_dir)\n        _import_file(name)\n        _clear_jit_cache()\n        yield newInstances\n        _clear_jit_cache()\n        sys.path.remove(temp_dir)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    with ExitStack() as stack:\n        # create a new module\n        mod = stack.enter_context(tempfile.TemporaryDirectory(prefix=\"instances_\"))\n        # create a new class\n        cls_name = f\"Instances{_counter}\"\n        newInstances = type(cls_name, (Instances,), {})\n        # add fields\n        for name, typ in fields:\n            newInstances._add_field(name, typ)\n        # add from_instances method\n        _add_instances_conversion_methods(newInstances)\n        # write the module to a temporary file\n        filename = os.path.join(mod, \"instances.py\")\n        with open(filename, \"w\") as f:\n            f.write(f\"class {cls_name}(Instances):\\n\")\n            for name, typ in fields:\n                f.write(f\"    {name}: {typ.__name__}\\n\")\n        # import the module\n        sys.path.insert(0, mod)\n        _import_file(filename)\n        # clear jit cache\n        _clear_jit_cache()\n        # yield the new class\n        yield newInstances\n        # clean up\n        sys.path.remove(mod)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    with ExitStack() as stack:\n        # Create a new module for the class\n        module = stack.enter_context(tempfile.TemporaryDirectory())\n        module_name = f\"module_{_counter}\"\n        sys.modules[module_name] = module\n\n        # Define the new class\n        fields_str = \"\".join(\n            [\n                f\"{name}: {type_}\"\n                for name, type_ in fields\n            ]\n        )\n        class_def = f\"class NewInstances(Instances):\\n\\tdef __init__(self, image_size):\\n\\t\\t{fields_str}\\n\\t\\tself.image_size = image_size\"\n\n        # Write the class definition to a temporary file\n        tmp_file = os.path.join(module, \"new_instances.py\")\n        with open(tmp_file, \"w\") as f:\n            f.write(class_def)\n\n        # Import the module and get the new class\n        newInstances = _import_file(tmp_file, module_name)\n        newInstances = newInstances.NewInstances\n\n        # Add from_instances method to the class\n        _add_instances_conversion_methods(newInstances)\n\n        # Patch the Instances class\n        with mock.patch(\"detectron2.structures.Instances\", newInstances):\n            yield newInstances\n\n        # Clean up\n        del sys.modules[module_name]\n        _clear_jit_cache()"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    # Create a new class with the specified fields\n    class NewInstances(nn.Module):\n        def __init__(self, image_size):\n            super().__init__()\n            self.image_size = image_size\n            for name, field in fields:\n                setattr(self, f\"_{name}\", field)\n\n    # Add a from_instances method to the new class\n    _add_instances_conversion_methods(NewInstances)\n\n    # Create a temporary file to store the new class\n    with tempfile.NamedTemporaryFile(suffix=f\"{_counter}.py\") as f:\n        # Write the new class to the temporary file\n        f.write(\n            f\"from typing import List, Tuple\\n\\n\"\n            f\"from torch import Tensor\\n\\n\"\n            f\"from detectron2.structures import Instances\\n\\n\"\n            f\"class NewInstances(Instances):\\n\"\n        )\n        for name, field in fields:\n            f.write(f\"    {name}: {field.__name__}\\n\")\n        f.write(\"\\n\")\n        f.flush()\n\n        # Import the new class from the temporary file\n        module = _import_file(f.name)\n        NewInstances = module.NewInstances\n\n        # Patch the Instances class to use the new class\n        with ExitStack() as stack:\n            stack.enter_context(mock.patch.object(sys.modules[\"detectron2.structures.instances\"], \"Instances\", NewInstances))\n            stack.enter_context(mock.patch.object(sys.modules[\"detectron2.structures.instances\"], \"__all__\", [\"Instances\"]))\n\n            # Clear the JIT cache to ensure that the new class is recognized\n            _clear_jit_cache()\n\n            # Yield the new class\n            yield NewInstances\n\n        # Clean up by removing the temporary file\n        os.remove(f.name)"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n\n    with ExitStack() as stack:\n        newInstances = nn.Module\n        for name, dtype in fields:\n            newInstances = torch.jit.script(\n                torch.jit.annotate(\n                    newInstances,\n                    (name, dtype),\n                )\n            )\n        _add_instances_conversion_methods(newInstances)\n\n        _clear_jit_cache()\n\n        fd, fname = tempfile.mkstemp(suffix=\".py\", text=True)\n        stack.enter_context(mock.patch.object(sys, \"path\", [os.path.dirname(fname)]))\n\n        with os.fdopen(fd, \"w\") as f:\n            f.write(\n                f\"\"\""}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n\n    # Create a new class with the specified fields\n    new_class_name = f\"Instances{_counter}\"\n    _counter += 1\n    new_class = type(new_class_name, (Instances,), {})\n    for field_name, field_type in fields:\n        new_class.register_buffer(field_name, torch.empty(0))\n\n    # Create a temporary file to write the new class to\n    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".py\")\n    temp_file.write(f\"from detectron2.structures import Instances\\n\\n\".encode(\"utf-8\"))\n    temp_file.write(f\"class {new_class_name}(Instances):\\n\".encode(\"utf-8\"))\n    for field_name, field_type in fields:\n        temp_file.write(f\"    {field_name}: {field_type}\\n\".encode(\"utf-8\"))\n    temp_file.close()\n\n    # Import the new class from the temporary file\n    sys.path.append(os.path.dirname(temp_file.name))\n    new_class = _import_file(new_class_name, os.path.basename(temp_file.name))\n\n    # Add from_instances method to the scripted Instances class\n    _add_instances_conversion_methods(new_class)\n\n    # Set up the environment to recognize the new class\n    with mock.patch(\"detectron2.structures.Instances\", new_class):\n        # Clear the JIT cache to ensure that the new class is recognized\n        _clear_jit_cache()\n        yield new_class\n\n    # Clean up after the context manager\n    sys.path.remove(os.path.dirname(temp_file.name))\n    os.remove(temp_file.name)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n\n    # create a new class for the instances\n    cls_name = f\"Instances_{_counter}\"\n    _counter += 1\n    newInstances = type(cls_name, (Instances,), {})\n\n    # add fields to the new class\n    for name, (type_, default) in fields:\n        setattr(newInstances, name, default)\n\n    # add a from_instances method to the new class\n    _add_instances_conversion_methods(newInstances)\n\n    # write the new class to a temporary file\n    with tempfile.NamedTemporaryFile(suffix=\".py\") as f:\n        f.write(f\"class {cls_name}(Instances):\\n\".encode())\n        for name, (type_, default) in fields:\n            f.write(f\"    {name}: {type_}\\n\".encode())\n        f.write(f\"\\n\".encode())\n        f.write(f\"    def __init__(self, image_size):\\n\".encode())\n        for name, (type_, default) in fields:\n            f.write(f\"        self.{name} = {default}\\n\".encode())\n        f.flush()\n\n        # import the new class\n        module_name = f\"temp_module_{_counter}\"\n        _counter += 1\n        module = _import_file(f.name, module_name, \"\")\n        newInstances = getattr(module, cls_name)\n\n        # patch the Instances class\n        with mock.patch(\"detectron2.structures.Instances\", new=newInstances):\n            yield newInstances\n\n    # clean up\n    del sys.modules[module_name]\n    _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    with ExitStack() as stack:\n        # create a new module\n        new_module = stack.enter_context(tempfile.TemporaryDirectory())\n        # create a new class\n        new_instances = type(\"Instances\", (Instances,), fields)\n        _add_instances_conversion_methods(new_instances)\n        # write the new class to a temporary file\n        new_instances_file = os.path.join(new_module, f\"Instances_{_counter}.py\")\n        with open(new_instances_file, \"w\") as f:\n            f.write(f\"from detectron2.structures import Instances\\n\\n\")\n            f.write(f\"class Instances(Instances):\\n\")\n            for field in fields:\n                f.write(f\"    {field[0]} = {field[1]}\\n\")\n        # import the new class\n        sys.path.append(new_module)\n        new_instances = _import_file(new_instances_file)\n        # set up the environment\n        with mock.patch.object(Instances, \"__class__\", new_instances):\n            yield new_instances\n        # clean up\n        _clear_jit_cache()\n        sys.path.remove(new_module)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n\n    with ExitStack() as stack:\n        # create a new module for the new class\n        new_module = sys.modules[f\"new_module_{_counter}\"] = mock.Mock()\n        _counter += 1\n\n        # create a new class that inherits from Instances\n        new_cls = type(\n            \"NewInstances\",\n            (Instances,),\n            {\n                \"__module__\": new_module.__name__,\n                \"__qualname__\": \"NewInstances\",\n                \"__annotations__\": {\n                    name: type(val)\n                    for name, val in fields\n                },\n            },\n        )\n        # add a from_instances method to the new class\n        _add_instances_conversion_methods(new_cls)\n\n        # create a temporary file to write the new module to\n        with tempfile.NamedTemporaryFile(suffix=\".py\") as f:\n            # write the new module to the temporary file\n            f.write(f\"from detectron2.structures import Instances\\n\\n\".encode())\n            f.write(f\"class {new_cls.__name__}(Instances):\\n\".encode())\n            for name, val in fields:\n                f.write(f\"    {name}: {type(val).__name__}\\n\".encode())\n            f.write(b\"\\n\")\n            f.flush()\n\n            # import the new module\n            _import_file(f.name, new_module)\n\n            # set the new class as the Instances class\n            stack.enter_context(mock.patch.object(sys.modules[__name__], \"Instances\", new_cls))\n\n            # yield the new class\n            yield new_cls\n\n        # clean up the temporary file\n        del sys.modules[f.name]\n\n        # clear the jit cache\n        _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Create a new class for Instances with the specified fields\n    newInstances = type('Instances', (Instances,), fields)\n    _add_instances_conversion_methods(newInstances)\n\n    # Create a temporary file for the new module\n    with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as f:\n        module_name = 'tmp_module'\n        f.write(f'{module_name} = {newInstances.__module__}\\n'.encode('utf-8'))\n        f.write(f'{newInstances.__name__} = {newInstances.__name__}\\n'.encode('utf-8'))\n        f.close()\n\n        # Import the new module\n        module = _import_file(f.name, module_name)\n\n        # Clean up the temporary file\n        os.remove(f.name)\n\n        # Set up the environment\n        with mock.patch.dict('sys.modules', {newInstances.__module__: module}):\n            # Clear the JIT cache to ensure that the new class is used\n            _clear_jit_cache()\n\n            # Yield the new class\n            yield newInstances\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with ExitStack() as stack:\n            stack.enter_context(mock.patch.dict(sys.modules, {}))\n            stack.enter_context(mock.patch.dict(os.environ, {\"DETECTRON2_DATASETS\": tmpdir}))\n\n            # create a new class\n            newInstances = type(\"Instances\", (Instances,), {})\n            for name, typ in fields:\n                newInstances.register_buffer_attribute(name, typ)\n            _add_instances_conversion_methods(newInstances)\n\n            # write the class to a file\n            filename = os.path.join(tmpdir, f\"instances_{_counter}.py\")\n            with open(filename, \"w\") as f:\n                f.write(f\"from detectron2.structures import Instances\\n\")\n                f.write(f\"class Instances({newInstances.__bases__[0].__name__}):\\n\")\n                for name, typ in fields:\n                    f.write(f\"    {name}: {typ.__name__}\\n\")\n\n            # import the class\n            _import_file(filename, \"instances\")\n\n            # clear jit cache\n            _clear_jit_cache()\n\n            yield newInstances\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    suffix = str(_counter)\n    module_name = f\"test_instances_scripting_module_{suffix}\"\n    module_file = f\"test_instances_scripting_module_{suffix}.py\"\n    module_path = os.path.join(tempfile.gettempdir(), module_file)\n\n    with open(module_path, \"w\") as f:\n        f.write(f\"import torch\\n\")\n        f.write(f\"from detectron2.structures import Instances\\n\")\n        f.write(f\"class {module_name}(Instances):\\n\")\n        for field in fields:\n            f.write(f\"\\t{field[0]} = {field[1]}\\n\")\n\n    _import_file(module_name, module_path)\n    newInstances = sys.modules[module_name].__dict__[module_name]\n    _add_instances_conversion_methods(newInstances)\n    newInstances = torch.jit.script(newInstances)\n    sys.modules[module_name].__dict__[module_name] = newInstances\n\n    with ExitStack() as stack:\n        stack.enter_context(mock.patch.dict(\"sys.modules\", {module_name: sys.modules[module_name]}))\n        stack.enter_context(mock.patch.dict(\"sys.modules\", {\"detectron2.structures\": sys.modules[\"detectron2.structures\"]}))\n        yield newInstances\n\n    _clear_jit_cache()\n    del sys.modules[module_name]\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n\n    with ExitStack() as stack:\n        # create a new module\n        module_name = f\"temp_instances_module_{_counter}\"\n        module = stack.enter_context(tempfile.TemporaryDirectory())\n        _counter += 1\n        sys.path.insert(0, module)\n\n        # create a new class\n        class_name = \"Instances\"\n        fields = [(name, torch.Tensor) for name in fields]\n        newInstances = type(class_name, (Instances,), dict(fields))\n        _add_instances_conversion_methods(newInstances)\n        scripted_newInstances = torch.jit.script(newInstances)\n\n        # write the new class to a temporary file\n        script_path = os.path.join(module, \"scripted_instances.py\")\n        with open(script_path, \"w\") as f:\n            f.write(f\"from torch import nn\\n\")\n            f.write(f\"class {class_name}(nn.Module):\\n\")\n            for name, type in fields:\n                f.write(f\"    def __init__(self):\\n\")\n                f.write(f\"        super().__init__()\\n\")\n                f.write(f\"        self.{name} = nn.Parameter(torch.zeros({type}))\\n\")\n            f.write(f\"\\n\")\n            f.write(f\"    def forward(self):\\n\")\n            f.write(f\"        return self\\n\")\n\n        # import the new class\n        _import_file(script_path)\n\n        # replace Instances with the new class\n        with mock.patch(\"detectron2.structures.Instances\", scripted_newInstances):\n            yield scripted_newInstances"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    with ExitStack() as stack:\n        # Create a temporary file to hold the new class\n        temp_file = tempfile.NamedTemporaryFile(\n            suffix=f\"_new_instances_{_counter}.py\", delete=False\n        )\n        stack.enter_context(temp_file)\n\n        # Define the new class and write it to the temporary file\n        new_class_name = f\"NewInstances_{_counter}\"\n        temp_file.write(\n            f\"from detectron2.structures import Instances\\n\\n\"\n            f\"class {new_class_name}(Instances):\\n\"\n        )\n        for field in fields:\n            field_name, field_type = field\n            temp_file.write(f\"    {field_name}: {field_type}\\n\")\n        temp_file.write(\"\\n\")\n        temp_file.close()\n\n        # Import the new class and replace 'Instances' with it\n        new_module = _import_file(temp_file.name, sys.modules[__name__])\n        newInstances = getattr(new_module, new_class_name)\n        _add_instances_conversion_methods(newInstances)\n\n        with mock.patch(\"detectron2.structures.Instances\", newInstances):\n            yield newInstances\n\n        # Clean up\n        os.remove(temp_file.name)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    with ExitStack() as stack:\n        _clear_jit_cache()\n        with tempfile.TemporaryDirectory() as tmpdir:\n            filename = f\"instances_{_counter}.py\"\n            filepath = os.path.join(tmpdir, filename)\n\n            with open(filepath, \"w\") as f:\n                f.write(\"from torch import nn\\n\")\n                f.write(\"from detectron2.structures import Boxes, Instances\\n\")\n                f.write(f\"class {filename}(Instances):\\n\")\n                for name, shape, dtype in fields:\n                    f.write(f\"    {name}: {dtype}\\n\")\n                f.write(\"\\n\")\n\n            stack.enter_context(mock.patch.dict(sys.modules, {\"instances\": _import_file(filepath)}))\n            newInstances = getattr(instances, filename)\n            newInstances.__module__ = \"instances\"\n            _add_instances_conversion_methods(newInstances)\n            yield newInstances\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # PIL only supports RGB, so convert to RGB and flip channels over below\n    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n\n    return convert_PIL_to_numpy(image, format)\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    if format is not None and format not in [\"BGR\", \"YUV-BT.601\"]:\n        assert format in Image.modes, \"Specified format is not supported by PIL\"\n\n    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # PIL only supports RGB, so convert to RGB and flip channels over below\n    conversion_format = format\n    if format in [\"BGR\", \"YUV-BT.601\"]:\n        conversion_format = \"RGB\"\n\n    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, conversion_format)\n\n    if format == \"BGR\":\n        image = image[:, :, [2, 1, 0]]\n    elif format == \"YUV-BT.601\":\n        image = image / 255.0\n        image = np.dot(image, np.array(_M_RGB2YUV).T)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # PIL only supports RGB, so convert to RGB and flip channels over below\n    conversion_format = format\n    if format in [\"BGR\", \"YUV-BT.601\"]:\n        conversion_format = \"RGB\"\n\n    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, conversion_format)\n\n    if format == \"BGR\":\n        image = image[:, :, [2, 1, 0]]\n    elif format == \"YUV-BT.601\":\n        image = image / 255.0\n        image = np.dot(image, np.array(_M_RGB2YUV).T)\n    elif format is not None and format != image.mode:\n        raise ValueError(\"Image mode not equal to given format. Got {}, expected {}.\".format(image.mode, format))\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image_np = convert_PIL_to_numpy(image, format)\n\n    return image_np\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n        return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image_np = convert_PIL_to_numpy(image, format)\n    return image_np\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    if format is not None:\n        format = format.upper()\n    file_obj = PathManager.open(file_name, \"rb\")\n    image = Image.open(file_obj)\n    image = _apply_exif_orientation(image)\n    image_np = convert_PIL_to_numpy(image, format)\n    file_obj.close()\n    return image_np\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n    image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    file_name = str(file_name)\n    if format is not None:\n        format = format.upper()\n    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image_np = convert_PIL_to_numpy(image, format)\n    return image_np\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image_bytes = f.read()\n    image = Image.open(BytesIO(image_bytes))\n    image = _apply_exif_orientation(image)\n    image = convert_PIL_to_numpy(image, format)\n    return image\n\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n\n    # transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                seg = transforms.apply_segmentation(seg)\n                annotation[\"segmentation\"][i] = seg\n        else:\n            # rle\n            seg = annotation[\"segmentation\"]\n            seg = transforms.apply_segmentation(seg)\n            annotation[\"segmentation\"] = seg\n\n    # transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transforms.apply_coords(keypoints.reshape(-1, 3)).reshape(-1, 3)\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n        annotation[\"keypoints\"] = keypoints\n\n    # clip transformed bbox\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = Boxes(bbox).clip(image_size)\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    # clip transformed segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"seg"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon\n            for i in range(len(segm)):\n                segm[i] = transforms.apply_segmentation(segm[i])[0]\n        else:\n            # RLE\n            segm = transforms.apply_segmentation([segm])[0]\n        annotation[\"segmentation\"] = segm\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transforms.apply_coords(keypoints.reshape(-1, 3)).reshape(-1, 3)\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n        annotation[\"keypoints\"] = keypoints\n\n    # clip transformed bbox\n    annotation[\"bbox\"] = BoxMode.convert(\n        annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS\n    )\n    annotation[\"bbox\"] = Boxes(annotation[\"bbox\"]).clip(image_size)\n    annotation[\"bbox\"] = BoxMode.convert(\n        annotation[\"bbox\"], BoxMode.XYXY_ABS, annotation[\"bbox_mode\"]\n    )\n\n    # clip transformed segmentation\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon\n            for i in range("}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon\n            segm = [\n                transforms.apply_coords(np.array(p, dtype=np.float32).reshape(-1, 2)).reshape(-1)\n                for p in segm\n            ]\n        else:\n            # RLE\n            segm = transforms.apply_coords(\n                np.array(segm[\"counts\"], dtype=np.uint8).reshape(-1, 2)\n            ).reshape(-1)\n            segm = mask_util.encode(np.asarray(segm.reshape(-1, 2), order=\"F\"))\n        annotation[\"segmentation\"] = segm\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transforms.apply_coords(keypoints.reshape(-1, 3)).reshape(-1)\n        if keypoint_hflip_indices is not None:\n            keypoints = keypoints.reshape(-1, 3)\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, [0, 2, 1]]\n            keypoints = keypoints.reshape(-1)\n        annotation[\"keypoints\"] = keypoints\n\n    # Clip the bbox to image size\n    h, w ="}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                annotation[\"segmentation\"][i] = transforms.apply_coords(np.asarray(seg).reshape(-1, 2)).reshape(-1)\n        else:\n            # RLE\n            if isinstance(annotation[\"segmentation\"][\"counts\"], list):\n                # uncompressed RLE\n                for i, count in enumerate(annotation[\"segmentation\"][\"counts\"]):\n                    annotation[\"segmentation\"][\"counts\"][i] = transforms.apply_coords(\n                        np.fromstring(count, dtype=np.uint8, sep=\" \").reshape(-1, 2)\n                    ).reshape(-1).tostring()\n            else:\n                # compressed RLE\n                annotation[\"segmentation\"][\"counts\"] = transforms.apply_coords(\n                    np.frombuffer(annotation[\"segmentation\"][\"counts\"].encode(), dtype=np.uint8)\n                ).tostring().decode()\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = keypoints.reshape(-1, 3)\n        keypoints[:, :2] = transforms.apply_coords(keypoints[:, :2]).reshape(-1)\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_ind"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"bbox\" in annotation:\n        # Transform bbox\n        bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        bbox = transforms.apply_box([bbox])[0]\n        annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    if \"segmentation\" in annotation:\n        # Transform segmentation\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segm = transforms.apply_segmentation(annotation[\"segmentation\"])\n            annotation[\"segmentation\"] = transformed_segm\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            transformed_segm = transforms.apply_segmentation(\n                [mask_util.frPyObjects(annotation[\"segmentation\"], *image_size)]\n            )\n            annotation[\"segmentation\"] = mask_util.encode(\n                np.asarray(\n                    transformed_segm[0][0], order=\"F\", dtype=\"uint8\"\n                )\n            )\n        else:\n            raise ValueError(\n                \"Cannot convert segmentation of type '{}'\".format(\n                    type(annotation[\"segmentation\"])\n                )\n            )\n\n    if \"keypoints\" in annotation:\n        # Transform keypoints\n        keypoints = annotation[\"keypoints\"]\n        if len(keypoints) > 0:\n            keypoints = keypoints.reshape(-1, 3)\n            keypoints[:, :2] = transforms.apply_coords(keypoints[:, :2]).reshape(-1, 2)\n            if keypoint_hflip_indices is not None:\n                keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n            annotation[\"keypoints\"] = ke"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                annotation[\"segmentation\"][i] = transforms.apply_segmentation(seg)[0]\n        else:\n            # RLE\n            annotation[\"segmentation\"] = transforms.apply_segmentation(annotation[\"segmentation\"])[0]\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n        keypoints = keypoints.reshape(-1, 3)\n        keypoints[:, :2] = transforms.apply_coords(keypoints[:, :2]).reshape(-1, 2)\n        annotation[\"keypoints\"] = keypoints.reshape(-1)\n\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bbox\n    bbox = transforms.apply_box([annotation[\"bbox\"]])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_polygons = transforms.apply_segmentation(\n                [np.asarray(annotation[\"segmentation\"], dtype=\"float32\")]\n            )[0]\n            annotation[\"segmentation\"] = transformed_polygons.tolist()\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            transformed_rle = mask_util.merge(\n                mask_util.frPyObjects(\n                    transforms.apply_segmentation([annotation[\"segmentation\"]]),\n                    annotation[\"height\"],\n                    annotation[\"width\"],\n                )[0]\n            )\n            annotation[\"segmentation\"] = mask_util.decode(transformed_rle)\n        else:\n            raise ValueError(\n                \"Unknown segmentation format: {}\".format(annotation[\"segmentation\"])\n            )\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = np.asarray(annotation[\"keypoints\"])\n        keypoints = keypoints.reshape(-1, 3)\n        keypoints[:, :2] = transforms.apply_coords(keypoints[:, :2]).reshape(-1, 2)\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices, :2] = (\n                image_size[1] - keypoints[keypoint_hflip_indices, :2] - 1\n            )\n        annotation[\"ke"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            if len(annotation[\"segmentation\"]) > 0:\n                # Filter out invalid polygons\n                valid_polygons = []\n                for poly in annotation[\"segmentation\"]:\n                    poly = np.asarray(poly).reshape(-1, 2)\n                    if poly.shape[0] <= 2:\n                        continue\n                    valid_polygons.append(poly)\n\n                if len(valid_polygons) == 0:\n                    annotation[\"segmentation\"] = []\n                else:\n                    valid_polygons = transforms.apply_coords(np.concatenate(valid_polygons, axis=0)).reshape(\n                        -1, len(valid_polygons), 2\n                    )\n                    segmentation = valid_polygons.tolist()\n                    annotation[\"segmentation\"] = segmentation\n\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            if \"counts\" in annotation[\"segmentation\"]:\n                counts = annotation[\"segmentation\"][\"counts\"]\n                counts = transforms.apply_segmentation(counts)\n                annotation[\"segmentation\"][\"counts\"] = counts\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if keypoint_hflip_indices is not None:\n            keypoints = keypoints[keypoint_hflip_indices]\n        keypoints = transforms.apply_coords(keypoints.reshape(-1, 2)).reshape(-1, 3)"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            segmentation = transforms.apply_segmentation(annotation[\"segmentation\"])[0]\n            segmentation = [np.asarray(p).reshape(-1) for p in segmentation]\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            segmentation = transforms.apply_segmentation(annotation[\"segmentation\"])\n        else:\n            raise ValueError(\n                \"Only support segmentation of poligon or RLE, but got {}\".format(\n                    annotation[\"segmentation\"]\n                )\n            )\n        annotation[\"segmentation\"] = segmentation\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        num_keypoints = len(keypoints) // 3\n        if num_keypoints > 0:\n            keypoints = keypoints.reshape((-1, 3))\n            keypoints, _ = transforms.apply_keypoints([keypoints])\n            keypoints = keypoints[0]\n            if keypoint_hflip_indices is not None:\n                keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][\n                    :, ::-1\n                ]\n            keypoints = keypoints.reshape((-1))\n            annotation[\"keypoints\"] = keypoints\n\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            if isinstance(annotation[\"segmentation\"][0], list):\n                # run-length encoding\n                rles = mask_util.frPyObjects(annotation[\"segmentation\"], *image_size)\n                rle = mask_util.merge(rles)\n                annotation[\"segmentation\"] = mask_util.decode(rle)\n            else:\n                # Assume it is a polygon\n                segmentation = np.array(annotation[\"segmentation\"])\n                segmentation = transforms.apply_coords(segmentation.reshape(-1, 2)).reshape(\n                    segmentation.shape\n                )\n                annotation[\"segmentation\"] = segmentation\n        else:\n            # Assume it is a bitmask\n            if isinstance(annotation[\"segmentation\"], PolygonMasks):\n                segmentation = annotation[\"segmentation\"].polygons[0]\n            else:\n                segmentation = annotation[\"segmentation\"].get_mask_tensor().nonzero().cpu()\n                segmentation = transforms.apply_coords(\n                    torch.stack([segmentation[0], segmentation[1]], dim=1).float()\n                ).long()\n                segmentation = segmentation.cpu().numpy()\n            annotation[\"segmentation\"] = BitMasks(\n                polygons_to_bitmask(segmentation, *image_size)\n            )\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if len(keypoints) > 0:\n            keypoints = keypoints"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        # Note that some dataset may not provide segmentation annotation\n        # and only provide bbox/keypoints annotation.\n        # In this case, we simply skip the transformation.\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon\n            transformed_segm = transforms.apply_segmentation(segm)\n            transformed_segm = [polygons_to_bitmask(x, *image_size) for x in transformed_segm]\n            annotation[\"segmentation\"] = BitMasks(transformed_segm)\n        elif isinstance(segm, dict):\n            # rle\n            transformed_segm = transforms.apply_segmentation([segm])[0]\n            annotation[\"segmentation\"] = transformed_segm\n        else:\n            raise ValueError(\"Unknown segmentation format: {}\".format(segm))\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = keypoints.reshape(-1, 3)\n        keypoints, _ = transforms.apply_keypoints([keypoints])\n        keypoints = keypoints.reshape(-1)\n        if keypoint_hflip_indices is not None:\n            keypoints = keypoints.copy()\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n        annotation[\"keypoints\"] = keyp"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                annotation[\"segmentation\"][i] = transforms.apply_coords(np.array(seg).reshape(-1, 2)).reshape(-1)\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # rle\n            rle = annotation[\"segmentation\"]\n            if \"counts\" in rle:\n                # binary mask\n                mask = mask_util.decode(rle)\n                mask = transforms.apply_segmentation(mask)\n                annotation[\"segmentation\"] = mask_util.encode(\n                    np.asfortranarray(mask.astype(np.uint8)))\n            elif \"size\" in rle:\n                # size is (height, width)\n                height, width = rle[\"size\"]\n                mask = mask_util.decode(rle)\n                mask = transforms.apply_segmentation(mask)\n                mask = mask[:height, :width]\n                annotation[\"segmentation\"] = mask_util.encode(\n                    np.asfortranarray(mask.astype(np.uint8)))\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n        keypoints = transforms.apply_coords(keypoints."}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            valid_polygons = []\n            for seg in annotation[\"segmentation\"]:\n                seg = transforms.apply_polygon(seg)\n                if not np.any(seg == 0):\n                    valid_polygons.append(seg)\n            annotation[\"segmentation\"] = valid_polygons\n        elif annotation[\"segmentation\"][\"counts\"] is not None:\n            # rle\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"]\n            )\n        else:\n            raise ValueError(\"Unknown segmentation format: {}\".format(annotation))\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n        keypoints = transforms.apply_coords(keypoints.reshape(-1, 3)).reshape(-1, 3)\n        keypoints = keypoints.clip(0, image_size[0] - 1, image_size[1] - 1)\n        annotation[\"keypoints\"] = keypoints\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox_mode = annotation[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, bbox_mode)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon\n            segm = PolygonMasks(segm)\n        else:\n            # RLE\n            segm = mask_util.decode(segm)\n        segm = transforms.apply_segmentation(segm)\n        segm = segm.resize(image_size)\n        segm = segm.polygons[0]\n        annotation[\"segmentation\"] = segm\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n        keypoints = transforms.apply_coords(keypoints.reshape(-1, 3)).reshape(-1, 3)\n        keypoints = keypoints[keypoints[:, 2] > 0]\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"bbox\" in annotation:\n        # Transform bbox\n        bbox = transforms.apply_box([annotation[\"bbox\"]])[0]\n        annotation[\"bbox\"] = bbox\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        # Transform segmentation\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segmentations = []\n            for seg in annotation[\"segmentation\"]:\n                transformed_segmentations.append(transforms.apply_coords(np.array(seg).reshape(-1, 2)))\n            annotation[\"segmentation\"] = transformed_segmentations\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # rle\n            transformed_segmentations = []\n            for seg in annotation[\"segmentation\"]:\n                mask = mask_util.decode(seg)\n                mask = mask_util.encode(\n                    np.array(\n                        transforms.apply_coords(mask.astype(np.float32)).reshape(-1, 2),\n                        dtype=np.uint8,\n                    )\n                )\n                transformed_segmentations.append(mask)\n            annotation[\"segmentation\"] = transformed_segmentations\n        else:\n            raise ValueError(\"Unknown segmentation format: {}\".format(annotation[\"segmentation\"]))\n\n    if \"keypoints\" in annotation:\n        # Transform keypoints\n        keypoints = np.asarray(annotation[\"keypoints\"], dtype=np.float32)\n        keypoints = keypoints.reshape(-1, 3)\n        keypoints = transforms.apply_coords(keypoints)\n        keypoints = keypoints.reshape(-1)\n        # Flip horizontally\n        if keypoint_hflip_indices is not None:\n            keypoints = keypoints.reshape(-1, 3)\n            keypoints[keypoint_hflip"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Transform bbox\n    bbox = transforms.apply_box([bbox])[0]\n    # Convert to XYXY_ABS\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            segm = [\n                transforms.apply_segmentation([np.asarray(p, dtype=np.float32)])[0]\n                for p in annotation[\"segmentation\"]\n            ]\n            segm = [p for p in segm if p.shape[0] >= 6]\n            annotation[\"segmentation\"] = segm\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # rle\n            segm = transforms.apply_segmentation(\n                [mask_util.frPyObjects(annotation[\"segmentation\"], *image_size)]\n            )[0]\n            annotation[\"segmentation\"] = mask_util.encode(\n                segm[:, :, np.newaxis]\n            )  # RLE format need to convert to binary mask\n        else:\n            raise NotImplementedError(\n                \"Unknown segmentation format: {}\".format(annotation[\"segmentation\"])\n            )\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n        keypoints = keypoints.reshape(-1, 3)\n        keypoints[:, :2] = transforms"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox_mode = BoxMode.convert(annotation[\"bbox_mode\"], BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, bbox_mode)\n    bbox = transforms.clip(bbox, image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                annotation[\"segmentation\"][i] = transforms.apply_polygon(seg)\n        else:\n            # RLE\n            segm = annotation[\"segmentation\"]\n            if isinstance(segm, dict):\n                segm = segm[\"counts\"]\n            segm = mask_util.frPyObjects(segm, *image_size)\n            segm = segm.astype(\"uint8\")\n            mask = transforms.apply_segmentation(segm)\n            mask = mask[:, :, None]\n            annotation[\"segmentation\"] = mask\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if keypoint_hflip_indices is not None:\n            keypoints = keypoints[keypoint_hflip_indices]\n        keypoints = transforms.apply_coords(keypoints.reshape(-1, 3)).reshape(-1, 3)\n        keypoints = Keypoints(keypoints, image_size)\n        annotation[\"keypoints\"] = keypoints\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if not isinstance(annotation, dict):\n        raise ValueError(\n            \"annotation has to be a dict, got {}\".format(type(annotation))\n        )\n    if \"bbox\" not in annotation or \"bbox_mode\" not in annotation:\n        raise ValueError(\n            'annotation must contain fields \"bbox\" and \"bbox_mode\"'\n        )\n    if \"segmentation\" not in annotation:\n        raise ValueError('annotation must contain field \"segmentation\"')\n\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = keypoints.reshape(-1, 3)\n        keypoints = transforms.apply_coords(keypoints)\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n        keypoints = keypoints.reshape(-1)\n        annotation[\"keypoints\"] = keypoints\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segm = []\n            for seg in annotation[\"segmentation\"]:\n                seg = np.array(seg).reshape(-1, 2)\n                seg = transforms.apply_coords(seg).reshape(-1).tolist()\n                transformed_segm.append(seg)\n            annotation[\"segmentation\"] = transformed_segm\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            segm"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Transform bbox\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        # Convert polygons to a bitmask of shape (H, W)\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            segmentations = [\n                transforms.apply_segmentation(np.array([poly], dtype=np.float32))[0]\n                for poly in annotation[\"segmentation\"]\n            ]\n            segmentations = [\n                p for p in segmentations if p.shape[0] >= 3 and p.shape[1] >= 3\n            ]\n            if segmentations:\n                # Create a mask of shape (H, W)\n                masks = [polygons_to_bitmask(p, *image_size) for p in segmentations]\n                masks = BitMasks(masks)\n                # Apply mask to the original image\n                masks = masks.crop_and_resize(\n                    np.array([bbox], dtype=np.float32), *image_size\n                )\n                annotation[\"segmentation\"] = mask_util.encode(\n                    np.ascontiguousarray(masks.tensor, dtype=np.uint8)\n                )\n        if isinstance(annotation[\"segmentation\"], dict):\n            # rle\n            segmentations = annotation[\"segmentation\"]\n            for k, segm in segmentations.items():\n                if isinstance(segm, dict):\n                    segm = mask_util.frPyObjects(segm, *image_size)\n                    segm = mask_util.decode(segm)\n                    segm = transforms.apply_segmentation(segm)[0]\n                    segmentations[k] = mask_util"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # TODO: This function is very similar to the one in detectron2/data/transforms/transform_instance_annotations.py\n    # TODO: We should refactor this function to avoid code duplication.\n\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # convert to XYXY_ABS\n    if isinstance(transforms, T.TransformList):\n        transforms = transforms.transforms\n    if isinstance(transforms, list):\n        transforms = T.TransformList(transforms)\n\n    # Transform bbox\n    if isinstance(transforms, T.TransformList):\n        bbox = transforms.apply_box(bbox)\n        bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    else:\n        for t in transforms:\n            bbox = t(np.asarray([bbox]), image_size)[0]\n        bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                if isinstance(seg, list):\n                    annotation[\"segmentation\"][i] = transforms.apply_segmentation(seg)\n            # TODO: this is inplace\n            annotation[\"segmentation\"] = polygons_to_bitmask(annotation[\"segmentation\"], *image_size)\n        else:\n            # RLE\n            annotation[\"segmentation\"] = transforms.apply_segmentation(annotation[\"segmentation\"])\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        num_keypoints = len(keypoints) // 3\n        if num_keypoints:\n            if isinstance(transform"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        # Convert the input coordinates to homogeneous coordinates\n        coords = np.hstack((coords, np.ones((coords.shape[0], 1))))\n\n        # Apply the rotation transformation to the coordinates\n        rotated_coords = np.dot(coords, self.rm_coords.T)\n\n        # Convert the rotated coordinates back to (x, y) format\n        rotated_coords = rotated_coords[:, :2]\n\n        return rotated_coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.size == 0 or self.angle % 360 == 0:\n            return coords\n\n        # Transform image center from source coordinates into output coordinates\n        # and then map the new origin to the corner of the output image.\n        new_coords = coords.astype(np.float32)\n        new_coords[:, 0] -= self.center[0]\n        new_coords[:, 1] -= self.center[1]\n        new_coords = np.dot(new_coords, self.rm_coords.T)\n        new_coords[:, 0] += self.center[0]\n        new_coords[:, 1] += self.center[1]\n        return new_coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return np.dot(coords, self.rm_coords.T)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.concatenate([coords, np.ones((len(coords), 1))], axis=1)\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords[:, :2]\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        # Convert the coordinates to homogeneous coordinates\n        hom_coords = np.ones((len(coords), 3))\n        hom_coords[:, :2] = coords\n\n        # Apply the rotation transformation\n        transformed_coords = np.dot(hom_coords, self.rm_coords.T)\n\n        # Convert the transformed coordinates back to (x, y) format\n        transformed_coords = transformed_coords[:, :2]\n\n        return transformed_coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords, dtype=np.float32)\n        coords = coords - self.center\n        coords = np.dot(coords, self.rm_coords.T)\n        coords = coords + self.center\n        return coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords)\n        coords = np.concatenate((coords, np.ones((coords.shape[0], 1))), axis=1)\n        coords = np.matmul(coords, self.rm_coords.T)\n        return coords[:, :2]\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        coords = np.hstack((coords, np.ones((coords.shape[0], 1))))\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords[:, :2]\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords)\n        coords = np.concatenate([coords, np.ones((len(coords), 1))], axis=1)\n        coords = coords @ self.rm_coords.T\n        return coords[:, :2]\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.size == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords, dtype=np.float32)\n        coords = np.concatenate([coords, np.ones((coords.shape[0], 1))], axis=1)\n        coords = np.matmul(coords, self.rm_coords.T)\n        return coords[:, :2]\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords)\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        coords = np.array(coords)\n        coords = coords.reshape(-1, 2)\n        coords = np.append(coords, np.ones((coords.shape[0], 1)), axis=1)\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords[:, :2]\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords)\n        coords = coords.reshape(-1, 2)\n        coords = np.concatenate((coords, np.ones((coords.shape[0], 1))), axis=1)\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords[:, :2]\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.asarray(coords)\n        coords = np.concatenate([coords, np.ones((coords.shape[0], 1))], axis=1)\n        coords = np.matmul(coords, self.rm_coords.T)\n        coords = coords[:, :2]\n        return coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        # Transform the coordinates using the rotation matrix\n        transformed_coords = np.dot(coords, self.rm_coords.T)\n\n        return transformed_coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return np.dot(coords, self.rm_coords.T)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.shape[0] == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # Apply the rotation matrix to the input coordinates\n        rotated_coords = np.dot(self.rm_coords, np.vstack((coords.T, np.ones((1, coords.shape[0])))))\n        rotated_coords = rotated_coords[:2, :].T\n        return rotated_coords\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return np.dot(coords, self.rm_coords.T)\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # if the angle is a multiple of 360, there is no need to rotate the coordinates\n        if self.angle % 360 == 0 or len(coords) == 0:\n            return coords\n\n        # convert the input coordinates to a numpy array\n        coords = np.array(coords)\n\n        # apply the rotation transformation to the coordinates using matrix multiplication\n        coords = np.dot(coords, self.rm_coords.T)\n\n        # return the transformed coordinates as a list of tuples\n        return coords.tolist()\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flop_count_dict = flop_count(model, inputs)\n    return flop_count_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flop_dict = flop_count(model, inputs)\n    return flop_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flop_counts = flop_count(model, inputs)\n    return flop_counts\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flops_dict = flop_count(model, inputs)\n    flops_dict = {k: v / 1e9 for k, v in flops_dict.items()}\n    return flops_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flop_dict = {}\n    for input in inputs:\n        flop_dict.update(flop_count(model, input))\n    return flop_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flop_count_dict = flop_count(model, inputs)\n    flop_count_dict = {k: v for k, v in flop_count_dict.items() if v > 0}\n    return flop_count_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flop_count_dict = flop_count(model, inputs)\n    flop_count_dict = {k: v / 1e9 for k, v in flop_count_dict.items()}\n    return flop_count_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flop_counts = fvcore.nn.flop_count.flop_count_operators(model, inputs)\n    return flop_counts\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flops = FlopCountAnalysis(model, inputs)\n    return flops.total()\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Get flops for each operator\n    flops_per_operator = flop_count(model, inputs)\n\n    # Convert to Gflops\n    flops_per_operator = {k: v / 1e9 for k, v in flops_per_operator.items()}\n\n    return flops_per_operator\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flops_dict = flop_count(model, inputs)\n    flops_dict = {k.split(\"::\")[-1]: v for k, v in flops_dict.items()}\n    return flops_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a FlopCountAnalysis object and run the model with the given inputs\n    flop_counts = FlopCountAnalysis(model, inputs)\n\n    # Extract the flops count from the FlopCountAnalysis object\n    flops_count = flop_counts.total()\n\n    # Convert the flops count to Gflop\n    flops_count_gflop = flops_count / 1e9\n\n    # Return the flops count in Gflop\n    return flops_count_gflop\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    flops_dict = flop_count(model, inputs)\n    return flops_dict\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Compute the flops for each operator in the model\n    flops_dict = flop_count(model, inputs)\n\n    # Extract the operator names and flops from the dictionary\n    operator_names = list(flops_dict.keys())\n    operator_flops = list(flops_dict.values())\n\n    # Create a dictionary to store the flops for each operator\n    flops_per_operator = {op: flops for op, flops in zip(operator_names, operator_flops)}\n\n    return flops_per_operator\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Convert inputs to a list of dictionaries, with only the \"image\" key present\n    inputs = [{k: v for k, v in input_dict.items() if k == \"image\"} for input_dict in inputs]\n\n    # Run the model with the provided inputs\n    model.eval()\n    with torch.no_grad():\n        _ = model(inputs)\n\n    # Compute the flops using the tracing adapter\n    adapter = TracingAdapter(model, inputs)\n    flops = flop_count(adapter, adapter.flattened_inputs)\n\n    # Convert the flops to Gflop counts\n    flops_per_operator = flops.total() / 1e9\n\n    return flops_per_operator\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Convert the inputs to a list of dictionaries\n    if not isinstance(inputs, list):\n        inputs = [inputs]\n\n    # Create a list of dictionaries with only the \"image\" key\n    inputs_images = [{\"image\": x[\"image\"]} for x in inputs]\n\n    # Create a FlopCountAnalysis object\n    flop_counter = FlopCountAnalysis(model, inputs_images)\n\n    # Run the model with the inputs\n    model(*inputs_images)\n\n    # Return the flops count\n    return flop_counter.total()\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Initialize a FlopCountAnalysis object with the provided model and inputs\n    flop_count_analysis = FlopCountAnalysis(model, inputs)\n\n    # Extract the flops count from the FlopCountAnalysis object\n    flop_count = flop_count_analysis.total()\n\n    # Convert the flops count to Gflop\n    flop_count_gflop = flop_count / 1e9\n\n    # Return the flops count as a dictionary\n    return flop_count_gflop\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Convert the inputs to a list of tensors\n    inputs = [input[\"image\"] for input in inputs]\n\n    # Create a FlopCountAnalysis object with the model and inputs\n    flop_counter = FlopCountAnalysis(model, inputs)\n\n    # Compute the flops\n    flop_counter.count()\n\n    # Get the flops for each operator\n    flops_per_operator = flop_counter.by_operator()\n\n    # Convert the flops to Gflop\n    flops_per_operator_gflop = {\n        op_name: flops / 1e9 for op_name, flops in flops_per_operator.items()\n    }\n\n    return flops_per_operator_gflop\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # TODO: this function is not very efficient and should be improved\n\n    # Get the flops count for the model\n    flops_count_dict = flop_count(model, inputs)\n\n    # Convert the flops count dictionary to a list of tuples\n    flops_count_list = [(op_name, flops_count_dict[op_name]) for op_name in flops_count_dict]\n\n    # Sort the list of tuples based on the flops count in descending order\n    flops_count_list.sort(key=lambda x: x[1], reverse=True)\n\n    # Extract the operator names and flops count from the sorted list\n    op_names = [op_name for op_name, _ in flops_count_list]\n    flops_counts = [flops_count for _, flops_count in flops_count_list]\n\n    return op_names, flops_counts\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # create a dictionary to store the flops count for each operator\n    flops_count = {}\n\n    # loop over each input and compute the flops\n    for input_data in inputs:\n        # extract the image tensor from the input dictionary\n        image = input_data[\"image\"]\n\n        # create a dummy model that takes the image tensor as input\n        dummy_model = nn.Sequential(model, nn.Flatten())\n\n        # create a dummy input for the dummy model\n        dummy_input = image\n\n        # use jit to compute the flops\n        flops = flop_count(dummy_model, dummy_input)\n\n        # add the flops to the flops count dictionary\n        for op, count in flops.items():\n            flops_count[op] = flops_count.get(op, 0) + count\n\n    # compute the average flops count for each operator\n    avg_flops_count = {op: count / len(inputs) for op, count in flops_count.items()}\n\n    return avg_flops_count\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] == (self.h, self.w):\n            if img.dtype == np.uint8:\n                if len(img.shape) > 2 and img.shape[2] == 1:\n                    pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n                else:\n                    pil_image = Image.fromarray(img)\n                pil_image = pil_image.rotate(self.angle, expand=self.expand)\n                ret = np.asarray(pil_image)\n                if len(img.shape) > 2 and img.shape[2] == 1:\n                    ret = np.expand_dims(ret, -1)\n            else:\n                # PIL only supports uint8\n                if any(x < 0 for x in img.strides):\n                    img = np.ascontiguousarray(img)\n                img = torch.from_numpy(img)\n                shape = list(img.shape)\n                shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n                img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n                _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                    Image.NEAREST: \"nearest\",\n                    Image.BILINEAR: \"bilinear\",\n                    Image.BICUBIC: \"bicubic\",\n                }\n                mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\n                align_corners = None if mode == \"nearest\" else False\n                img = F.interpolate(\n                    img, (self.new_h, self.new_w), mode=mode, align_corners=align_corners\n                )\n                shape[:2] = (self.new_h, self.new_w)\n                ret = img.permute(2, 3, 0, 1).view(shape).n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        assert img.shape[:2] == (self.h, self.w)\n        assert len(img.shape) <= 4\n        interp_method = interp if interp is not None else self.interp\n\n        if img.dtype == np.uint8:\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(\n                angle=self.angle,\n                resample=interp_method,\n                expand=self.expand,\n                center=self.center,\n            )\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                Image.NEAREST: \"nearest\",\n                Image.BILINEAR: \"bilinear\",\n                Image.BICUBIC: \"bicubic\",\n            }\n            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\n            align_corners = None if mode == \"nearest\" else False\n            img = F.interpolate(\n                img, (self.new_h, self.new_w), mode=mode"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        assert img.shape[:2] == (self.h, self.w)\n        assert len(img.shape) <= 4\n        interp_method = interp if interp is not None else self.interp\n\n        if img.dtype == np.uint8:\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(self.angle, resample=interp_method, expand=True)\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                Image.NEAREST: \"nearest\",\n                Image.BILINEAR: \"bilinear\",\n                Image.BICUBIC: \"bicubic\",\n            }\n            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\n            align_corners = None if mode == \"nearest\" else False\n            img = F.interpolate(\n                img, (self.new_h, self.new_w), mode=mode, align_corners=align_corners\n            )\n            shape[:2] = (self"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        # TODO decide on PIL vs opencv\n        assert img.shape[:2] == (self.h, self.w)\n        assert len(img.shape) <= 4\n        interp_method = interp if interp is not None else self.interp\n\n        if img.dtype == np.uint8:\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(self.angle, expand=self.expand, center=self.center)\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                Image.NEAREST: \"nearest\",\n                Image.BILINEAR: \"bilinear\",\n                Image.BICUBIC: \"bicubic\",\n            }\n            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\n            align_corners = None if mode == \"nearest\" else False\n            img = F.interpolate(\n                img, (self.new_h, self.new_w), mode=mode, align_corners=align_corners"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        assert img.shape[:2] == (self.h, self.w)\n        assert len(img.shape) <= 4\n        interp_method = interp if interp is not None else self.interp\n\n        if img.dtype == np.uint8:\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(\n                self.angle,\n                resample=interp_method,\n                expand=self.expand,\n                center=self.center,\n            )\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                Image.NEAREST: \"nearest\",\n                Image.BILINEAR: \"bilinear\",\n                Image.BICUBIC: \"bicubic\",\n            }\n            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\n            align_corners = None if mode == \"nearest\" else False\n            img = F.interpolate(\n                img, (self.new_h, self.new_w), mode=mode, align"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        assert img.shape[:2] == (self.h, self.w)\n        assert len(img.shape) <= 4\n        if img.dtype == np.uint8:\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(angle=self.angle, resample=self.interp)\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                Image.NEAREST: \"nearest\",\n                Image.BILINEAR: \"bilinear\",\n                Image.BICUBIC: \"bicubic\",\n            }\n            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\n            align_corners = None if mode == \"nearest\" else False\n            img = F.interpolate(\n                img, (self.new_h, self.new_w), mode=mode, align_corners=align_corners\n            )\n            shape[:2] = (self.new_h, self.new_w)\n            ret = img.permute(2"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        assert img.shape[:2] == (self.h, self.w)\n        assert len(img.shape) <= 4\n\n        if img.dtype == np.uint8:\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(angle=self.angle, resample=interp if interp else self.interp, center=self.center, expand=self.expand)\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                Image.NEAREST: \"nearest\",\n                Image.BILINEAR: \"bilinear\",\n                Image.BICUBIC: \"bicubic\",\n            }\n            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\n            align_corners = None if mode == \"nearest\" else False\n            img = F.interpolate(\n                img, (self.new_h, self.new_w), mode=mode, align_corners=align_corners\n            )\n            shape[:2] = (self.new_h"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] == (self.h, self.w) and self.angle % 360 == 0:\n            return img\n\n        if img.dtype == np.uint8:\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(self.angle, resample=interp if interp else self.interp)\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                Image.NEAREST: \"nearest\",\n                Image.BILINEAR: \"bilinear\",\n                Image.BICUBIC: \"bicubic\",\n            }\n            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp]\n            img = F.interpolate(img, (self.bound_h, self.bound_w), mode=mode)\n            shape[:2] = (self.bound_h, self.bound_w)\n            ret = img.permute(2, 3, 0, 1).view(shape).numpy()  # nchw"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        assert img.shape[:2] == (self.h, self.w)\n        assert len(img.shape) <= 4\n\n        if self.angle == 0.0:\n            return img\n\n        interp_method = interp if interp is not None else self.interp\n        if img.dtype == np.uint8:\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(self.angle, expand=self.expand, center=self.center)\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                Image.NEAREST: \"nearest\",\n                Image.BILINEAR: \"bilinear\",\n                Image.BICUBIC: \"bicubic\",\n            }\n            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\n            align_corners = None if mode == \"nearest\" else False\n            img = F.interpolate(\n                img, (self.bound_h, self.bound_w), mode=mode, align_corners="}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        assert img.shape[:2] == (self.h, self.w)\n        assert len(img.shape) <= 4\n        if img.dtype == np.uint8:\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(self.angle, expand=self.expand, center=self.center)\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            interp_method = interp if interp is not None else self.interp\n            align_corners = None if interp_method == cv2.INTER_NEAREST else False\n            img = F.interpolate(\n                img, (self.bound_h, self.bound_w), mode=interp_method, align_corners=align_corners\n            )\n            shape[:2] = (self.bound_h, self.bound_w)\n            ret = img.permute(2, 3, 0, 1).view(shape).numpy()  # nchw -> hw(c)\n\n        return ret\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] != (self.h, self.w):\n            raise ValueError(\"Image shape must be ({}, {})\".format(self.h, self.w))\n        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n        else:\n            pil_image = Image.fromarray(img)\n        pil_image = pil_image.rotate(angle=self.angle, resample=interp, expand=True)\n        ret = np.asarray(pil_image)\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            ret = np.expand_dims(ret, -1)\n        return ret\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] != (self.h, self.w):\n            raise ValueError(\"Input image shape must match initial image shape.\")\n        if self.angle % 360 == 0:\n            return img\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n        else:\n            pil_image = Image.fromarray(img)\n        pil_image = pil_image.rotate(self.angle, resample=interp if interp else self.interp)\n        ret = np.asarray(pil_image)\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            ret = np.expand_dims(ret, -1)\n        return ret\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        # Check if the input image is empty or the angle results in no change\n        if img.shape[0] == 0 or img.shape[1] == 0 or self.angle % 360 == 0:\n            return img\n\n        # If the input image is a single channel image, convert it to a 3-channel image\n        if len(img.shape) == 2:\n            img = np.stack([img] * 3, axis=-1)\n\n        # If the interpolation method is not provided, use the instance's default interpolation method\n        interp = interp if interp is not None else self.interp\n\n        # Rotate the image using OpenCV's warpAffine function\n        rotated_img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        # Return the rotated image\n        return rotated_img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] == (self.h, self.w):\n            if interp is None:\n                interp = self.interp\n            ret = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n        else:\n            # TODO: this is a hack, we should not be using PIL here\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.rotate(self.angle, resample=interp)\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        return ret\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] != (self.h, self.w):\n            raise RuntimeError(\"Size mismatch: %d x %d vs %d x %d\" % (self.h, self.w, img.shape[0], img.shape[1]))\n        if self.angle == 0:\n            return img\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n        else:\n            pil_image = Image.fromarray(img)\n        pil_image = pil_image.rotate(self.angle, resample=interp if interp else self.interp, center=self.center)\n        ret = np.asarray(pil_image)\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            ret = np.expand_dims(ret, -1)\n        return ret\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        # If the angle is 0 or a multiple of 360, no rotation is needed\n        if self.angle == 0 or self.angle % 360 == 0:\n            return img\n\n        # If the image is empty, return it unchanged\n        if img is None:\n            return img\n\n        # If the interpolation method is not provided, use the instance's default\n        if interp is None:\n            interp = self.interp\n\n        # If the angle is not 0, rotate the image\n        if self.angle != 0:\n            # Rotate the image using OpenCV's warpAffine function\n            img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] != (self.h, self.w):\n            raise ValueError(\"Input image size must match initial image size.\")\n        if self.angle % 360 == 0:\n            return img\n\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n        else:\n            pil_image = Image.fromarray(img)\n\n        interp_method = interp if interp is not None else self.interp\n        pil_image = pil_image.rotate(self.angle, expand=True, resample=interp_method)\n        ret = np.asarray(pil_image)\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            ret = np.expand_dims(ret, -1)\n        return ret\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        # check if the image is empty or the angle results in no change\n        if img.size == 0 or (self.angle % 360 == 0):\n            return img\n\n        # check if the image is a single channel image\n        if len(img.shape) == 2:\n            img = np.expand_dims(img, axis=2)\n\n        # check if the image has 3 channels\n        if img.shape[2] != 3:\n            raise ValueError(\"Image must have 3 channels (RGB)\")\n\n        # check if the image is a numpy array\n        if not isinstance(img, np.ndarray):\n            raise TypeError(\"Image must be a numpy array\")\n\n        # check if the image has the correct dimensions\n        if img.shape[:2] != (self.h, self.w):\n            raise ValueError(\"Image dimensions must match original image dimensions\")\n\n        # check if the interpolation method is provided\n        if interp is None:\n            interp = self.interp\n\n        # rotate the image using OpenCV's warpAffine function\n        img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n        return img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        # Check if the image is empty or the angle results in no change\n        if img is None or (self.angle % 360 == 0 and self.angle != 0):\n            return img\n\n        # If the image is a single channel image, convert it to a 3-channel image\n        if len(img.shape) == 2:\n            img = np.expand_dims(img, axis=-1)\n\n        # Rotate the image using OpenCV's warpAffine function\n        rotated_img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR)\n\n        # If the image is a 3-channel image, convert it back to a single channel image\n        if len(rotated_img.shape) == 3 and rotated_img.shape[2] == 3:\n            rotated_img = rotated_img[:, :, 0]\n\n        return rotated_img\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if not np.all(self.rm_image == self.rm_image[0, 0]):\n            # OpenCV uses a different coordinate system than PIL, so we need to\n            # adjust the rotation matrix accordingly\n            img = img[:, :, ::-1]\n            img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp if interp else self.interp)\n            img = img[:, :, ::-1]\n        else:\n            img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp if interp else self.interp)\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if not isinstance(predictions, list):\n            predictions = [predictions]\n        for pred in predictions:\n            boxes = pred.pred_boxes if pred.has(\"pred_boxes\") else None\n            scores = pred.scores if pred.has(\"scores\") else None\n            classes = pred.pred_classes if pred.has(\"pred_classes\") else None\n            masks = pred.pred_masks if pred.has(\"pred_masks\") else None\n            keypoints = pred.pred_keypoints if pred.has(\"pred_keypoints\") else None\n            is_crowd = pred.pred_crowd if pred.has(\"pred_crowd\") else None\n            if isinstance(pred.pred_masks, BitMasks):\n                masks = pred.pred_masks.to(self.cpu_device)\n            if isinstance(pred.pred_masks, PolygonMasks):\n                masks = pred.pred_masks.to(self.cpu_device)\n            if isinstance(pred.pred_masks, torch.Tensor):\n                masks = pred.pred_masks.to(self.cpu_device)\n\n            if isinstance(pred.pred_masks_rle, list):\n                masks = [GenericMask(x, self.output.height, self.output.width) for x in pred.pred_masks_rle]\n            if isinstance(pred.pred_masks_rle, dict):\n                masks = [GenericMask(pred.pred_masks_rle, self.output.height, self.output.width)]\n\n            if isinstance(pred.pred_masks_rle, torch.Tensor):\n                masks = [GenericMask(pred.pred_masks_rle, self.output.height, self.output.width)]\n\n            if isinstance(pred.pred_masks_rle, list) and isinstance(pred.pred_masks_rle[0], dict):\n                masks = [GenericMask(x, self.output.height, self.output.width)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Adds boxes and labels to the image.\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        self.draw_boxes(boxes, labels=labels)\n\n        # Adds masks to the image.\n        if predictions.has(\"pred_masks\"):\n            masks = predictions.pred_masks\n            if isinstance(masks, BitMasks):\n                masks = masks.to(self.cpu_device)\n            masks = masks.to(self.cpu_device)\n            masks = [mask.numpy() for mask in masks]\n            self.draw_masks(masks)\n\n        # Adds keypoints to the image.\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n            if isinstance(keypoints, Keypoints):\n                keypoints = keypoints.to(self.cpu_device)\n            keypoints = keypoints.to(self.cpu_device)\n            keypoints = [keypoint.numpy() for keypoint in keypoints]\n            self.draw_keypoints(keypoints)\n\n        # Adds segmentation masks to the image.\n        if predictions.has(\"pred_masks_rle\"):\n            masks = predictions.pred_masks_rle\n            if isinstance(masks, list):\n                masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n            else:\n                masks = GenericMask(masks, self.output.height, self.output.width)\n            self.draw_masks(masks)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes.tensor.cpu()\n        scores = predictions.scores.tolist()\n        classes = predictions.pred_classes.tolist()\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        self.draw_box(boxes, labels=labels)\n\n        if predictions.has(\"pred_masks\"):\n            masks = predictions.pred_masks\n            if isinstance(masks, BitMasks):\n                masks = masks.to(self.cpu_device)\n            if isinstance(masks, PolygonMasks):\n                masks = masks.to_bitmasks(self.output.height, self.output.width)\n            masks = masks.to(self.cpu_device)\n            masks = masks.numpy()\n            masks = np.asarray(masks, dtype=\"bool\")\n            if self._instance_mode == ColorMode.IMAGE_BW:\n                masks_color = np.zeros((masks.shape[0], masks.shape[1], 3), dtype=\"uint8\")\n            else:\n                masks_color = self.metadata.get(\"thing_colors\", None)\n            self.draw_mask(masks, masks_color)\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n            keypoints = keypoints.to(self.cpu_device)\n            keypoints = keypoints.numpy()\n            keypoints = keypoints[keypoints[:, 2] > self.keypoint_threshold, :2]\n            keypoints = keypoints.reshape(-1, len(keypoints[0]) // 3, 3)\n            self.draw_keypoints(keypoints)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Draw predictions\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        self.draw_boxes(boxes, labels=labels)\n\n        # Draw segmentation masks\n        if self._instance_mode == ColorMode.SEGMENTATION and predictions.has(\"pred_masks\"):\n            masks = predictions.pred_masks\n            if isinstance(masks, BitMasks):\n                masks = masks.to(self.cpu_device)\n            masks = masks.to(torch.bool)\n            masks = [mask[0] for mask in masks]\n            self.draw_masks(masks, alpha=0.5)\n\n        # Draw keypoints\n        if predictions.has(\"pred_keypoints\"):\n            kps = predictions.pred_keypoints\n            if isinstance(kps, Keypoints):\n                kps = kps.to(self.cpu_device)\n            kps = kps.to(torch.bool)\n            kps = [kp[0] for kp in kps]\n            self.draw_keypoints(kps)\n\n        # Draw polygons\n        if predictions.has(\"pred_polygons\"):\n            polys = predictions.pred_polygons\n            if isinstance(polys, PolygonMasks):\n                polys = polys.to(self.cpu_device)\n            polys = polys.to(torch.bool)\n            polys = [poly[0] for poly in polys]\n            self.draw_polygons(polys)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # assert isinstance(predictions, Instances), predictions\n        boxes = predictions.pred_boxes\n        scores = predictions.scores\n        classes = predictions.pred_classes\n        if predictions.has(\"pred_masks\"):\n            masks = predictions.pred_masks\n        elif predictions.has(\"pred_masks_rle\"):\n            masks = predictions.pred_masks_rle\n        else:\n            masks = None\n\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n        else:\n            keypoints = None\n\n        if boxes.tensor.shape[0] == 0:\n            return self.output\n\n        boxes = boxes.to(self.cpu_device)\n        scores = scores.to(self.cpu_device)\n        classes = classes.to(self.cpu_device)\n        if masks is not None:\n            masks = masks.to(self.cpu_device)\n        if keypoints is not None:\n            keypoints = keypoints.to(self.cpu_device)\n\n        self.draw_boxes(boxes, classes, scores)\n        if masks is not None:\n            self.draw_masks(masks, classes)\n        if keypoints is not None:\n            self.draw_keypoints(keypoints, classes)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        self.output.reset_image(self.img)\n        self._draw_instance_predictions(predictions)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # filter out predictions with low confidence score\n        scores = predictions.get(\"scores\")\n        if scores is not None and \"scores\" in predictions:\n            indices = np.where(scores > self.keypoint_threshold)[0]\n            predictions = predictions[indices]\n\n        # create a figure for drawing\n        self.output.reset_image(self.img)\n\n        # draw boxes\n        if \"pred_boxes\" in predictions:\n            boxes = predictions.pred_boxes if predictions.pred_boxes.tensor.ndim == 2 else predictions.pred_boxes.tensor[None]\n            self.draw_boxes(boxes, predictions.get(\"pred_classes\"), predictions.get(\"scores\"))\n\n        # draw masks\n        if \"pred_masks\" in predictions:\n            masks = predictions.pred_masks if isinstance(predictions.pred_masks, BitMasks) else \\\n                BitMasks(predictions.pred_masks)\n            self.draw_instance_masks(masks, predictions.get(\"pred_classes\"), alpha=0.3)\n\n        # draw keypoints\n        if \"pred_keypoints\" in predictions:\n            keypoints = predictions.pred_keypoints\n            if keypoints.shape[1] == 3:\n                keypoints = keypoints[:, :2]\n            self.draw_keypoints(keypoints, connections=self.metadata.keypoint_connections)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if len(predictions) == 0:\n            return self.output\n        if not predictions.has(\"pred_boxes\"):\n            return self.output\n\n        # Draw boxes.\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            boxes_vis_value = self._create_grayscale_box_vis(predictions.pred_boxes)\n        elif self._instance_mode == ColorMode.IMAGE:\n            boxes_vis_value = self._create_passthrough_box_vis(predictions.pred_boxes)\n        elif self._instance_mode == ColorMode.SEGMENTATION:\n            boxes_vis_value = self._create_thing_box_vis(predictions.pred_boxes)\n        else:\n            raise ValueError(f\"Unknown mode: {self._instance_mode}\")\n        self.output.ax.add_patch(boxes_vis_value)\n\n        # Draw labels.\n        labels = _create_text_labels(\n            predictions.pred_classes,\n            predictions.scores,\n            self.metadata.get(\"thing_classes\", None),\n            predictions.pred_classes == self.metadata.thing_dataset_id_to_contiguous_id[\n                \"is_crowd\"\n            ],\n        )\n        if labels is not None:\n            self.draw_text(predictions.pred_boxes, labels)\n\n        # Draw masks.\n        if predictions.has(\"pred_masks\"):\n            if self._instance_mode == ColorMode.IMAGE_BW:\n                masks = predictions.pred_masks\n            elif self._instance_mode == ColorMode.IMAGE:\n                masks = predictions.pred_masks\n            elif self._instance_mode == ColorMode.SEGMENTATION:\n                masks = predictions.pred_masks\n            else:\n                raise ValueError(f\"Unknown mode: {self._instance_mode}\")\n            self.draw_mask(masks, alpha=0.5)\n\n        # Draw keypoints.\n        if predictions.has(\"pred_keypoint"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # convert to Instances\n        if isinstance(predictions, torch.Tensor):\n            predictions = Instances(self.output.height, self.output.width, predictions)\n\n        # convert to numpy array\n        if isinstance(predictions, torch.Tensor):\n            predictions = predictions.to(self.cpu_device).numpy()\n\n        # if there are no predictions, return the image as is\n        if len(predictions) == 0:\n            return self.output\n\n        # if we have masks, draw them\n        if \"pred_masks\" in predictions:\n            self.draw_predictions_masks(predictions)\n\n        # draw keypoints\n        if \"pred_keypoints\" in predictions:\n            self.draw_predictions_keypoints(predictions)\n\n        # draw bounding boxes\n        if \"pred_boxes\" in predictions:\n            self.draw_predictions_bboxes(predictions)\n\n        # draw classes\n        if \"pred_classes\" in predictions:\n            self.draw_predictions_classes(predictions)\n\n        # draw scores\n        if \"scores\" in predictions:\n            self.draw_predictions_scores(predictions)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Draw predictions\n        self.output.reset_image(self.img)\n        self.draw_boxes(predictions.pred_boxes, thickness=2)\n        self.draw_text(\n            predictions.get_fields()[\"pred_classes\"].tolist(),\n            predictions.get_fields()[\"scores\"].tolist(),\n            box_mode=BoxMode.XYXY_ABS,\n            box_color=_OFF_WHITE,\n            text_color=_BLACK,\n        )\n        if predictions.has(\"pred_masks\"):\n            self.draw_sem_seg(predictions.pred_masks, area_threshold=_SMALL_OBJECT_AREA_THRESH)\n        if predictions.has(\"pred_masks_rle\"):\n            self.draw_panoptic_seg_predictions(predictions.pred_masks_rle)\n        if predictions.has(\"pred_keypoints\"):\n            self.draw_keypoints(predictions.pred_keypoints, self.keypoint_threshold)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # if not isinstance(predictions, Instances):\n        #     raise TypeError(\n        #         \"This function expects an instance of Instances, \"\n        #         \"but got {}\".format(type(predictions))\n        #     )\n\n        # self.output.reset_image(self.img)\n        self._draw_predictions(predictions)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # convert everything to CPU\n        predictions = predictions.to(self.cpu_device)\n\n        # add a mask to the image if we have masks\n        if \"pred_masks\" in predictions:\n            masks = predictions[\"pred_masks\"]\n            masks = masks.to(self.cpu_device)\n            masks = [m for m in masks]\n            masks = [m for m in masks if m.shape[0] > 0]\n            masks = [m for m in masks if m.shape[1] > 0]\n            masks = [m for m in masks if m.sum() > 0]\n            if len(masks) > 0:\n                if isinstance(masks[0], PolygonMasks):\n                    masks = [x.to_bitmasks(self.output.height, self.output.width) for x in masks]\n                    masks = BitMasks(torch.cat(masks, dim=0))\n                elif isinstance(masks[0], BitMasks):\n                    masks = BitMasks(torch.cat(masks, dim=0))\n                else:\n                    masks = PolygonMasks(torch.cat(masks, dim=0))\n                masks = masks.to(self.cpu_device)\n                self.draw_polygon_masks(masks, alpha=0.3)\n\n        # add a mask to the image if we have masks\n        if \"pred_masks_rle\" in predictions:\n            masks = predictions[\"pred_masks_rle\"]\n            masks = masks.to(self.cpu_device)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n            masks = [x for x in masks if x.area() > 0]\n            if len(masks) > 0:\n                masks = PolygonMasks(torch.cat([x.mask for x in masks], dim=0))\n                masks = masks.to(self.cpu_device)\n               "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Draw predictions\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self.img)\n        self.output.reset_image(self.img)\n        if len(predictions) == 0:\n            return self.output\n\n        assert isinstance(predictions, list), predictions\n        if len(predictions) == 0:\n            return self.output\n\n        if isinstance(predictions[0], dict):\n            # Convert the list of dicts to a single dict\n            predictions = {k: [dic[k] for dic in predictions] for k in predictions[0]}\n\n        # Add custom attributes for visualization\n        if \"pred_masks_rle\" in predictions:\n            predictions[\"pred_masks\"] = [\n                GenericMask(x, self.output.height, self.output.width)\n                for x in predictions[\"pred_masks_rle\"]\n            ]\n        if \"pred_masks\" in predictions:\n            predictions[\"pred_masks\"] = [\n                x.mask for x in predictions[\"pred_masks\"]\n            ]\n\n        if \"pred_keypoints\" in predictions:\n            predictions[\"pred_keypoints\"] = [\n                x.reshape(-1, 3) for x in predictions[\"pred_keypoints\"]\n            ]\n\n        colors = self._get_colors(predictions)\n        labels = _create_text_labels(\n            predictions[\"pred_classes\"],\n            predictions[\"scores\"],\n            self.metadata.get(\"thing_classes\", None),\n            predictions.get(\"pred_crowd\", None),\n        )\n        self.draw_boxes(\n            predictions[\"pred_boxes\"],\n            width=self._default_font_size / 2,\n            alpha=0.7,\n            colors=colors,\n            labels=labels,\n        )\n        if \"pred_masks\" in predictions:\n            self.draw_masks(predictions[\"pred_masks\"], alpha=0.5, colors=colors)\n        if \"pred_keypoints\" in"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Draw predictions.\n        if not self.metadata.use_keypoint:\n            self.draw_box(predictions.pred_boxes, thickness=2)\n        self.draw_text(predictions.get_fields(), box_mode=predictions.pred_boxes.mode)\n        if self.metadata.use_keypoint:\n            self.draw_keypoints(predictions.pred_keypoints)\n        if \"pred_masks\" in predictions:\n            self.draw_mask(predictions.pred_masks, alpha=0.5)\n        if \"pred_masks_rle\" in predictions:\n            self.draw_mask(predictions.pred_masks_rle, alpha=0.5)\n        if \"pred_keypoints\" in predictions:\n            self.draw_keypoints(predictions.pred_keypoints)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Adds predictions to the image.\n        self.output.reset_image(self.img)\n        self.output.ax.axis(\"off\")\n\n        # Draws bounding boxes, classes, and scores for each instance.\n        for i, pred in enumerate(predictions):\n            if not self.metadata.get(\"thing_colors\"):\n                color = random_color()\n            else:\n                color = self.metadata.thing_colors[pred[\"pred_classes\"]]\n\n            self.draw_box(pred[\"pred_boxes\"], edge_color=color, linewidth=1)\n\n            if \"pred_classes\" in pred:\n                class_name = self.metadata.thing_classes[pred[\"pred_classes\"]]\n                self.draw_text(\n                    pred[\"pred_boxes\"],\n                    text=class_name,\n                    color=color,\n                    horizontalalignment=\"left\",\n                    verticalalignment=\"top\",\n                    fontsize=self._default_font_size,\n                )\n            if \"scores\" in pred:\n                score = pred[\"scores\"]\n                self.draw_text(\n                    pred[\"pred_boxes\"],\n                    text=\"{:.2f}\".format(score),\n                    color=color,\n                    horizontalalignment=\"right\",\n                    verticalalignment=\"top\",\n                    fontsize=self._default_font_size,\n                )\n\n        # Draws masks for each instance.\n        if \"pred_masks\" in predictions[0]:\n            for i, pred in enumerate(predictions):\n                if \"pred_masks\" in pred:\n                    self.draw_mask(pred[\"pred_masks\"], color=color)\n\n        # Draws keypoints for each instance.\n        if \"pred_keypoints\" in predictions[0]:\n            for i, pred in enumerate(predictions):\n                if \"pred_keypoints\" in pred:\n                    self.draw_keypoints(pred[\"pred_keypoints\"], color=color)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Convert predictions to a format that's compatible with the drawing functions.\n        if isinstance(predictions, torch.Tensor):\n            predictions = Instances.from_tensor(predictions)\n        elif isinstance(predictions, list):\n            predictions = Instances.from_list(predictions)\n        elif isinstance(predictions, dict):\n            predictions = Instances.from_dict(predictions)\n        else:\n            raise TypeError(\n                \"Unsupported type for visualization: {}\".format(type(predictions))\n            )\n        predictions = predictions.to(self.cpu_device)\n\n        # Draw the predictions.\n        self.draw_box(predictions.pred_boxes, thickness=2)\n        self.draw_text(\n            predictions.get_fields()[\"pred_classes\"],\n            predictions.get_fields()[\"scores\"],\n            box_mode=BoxMode.XYXY_ABS,\n            text_color=\"white\",\n            text_size=self._default_font_size,\n        )\n        self.draw_mask(predictions.pred_masks, alpha=0.5)\n        self.draw_keypoints(predictions.pred_keypoints, connections=None)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Adds the predicted boxes, scores, and labels to the Visualizer object.\n        self.output.reset_image(self.img)\n        self.output.ax.axis(\"off\")\n\n        # If there are no predictions, return the original image.\n        if len(predictions) == 0:\n            return self.output\n\n        # Get the class names from the metadata object.\n        class_names = self.metadata.get(\"thing_classes\", None)\n\n        # Get the colors for the class names from the metadata object.\n        colors = self.metadata.get(\"thing_colors\", None)\n\n        # If there are no class names or colors, use random colors.\n        if class_names is None or colors is None:\n            class_names = [\"instance\"]\n            colors = [random_color() for _ in class_names]\n\n        # Convert the class names and colors to numpy arrays.\n        class_names = np.array(class_names)\n        colors = np.array(colors)\n\n        # Get the predicted boxes, classes, scores, and masks from the predictions.\n        pred_boxes = predictions.pred_boxes\n        pred_classes = predictions.pred_classes\n        scores = predictions.scores\n        pred_masks = predictions.pred_masks\n\n        # Convert the predicted boxes and classes to numpy arrays.\n        pred_boxes = pred_boxes.tensor.cpu().numpy()\n        pred_classes = pred_classes.cpu().numpy()\n\n        # If the instance mode is set to \"image\", draw the masks as grayscale images.\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            masks = pred_masks.cpu().numpy()\n            masks = np.asarray(Image.fromarray(masks.astype(np.uint8)).convert(\"L\"))\n            masks = torch.from_numpy(masks.astype(np.bool))\n            pred_masks = BitMasks(masks)\n\n        # If the instance mode is set to \"segmentation\", draw the masks as"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Draw instance-level prediction results\n        if predictions.has(\"pred_masks\"):\n            if self._instance_mode == ColorMode.IMAGE_BW:\n                # use grayscale image to draw segmentation masks\n                self.output.reset_image(self.img)\n            self.draw_sem_seg(predictions.pred_masks, area_threshold=_SMALL_OBJECT_AREA_THRESH)\n            if self._instance_mode == ColorMode.IMAGE_BW:\n                # add contour around segmented areas\n                self.draw_contour(predictions.pred_masks, alpha=0.5)\n        if predictions.has(\"pred_boxes\"):\n            self.draw_box(predictions.pred_boxes, None, None, alpha=0.5)\n        if predictions.has(\"pred_classes\"):\n            self.draw_text(predictions.pred_classes, predictions.scores, alpha=0.5)\n        if predictions.has(\"pred_keypoints\"):\n            self.draw_keypoints(predictions.pred_keypoints, alpha=0.5)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Draw all the instances\n        if \"pred_masks\" in predictions:\n            self.draw_masks(predictions)\n        if \"pred_masks_rle\" in predictions:\n            self.draw_masks(predictions, rles=True)\n        if \"pred_boxes\" in predictions:\n            self.draw_boxes(predictions)\n        if \"pred_keypoints\" in predictions:\n            self.draw_keypoints(predictions)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # convert everything to CPU\n        predictions = predictions.to(self.cpu_device)\n        # predictions = predictions.to(\"cpu\")\n\n        # convert everything to a format we're ready to draw\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n        # masks = predictions.pred_masks if predictions.has(\"pred_masks\") else None\n        # panoptic_seg = predictions.panoptic_seg if predictions.has(\"panoptic_seg\") else None\n        # segments_info = predictions.panoptic_seg_masks if predictions.has(\"panoptic_seg_masks\") else None\n\n        # if panoptic_seg is not None:\n        #     panoptic_seg = panoptic_seg.to(self.cpu_device)\n        #     segments_info = segments_info.to(self.cpu_device)\n        #     panoptic_pred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\n\n        # if masks is not None:\n        #     masks = masks.to(self.cpu_device)\n        #     if panoptic_seg is None:\n        #         # If there are no panoptic segmentations, we can still draw masks\n        #         # for instance-level visualization.\n        #         masks = masks[predictions.pred_classes]\n        #     else:\n        #         # If there are panoptic segmentations, we only draw masks for\n        #         # instance-level visualization.\n        #         masks = None\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            # In IMAGE_BW mode, we draw all masks in one color.\n            # We choose to draw the masks in black"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to an RGBA image\n        rgba_image = self.canvas.buffer_rgba()\n        # Convert the RGBA image to an RGB image\n        rgb_image = np.frombuffer(rgba_image, dtype=np.uint8).reshape(self.height, self.width, 4)[:, :, :3]\n        return rgb_image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.fig.canvas.draw()\n        img_rgba = np.array(self.fig.canvas.renderer._renderer)\n        img_rgb = cv2.cvtColor(img_rgba, cv2.COLOR_RGBA2RGB)\n        return img_rgb\n\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to an RGBA image\n        canvas = self.canvas\n        canvas.draw()\n        rgba = np.array(canvas.buffer_rgba())\n\n        # Convert the RGBA image to RGB format\n        img = rgba[:, :, :3]\n\n        # Return the image as a numpy ndarray of uint8 type\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to a numpy ndarray of uint8 type.\n        canvas_image = np.array(self.canvas.buffer_rgba()).astype(np.uint8)\n\n        # Convert the image from RGBA to RGB format.\n        canvas_image = cv2.cvtColor(canvas_image, cv2.COLOR_RGBA2RGB)\n\n        # Return the visualized image as a numpy ndarray of uint8 type.\n        return canvas_image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to an image and return it\n        self.fig.canvas.draw()\n        image = np.array(self.fig.canvas.renderer._renderer)\n        return image\n\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to RGBA format\n        self.fig.canvas.draw()\n        rgba_image = np.frombuffer(self.fig.canvas.tostring_rgba(), dtype=np.uint8).reshape(self.height, self.width, 4)\n\n        # Convert the RGBA image to RGB format\n        rgb_image = rgba_image[:, :, :3].copy()\n\n        # Return the RGB image as a numpy ndarray of uint8 type\n        return rgb_image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to RGBA format\n        rgba_image = self.canvas.buffer_rgba()\n\n        # Create a new numpy array to store the RGB image\n        rgb_image = np.zeros((rgba_image.height, rgba_image.width, 3), dtype=np.uint8)\n\n        # Copy the RGB values from the RGBA array to the RGB array\n        for y in range(rgba_image.height):\n            for x in range(rgba_image.width):\n                rgb_image[y, x, 0] = rgba_image[y, x, 0]\n                rgb_image[y, x, 1] = rgba_image[y, x, 1]\n                rgb_image[y, x, 2] = rgba_image[y, x, 2]\n\n        return rgb_image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to a numpy array in RGB format\n        canvas_array = np.array(self.canvas.renderer._renderer)\n        # Convert the canvas array from RGBA to RGB format\n        canvas_array = canvas_array[:, :, :3]\n        # Convert the canvas array to a numpy ndarray of uint8 type\n        canvas_array = canvas_array.astype(np.uint8)\n        # Return the canvas array\n        return canvas_array\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to an RGBA image\n        canvas = self.canvas\n        canvas.draw()\n        rgba_img = np.fromstring(canvas.tostring_rgba(), dtype=np.uint8, sep='')\n        rgba_img = rgba_img.reshape(canvas.get_width_height()[::-1] + (4,))\n\n        # Convert the RGBA image to RGB\n        rgb_img = rgba_img[:, :, :3].copy()\n\n        # Convert the image from uint8 to uint8\n        rgb_img = rgb_img.astype(np.uint8)\n\n        return rgb_img\n\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the current state of the canvas associated with the VisImage instance\n        self.canvas.draw()\n\n        # Convert the canvas to an image in RGB format\n        img_rgba = np.frombuffer(self.canvas.tostring_rgb(), dtype=\"uint8\").reshape(\n            self.height, self.width, 3\n        )\n\n        # Convert the image from RGBA to RGB format\n        img_rgb = cv2.cvtColor(img_rgba, cv2.COLOR_RGBA2RGB)\n\n        # Return the image as a numpy ndarray of uint8 type\n        return img_rgb\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Get the current canvas from the VisImage instance\n        canvas = self.canvas\n\n        # Convert the canvas to an RGBA image\n        canvas.draw()\n        rgba = np.frombuffer(canvas.tostring_rgba(), dtype=np.uint8).reshape(canvas.get_width_height()[::-1] + (4,))\n\n        # Convert the RGBA image to RGB\n        img = rgba[:, :, :3]\n\n        # Convert the image to uint8 type\n        img = img.astype(np.uint8)\n\n        return img\n\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.fig.canvas.draw()\n        img = np.array(self.fig.canvas.renderer._renderer)\n        # img = np.array(self.fig.canvas.renderer.buffer_rgba())\n        # img = np.array(self.fig.canvas.renderer.buffer_rgba())\n        img = img.transpose((1, 0, 2))\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Retrieve the image from the canvas\n        self.fig.canvas.draw()\n        img = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=np.uint8)\n        img = img.reshape(self.height, self.width, 3)\n\n        # Convert the image from RGBA to RGB format\n        img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.fig.canvas.draw()\n        image = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype='uint8')\n        image = image.reshape(self.height, self.width, 3)\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to an image and return it\n        self.fig.canvas.draw()\n        img = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=np.uint8)\n        img = img.reshape(self.height, self.width, 3)\n        return img\n\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to a numpy array and convert the RGBA channels to RGB channels\n        img = np.array(self.canvas.renderer._renderer)\n        img = img[:, :, :3]\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.fig.canvas.draw()\n        image = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=\"uint8\")\n        image = image.reshape(self.height, self.width, 3)\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to a numpy ndarray of uint8 type.\n        self.canvas.draw()\n        img = np.frombuffer(self.canvas.tostring_rgb(), dtype=np.uint8).reshape(self.height, self.width, 3)\n\n        # Convert the image from RGBA to RGB format.\n        img = img[:, :, :3]\n\n        # Return the image as a numpy ndarray of uint8 type.\n        return img\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to an RGBA image\n        rgba_img = self.canvas.buffer_rgba()\n\n        # Convert the RGBA image to RGB format\n        rgb_img = np.empty((rgba_img.shape[0], rgba_img.shape[1], 3), dtype=np.uint8)\n        for i in range(3):\n            rgb_img[:, :, i] = rgba_img[:, :, i]\n\n        return rgb_img\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to a numpy array\n        canvas_array = np.array(self.canvas.renderer._renderer)\n        # Convert the array from RGBA to RGB format\n        image_rgb = cv2.cvtColor(canvas_array, cv2.COLOR_RGBA2RGB)\n        # Return the image as a numpy array of uint8 type\n        return image_rgb\n\n\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic.get(\"segments_info\"))\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw segmentation masks\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], alpha=0.5)\n\n        # Draw panoptic segmentation\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic[\"segments_info\"])\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            self.draw_keypoints(dic[\"keypoints\"], dic.get(\"keypoint_colors\", None))\n\n        # Draw bounding boxes\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n\n        # Draw bounding boxes\n        if \"proposals\" in dic:\n            self.draw_proposals(dic[\"proposals\"])\n\n        # Draw bounding boxes\n        if \"boxes\" in dic:\n            self.draw_boxes(dic[\"boxes\"])\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic.get(\"segments_info\", None))\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic.get(\"segments_info\", None))\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"].to(self.cpu_device))\n\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"].to(self.cpu_device))\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw segmentation masks\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], alpha=0.8)\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            keypoints = dic[\"keypoints\"]\n            if keypoints.shape[1] == 3:\n                keypoints = keypoints[:, :2]\n            self.draw_keypoints(keypoints, connections=self.metadata.keypoint_connections)\n\n        # Draw bounding boxes\n        if \"instances\" in dic:\n            instances = dic[\"instances\"].to(self.cpu_device)\n            if instances.has(\"pred_boxes\"):\n                boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else instances.pred_masks\n            else:\n                boxes = instances.pred_masks\n            scores = instances.scores if instances.has(\"scores\") else None\n            classes = instances.pred_classes.tolist() if instances.has(\"pred_classes\") else None\n            labels = _create_text_labels(\n                classes, scores, self.metadata.get(\"thing_classes\", None)\n            )\n            self.draw_instance_predictions(instances)\n\n        # Draw semantic segmentation\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], alpha=0.8)\n\n        # Draw panoptic segmentation\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic[\"segments_info\"])\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annotations = dic[\"annotations\"]\n            if \"segmentation\" in annotations:\n                if isinstance(annotations[\"segmentation\"], list):\n                    for seg in annotations[\"segmentation\"]:\n                        self.draw_binary_mask(seg, color=_RED, edge_color=_OFF_WHITE, alpha=0.5)\n                else:\n                    self.draw_binary_mask(annotations[\"segmentation\"], color=_RED, edge_color=_OFF_WHITE, alpha=0.5)\n\n            if \"keypoints\" in annotations:\n                keypoints = annotations[\"keypoints\"]\n                if keypoints.shape[0] > 0:\n                    keypoints = keypoints[keypoints[:, 2] > self.keypoint_threshold, :2]\n                    keypoints = keypoints.reshape(-1, 2)\n                    self.draw_keypoints(keypoints, color=_RED)\n\n            if \"bbox\" in annotations:\n                bbox = annotations[\"bbox\"]\n                self.draw_box(bbox, color=_RED)\n\n        if \"sem_seg\" in dic:\n            sem_seg = dic[\"sem_seg\"]\n            self.draw_sem_seg(sem_seg)\n\n        if \"panoptic_seg\" in dic:\n            panoptic_seg = dic[\"panoptic_seg\"]\n            segments_info = dic.get(\"segments_info\", None)\n            self.draw_panoptic_seg(panoptic_seg, segments_info)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw segmentation masks\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], area_threshold=_SMALL_OBJECT_AREA_THRESH)\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            keypoints = dic[\"keypoints\"]\n            keypoint_colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[k[0]]])\n                for k in keypoints\n                if k[2] > self.keypoint_threshold\n            ]\n            self.draw_keypoints(\n                keypoints,\n                keypoint_colors=keypoint_colors,\n                radius=self._default_font_size // 2,\n            )\n\n        # Draw bounding boxes\n        if \"instances\" in dic:\n            instances = dic[\"instances\"]\n            if instances.has(\"pred_masks\"):\n                masks = instances.pred_masks\n                if isinstance(masks, BitMasks):\n                    masks = masks.to_polygons()\n                elif isinstance(masks, PolygonMasks):\n                    masks = masks.polygons\n                elif isinstance(masks, torch.Tensor):\n                    masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n                else:\n                    raise NotImplementedError(\n                        \"Invalid mask type: {}\".format(type(masks))\n                    )\n            else:\n                masks = None\n\n            boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else None\n            scores = instances.scores if instances.has(\"scores\") else None\n            classes = instances.pred_classes.tolist() if instances.has(\"pred_classes\") else None\n            labels = _create_text_labels(\n                classes, scores, self.metadata.thing_classes, instances.get(\"is_crowd\", None)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw segmentation masks\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            self.draw_keypoints(dic[\"keypoints\"], dic[\"keypoint_scores\"])\n\n        # Draw bounding boxes\n        if \"bbox\" in dic:\n            self.draw_box(dic[\"bbox\"])\n\n        # Draw panoptic segmentation\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic.get(\"panoptic_seg_info\", None))\n\n        # Draw instance segmentation\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], area_threshold=_SMALL_OBJECT_AREA_THRESH)\n\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic[\"segments_info\"])\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # TODO: support keypoints\n        # TODO: support rotated boxes\n        # TODO: support polygons\n        # TODO: support panoptic segmentation\n        # TODO: support instance segmentation\n\n        if \"annotations\" in dic:\n            for ann in dic[\"annotations\"]:\n                if ann[\"iscrowd\"] == 1:\n                    continue\n                if ann[\"category_id\"] not in self.metadata.thing_dataset_id_to_contiguous_id:\n                    continue\n                if \"bbox\" in ann:\n                    self.draw_box(ann[\"bbox\"])\n                elif \"segmentation\" in ann:\n                    self.draw_binary_mask(ann[\"segmentation\"])\n                elif \"keypoints\" in ann:\n                    self.draw_keypoints(ann[\"keypoints\"])\n                elif \"polygon\" in ann:\n                    self.draw_polygon(ann[\"polygon\"])\n                elif \"rotated_box\" in ann:\n                    self.draw_rotated_box(ann[\"rotated_box\"])\n\n        if \"sem_seg_file_name\" in dic:\n            sem_seg = self.load_sem_seg(dic[\"sem_seg_file_name\"])\n            self.draw_sem_seg(sem_seg)\n\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg = self.load_panoptic_seg(dic[\"panoptic_seg_file_name\"])\n            self.draw_panoptic_seg(panoptic_seg)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic.get(\"segments_info\", None))\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n        if \"sem_seg_file_name\" in dic:\n            sem_seg = self._read_image(dic[\"sem_seg_file_name\"], format=\"L\")\n            self.draw_sem_seg(sem_seg)\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg = self._read_image(dic[\"panoptic_seg_file_name\"], format=\"I;16\")\n            segments_info = dic.get(\"panoptic_seg_info\", None)\n            self.draw_panoptic_seg(panoptic_seg, segments_info)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw segmentation masks\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"].to(self.cpu_device), alpha=0.5)\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            keypoints = dic[\"keypoints\"]\n            if isinstance(keypoints, torch.Tensor):\n                keypoints = keypoints.numpy()\n            keypoints = Keypoints(keypoints, keypoints.shape[1])\n            self.draw_keypoints(keypoints, keypoint_threshold=self.keypoint_threshold)\n\n        # Draw bounding boxes\n        if \"instances\" in dic:\n            instances = dic[\"instances\"].to(self.cpu_device)\n            if instances.has(\"pred_boxes\"):\n                boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else None\n                scores = instances.scores if instances.has(\"scores\") else None\n                classes = (\n                    instances.pred_classes.tolist() if instances.has(\"pred_classes\") else None\n                )\n                labels = _create_text_labels(\n                    classes, scores, self.metadata.get(\"thing_classes\", None)\n                )\n                keypoints = instances.pred_keypoints if instances.has(\"pred_keypoints\") else None\n                self.draw_instance_predictions(\n                    instances,\n                    keypoint_threshold=self.keypoint_threshold,\n                    show_class=True,\n                    show_score=True,\n                    box_alpha=0.3,\n                    keypoint_alpha=0.7,\n                    show_keypoint_edges=True,\n                    show_keypoint_labels=True,\n                    show_keypoint_edge_labels=True,\n                    keypoint_labels=labels,\n                )\n\n        # Draw panoptic segmentation\n        if \"panoptic_seg\" in dic:"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            for ann in dic[\"annotations\"]:\n                if \"bbox\" in ann:\n                    x1, y1, x2, y2 = ann[\"bbox\"]\n                    self.draw_box(\n                        [x1, y1, x2, y2],\n                        edge_color=ann.get(\"bbox_color\", _RED),\n                        thickness=2,\n                        text=ann.get(\"category_id\", None),\n                    )\n                if \"keypoints\" in ann:\n                    self.draw_keypoints(ann[\"keypoints\"], color=_RED)\n\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"].as_tensor(self.cpu_device))\n\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(\n                dic[\"panoptic_seg\"].as_tensor(self.cpu_device),\n                segments_info=dic.get(\"panoptic_seg_info\", None),\n            )\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # TODO: add support for keypoints, bounding boxes, etc.\n\n        # Segmentation\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic.get(\"panoptic_seg_info\", None))\n        if \"seg_masks\" in dic:\n            for seg_mask in dic[\"seg_masks\"]:\n                self.draw_binary_mask(seg_mask)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw segmentation masks\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], alpha=0.5)\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic[\"seg_info\"], alpha=0.5)\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            keypoints = dic[\"keypoints\"]\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in keypoints.colors\n            ]\n            self.draw_keypoints(keypoints, colors=colors)\n\n        # Draw bounding boxes\n        if \"bbox\" in dic:\n            bboxes = BoxMode.convert(dic[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)\n            self.draw_box(bboxes)\n\n        # Draw bounding boxes and labels\n        if \"instances\" in dic:\n            instances = dic[\"instances\"]\n            scores = instances.get(\"scores\")\n            classes = instances.get(\"pred_classes\")\n            if classes is None:\n                classes = instances.get(\"gt_classes\")\n            labels = _create_text_labels(classes, scores, self.metadata.thing_classes)\n            self.draw_instance_predictions(instances, labels=labels)\n\n        # Draw bounding boxes and labels\n        if \"pred_boxes\" in dic:\n            pred_boxes = dic[\"pred_boxes\"]\n            pred_classes = dic[\"pred_classes\"]\n            scores = dic[\"scores\"]\n            labels = _create_text_labels(pred_classes, scores, self.metadata.thing_classes)\n            self.draw_instance_predictions(pred_boxes, labels=labels)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic[\"segments_info\"])\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw segmentation masks\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], alpha=0.5)\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            self.draw_keypoints(\n                dic[\"keypoints\"],\n                connections=self.metadata.keypoint_connections,\n                radius=3,\n                alpha=0.7,\n            )\n\n        # Draw bounding boxes\n        if \"bbox\" in dic:\n            self.draw_box(dic[\"bbox\"], alpha=0.7)\n\n        # Draw panoptic segmentation\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(\n                dic[\"panoptic_seg\"],\n                segments_info=dic.get(\"panoptic_seg_info\"),\n                alpha=0.5,\n            )\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # TODO: This function is very long and complicated. It needs to be refactored.\n        # TODO: It is not very flexible to add new types of annotations.\n        # TODO: It is not very flexible to change the visualization style.\n\n        # Draw segmentation masks\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"].argmax(dim=0), alpha=0.6)\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic.get(\"panoptic_seg_info\", None))\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            keypoints = dic[\"keypoints\"]\n            if keypoints.shape[0] > 0:\n                keypoints = keypoints.cpu()\n                if \"keypoints_vis\" in dic:\n                    keypoints_vis = dic[\"keypoints_vis\"]\n                else:\n                    keypoints_vis = None\n                self.draw_keypoints(keypoints, keypoints_vis)\n\n        # Draw bounding boxes\n        if \"instances\" in dic:\n            instances = dic[\"instances\"]\n            if instances.has(\"pred_boxes\"):\n                pred_boxes = instances.pred_boxes\n                pred_classes = instances.pred_classes\n                scores = instances.scores\n                if instances.has(\"pred_masks\"):\n                    pred_masks = instances.pred_masks\n                else:\n                    pred_masks = None\n                if instances.has(\"pred_keypoints\"):\n                    pred_keypoints = instances.pred_keypoints\n                else:\n                    pred_keypoints = None\n                if instances.has(\"pred_keypoints_heatmap\"):\n                    pred_keypoints_heatmap = instances.pred_keypoints_heatmap\n                else:\n                    pred_keypoints_heatmap = None"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if edge_color is None:\n            edge_color = _OFF_WHITE\n\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if text is not None:\n            # first get a box\n            x0, y0, x1, y1 = binary_mask.bbox()\n            # draw text in the center (defined by median) when box is not drawn\n            # median is less sensitive to outliers.\n            text_pos = np.median(binary_mask.mask.nonzero(), axis=1)[::-1]\n            # for small objects, draw text at the side to avoid occlusion\n            instance_area = (y1 - y0) * (x1 - x0)\n            if (\n                instance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\n                or y1 - y0 < 40 * self.output.scale\n            ):\n                if y1 >= self.output.height - 5:\n                    text_pos = (x1, y0)\n                else:\n                    text_pos = (x0, y1)\n\n            height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            font_size = (\n                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n                * 0.5\n                * self._default_font_size\n            )\n            self.draw_text(\n                text,\n                text_pos,\n                color=lighter_color,\n                horizontal_alignment=\"center\",\n                font_size=font_size,\n            )\n\n        # draw mask\n        for segment in binary_mask.polygons:\n            self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha, edge_color=edge_color)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Convert the binary mask to a polygons mask\n        polygons = mask_util.encode(np.asfortranarray(binary_mask))\n        polygons = GenericMask(polygons, self.output.height, self.output.width)\n\n        # Draw the polygons mask\n        self.draw_polygon(\n            polygons.polygons[0].reshape(-1, 2),\n            color=color,\n            edge_color=edge_color,\n            alpha=alpha,\n            area_threshold=area_threshold,\n        )\n\n        # Draw the text on the mask\n        if text is not None:\n            x0, y0, x1, y1 = polygons.bbox()\n            text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\n            height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            font_size = (\n                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n                * 0.5\n                * self._default_font_size\n            )\n            self.draw_text(\n                text,\n                text_pos,\n                color=lighter_color,\n                horizontal_alignment=\"left\",\n                font_size=font_size,\n            )\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = _OFF_WHITE\n\n        # convert to uint8\n        binary_mask = np.asarray(binary_mask, dtype=np.uint8)\n\n        # convert mask to polygons\n        polygons = mask_util.encode(np.asfortranarray(binary_mask))\n        polygons = GenericMask(polygons, self.output.height, self.output.width)\n\n        # draw polygons\n        for segment in polygons.polygons:\n            self.draw_polygon(segment.reshape(-1, 2), color, edge_color=edge_color, alpha=alpha)\n\n        # draw text\n        if text is not None:\n            # get a box\n            x0, y0, x1, y1 = polygons.bbox()\n            # draw text in the center (defined by median) when box is not drawn\n            # median is less sensitive to outliers.\n            text_pos = np.median(binary_mask.nonzero(), axis=1)[::-1]\n            # for small objects, draw text at the side to avoid occlusion\n            instance_area = (y1 - y0) * (x1 - x0)\n            if (\n                instance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\n                or y1 - y0 < 40 * self.output.scale\n            ):\n                if y1 >= self.output.height - 5:\n                    text_pos = (x1, y0)\n                else:\n                    text_pos = (x0, y1)\n\n            height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            font_size = (\n                np.clip((height_ratio - 0.02) / 0.08 + 1"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n\n        if not np.any(binary_mask):\n            return self.output\n\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = _OFF_WHITE\n\n        if text is not None:\n            text_color = self._change_color_brightness(color, brightness_factor=0.7)\n\n        if not isinstance(binary_mask, GenericMask):\n            binary_mask = GenericMask(binary_mask, self.output.height, self.output.width)\n\n        if binary_mask.area() < area_threshold:\n            return self.output\n\n        if binary_mask.has_holes:\n            # draw mask with holes\n            for segment in binary_mask.polygons:\n                self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha, edge_color=edge_color)\n        else:\n            # draw mask without holes\n            self.draw_polygon(binary_mask.mask.flatten(), color, alpha=alpha, edge_color=edge_color)\n\n        if text is not None:\n            text_pos = np.median(binary_mask.mask.nonzero(), axis=1)[::-1]\n            height_ratio = (\n                binary_mask.height / np.sqrt(self.output.height * self.output.width)\n            )\n            font_size = (\n                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n            )\n            self.draw_text(\n                text,\n                text_pos,\n                color=text_color,\n                horizontal_alignment=\"center\",\n                font_size=font_size,\n            )\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, BitMasks):\n            binary_mask = binary_mask.get_mask_tensor(self.output.height, self.output.width).numpy()\n        if isinstance(binary_mask, PolygonMasks):\n            binary_mask = binary_mask.get_masks(self.output.height, self.output.width).numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(bool)\n\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = color\n\n        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n        if mask.area() < area_threshold:\n            return self.output\n\n        if mask.has_holes:\n            for segment in mask.polygons:\n                self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\n        else:\n            self.draw_polygon(mask.mask.flatten(), color, alpha=alpha)\n\n        if text is not None:\n            text_pos = np.median(mask.mask.nonzero(), axis=1)[::-1]\n            height_ratio = mask.mask.shape[0] / np.sqrt(self.output.height * self.output.width)\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            font_size = (\n                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n                * 0.5\n                * self._default_font_size\n            )\n            self.draw_text(\n                text,\n                text_pos,\n                color=lighter_color,\n                horizontal_alignment=\"center\","}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=0.7)\n\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(\"uint8\")\n\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.cpu().numpy()\n\n        if isinstance(binary_mask, BitMasks):\n            binary_mask = binary_mask.tensor.numpy()\n\n        if isinstance(binary_mask, PolygonMasks):\n            binary_mask = binary_mask.polygons\n\n        if isinstance(binary_mask, GenericMask):\n            binary_mask = binary_mask.polygons\n\n        if isinstance(binary_mask, list):\n            if len(binary_mask) == 0:\n                return self.output\n            if isinstance(binary_mask[0], np.ndarray):\n                binary_mask = [x.reshape(-1, 2) for x in binary_mask]\n            else:\n                binary_mask = [x.polygons for x in binary_mask]\n\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = [binary_mask]\n\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = [x.numpy() for x in binary_mask]\n\n        if isinstance(binary_mask, tuple):\n            binary_mask = list(binary_mask)\n\n        if isinstance(binary_mask, dict):\n            binary_mask = [x for x in binary_mask.values()]\n\n        if isinstance(binary_mask, list) and isinstance(binary_mask[0], dict):\n            binary_mask = [x[\"segmentation\"] for x in binary_mask]\n\n        if isinstance(binary_mask, list) and isinstance(binary_mask[0], list):\n            binary"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=0.7)\n\n        # Convert the binary mask to a polygon mask\n        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n\n        # Draw the polygon mask\n        if mask.area() > area_threshold:\n            self.draw_polygon(mask.mask.nonzero()[::-1].T, color, alpha=alpha)\n\n        # Draw the text on the mask\n        if text is not None:\n            height_ratio = mask.height / np.sqrt(self.output.height * self.output.width)\n            text_color = self._change_color_brightness(color, brightness_factor=0.7)\n            font_size = (\n                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n                * 0.5\n                * self._default_font_size\n            )\n            text_pos = np.median(mask.mask.nonzero(), axis=1)[::-1]\n            self.draw_text(text, text_pos, color=text_color, font_size=font_size)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=0.7)\n\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(np.bool)\n\n        if isinstance(binary_mask, PolygonMasks):\n            binary_mask = binary_mask.polygons\n        elif isinstance(binary_mask, BitMasks):\n            binary_mask = binary_mask.get_mask_tensor().numpy()\n        elif isinstance(binary_mask, list):\n            # assume it's a list of polygons already\n            pass\n        else:\n            raise ValueError(\n                \"binary_mask should be of type PolygonMasks, BitMasks, or list, but got {}\".format(\n                    type(binary_mask)\n                )\n            )\n\n        if isinstance(binary_mask, list):\n            # assume it's a list of polygons already\n            pass\n        else:\n            binary_mask = [binary_mask]\n\n        if isinstance(binary_mask[0], np.ndarray):\n            # assume it's a list of polygons already\n            pass\n        else:\n            binary_mask = [x.numpy() for x in binary_mask]\n\n        if isinstance(binary_mask[0], np.ndarray):\n            # assume it's a list of polygons already\n            pass\n        else:\n            binary_mask = [x.numpy() for x in binary_mask]\n\n        if isinstance(binary_mask[0], np.ndarray):\n            # assume it's a list of polygons already\n            pass\n        else:\n            binary_mask = [x.numpy() for x in binary_mask]\n\n        if isinstance(binary_mask[0], np.nd"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=0.7)\n        binary_mask = binary_mask.astype(np.uint8)\n        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n        if mask.area() < area_threshold:\n            return self.output\n        if mask.has_holes:\n            for polygon in mask.polygons:\n                self.draw_polygon(polygon.reshape(-1, 2), color, alpha=alpha)\n        else:\n            self.draw_polygon(mask.mask.nonzero(), color, alpha=alpha)\n        if text is not None:\n            x0, y0, x1, y1 = mask.bbox()\n            text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\n            height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            font_size = (\n                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n                * 0.5\n                * self._default_font_size\n            )\n            self.draw_text(\n                text,\n                text_pos,\n                color=lighter_color,\n                horizontal_alignment=\"left\",\n                font_size=font_size,\n            )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, BitMasks):\n            binary_mask = binary_mask.get_mask_tensor(self.output.height, self.output.width).numpy()\n        if isinstance(binary_mask, PolygonMasks):\n            binary_mask = binary_mask.get_mask_tensor(self.output.height, self.output.width).numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(np.uint8)\n        if isinstance(binary_mask, GenericMask):\n            binary_mask = binary_mask.mask\n        if isinstance(binary_mask, dict):\n            binary_mask = mask_util.decode(binary_mask)\n\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=0.7)\n\n        if not isinstance(binary_mask, np.ndarray):\n            raise ValueError(\n                \"binary_mask should be a numpy array or a GenericMask, \"\n                f\"but got {type(binary_mask)}\"\n            )\n        if binary_mask.ndim != 2:\n            raise ValueError(\n                \"binary_mask should be of shape (H, W), \" f\"but got {binary_mask.shape}\"\n            )\n        if binary_mask.dtype != np.uint8:\n            raise ValueError(\n                \"binary_mask values should be of type uint8, \" f\"but got {binary_mask.dtype}\"\n            )\n        if not binary_mask.flags[\"C_CONTIGUOUS\"]:\n            binary_mask = np.ascontiguousarray(binary_mask)\n\n        if binary_mask.shape[0] != self.output.height or binary_mask.shape[1] != self.output.width"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = _OFF_WHITE\n\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(np.bool)\n        elif isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        else:\n            raise ValueError(\"binary_mask must be a numpy array or a torch tensor\")\n\n        if np.any(binary_mask):\n            # Convert binary mask to polygons\n            polygons, _ = mask_util.encode(\n                np.asfortranarray(binary_mask.astype(np.uint8))\n            )\n            polygons = polygons[\"counts\"]\n            polygons = mask_util.decode(polygons)\n\n            # Draw polygons\n            for polygon in polygons:\n                self.draw_polygon(polygon, color=color, edge_color=edge_color, alpha=alpha)\n\n            # Draw text on mask\n            if text is not None:\n                x0, y0, x1, y1 = mask_util.toBbox(polygons)\n                text_pos = np.median(binary_mask.nonzero(), axis=1)[::-1]\n                height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\n                lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n                font_size = (\n                    np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n                    * 0.5\n                    * self._default_font_size\n                )\n                self.draw_text(\n                    text,\n                    text_pos,\n                    color=lighter_color,\n                    horizontal_alignment=\"center\",\n                    font_size=font_size,\n                )\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(np.bool)\n\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = GenericMask(binary_mask, self.output.height, self.output.width)\n\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=0.7)\n\n        if binary_mask.area() < area_threshold:\n            return self.output\n\n        if isinstance(binary_mask, PolygonMasks):\n            # draw polygons\n            polygons = binary_mask.polygons\n            for polygon in polygons:\n                self.draw_polygon(polygon, color, edge_color, alpha)\n        elif isinstance(binary_mask, BitMasks):\n            # draw bit masks\n            bitmasks = binary_mask.masks\n            for bitmask in bitmasks:\n                self.draw_bitmask(bitmask, color, edge_color, alpha)\n        elif isinstance(binary_mask, GenericMask):\n            # draw polygons\n            polygons = binary_mask.polygons\n            for polygon in polygons:\n                self.draw_polygon(polygon, color, edge_color, alpha)\n        else:\n            raise ValueError(\n                \"Unsupported type for binary_mask: {}\".format(type(binary_mask))\n            )\n\n        # draw text\n        if text is not None:\n            text_pos = np.median(binary_mask.mask.nonzero(), axis=1)[::-1]\n            height_ratio = binary_mask.height / np.sqrt(self.output.height * self.output.width)\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=0.7)\n\n        if isinstance(binary_mask, (list, tuple)):\n            binary_mask = np.asarray(binary_mask)\n        if binary_mask.ndim == 3:\n            binary_mask = binary_mask.any(axis=-1)\n        if binary_mask.ndim != 2:\n            raise ValueError(\"binary_mask must be of shape (H, W) or (H, W, 1)\")\n        if binary_mask.dtype != np.uint8:\n            binary_mask = binary_mask.astype(np.uint8)\n\n        if binary_mask.shape[0] != self.output.height or binary_mask.shape[1] != self.output.width:\n            binary_mask = cv2.resize(binary_mask, (self.output.width, self.output.height))\n\n        # draw mask\n        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n        if mask.area() > area_threshold:\n            for segment in mask.polygons:\n                self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\n\n        # draw text\n        if text is not None:\n            # first get a box\n            x0, y0, x1, y1 = mask.bbox()\n            text_pos = np.median(binary_mask.nonzero(), axis=1)[::-1]\n\n            # for small objects, draw text at the side to avoid occlusion\n            instance_area = (y1 - y0) * (x1 - x0)\n            if instance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=0.5)\n\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(bool)\n\n        if not isinstance(binary_mask, np.ndarray):\n            raise TypeError(\n                f\"binary_mask must be a numpy array or a torch tensor, got {type(binary_mask)}\"\n            )\n\n        if not binary_mask.dtype == np.bool_:\n            raise ValueError(f\"binary_mask must be a boolean numpy array, got {binary_mask.dtype}\")\n\n        if binary_mask.ndim != 2:\n            raise ValueError(f\"binary_mask must be a 2D array, got {binary_mask.ndim}D\")\n\n        if binary_mask.shape[0] != self.output.height or binary_mask.shape[1] != self.output.width:\n            raise ValueError(\n                f\"binary_mask must have shape ({self.output.height}, {self.output.width}), got {binary_mask.shape}\"\n            )\n\n        if np.sum(binary_mask) < area_threshold:\n            return self.output\n\n        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n\n        if mask.has_holes:\n            for segment in mask.polygons:\n                self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\n        else:\n            self.draw_polygon(mask.mask.flatten(), color, alpha=alpha)\n\n        if text is not None:\n            x0, y0, x1, y1 = mask.bbox()\n            text_pos = ("}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=0.7)\n\n        # convert mask to polygon\n        polygons = mask_util.encode(\n            np.asfortranarray(binary_mask.astype(np.uint8))\n        )  # Encode mask to RLE\n        polygons = mask_util.frPyObjects(polygons, binary_mask.shape[0], binary_mask.shape[1])\n        polygons = mask_util.merge(polygons)\n        polygons = mask_util.decode(polygons)\n\n        # draw polygon\n        if len(polygons) > 0:\n            polygons = np.concatenate(polygons)\n            polygons = polygons.reshape((-1, 2))\n            self.draw_polygon(polygons, color, edge_color, alpha)\n\n        # draw text\n        if text is not None:\n            text_pos = np.median(binary_mask.nonzero(), axis=1)[::-1]\n            self.draw_text(text, text_pos, color=color)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = _OFF_WHITE\n\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(\"uint8\")\n        if not isinstance(binary_mask, np.ndarray):\n            raise ValueError(\n                \"binary_mask should be of type np.ndarray or torch.Tensor, but got {}\".format(\n                    type(binary_mask)\n                )\n            )\n        if binary_mask.shape != (self.output.height, self.output.width):\n            raise ValueError(\n                \"binary_mask should have shape ({}, {}), but got {}\".format(\n                    self.output.height, self.output.width, binary_mask.shape\n                )\n            )\n\n        # convert to polygons\n        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n\n        # draw mask\n        if mask.area() > area_threshold:\n            if mask.has_holes:\n                # draw polygon with holes\n                for polygon in mask.polygons:\n                    self.draw_polygon(polygon.reshape(-1, 2), color, edge_color, alpha)\n            else:\n                # draw polygon without holes\n                self.draw_polygon(mask.polygons[0].reshape(-1, 2), color, edge_color, alpha)\n\n        # draw text\n        if text is not None:\n            text_pos = np.median(binary_mask.nonzero(), axis=1)[::-1]\n            self.draw_text(text, text_pos, color=color)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if not isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.cpu()\n        if binary_mask.dtype != np.uint8:\n            binary_mask = binary_mask.numpy().astype(np.uint8)\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=0.7)\n\n        if area_threshold is not None:\n            # convert to boolean mask and perform morphology operation\n            # to fill the holes in the mask\n            binary_mask = binary_mask.astype(bool)\n            kernel = np.ones((3, 3), np.uint8)\n            binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n            binary_mask = binary_mask.astype(np.uint8)\n\n        # convert to polygons\n        polygons = mask_util.encode(np.asfortranarray(binary_mask))\n        polygons = polygons[\"counts\"]\n        polygons = mask_util.frPyObjects(polygons, binary_mask.shape[0], binary_mask.shape[1])\n        polygons = mask_util.decode(polygons)\n\n        # draw polygons\n        for polygon in polygons:\n            if polygon.size < 6:\n                continue\n            self.draw_polygon(polygon.reshape(-1, 2), color, edge_color, alpha)\n\n        # draw text\n        if text is not None:\n            text_pos = np.median(binary_mask.nonzero(), axis=1)[::-1]\n            height_ratio = (binary_mask.shape[0] - 1) / np.sqrt(self.output.height * self.output.width)\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            font_size = (\n                np.clip"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = _OFF_WHITE\n\n        # Convert binary mask to polygons.\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(\"uint8\")\n\n        # Convert to polygons.\n        if isinstance(binary_mask, np.ndarray):\n            polygons, _ = mask_util.to_polygon(binary_mask, area_threshold=area_threshold)\n        elif isinstance(binary_mask, dict):\n            polygons = [np.asarray(x).reshape(-1) for x in binary_mask[\"counts\"]]\n        else:\n            raise ValueError(\"binary_mask is expected to be a numpy array or a dict\")\n\n        # Draw polygons.\n        for polygon in polygons:\n            self.draw_polygon(polygon.reshape(-1, 2), color, edge_color, alpha)\n\n        # Draw text.\n        if text is not None:\n            text_pos = np.median(binary_mask.nonzero(), axis=1)[::-1]\n            self.draw_text(text, text_pos, color=color, horizontal_alignment=\"center\")\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            edge_color = _OFF_WHITE\n\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(np.bool)\n        if not isinstance(binary_mask, np.ndarray) or binary_mask.ndim != 2:\n            raise ValueError(\"binary_mask must be a numpy array of shape (H, W)\")\n        if not np.issubdtype(binary_mask.dtype, np.bool_):\n            raise ValueError(\"binary_mask must be a numpy array of dtype bool\")\n\n        # Get the connected components\n        labels = measure.label(binary_mask)\n\n        # Get the bounding boxes of the connected components\n        bboxes = []\n        for label in np.unique(labels):\n            if label == 0:\n                continue\n            bbox = measure.regionprops(labels == label)[0].bbox\n            bboxes.append(bbox)\n\n        # Draw the bounding boxes\n        for bbox in bboxes:\n            if np.prod(bbox[2:] - bbox[:2]) < area_threshold:\n                continue\n            self.draw_box(bbox, color=color, alpha=alpha, edge_color=edge_color)\n\n            # Draw the text on the mask\n            if text is not None:\n                text_pos = np.median(binary_mask[bbox[0] : bbox[2], bbox[1] : bbox[3]].nonzero(), axis=1)[\n                    ::-1\n                ]\n                self.draw_text(text, text_pos, color=color)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = _OFF_WHITE\n\n        binary_mask = np.asarray(binary_mask, dtype=\"uint8\")\n        if binary_mask.ndim != 2:\n            raise ValueError(\n                \"binary_mask should be of shape (H, W), got {}\".format(binary_mask.shape)\n            )\n        if binary_mask.dtype != \"uint8\":\n            raise ValueError(\"binary_mask values should be in list of int, got {}\".format(binary_mask.dtype))\n        if binary_mask.shape[0] * binary_mask.shape[1] > _LARGE_MASK_AREA_THRESH:\n            # for large mask, we use a different approach to draw the mask.\n            # we draw the mask as a polygon and add a small polygon edge to make it look like a mask.\n            binary_mask = binary_mask.astype(\"bool\")\n            contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n            if len(contours) > 0:\n                contours = sorted(contours, key=cv2.contourArea, reverse=True)\n                contours = [c.reshape(-1, 2) for c in contours]\n                for contour in contours:\n                    if cv2.contourArea(contour) < area_threshold:\n                        break\n                    self.draw_polygon(contour, color=color, edge_color=edge_color, alpha=alpha)\n        else:\n            # for small mask, we draw the mask as a polygon\n            contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n            if len(contours) > 0:\n                contours = sorted(contours, key=cv2."}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"Expect an Instances object, but got {type(other)}!\"\n    assert input.image_size == other.image_size, f\"{msg}Expect image_size {input.image_size} == {other.image_size}\"\n\n    for name in input._field_names:\n        if name == \"pred_boxes\":\n            assert isinstance(input.pred_boxes, Boxes), f\"Expect a Boxes object, but got {type(input.pred_boxes)}!\"\n            assert isinstance(other.pred_boxes, Boxes), f\"Expect a Boxes object, but got {type(other.pred_boxes)}!\"\n            assert input.pred_boxes.tensor.shape == other.pred_boxes.tensor.shape, f\"{msg}Expect pred_boxes.tensor.shape {input.pred_boxes.tensor.shape} == {other.pred_boxes.tensor.shape}\"\n            assert torch.allclose(\n                input.pred_boxes.tensor, other.pred_boxes.tensor, rtol=rtol\n            ), f\"{msg}Expect pred_boxes.tensor {input.pred_boxes.tensor} == {other.pred_boxes.tensor}\"\n        elif name == \"pred_masks\":\n            assert isinstance(input.pred_masks, ROIMasks), f\"Expect a ROIMasks object, but got {type(input.pred_masks)}!\"\n            assert isinstance(other.pred_masks, ROIMasks), f\"Expect a ROIMasks object, but got {type(other.pred_masks)}!\"\n            assert input.pred_masks.tensor.shape == other.pred_masks.tensor.shape, f\"{msg}Expect pred_masks.tensor.shape {input.pred_masks.tensor.shape} == {other.pred_masks.tensor.shape}\"\n            assert torch.allclose(\n                input"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # check image_size\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), msg + \"image_size mismatch\"\n    else:\n        assert input.image_size == other.image_size, msg + \"image_size mismatch\"\n\n    # check fields\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None or val2 is None:\n            assert val1 == val2, msg + f\"field {name} mismatch\"\n        elif isinstance(val1, (Boxes, ROIMasks)):\n            assert val1 == val2, msg + f\"field {name} mismatch\"\n        elif isinstance(val1, torch.Tensor):\n            assert torch.allclose(val1, val2, rtol=rtol), msg + f\"field {name} mismatch\"\n        else:\n            raise ValueError(f\"Unknown field type: {type(val1)}\")\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"Expect an Instances object, but got {type(other)}!\"\n\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), f\"{msg}image_size mismatch\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size mismatch\"\n\n    for name in input._field_names:\n        val = getattr(input, \"_\" + name, None)\n        other_val = getattr(other, \"_\" + name, None)\n        if val is None:\n            assert other_val is None, f\"{msg}{name} should be None\"\n        elif other_val is None:\n            assert False, f\"{msg}{name} should not be None\"\n        else:\n            if isinstance(val, (Boxes, ROIMasks)):\n                assert val == other_val, f\"{msg}{name} mismatch\"\n            elif isinstance(val, torch.Tensor):\n                assert torch.allclose(val, other_val, rtol=rtol), f\"{msg}{name} mismatch\"\n            else:\n                assert val == other_val, f\"{msg}{name} mismatch\"\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), \"Expect an Instances object, but got {}!\".format(\n        type(input)\n    )\n    assert isinstance(other, Instances), \"Expect an Instances object, but got {}!\".format(\n        type(other)\n    )\n\n    assert input.image_size == other.image_size, \"{}: image_size mismatch: {} vs {}\".format(\n        msg, input.image_size, other.image_size\n    )\n\n    for name in input._field_names:\n        val1 = getattr(input, name)\n        val2 = getattr(other, name)\n        if isinstance(val1, (Boxes, ROIMasks)):\n            assert val1 == val2, \"{}: {} mismatch: {} vs {}\".format(\n                msg, name, val1, val2\n            )\n        elif isinstance(val1, torch.Tensor):\n            if size_as_tensor:\n                assert torch.equal(\n                    val1, val2\n                ), \"{}: {} mismatch: {} vs {}\".format(msg, name, val1, val2)\n            else:\n                assert torch.allclose(\n                    val1, val2, rtol=rtol\n                ), \"{}: {} mismatch: {} vs {}\".format(msg, name, val1, val2)\n        else:\n            assert val1 == val2, \"{}: {} mismatch: {} vs {}\".format(\n                msg, name, val1, val2\n            )\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"{msg}Expect an Instances object, but got {type(other)}!\"\n\n    # Compare image_size\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    # Compare fields\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise ValueError(f\"{msg}field {name} has different number of elements: {val1} vs {val2}\")\n        if isinstance(val1, Boxes) and isinstance(val2, Boxes):\n            assert torch.equal(val1.tensor, val2.tensor), f\"{msg}field {name} mismatch: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, ROIMasks) and isinstance(val2, ROIMasks):\n            assert torch.equal(val1.tensor, val2.tensor), f\"{msg}field {name} mismatch: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, torch.Tensor) and isinstance(val2, torch.Tensor):\n            assert torch.allclose(val1, val2, rtol=rtol), f\"{msg}field {name} mismatch: {val1} vs {val2}\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances):\n        raise ValueError(f\"{msg}Expect an Instances object, but got {type(input)}\")\n    if not isinstance(other, Instances):\n        raise ValueError(f\"{msg}Expect an Instances object, but got {type(other)}\")\n\n    if size_as_tensor:\n        input_size = torch.tensor(input.image_size)\n        other_size = torch.tensor(other.image_size)\n    else:\n        input_size = input.image_size\n        other_size = other.image_size\n    assert torch.equal(\n        input_size, other_size\n    ), f\"{msg}image_size does not match: {input_size} vs {other_size}\"\n\n    for name in input._field_names:\n        input_val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(input_val, (Boxes, ROIMasks)):\n            assert input_val == other_val, f\"{msg}field {name} does not match\"\n        elif isinstance(input_val, torch.Tensor):\n            assert torch.allclose(\n                input_val, other_val, rtol=rtol\n            ), f\"{msg}field {name} does not match\"\n        else:\n            assert input_val == other_val, f\"{msg}field {name} does not match\"\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for name in input._field_names:\n        val1 = getattr(input, name, None)\n        val2 = getattr(other, name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise AssertionError(\n                f\"{msg} field {name} is missing in one of the instances\"\n            )\n        if isinstance(val1, Boxes):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg} field {name} has different shape: {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(\n                val1.tensor, val2.tensor\n            ), f\"{msg} field {name} is not close\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg} field {name} has different shape: {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(\n                val1.tensor, val2.tensor\n            ), f\"{msg} field {name} is not close\"\n        elif isinstance(val1, torch.Tensor):\n            assert val1.shape == val2.shape, f\"{msg} field {name} has different shape: {val1.shape} vs {val2.shape}\"\n            assert torch.allclose(\n                val1, val2, rtol=rtol\n            ), f\"{msg} field {name} is"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if image_size is the same\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}Image sizes are not equal: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Image sizes are not equal: {input.image_size} vs {other.image_size}\"\n\n    # Check if all fields are equal\n    for field_name in input._field_names:\n        if field_name == \"image_size\":\n            continue\n        input_field = getattr(input, field_name)\n        other_field = getattr(other, field_name)\n\n        if isinstance(input_field, Boxes):\n            assert torch.allclose(\n                input_field.tensor, other_field.tensor\n            ), f\"{msg}Field '{field_name}' is not equal: {input_field.tensor} vs {other_field.tensor}\"\n        elif isinstance(input_field, ROIMasks):\n            assert torch.allclose(\n                input_field.tensor, other_field.tensor\n            ), f\"{msg}Field '{field_name}' is not equal: {input_field.tensor} vs {other_field.tensor}\"\n        elif isinstance(input_field, torch.Tensor):\n            assert torch.allclose(\n                input_field, other_field, rtol=rtol\n            ), f\"{msg}Field '{field_name}' is not equal: {input_field} vs {other_field}\"\n        else:\n            assert input_field == other_field, f\"{msg}Field '{field_name}' is not equal: {input_field} vs {other_field}\"\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check that the image sizes are equal\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), msg\n    else:\n        assert input.image_size == other.image_size, msg\n\n    # Loop through all fields and compare them\n    for field_name in input._field_names:\n        input_val = getattr(input, field_name)\n        other_val = getattr(other, field_name)\n\n        if isinstance(input_val, Boxes):\n            assert input_val.tensor.shape == other_val.tensor.shape, msg\n            assert torch.allclose(input_val.tensor, other_val.tensor), msg\n        elif isinstance(input_val, ROIMasks):\n            assert input_val.tensor.shape == other_val.tensor.shape, msg\n            assert torch.allclose(input_val.tensor, other_val.tensor), msg\n        elif isinstance(input_val, torch.Tensor):\n            assert input_val.shape == other_val.shape, msg\n            assert torch.allclose(input_val, other_val, rtol=rtol), msg\n        elif isinstance(input_val, list):\n            assert len(input_val) == len(other_val), msg\n            for i in range(len(input_val)):\n                assert_instances_allclose(\n                    input_val[i], other_val[i], rtol=rtol, msg=msg\n                )\n        else:\n            raise ValueError(f\"Unknown field type: {type(input_val)}\")\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are equal\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n\n    # Iterate over all fields and check their values\n    for field in input._field_names:\n        val_1 = getattr(input, field)\n        val_2 = getattr(other, field)\n\n        if isinstance(val_1, Boxes):\n            # Special handling for Boxes\n            assert val_1.tensor.shape == val_2.tensor.shape, f\"{msg}Boxes tensor shapes do not match: {val_1.tensor.shape} vs {val_2.tensor.shape}\"\n            assert torch.allclose(val_1.tensor, val_2.tensor), f\"{msg}Boxes tensors are not equal: {val_1.tensor} vs {val_2.tensor}\"\n        elif isinstance(val_1, ROIMasks):\n            # Special handling for ROIMasks\n            assert val_1.tensor.shape == val_2.tensor.shape, f\"{msg}ROIMasks tensor shapes do not match: {val_1.tensor.shape} vs {val_2.tensor.shape}\"\n            assert torch.allclose(val_1.tensor, val_2.tensor), f\"{msg}ROIMasks tensors are not equal: {val_1.tensor} vs {val_2.tensor}\"\n        elif isinstance(val_1, torch.Tensor):\n            # Special handling for torch.Tensor\n            assert val_1.shape == val_2.shape, f\"{msg}Tensor shapes do not match: {val_1.shape} vs {val_2.shape}\"\n            assert torch.all"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check that the image sizes are the same\n    if size_as_tensor:\n        assert torch.equal(\n            input.image_size, other.image_size\n        ), f\"{msg} image_size: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size: {input.image_size} vs {other.image_size}\"\n\n    # Check that all fields are equal or close\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n\n        if isinstance(val, Boxes):\n            assert val.tensor.equal(other_val.tensor), f\"{msg} {name}: {val.tensor} vs {other_val.tensor}\"\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.equal(other_val.tensor), f\"{msg} {name}: {val.tensor} vs {other_val.tensor}\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), f\"{msg} {name}: {val} vs {other_val}\"\n        else:\n            raise ValueError(f\"Unknown field type: {type(val)}\")\n\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg} Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"{msg} Expect an Instances object, but got {type(other)}!\"\n\n    # check image_size\n    if size_as_tensor:\n        assert torch.equal(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), f\"{msg} image_size is not equal\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size is not equal\"\n\n    # check fields\n    for name in input._field_names:\n        val = getattr(input, \"_\" + name, None)\n        other_val = getattr(other, \"_\" + name, None)\n        if val is not None and other_val is not None:\n            if isinstance(val, (Boxes, ROIMasks)):\n                assert val == other_val, f\"{msg} {name} is not equal\"\n            elif isinstance(val, torch.Tensor):\n                assert torch.allclose(val, other_val, rtol=rtol), f\"{msg} {name} is not close\"\n            else:\n                assert val == other_val, f\"{msg} {name} is not equal\"\n        elif val is None and other_val is None:\n            pass\n        else:\n            raise ValueError(f\"{msg} {name} is not None\")\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are equal\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg} Expected image sizes to be equal, but got {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} Expected image sizes to be equal, but got {input.image_size} vs {other.image_size}\"\n\n    # Check if all fields are equal or close to each other\n    for field_name in input._field_names:\n        input_field = getattr(input, field_name)\n        other_field = getattr(other, field_name)\n\n        if isinstance(input_field, (Boxes, ROIMasks)):\n            assert input_field.tensor.shape == other_field.tensor.shape, f\"{msg} Expected field '{field_name}' to have the same shape, but got {input_field.tensor.shape} vs {other_field.tensor.shape}\"\n            assert torch.allclose(input_field.tensor, other_field.tensor), f\"{msg} Expected field '{field_name}' to be equal or close to each other, but got {input_field.tensor} vs {other_field.tensor}\"\n        elif isinstance(input_field, torch.Tensor):\n            assert input_field.shape == other_field.shape, f\"{msg} Expected field '{field_name}' to have the same shape, but got {input_field.shape} vs {other_field.shape}\"\n            assert torch.allclose(input_field, other_field, rtol=rtol), f\"{msg} Expected field '{field_name}' to be equal or close to each other, but got {input_field} vs {other_field}\"\n        elif isinstance(input_field, np.ndarray):\n            assert np.allclose(input_field, other_field, rtol=rtol), f\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are the same\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), msg\n    else:\n        assert input.image_size == other.image_size, msg\n\n    # Iterate over all fields and compare their values\n    for field_name in input._field_names:\n        input_field = getattr(input, field_name)\n        other_field = getattr(other, field_name)\n\n        # Handle Boxes and ROIMasks separately\n        if isinstance(input_field, (Boxes, ROIMasks)):\n            assert input_field.tensor.shape == other_field.tensor.shape, msg\n            assert torch.allclose(input_field.tensor, other_field.tensor), msg\n        elif isinstance(input_field, torch.Tensor):\n            assert input_field.shape == other_field.shape, msg\n            assert torch.allclose(input_field, other_field, rtol=rtol), msg\n        else:\n            # If the field is not a tensor, compare directly\n            assert input_field == other_field, msg\n\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances)\n    assert input.image_size == other.image_size, f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise AssertionError(f\"{msg}Field {name} is missing in one of the instances\")\n        if isinstance(val1, torch.Tensor):\n            if val1.shape != val2.shape:\n                raise AssertionError(\n                    f\"{msg}Field {name} has different shapes: {val1.shape} vs {val2.shape}\"\n                )\n            if not torch.allclose(val1, val2, rtol=rtol):\n                raise AssertionError(f\"{msg}Field {name} does not match\")\n        elif isinstance(val1, Boxes):\n            if not val1.tensor.allclose(val2.tensor, rtol=rtol):\n                raise AssertionError(f\"{msg}Field {name} does not match\")\n        elif isinstance(val1, ROIMasks):\n            if not val1.tensor.allclose(val2.tensor, rtol=rtol):\n                raise AssertionError(f\"{msg}Field {name} does not match\")\n        elif isinstance(val1, Instances):\n            assert_instances_allclose(val1, val2, rtol=rtol, msg=msg)\n        else:\n            raise Value"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"{msg}Expect an Instances object, but got {type(other)}!\"\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), f\"{msg}Image sizes do not match!\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Image sizes do not match!\"\n    for name in input._field_names:\n        val = getattr(input, \"_\" + name, None)\n        other_val = getattr(other, \"_\" + name, None)\n        if val is not None:\n            if other_val is None:\n                raise AssertionError(f\"{msg}Field {name} is in first Instances but not in second Instances\")\n            if isinstance(val, Boxes):\n                assert torch.allclose(val.tensor, other_val.tensor), f\"{msg}Field {name} does not match!\"\n            elif isinstance(val, ROIMasks):\n                assert val.tensor.equal(other_val.tensor), f\"{msg}Field {name} does not match!\"\n            elif isinstance(val, torch.Tensor):\n                assert torch.allclose(val, other_val, rtol=rtol), f\"{msg}Field {name} does not match!\"\n            else:\n                raise ValueError(f\"{msg}Unknown field type {type(val)} for field {name}\")\n        elif other_val is not None:\n            raise AssertionError(f\"{msg}Field {name} is in second Instances but not in first Instances\")\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"{msg}Expect an Instances object, but got {type(other)}!\"\n\n    # Check image sizes\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}image_size is different!\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size is different!\"\n\n    # Check all fields\n    for name in input._field_names:\n        val = getattr(input, \"_\" + name, None)\n        other_val = getattr(other, \"_\" + name, None)\n        if val is None and other_val is None:\n            continue\n        if val is None or other_val is None:\n            raise AssertionError(f\"{msg}field {name} is missing in one of the instances\")\n        if isinstance(val, (Boxes, ROIMasks)):\n            # Boxes and ROIMasks are compared as tensors\n            assert torch.allclose(\n                val.tensor, other_val.tensor\n            ), f\"{msg}field {name} is different!\"\n        elif isinstance(val, torch.Tensor):\n            # Tensors are compared with rtol\n            assert torch.allclose(\n                val, other_val, rtol=rtol\n            ), f\"{msg}field {name} is different!\"\n        elif isinstance(val, (list, tuple)):\n            # Lists and tuples are compared recursively\n            assert len(val) == len(other_val), f\"{msg}field {name} has different lengths!\"\n            for i, (x, y) in enumerate(zip(val, other_val)):\n                assert_instances_allclose(\n                    x,\n                    y,\n                    rtol=rtol,\n                    msg=f\"{"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), \"input must be an instance of Instances\"\n    assert isinstance(other, Instances), \"other must be an instance of Instances\"\n\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}image_size is not equal\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size is not equal\"\n\n    for field in input._field_names:\n        input_val = getattr(input, \"_\" + field, None)\n        other_val = getattr(other, \"_\" + field, None)\n        if input_val is None or other_val is None:\n            assert (\n                input_val == other_val\n            ), f\"{msg}field {field} is not equal (None)\"\n            continue\n\n        if isinstance(input_val, (Boxes, ROIMasks)):\n            assert input_val.tensor.shape == other_val.tensor.shape, f\"{msg}field {field} shape is not equal\"\n            assert torch.allclose(\n                input_val.tensor, other_val.tensor\n            ), f\"{msg}field {field} is not equal\"\n        elif isinstance(input_val, torch.Tensor):\n            assert input_val.shape == other_val.shape, f\"{msg}field {field} shape is not equal\"\n            assert torch.allclose(input_val, other_val, rtol=rtol), f\"{msg}field {field} is not equal\"\n        else:\n            raise ValueError(f\"{msg}field {field} type {type(input_val)} is not supported\")\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if image sizes are the same\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    # Check if all fields match\n    for name in input._field_names:\n        val = getattr(input, \"_\" + name, None)\n        other_val = getattr(other, \"_\" + name, None)\n        if val is not None and other_val is not None:\n            if isinstance(val, (Boxes, ROIMasks)):\n                assert val.tensor.shape == other_val.tensor.shape, f\"{msg}{name} shape mismatch: {val.tensor.shape} vs {other_val.tensor.shape}\"\n                assert torch.allclose(val.tensor, other_val.tensor), f\"{msg}{name} mismatch: {val.tensor} vs {other_val.tensor}\"\n            elif isinstance(val, torch.Tensor):\n                assert torch.allclose(val, other_val, rtol=rtol), f\"{msg}{name} mismatch: {val} vs {other_val}\"\n            else:\n                assert val == other_val, f\"{msg}{name} mismatch: {val} vs {other_val}\"\n        else:\n            assert val == other_val, f\"{msg}{name} mismatch: {val} vs {other_val}\"\n\n\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), \"Expect an Instances object!\"\n    assert isinstance(other, Instances), \"Expect an Instances object!\"\n\n    # Check if image sizes are equal\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), msg + \"image_size\"\n    else:\n        assert input.image_size == other.image_size, msg + \"image_size\"\n\n    # Check if fields are equal or close\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise AssertionError(msg + f\"Field '{name}' is not present in both instances.\")\n        if isinstance(val1, Boxes) or isinstance(val2, Boxes):\n            assert torch.allclose(val1.tensor, val2.tensor), msg + f\"Field '{name}'\"\n        elif isinstance(val1, ROIMasks) or isinstance(val2, ROIMasks):\n            assert val1.tensor.equal(val2.tensor), msg + f\"Field '{name}'\"\n        elif isinstance(val1, torch.Tensor) or isinstance(val2, torch.Tensor):\n            assert torch.allclose(val1, val2, rtol=rtol), msg + f\"Field '{name}'\"\n        else:\n            raise ValueError(f\"Unknown field type: {type(val1)}\")\n\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # TODO: Implement this function\n        # The area of a rotated box is calculated as the product of its width and height.\n        # The width and height of a rotated box can be obtained from its width and height attributes.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rotated box can be calculated as the product of its width and height.\n        # The area of a rot"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Compute area of each box\n        area = self.tensor[:, 2] * self.tensor[:, 3]\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return (self.tensor[:, 2] * self.tensor[:, 3]).to(dtype=torch.float32)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return (self.tensor[:, 2] * self.tensor[:, 3]).to(dtype=torch.float32)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 2] * self.tensor[:, 3]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Calculate the area of each box\n        area = (self.tensor[:, 2] * self.tensor[:, 3]) * torch.sin(self.tensor[:, 4] * math.pi / 180)\n\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 2] * self.tensor[:, 3]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 2] * self.tensor[:, 3]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Compute the area of each rotated box\n        width, height = self.tensor[:, 2], self.tensor[:, 3]\n        area = width * height\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Compute the area of each box\n        area = self.tensor[:, 2] * self.tensor[:, 3]\n\n        # Return the computed area\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return (self.tensor[:, 2] * self.tensor[:, 3]).to(dtype=torch.float32)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Compute the area of each box\n        area = self.tensor[:, 2] * self.tensor[:, 3]\n\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Calculate the area of each box\n        area = self.tensor[:, 2] * self.tensor[:, 3]\n\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 2] * self.tensor[:, 3]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Compute the area of the boxes\n        area = self.tensor[:, 2] * self.tensor[:, 3]\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return (self.tensor[:, 2] * self.tensor[:, 3]).to(dtype=torch.float32)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return (self.tensor[:, 2] * self.tensor[:, 3])\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # Calculate the area of each rotated box using the width and height\n        area = self.tensor[:, 2] * self.tensor[:, 3]\n\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 2] * self.tensor[:, 3]\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(cfg.MODEL.PROPOSAL_GENERATOR.NAME)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.RPN_ON and cfg.MODEL.RPN.NAME == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(cfg.MODEL.RPN.NAME)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(cfg.MODEL.PROPOSAL_GENERATOR.NAME)(cfg, input_shape)\n\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Extract the ground truth boxes, classes, and image sizes from the proposals\n        gt_boxes = cat([proposal.gt_boxes for proposal in proposals], dim=0)\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        image_sizes = [(proposal.image_size[0], proposal.image_size[1]) for proposal in proposals]\n\n        # Calculate the classification loss\n        loss_cls = cross_entropy(scores, gt_classes)\n        loss_cls = loss_cls * self.loss_weight.get(\"loss_cls\", 1.0)\n\n        # Calculate the box regression loss\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = _dense_box_regression_loss(\n                proposal_deltas,\n                gt_boxes,\n                weights=self.box2box_transform.weights,\n                box_dim=proposal_deltas.size(1) // self.num_classes,\n                beta=self.smooth_l1_beta,\n            )\n        elif self.box_reg_loss_type == \"giou\":\n            loss_box_reg = self.box2box_transform.loss_box_reg(proposal_deltas, gt_boxes)\n        elif self.box_reg_loss_type == \"diou\":\n            loss_box_reg = self.box2box_transform.loss_box_reg(proposal_deltas, gt_boxes, giou=False)\n        elif self.box_reg_loss_type == \"ciou\":\n            loss_box_reg = self.box2box_transform.loss_box_reg(proposal_deltas, gt_boxes, giou=False, diou=False)\n        else:\n            raise ValueError(f\"Unknown box_reg_loss_"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Extract the ground truth boxes and classes from the proposals\n        gt_boxes = cat([proposal.gt_boxes for proposal in proposals], dim=0)\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n\n        # Calculate the classification loss\n        loss_cls = cross_entropy(scores, gt_classes)\n\n        # Calculate the box regression loss\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = _dense_box_regression_loss(\n                self.box2box_transform,\n                proposal_deltas,\n                gt_boxes,\n                gt_classes,\n                self.smooth_l1_beta,\n            )\n        elif self.box_reg_loss_type == \"giou\":\n            loss_box_reg = F.l1_loss(\n                self.box2box_transform.decode(proposal_deltas),\n                gt_boxes.tensor,\n                reduction=\"none\",\n            ).sum(dim=-1)\n        elif self.box_reg_loss_type == \"diou\":\n            loss_box_reg = -self.box2box_transform.decode_diou(proposal_deltas, gt_boxes.tensor).diag()\n        elif self.box_reg_loss_type == \"ciou\":\n            loss_box_reg = -self.box2box_transform.decode_ciou(proposal_deltas, gt_boxes.tensor).diag()\n        else:\n            raise NotImplementedError(f\"{self.box_reg_loss_type} is not supported\")\n\n        # Apply weights to the losses\n        if self.loss_weight:\n            loss_cls = loss_cls * self.loss_weight.get(\"loss_cls\", 1.0)\n            loss_box_reg = loss_box_reg"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # This function calculates and returns the classification and box regression losses for object detection, based on the predictions from the model and the ground truth proposals.\n\n        # Input-Output Arguments\n        # self: FastRCNNOutputLayers. An instance of the FastRCNNOutputLayers class.\n        # predictions: Tuple containing the scores and proposal deltas returned by the `forward()` method. The scores are used for calculating classification loss, and the proposal deltas are used for calculating box regression loss.\n        # proposals: list[Instances]. A list of Instances objects that match the features used to compute predictions. These objects are expected to contain the fields `proposal_boxes`, `gt_boxes`, and `gt_classes`, which are used for calculating the losses.\n        # return: Dict[str, Tensor]. A dictionary of losses where the keys are strings indicating the type of loss ('loss_cls' for classification loss and 'loss_box_reg' for box regression loss) and the values are Tensors representing the calculated losses, scaled by their respective weights defined in `self.loss_weight`.\n\n        # Unpack the predictions\n        scores, proposal_deltas = predictions\n\n        # Compute the classification loss\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight.get(\"loss_cls\", 1.0))\n        _log_classification_stats(scores, gt_classes)\n\n        # Compute the box regression loss\n        gt_boxes = cat([proposal.gt_boxes for proposal in proposals], dim=0)\n        num_classes = self.num_classes if self.box2box_transform.weights is not None else 1\n        gt_boxes = self.box2box_transform.encode_with_extra_fields(gt_boxes, {\"num_classes\": num_classes})\n        loss_box_reg = _dense_box_regression_loss(\n            self.box_reg_loss_type,\n            proposal_deltas,"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Retrieve the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Calculate the classification loss\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight[\"loss_cls\"])\n        _log_classification_stats(scores, gt_classes)\n\n        # Calculate the box regression loss\n        gt_boxes = cat([proposal.gt_boxes for proposal in proposals], dim=0)\n        loss_box_reg = _dense_box_regression_loss(\n            proposal_deltas,\n            gt_boxes,\n            weights=self.loss_weight[\"loss_box_reg\"],\n            box_dim=self.box2box_transform.weights.shape[0],\n            box_reg_loss_type=self.box_reg_loss_type,\n            smooth_l1_beta=self.smooth_l1_beta,\n        )\n\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # Classification loss\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight.get(\"loss_cls\", 1.0))\n        _log_classification_stats(scores, gt_classes, prefix=\"fast_rcnn\")\n\n        # Box regression loss\n        gt_boxes = cat([proposal.gt_boxes for proposal in proposals], dim=0)\n        num_bbox_reg_classes = scores.shape[1] - 1\n        if num_bbox_reg_classes == 1:\n            proposal_boxes = cat([proposal.proposal_boxes for proposal in proposals], dim=0)\n            proposal_boxes = proposal_boxes[:, None]\n        else:\n            proposal_boxes = cat([proposal.proposal_boxes for proposal in proposals], dim=0)\n        gt_boxes = self.box2box_transform.encode(proposal_boxes, gt_boxes)\n        loss_box_reg = _dense_box_regression_loss(\n            proposal_deltas,\n            gt_boxes,\n            box_dim=self.box2box_transform.weights.shape[0],\n            scale=self.loss_weight.get(\"loss_box_reg\", 1.0),\n            loss_type=self.box_reg_loss_type,\n            smooth_l1_beta=self.smooth_l1_beta,\n        )\n\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In training, the proposals are the ground-truth, so the network is supervised\n        # to produce correct classification and box regression results.\n        # In inference, the proposals are produced by the model. They are not\n        # known during training, so we need to do it during test-time.\n        if self.training:\n            gt_boxes, gt_classes = cat(\n                [proposal.gt_boxes for proposal in proposals], dim=0\n            ), cat([proposal.gt_classes for proposal in proposals], dim=0)\n        else:\n            gt_boxes = cat([proposal.proposal_boxes for proposal in proposals], dim=0)\n            gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n\n        scores, proposal_deltas = predictions\n        num_classes = self.num_classes\n        num_boxes = len(proposal_deltas)\n\n        # classification loss\n        loss_cls = cross_entropy(scores, gt_classes)\n        _log_classification_stats(scores, gt_classes)\n\n        # box regression loss\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = _dense_box_regression_loss(\n                self.box2box_transform,\n                proposal_deltas,\n                gt_boxes,\n                gt_classes,\n                weights=self.loss_weight[\"loss_box_reg\"],\n                box_dim=num_classes * 4,\n                smooth_l1_beta=self.smooth_l1_beta,\n            )\n        elif self.box_reg_loss_type == \"giou\":\n            loss_box_reg = _dense_box_regression_loss(\n                self.box2box_transform,\n                proposal_deltas,\n                gt_boxes,\n                gt_classes,\n                weights=self.loss_weight[\"loss_box_reg\"],\n                box_dim=num_classes * 4,\n                g"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In training, the proposals are the ground-truth, and the network needs\n        # to bring the proposal box deltas to match the ground-truth.\n        # In inference, the proposals are the predicted boxes from the R-CNN,\n        # and only the network needs to bring the deltas to correct\n        # the box coordinates and return the final detection boxes.\n        if self.training:\n            num_classes = self.num_classes\n            gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n            regression_targets = self.box2box_transform.get_deltas(\n                proposals, cat([proposal.gt_boxes for proposal in proposals], dim=0)\n            )\n        else:\n            num_classes = self.num_classes + 1\n            gt_classes = None\n            regression_targets = None\n\n        scores, proposal_deltas = predictions\n        scores = scores.reshape(-1, num_classes)\n        proposal_deltas = proposal_deltas.reshape(-1, 4)\n\n        if self.training:\n            # Classification loss\n            labels = cat([proposal.gt_classes for proposal in proposals], dim=0)\n            labels = labels.to(dtype=torch.int64)\n            classification_loss = cross_entropy(scores, labels)\n            classification_loss = classification_loss * self.loss_weight.get(\"loss_cls\", 1.0)\n\n            # Box regression loss\n            if self.box_reg_loss_type == \"smooth_l1\":\n                # Filter out background boxes\n                fg_inds = torch.nonzero(labels != num_classes).squeeze(1)\n                regression_targets = regression_targets[fg_inds]\n                proposal_deltas = proposal_deltas[fg_inds]\n\n                box_reg_loss = F.smooth_l1_loss(\n                    proposal_deltas,\n                    regression_targets,\n                    beta=self.smooth_l1_beta,\n                    reduction="}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Compute the classification loss\n        scores, proposal_deltas = predictions\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        loss_cls = cross_entropy(\n            scores.view(-1, self.num_classes + 1),\n            gt_classes,\n            self.loss_weight[\"loss_cls\"],\n        )\n        _log_classification_stats(scores, gt_classes)\n\n        # Compute the box regression loss\n        gt_boxes = cat([proposal.gt_boxes for proposal in proposals], dim=0)\n        num_bbox_reg_classes = proposal_deltas.shape[1] // 4\n        if num_bbox_reg_classes == 1:\n            proposal_deltas = proposal_deltas.view(-1, 4)\n            gt_boxes = gt_boxes.view(-1, 4)\n        else:\n            proposal_deltas = proposal_deltas.view(-1, num_bbox_reg_classes, 4)\n            gt_boxes = gt_boxes.view(-1, num_bbox_reg_classes, 4)\n\n        loss_box_reg = _dense_box_regression_loss(\n            proposal_deltas,\n            gt_boxes,\n            self.box2box_transform,\n            self.loss_weight[\"loss_box_reg\"],\n            self.box_reg_loss_type,\n            self.smooth_l1_beta,\n        )\n\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [p for p in proposals if p.has_field(\"gt_classes\")]\n        if len(proposals) == 0:\n            return {}\n\n        gt_boxes = cat([p.gt_boxes for p in proposals], dim=0)\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n\n        # classification loss\n        loss_cls = self.loss_cls(scores, gt_classes)\n        # box regression loss\n        loss_box_reg = self.loss_box_reg(\n            [proposal_deltas],\n            [gt_boxes],\n            [gt_classes],\n        )\n        # total loss\n        loss = {\n            \"loss_cls\": loss_cls * self.loss_weight[\"loss_cls\"],\n            \"loss_box_reg\": loss_box_reg * self.loss_weight[\"loss_box_reg\"],\n        }\n        return loss\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        num_classes = self.num_classes\n        if num_classes == 1:\n            # A dummy variable to make the loss consistent with other models\n            gt_classes = torch.zeros_like(scores[:, :1])\n        else:\n            gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n\n        # classification loss\n        scores = scores.reshape(-1, num_classes + 1)\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight[\"loss_cls\"])\n        _log_classification_stats(scores, gt_classes)\n\n        # box regression loss\n        proposal_deltas = proposal_deltas.reshape(-1, 4)\n        gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0)\n        loss_box_reg = _dense_box_regression_loss(\n            proposal_deltas,\n            gt_boxes,\n            self.box2box_transform,\n            self.loss_weight[\"loss_box_reg\"],\n            self.box_reg_loss_type,\n            self.smooth_l1_beta,\n        )\n\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0)\n        # The following code is used to convert the gt_boxes to the format used by the Box2BoxTransform class.\n        if self.box2box_transform.weights is not None:\n            gt_boxes = gt_boxes / torch.tensor(self.box2box_transform.weights, device=gt_boxes.device)\n        if self.box2box_transform.angle_bound_weights is not None:\n            gt_boxes = gt_boxes / torch.tensor(\n                self.box2box_transform.angle_bound_weights, device=gt_boxes.device\n            )\n        num_bbox_reg_classes = scores.shape[1] // self.num_classes\n        if num_bbox_reg_classes > 1:\n            # split box regression predictions into two:\n            # one for boxes specific to the foreground class and one for background\n            gt_boxes_foreground = gt_boxes[:, self.num_classes :]\n            gt_boxes_background = gt_boxes[:, : self.num_classes]\n            proposal_deltas_foreground = proposal_deltas[:, self.num_classes :]\n            proposal_deltas_background = proposal_deltas[:, : self.num_classes]\n            scores_foreground = scores[:, self.num_classes :]\n            scores_background = scores[:, : self.num_classes]\n        else:\n            gt_boxes_foreground = gt_boxes\n            gt_boxes_background = gt_boxes\n            proposal_deltas_foreground = proposal_deltas\n            proposal_deltas_background = proposal_deltas\n            scores_foreground = scores\n            scores_background = scores\n        # classification loss\n        loss"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        num_proposals = len(proposals)\n        num_classes = scores.shape[1]\n\n        # In training, we perform classification and box regression loss calculation for each proposal.\n        # In testing, we perform classification and box regression loss calculation for each image.\n        # For each image, we keep the top `self.test_topk_per_image` proposals with the highest scores.\n        # We also filter out proposals with scores below `self.test_score_thresh`.\n\n        if self.training:\n            # Classification loss\n            gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n            classification_loss = F.cross_entropy(scores, gt_classes)\n\n            # Box regression loss\n            gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0)\n            box_regression_loss = _dense_box_regression_loss(\n                self.box2box_transform,\n                proposal_deltas,\n                gt_boxes,\n                weights=self.loss_weight[\"loss_box_reg\"],\n                box_reg_loss_type=self.box_reg_loss_type,\n                smooth_l1_beta=self.smooth_l1_beta,\n            )\n\n            losses = {\n                \"loss_cls\": classification_loss,\n                \"loss_box_reg\": box_regression_loss,\n            }\n        else:\n            # Classification loss\n            # During inference, we perform classification loss calculation for each image.\n            # We keep the top `self.test_topk_per_image` proposals with the highest scores.\n            # We also filter out proposals with scores below `self.test_score_thresh`.\n            # For each image, we keep the top `self.test_topk_per_image` proposals with the highest scores.\n            # We also filter out proposals with scores below `self.test_score_thresh`.\n            # For each image, we keep the top `self.test_"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        num_classes = scores.shape[1]\n        assert num_classes == self.num_classes + 1, (\n            \"Unexpected number of classes in scores {} != {}\".format(num_classes - 1, self.num_classes)\n        )\n\n        # Retrieve the device and dtype of the input tensors\n        device = scores.device\n        dtype = scores.dtype\n\n        # Convert the proposals to box regression targets\n        gt_boxes = cat([proposal.gt_boxes for proposal in proposals], dim=0)\n        gt_labels = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        proposal_boxes = cat([proposal.proposal_boxes for proposal in proposals], dim=0)\n\n        box_reg_targets = self.box2box_transform.get_deltas(proposal_boxes, gt_boxes)\n        box_reg_targets = box_reg_targets.to(dtype=dtype, device=device)\n\n        # Classification loss\n        loss_cls = cross_entropy(\n            scores.reshape(-1, self.num_classes),\n            gt_labels.reshape(-1),\n            self.loss_weight[\"loss_cls\"],\n        )\n\n        # Box regression loss\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = _dense_box_regression_loss(\n                self.box2box_transform,\n                proposal_deltas,\n                box_reg_targets,\n                gt_labels,\n                self.smooth_l1_beta,\n                self.loss_weight[\"loss_box_reg\"],\n            )\n        else:\n            raise NotImplementedError(\n                f\"box regression loss type {self.box_reg_loss_type} is not supported\"\n            )\n\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [x.detach() for x in proposals]\n        num_proposals = sum(len(p) for p in proposals)\n        num_classes = scores.shape[1] - 1\n\n        # In computing the losses, \"gt_classes\" field is used to indicate the class indices of the ground-truth\n        # For each box, the ground-truth class is the index in \"gt_classes\"\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n        if gt_boxes.numel() == 0:\n            return {}\n\n        # get indices of ground-truth objects\n        gt_inds = nonzero_tuple((gt_classes >= 0).any(dim=1))[0]\n        gt_classes = gt_classes[gt_inds]\n        gt_boxes = gt_boxes[gt_inds]\n\n        # classification/regression targets\n        labels = cat([p.gt_classes for p in proposals], dim=0)\n        regression_targets = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n\n        # Compute classification loss\n        scores = scores.detach()\n        labels = labels.detach()\n        classification_loss = cross_entropy(scores, labels)\n        classification_loss = classification_loss.sum() / max(num_proposals, 1.0)\n        self.loss_weight.setdefault(\"loss_cls\", 1.0)\n        losses = {\"loss_cls\": self.loss_weight[\"loss_cls\"] * classification_loss}\n\n        # Compute box regression loss\n        proposal_deltas = proposal_deltas.detach()\n        regression_targets = regression_targets.detach()\n        if self.box_reg_loss_type == \"smooth_l1\":\n            box_reg_loss = _dense_box_regression_loss("}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In training mode, the proposals are the ground-truth, so the network\n        # needs no proposal boxes.\n        if self.training:\n            num_proposals = len(proposals)\n            assert num_proposals == len(self.loss_weight)\n            gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n            gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0)\n            assert gt_classes.shape[0] == gt_boxes.shape[0]\n        else:\n            num_proposals = predictions[0].shape[0]\n            gt_classes = torch.zeros(num_proposals, dtype=torch.int64, device=proposals[0].device)\n            gt_boxes = torch.zeros((num_proposals, 4), dtype=torch.float32, device=proposals[0].device)\n\n        scores, proposal_deltas = predictions\n        # scores has shape (N, K + 1), where scores[:, :-1] are the class scores\n        # and scores[:, -1] is the background class score.\n        num_classes = scores.shape[1] - 1\n\n        # classification-only or box regression loss\n        if self.box_reg_loss_type == \"none\":\n            loss_box_reg = torch.tensor(0.0, device=scores.device)\n        else:\n            loss_box_reg = _dense_box_regression_loss(\n                self.box_reg_loss_type,\n                proposal_deltas,\n                gt_boxes,\n                weights=self.loss_weight[\"loss_box_reg\"],\n                box_dim=self.box2box_transform.weights.shape[0],\n                smooth_l1_beta=self.smooth_l1_beta,\n            )\n\n        # classification loss\n        # TODO: make this a sparse loss\n        # TODO: make this support empty classes, so we"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In training mode, the true boxes and the predicted boxes are known.\n        # In inference mode, we use proposal boxes, so we ignore the true\n        # boxes.\n        # rename variables for clarity\n        scores, proposal_deltas = predictions\n\n        num_proposals = len(proposals)\n        num_classes = scores.shape[1]\n\n        # The input boxes for box_i are the proposals for the image.\n        # The input proposals are expected to be in (x1, y1, x2, y2) format,\n        # where x1 < x2 and y1 < y2\n        proposal_boxes = [x.proposal_boxes for x in proposals]\n        proposal_boxes = cat(proposal_boxes, dim=0)\n\n        # The input classes for box_i are the class_ids for the ground-truth\n        # object for the box.\n        gt_classes = [x.gt_classes for x in proposals]\n        gt_classes = cat(gt_classes, dim=0)\n\n        # The input boxes for box_i are the ground-truth boxes for the box.\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_boxes = cat(gt_boxes, dim=0)\n\n        # The input is expected to be a list of `Instances`, and to contain\n        # the `gt_classes` and `gt_boxes` fields.\n        assert len(proposals) == gt_classes.shape[0] == gt_boxes.shape[0]\n\n        # get indices that sort the boxes\n        # The proposal_boxes are expected to be in (x1, y1, x2, y2) format,\n        # where x1 < x2 and y1 < y2\n        # The gt_boxes are expected to be in (x1, y1, x2, y2) format,\n        # where x1 < x2 and y1 < y2\n        # The proposal_boxes are sorted by score\n        # The gt_boxes are sorted by area"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In training, the proposals are the ground-truth, so the network can be\n        # trained to make predictions based on these proposals.\n        # In inference, when `proposals` is not given, the network computes\n        # proposals from the features to avoid making additional network\n        # predictions for these proposals.\n        if self.training:\n            assert proposals is not None\n\n        scores, proposal_deltas = predictions\n        num_proposals = len(proposals)\n        num_classes = scores.shape[1]\n\n        # The input boxes for box_reg_loss are normalized by the image size.\n        # During inference, proposal_boxes should be the unnormalized proposal\n        # boxes and proposal_boxes should be None if proposals are not given\n        # (for example, during visualization they are not).\n        if proposals is not None:\n            proposal_boxes = [x.proposal_boxes for x in proposals]\n        else:\n            assert self.training\n            proposal_boxes = None\n\n        # In inference mode, when `proposals` is None, let's decode proposals\n        # one by one.\n        if proposals is None:\n            # In detection, when there are multiple instances per image,\n            # we generate training targets of shape (N, L) for all images,\n            # where L == #detection_per_im.\n            # In detection, L == 1, in semantic segmentation, L == area.\n            num_top_level = len(self.box2box_transform.weights)\n            num_bbox_reg_classes = 1 if self.cls_agnostic_bbox_reg else num_classes\n            proposal_boxes = []\n            for level in range(num_top_level):\n                # pick the corresponding box2box transform\n                box2box_transform = self.box2box_transform.weights[level]\n                box2box_transform = Box2BoxTransform(weights=box2box_transform)\n                assert len(box2box_transform.weights) == 4\n                # We generate a set of ground-truth boxes from the gt_classes and gt_"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In training, the proposals are the ground-truth boxes.\n        # In inference, the proposals are predicted boxes.\n        # In training, the target boxes are known and should be used as input to the loss computation.\n        # In inference, the target boxes are unknown and should be ignored.\n        # In training, we also return the matched gt boxes so that the\n        # box_selector can select the training targets appropriately.\n        # In inference, we return None.\n        if self.training:\n            gt_boxes, gt_classes = cat(\n                [proposal.gt_boxes for proposal in proposals], dim=0\n            ), cat([proposal.gt_classes for proposal in proposals], dim=0)\n        else:\n            gt_boxes = None\n            gt_classes = None\n\n        # Get predicted boxes from rpn-based proposals\n        # Note that the second return value, `_`, is ignored.\n        # It is a placeholder that we should get from the call to `box_selector`.\n        # We should not use the second return value.\n        # TODO: Modify the box_selector so that it returns the matched gt boxes.\n        # Then we can remove the placeholder `_`.\n        _, proposal_deltas = predictions\n        boxes_per_image = [boxes_in_image.tensor for boxes_in_image in proposals]\n        box_regression_targets = self.box2box_transform.get_deltas(\n            boxes_per_image, gt_boxes.tensor if gt_boxes else None\n        )\n\n        # Classification loss\n        scores, proposal_deltas = predictions\n        loss_cls = cross_entropy(\n            scores.permute(0, 2, 1),\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n\n        # Box regression loss\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = _dense_box_regression_loss(\n                proposal_deltas,"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In training, the proposals are the ground-truth boxes.\n        # In inference, the proposals are the predicted boxes from the RPN.\n        # In RPN training, the proposals are anchors from the feature maps.\n        # In RPN inference, the boxes are the outputs of the RPN subnet.\n        #\n        # The following code computes the classification and box regression losses\n        # for each proposal, and then aggregates them into a single loss value.\n        #\n        # Note that the code currently only supports a single image per batch.\n        #\n        # The code is based on the implementation in\n        # https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/fast_rcnn.py\n        #\n        # TODO: Add support for multiple images per batch.\n\n        # The input to the model is a list of images, and each image is represented\n        # by a list of Instances. Each Instance contains the following fields:\n        # - proposal_boxes: Tensor of shape (num_proposals, 4) containing the coordinates of the proposal boxes.\n        # - gt_boxes: Tensor of shape (num_gt_boxes, 4) containing the coordinates of the ground-truth boxes.\n        # - gt_classes: Tensor of shape (num_gt_boxes,) containing the class labels for each ground-truth box.\n\n        # Extract the scores and proposal deltas from the predictions.\n        scores, proposal_deltas = predictions\n\n        # Extract the ground-truth boxes and class labels from the proposals.\n        gt_boxes = [proposal.gt_boxes for proposal in proposals]\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n\n        # Compute the classification loss.\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight[\"loss_cls\"])\n\n        # Compute the box regression loss.\n        num_classes = scores.shape[1]\n        if num_classes == 1:\n            # If there is only one"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In training, the proposals are the ground-truth, so we compute a loss for those\n        # proposals.\n        # During inference, self.training_mode is False. In this case, the proposals are\n        # the predicted box regression results and we use them to obtain a set of boxes to\n        # be used to take a prediction.\n        with torch.no_grad():\n            if self.training:\n                # Gather the input features corresponding to the given proposals\n                # Note: it's important to call `.detach()` here, otherwise the hooks\n                # in `Box2BoxTransform` will make the loss different even during inference.\n                x = cat([proposal.pred_boxes.tensor for proposal in proposals], dim=0).detach()\n                # Note: we use detach() here to avoid the Hooks in Box2BoxTransform\n                # from making the loss different during training and inference.\n                gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0).detach()\n                gt_classes = cat(\n                    [proposal.gt_classes for proposal in proposals], dim=0\n                ).detach()\n            else:\n                # For training, we want to find the predicted boxes.\n                # This is the foreground probability for the top class for each proposal.\n                scores = predictions[0]\n                # Get the foreground scores for each class\n                fg_scores = scores[:, :-1]\n                # Get the class with the highest foreground score for each proposal\n                fg_inds = fg_scores.argmax(dim=1, keepdim=True)\n                # Get the class-specific score for each proposal\n                fg_scores = fg_scores.gather(1, fg_inds).squeeze(1)\n                # Convert foreground scores to predicted boxes\n                if self.box2box_transform.weights.shape[0] == 4:\n                    # If we use standard (dx, dy, dw, dh) predictions,\n                    # perform decoding directly\n                    proposals = self.box2box_transform.decode("}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(name)\n    return tracker_head(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)\n    tracker_head_cfg = cfg.clone()\n    tracker_head_cfg.freeze()\n    return tracker_head(tracker_head_cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head\n\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head\n\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        src_widths = boxes[:, 2] - boxes[:, 0]\n        src_heights = boxes[:, 3] - boxes[:, 1]\n        src_ctr_x = boxes[:, 0] + 0.5 * src_widths\n        src_ctr_y = boxes[:, 1] + 0.5 * src_heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * src_widths[:, None] + src_ctr_x[:, None]\n        pred_ctr_y = dy * src_heights[:, None] + src_ctr_y[:, None]\n        pred_w = torch.exp(dw) * src_widths[:, None]\n        pred_h = torch.exp(dh) * src_heights[:, None]\n\n        dst_boxes = torch.zeros_like(deltas)\n        dst_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        dst_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        dst_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        dst_boxes[:, 3::4] = pred_ctr_"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        src_widths = boxes[:, 2] - boxes[:, 0]\n        src_heights = boxes[:, 3] - boxes[:, 1]\n        src_ctr_x = boxes[:, 0] + 0.5 * src_widths\n        src_ctr_y = boxes[:, 1] + 0.5 * src_heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * src_widths[:, None] + src_ctr_x[:, None]\n        pred_ctr_y = dy * src_heights[:, None] + src_ctr_y[:, None]\n        pred_w = torch.exp(dw) * src_widths[:, None]\n        pred_h = torch.exp(dh) * src_heights[:, None]\n\n        dst_boxes = torch.zeros_like(deltas)\n        dst_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        dst_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        dst_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        dst_boxes[:, 3::4] = pred_ctr_"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        src_widths = boxes[:, 2] - boxes[:, 0]\n        src_heights = boxes[:, 3] - boxes[:, 1]\n        src_ctr_x = boxes[:, 0] + 0.5 * src_widths\n        src_ctr_y = boxes[:, 1] + 0.5 * src_heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * src_widths[:, None] + src_ctr_x[:, None]\n        pred_ctr_y = dy * src_heights[:, None] + src_ctr_y[:, None]\n        pred_w = torch.exp(dw) * src_widths[:, None]\n        pred_h = torch.exp(dh) * src_heights[:, None]\n\n        dst_boxes = torch.zeros_like(deltas)\n        dst_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        dst_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        dst_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        dst_boxes[:, 3::4] = pred_ctr_"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        src_widths = boxes[:, 2] - boxes[:, 0]\n        src_heights = boxes[:, 3] - boxes[:, 1]\n        src_ctr_x = boxes[:, 0] + 0.5 * src_widths\n        src_ctr_y = boxes[:, 1] + 0.5 * src_heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * src_widths[:, None] + src_ctr_x[:, None]\n        pred_ctr_y = dy * src_heights[:, None] + src_ctr_y[:, None]\n        pred_w = torch.exp(dw) * src_widths[:, None]\n        pred_h = torch.exp(dh) * src_heights[:, None]\n\n        dst_boxes = torch.zeros_like(deltas)\n        dst_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        dst_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        dst_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        dst_boxes[:, 3::4] = pred_ctr_"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        # Apply deltas to each corner of each box\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # x1, y1, x2, y2\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + "}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.size(1) == boxes.size(1) * 4\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        # Apply scale and offset to box locations and sizes\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Convert center-xy/width/height to top-left, bottom-right\n        pred_boxes1 = pred_ctr_x - 0.5 * pred_w\n        pred_boxes3 = pred_ctr_x + 0.5 * pred_w\n        pred_boxes2 = pred_ctr_y - 0.5 * pred_h\n        pred_boxes4 = pred_ctr_y + 0.5 * pred_h\n\n        # Stack predictions into a single tensor\n        pred_boxes = torch.stack((pred_boxes1, pred_boxes2"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        # Convert to (x, y, w, h)\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Convert back to (x1, y1, x2, y2)\n        x1 = pred_ctr_x - 0.5 * pred_w\n        y1 = pred_ctr_y - 0.5 * pred_h\n        x2 = pred_ctr_x + 0.5 * pred_w\n        y2 = pred_ctr_y + 0.5 * pred_h\n\n        pred_boxes = torch.stack((x1, y1, x2, y2), dim=2).flatten(1)\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        # Apply deltas to each corner of each box\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Use the center of each box to derive the top-left and bottom-right corners\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] ="}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        # Convert to (x, y, w, h)\n        src_widths = boxes[:, 2] - boxes[:, 0]\n        src_heights = boxes[:, 3] - boxes[:, 1]\n        src_ctr_x = boxes[:, 0] + 0.5 * src_widths\n        src_ctr_y = boxes[:, 1] + 0.5 * src_heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * src_widths[:, None] + src_ctr_x[:, None]\n        pred_ctr_y = dy * src_heights[:, None] + src_ctr_y[:, None]\n        pred_w = torch.exp(dw) * src_widths[:, None]\n        pred_h = torch.exp(dh) * src_heights[:, None]\n\n        dst_boxes = torch.zeros_like(deltas)\n        # Convert to (x1, y1, x2, y2)\n        dst_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        dst_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        dst_boxes[:, 2::4] = pred"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        # Apply deltas to each corner of each box\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # For numerical stability\n        pred_w = torch.clamp(pred_w, min=1e-5)\n        pred_h = torch.clamp(pred_h, min=1e-5)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, "}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # For numerical stability\n        pred_ctr_x = torch.clamp(pred_ctr_x, min=1e-4, max=widths[:, None] - 1e-4)\n        pred_ctr_y = torch.clamp(pred_ctr_y, min=1e-4, max=heights[:, None] - 1e-4)\n        pred_w = torch.clamp(pred_w, min=1e-4, max=widths[:, None] - 1e-4)\n        pred_h = torch.clamp(pred_h, min=1e-4"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Perform translation of points to their respective boxes\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # x1, y1, x2, y2\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        # Apply deltas to each corner coordinate\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Clamp widths and heights to a maximum value\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        # Compute the new corners of the boxes\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Apply deltas to the corners\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Compute the new corners of the boxes\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        # Perform translation of points (x_center, y_center)\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Perform scaling of boxes' width and height\n        max_scale = math.log(self.scale_clamp / self.weights[2] + 1)\n        dw = torch.clamp(dw, max=max_scale)\n        dh = torch.clamp(dh, max=max_scale)\n\n        # Compute width, height, and center of each box\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Compute new center and size\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Compute top-left and bottom-right corners of the predicted boxes\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Perform deltas transformation to boxes\n        max_scale = math.log(1000.0 / 16)\n        scale_clamp = self.scale_clamp\n        if scale_clamp is not None:\n            scale_clamp = math.log(scale_clamp)\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n        dw = torch.clamp(dw, max=scale_clamp)\n        dh = torch.clamp(dh, max=scale_clamp)\n\n        pred_ctr_x = dx * boxes[:, 2] + boxes[:, 0] + 0.5 * boxes[:, 2]\n        pred_ctr_y = dy * boxes[:, 3] + boxes[:, 1] + 0.5 * boxes[:, 3]\n        pred_w = torch.exp(dw) * boxes[:, 2]\n        pred_h = torch.exp(dh) * boxes[:, 3]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes\n\n"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        # Apply transformation to each corner of each box\n        # x1, y1, x2, y2 = boxes.unbind(1)\n        # dx1, dy1, dx2, dy2 = deltas.unbind(1)\n        #\n        # # Apply deltas\n        # px = ((x1 + x2) / 2 + dx1).unsqueeze(1)\n        # py = ((y1 + y2) / 2 + dy1).unsqueeze(1)\n        # pw = (x2 - x1 + dx2).unsqueeze(1)\n        # ph = (y2 - y1 + dy2).unsqueeze(1)\n        #\n        # # Apply scaling and restoring\n        # pw = pw.clamp_min_(1.0)\n        # ph = ph.clamp_min_(1.0)\n        # px1 = px - pw / 2\n        # py1 = py - ph / 2\n        # px2 = px + pw / 2\n        # py2 = py + ph / 2\n        #\n        # # Concat back together into box\n        # # no need to clamp because it was done in the split above\n        # # y2 >= y1 and x2 >= x1\n        # # x1 >= 0 and y1 >= 0\n        # pred_boxes = torch.cat((px1, py1, px2, py2), dim=1)\n        #\n        # return pred_boxes\n\n        # Apply transformation to each corner of each box\n        x1, y1, x2, y2 = boxes.unbind(1)\n        dx1, dy1, dx2, dy2 = deltas.unbind(1)\n\n        # Apply deltas\n        px1 = dx1 + x1\n        py1 = dy1 + y1\n        px2 ="}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Apply deltas to each center-size pair\n        # dx = deltas[:, 0::4]\n        # dy = deltas[:, 1::4]\n        # dw = deltas[:, 2::4]\n        # dh = deltas[:, 3::4]\n        #\n        # # Compute width, height, and center\n        # # TODO: The next two lines seem redundant, since the width and height are computed from the boxes\n        # # widths = boxes[:, 2] - boxes[:, 0]\n        # # heights = boxes[:, 3] - boxes[:, 1]\n        # # ctr_x = boxes[:, 0] + 0.5 * widths\n        # # ctr_y = boxes[:, 1] + 0.5 * heights\n        #\n        # # Apply deltas\n        # # TODO: This is the same as the code in the original implementation, but it seems redundant.\n        # # TODO: The next two lines seem redundant, since the width and height are computed from the boxes\n        # # widths = boxes[:, 2] - boxes[:, 0]\n        # # heights = boxes[:, 3] - boxes[:, 1]\n        # # ctr_x = boxes[:, 0] + 0.5 * widths\n        # # ctr_y = boxes[:, 1] + 0.5 * heights\n        # dx = deltas[:, 0::4]\n        # dy = deltas[:, 1::4]\n        # dw = deltas[:, 2::4]\n        # dh = deltas[:, 3::4]\n        #\n        # # Compute width, height, and center\n        # widths = boxes[:, 2] - boxes[:, 0]\n        # heights = boxes[:, 3] - boxes[:, 1]\n        # ctr_x = boxes[:, 0] + 0.5 * widths\n        # ctr_y = boxes[:, 1] + 0.5 * heights\n        #\n        # # Apply deltas"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                pass\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n\n            out = self.general_ins(image)\n            if len(anno_type) == 1:\n                return out[anno_type[0]]\n            else:\n                return {tp: out[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        outputs = self.general_ins(image)\n        if anno_type is None:\n            return outputs\n        else:\n            anno_dict = {}\n            for anno_tp in anno_type:\n                if anno_tp in outputs:\n                    anno_dict[anno_tp] = outputs[anno_tp]\n            return anno_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is not None and isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif anno_type is not None and isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        if anno_type is not None:\n            if len(anno_type) == 1:\n                return output[anno_type[0]]\n            else:\n                return {tp: output[tp] for tp in anno_type}\n        else:\n            return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        out = self.general_ins(image)\n\n        if anno_type is None:\n            return out\n        else:\n            anno_dict = {}\n            for tp in anno_type:\n                if tp in out:\n                    anno_dict[tp] = out[tp]\n            return anno_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        if anno_type is None:\n            return self.general_ins(image)\n        else:\n            res = self.general_ins(image)\n            if len(anno_type) == 1:\n                return res[anno_type[0]]\n            else:\n                return {tp: res[tp] for tp in anno_type if tp in res.keys()}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n            output = self.general_ins(image)\n            return {tp: output[tp] for tp in anno_type}\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n\n        result = self.general_ins(image)\n        if anno_type is None:\n            return result\n\n        if isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n            result_dict = {}\n            for tp in anno_type:\n                if tp in result:\n                    result_dict[tp] = result[tp]\n            return result_dict\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins({'img': image})\n        if anno_type is None:\n            return output\n\n        anno_output = {}\n        for k in anno_type:\n            if k in output.keys():\n                anno_output[k] = output[k]\n        return anno_output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            output = self.general_ins(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            output = self.general_ins(image)\n            if isinstance(anno_type, (list, tuple)):\n                output = {\n                    k: v for k, v in output.items() if k in anno_type\n                }\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n        return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        if anno_type is None:\n            anno_type = self.anno_type_map.keys()\n\n        data_dict = {'img': image}\n        output_dict = self.general_ins(data_dict)\n\n        output_dict = {\n            k: v\n            for k, v in output_dict.items()\n            if k in self.anno_type_map.keys()\n        }\n\n        if len(anno_type) == 1:\n            return output_dict[anno_type[0]]\n        else:\n            return {k: v for k, v in output_dict.items() if k in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        out = self.general_ins(image)\n\n        if anno_type is None:\n            return out\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                pass\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n            anno_dict = {}\n            for tp in anno_type:\n                if tp in out.keys():\n                    anno_dict[tp] = out[tp]\n            return anno_dict\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        anno_type = [tp for tp in anno_type if tp in self.anno_type_map.keys()]\n        if len(anno_type) == 0:\n            return self.general_ins(image)\n        else:\n            anno_type = list(set(anno_type))\n\n        anno_output = self.general_ins(image)\n        anno_dict = {}\n        for tp in anno_type:\n            if tp in anno_output.keys():\n                anno_dict[tp] = anno_output[tp]\n\n        if len(anno_dict) == 1:\n            return anno_dict[anno_type[0]]\n        else:\n            return anno_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        # general_ins = self.general_ins\n        general_ins = self.general_ins\n        image = image.copy()\n        general_ins.set_image(image)\n        general_ins.run()\n        output = general_ins.get_output()\n\n        if anno_type is None:\n            return output\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            ret_dict = {}\n            for tp in anno_type:\n                if tp in output.keys():\n                    ret_dict[tp] = output[tp]\n            if len(ret_dict) == 0:\n                return None\n            elif len(ret_dict) == 1:\n                return list(ret_dict.values())[0]\n            else:\n                return ret_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n\n        res = self.general_ins(image)\n        if anno_type is None:\n            return res\n\n        if isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n            return {tp: res[tp] for tp in anno_type}\n\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        image_ori = image.copy()\n        image, k = resize_image(image, 512)\n        image = image.astype(np.float32) / 255.0\n        image = image.transpose(2, 0, 1)[None, :, :, :]\n        image = torch.from_numpy(image).to(we.device_id)\n        outputs = self.general_ins(image)\n\n        if anno_type is None:\n            return outputs\n        else:\n            anno_dict = {}\n            for k in anno_type:\n                if k in outputs.keys():\n                    anno_dict[k] = outputs[k]\n            if len(anno_dict) == 0:\n                return outputs\n            elif len(anno_dict) == 1:\n                return list(anno_dict.values())[0]\n            else:\n                return anno_dict\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        input_dict = {'img': image}\n        output_dict = self.general_ins(input_dict)\n        if anno_type is None:\n            return output_dict\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            assert all(tp in output_dict.keys() for tp in anno_type)\n            return {tp: output_dict[tp] for tp in anno_type}\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        res_dict = self.general_ins(image)\n        if anno_type is None:\n            return res_dict\n        else:\n            res_dict = {k: v for k, v in res_dict.items() if k in anno_type}\n            if len(res_dict) == 1:\n                return list(res_dict.values())[0]\n            else:\n                return res_dict\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins({'img': image})\n        if anno_type is None:\n            return output\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            anno_dict = {}\n            for tp in anno_type:\n                if tp in output.keys():\n                    anno_dict[tp] = output[tp]\n            return anno_dict\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        result = self.general_ins(image)\n        if anno_type is None:\n            return result\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            result_dict = {}\n            for key in anno_type:\n                if key in result.keys():\n                    result_dict[key] = result[key]\n            if len(result_dict) == 0:\n                return None\n            elif len(result_dict) == 1:\n                return result_dict[anno_type[0]]\n            else:\n                return result_dict\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        # 1. resize image\n        H, W, C = image.shape\n        image, k = resize_image(image, 512)\n\n        # 2. run general annotator\n        output = self.general_ins({'img': image})\n\n        # 3. filter output\n        if anno_type is None:\n            return output\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n\n            ret_dict = {}\n            for tp in anno_type:\n                if tp in output:\n                    ret_dict[tp] = output[tp]\n\n            if len(ret_dict) == 1:\n                return ret_dict[anno_type[0]]\n            else:\n                return ret_dict\n\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        normalized_query = normalize_string(query)\n        keywords = normalized_query.split()\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        result = {}\n        for kw in keywords:\n            result = update_url_scores(result, self.bm25(kw))\n        return result\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        scores = defaultdict(float)\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query_normalized = normalize_string(query)\n        query_keywords = query_normalized.split()\n        result = {}\n        for kw in query_keywords:\n            result = update_url_scores(result, self.bm25(kw))\n        return result\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        normalized_query = normalize_string(query)\n        keywords = normalized_query.split()\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        result = {}\n        for kw in keywords:\n            result = update_url_scores(result, self.bm25(kw))\n        return result\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        normalized_query = normalize_string(query)\n        query_keywords = normalized_query.split()\n        scores = {}\n        for kw in query_keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        result = {}\n        for kw in keywords:\n            result = update_url_scores(result, self.bm25(kw))\n        return result\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        normalized_query = normalize_string(query)\n        keywords = normalized_query.split()\n        scores = defaultdict(float)\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        scores = defaultdict(int)\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        normalized_query = normalize_string(query)\n        keywords = normalized_query.split()\n        url_scores = {}\n        for kw in keywords:\n            url_scores = update_url_scores(url_scores, self.bm25(kw))\n        return url_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        normalized_query = normalize_string(query)\n        query_keywords = normalized_query.split()\n        scores = {}\n        for kw in query_keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query_normalized = normalize_string(query)\n        query_keywords = query_normalized.split()\n        scores = {}\n        for keyword in query_keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        url_scores = defaultdict(float)\n        for kw in keywords:\n            url_scores = update_url_scores(url_scores, self.bm25(kw))\n        return dict(url_scores)\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        normalized_query = normalize_string(query)\n        keywords = normalized_query.split()\n        scores = {}\n        for kw in keywords:\n            bm25_score = self.bm25(kw)\n            scores = update_url_scores(scores, bm25_score)\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in normalize_string(query).split():\n            result = update_url_scores(result, self.bm25(kw))\n        return result\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n\n        # Identify indices of nearly horizontal boxes\n        nearly_horizontal_indices = torch.abs(box[:, 4]) < clip_angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1 = box[:, 0] - box[:, 2] / 2\n        y1 = box[:, 1] - box[:, 3] / 2\n        x2 = box[:, 0] + box[:, 2] / 2\n        y2 = box[:, 1] + box[:, 3] / 2\n\n        # Clamp x and y coordinates\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 + y2) / 2\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 + y2) / 2\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 +"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify indices of boxes that are nearly horizontal\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        angle_mask = torch.abs(self.tensor[:, 4]) < angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2.0\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2.0\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2.0\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2.0\n\n        # Clip x and y coordinates\n        x1 = torch.clamp(x1, min=0.0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0.0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0.0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0.0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0] = (x1 + x2) / 2.0\n        self.tensor[:, 1] = (y1 + y2) / 2.0\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0] = (x1 + x2) / 2.0\n        self.tensor[:, 1] = (y1 + y2) / 2.0\n        self.tensor[:, 2] = x2"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify indices of nearly horizontal boxes\n        nearly_horizontal_indices = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n\n        # Clamp x and y coordinates\n        x1 = torch.clamp(x1, min=0, max=box_size[1] - 1)\n        y1 = torch.clamp(y1, min=0, max=box_size[0] - 1)\n        x2 = torch.clamp(x2, min=0, max=box_size[1] - 1)\n        y2 = torch.clamp(y2, min=0, max=box_size[0] - 1)\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0] = (x1 + x2) / 2\n        self.tensor[:, 1] = (y1 + y2) / 2\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Convert angles to (-180, 180] degrees\n        self.tensor[:, 4] = (self.tensor[:, 4] + 180.0) % 360.0 - 180.0\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0] = (x1 + x2) / 2\n        self.tensor"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        angle_mask = torch.abs(self.tensor[:, 4]) < angle_threshold\n        angle_mask = angle_mask.nonzero().squeeze(1)\n\n        # Convert to x1y1x2y2 representation\n        x1y1x2y2 = self.tensor.clone()\n        x1y1x2y2[:, 0] -= x1y1x2y2[:, 2] / 2\n        x1y1x2y2[:, 1] -= x1y1x2y2[:, 3] / 2\n        x1y1x2y2[:, 2] += x1y1x2y2[:, 0]\n        x1y1x2y2[:, 3] += x1y1x2y2[:, 1]\n\n        # Clip x1, y1, x2, y2\n        x1y1x2y2[:, 0].clamp_(min=0, max=box_size[1])\n        x1y1x2y2[:, 1].clamp_(min=0, max=box_size[0])\n        x1y1x2y2[:, 2].clamp_(min=0, max=box_size[1])\n        x1y1x2y2[:, 3].clamp_(min=0, max=box_size[0])\n\n        # Convert back to xywha representation\n        self.tensor[:, 0] = (x1y1x2y2[:, 0] + x1y1x2y2[:, 2]) / 2\n        self.tensor[:, 1] = (x1y1x2y2[:, 1] + x1y1x2y2[:, 3]) / 2\n        self.tensor[:, 2] = x1y1x2y2[:, 2] - x1y1x2y2[:,"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify indices of boxes that are nearly horizontal\n        near_horizontal_indices = torch.abs(self.tensor[:, 4]) <= clip_angle_threshold\n\n        # Convert boxes to (x1, y1, x2, y2) representation\n        boxes_xyxy = self.tensor.clone()\n        boxes_xyxy[:, 0] -= boxes_xyxy[:, 2] / 2  # x1\n        boxes_xyxy[:, 1] -= boxes_xyxy[:, 3] / 2  # y1\n        boxes_xyxy[:, 2] += boxes_xyxy[:, 0]  # x2\n        boxes_xyxy[:, 3] += boxes_xyxy[:, 1]  # y2\n\n        # Clip x and y coordinates to ensure they fit within the specified box_size\n        boxes_xyxy[:, 0] = torch.clamp(boxes_xyxy[:, 0], min=0, max=box_size[1])\n        boxes_xyxy[:, 1] = torch.clamp(boxes_xyxy[:, 1], min=0, max=box_size[0])\n        boxes_xyxy[:, 2] = torch.clamp(boxes_xyxy[:, 2], min=0, max=box_size[1])\n        boxes_xyxy[:, 3] = torch.clamp(boxes_xyxy[:, 3], min=0, max=box_size[0])\n\n        # Convert boxes back to (center x, center y, width, height, angle) representation\n        boxes_xyxy[:, 0] = (boxes_xyxy[:, 0] + boxes_xyxy[:, 2]) / 2  # center x\n        boxes_xyxy[:, 1] = (boxes_xyxy[:, 1] + boxes_xyxy[:, 3]) / 2  # center y\n        boxes_xyxy[:, 2] = boxes_xyxy[:, 2] - boxes_"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify indices of nearly horizontal boxes\n        angle_threshold = math.radians(clip_angle_threshold)\n        near_horizontal_indices = torch.abs(self.tensor[:, 4]) < angle_threshold\n\n        # Convert boxes to (x1, y1, x2, y2) representation\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2.0\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2.0\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2.0\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2.0\n\n        # Clamp x and y coordinates\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert boxes back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0] = (x1 + x2) / 2.0\n        self.tensor[:, 1] = (y1 + y2) / 2.0\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Convert boxes to (center x, center y, width, height, angle) representation\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2.0\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2.0\n        x2 = self.tensor[:, 0] + self"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        indices = torch.abs(self.tensor[:, 4]) < angle_threshold\n        box = self.tensor[indices]\n        x1 = box[:, 0] - box[:, 2] / 2\n        y1 = box[:, 1] - box[:, 3] / 2\n        x2 = box[:, 0] + box[:, 2] / 2\n        y2 = box[:, 1] + box[:, 3] / 2\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 + y2) / 2\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n        self.tensor[indices] = box\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify indices of boxes that are nearly horizontal\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        angle_mask = torch.abs(self.tensor[:, 4]) < angle_threshold\n        angle_mask = angle_mask.nonzero().squeeze(1)\n\n        # Convert to (x1, y1, x2, y2) representation\n        box = self.tensor\n        x1 = box[:, 0] - box[:, 2] / 2\n        y1 = box[:, 1] - box[:, 3] / 2\n        x2 = box[:, 0] + box[:, 2] / 2\n        y2 = box[:, 1] + box[:, 3] / 2\n        box = torch.stack([x1, y1, x2, y2], dim=1)\n\n        # Clamp x and y coordinates\n        box[:, 0::2] = box[:, 0::2].clamp(min=0, max=box_size[1])\n        box[:, 1::2] = box[:, 1::2].clamp(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        x1 = box[:, 0]\n        y1 = box[:, 1]\n        x2 = box[:, 2]\n        y2 = box[:, 3]\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 + y2) / 2\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n\n        # Update tensor attribute\n        self.tensor = box\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        angle_mask = torch.abs(self.tensor[:, 4]) < angle_threshold\n        box = self.tensor[angle_mask]\n        x1 = box[:, 0] - box[:, 2] / 2.0\n        y1 = box[:, 1] - box[:, 3] / 2.0\n        x2 = box[:, 0] + box[:, 2] / 2.0\n        y2 = box[:, 1] + box[:, 3] / 2.0\n        x1 = torch.clamp(x1, 0, box_size[1])\n        y1 = torch.clamp(y1, 0, box_size[0])\n        x2 = torch.clamp(x2, 0, box_size[1])\n        y2 = torch.clamp(y2, 0, box_size[0])\n        box[:, 0] = (x1 + x2) / 2.0\n        box[:, 1] = (y1 + y2) / 2.0\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n        self.tensor[angle_mask] = box\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify indices of nearly horizontal boxes\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        is_near_horizontal = torch.abs(self.tensor[:, 4]) <= angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        box = self.tensor.clone()\n        box[:, 0] -= box[:, 2] / 2.0\n        box[:, 1] -= box[:, 3] / 2.0\n        box[:, 2] += box[:, 0]\n        box[:, 3] += box[:, 1]\n\n        # Clamp x and y coordinates\n        box[:, 0].clamp_(min=0, max=box_size[1])\n        box[:, 1].clamp_(min=0, max=box_size[0])\n        box[:, 2].clamp_(min=0, max=box_size[1])\n        box[:, 3].clamp_(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        box[:, 0] = (box[:, 0] + box[:, 2]) / 2.0\n        box[:, 1] = (box[:, 1] + box[:, 3]) / 2.0\n        box[:, 2] = box[:, 2] - box[:, 0]\n        box[:, 3] = box[:, 3] - box[:, 1]\n\n        # Update tensor attribute\n        self.tensor[is_near_horizontal] = box[is_near_horizontal]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        angle_mask = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n        angle_mask = angle_mask.nonzero().squeeze(1)\n        if angle_mask.numel() > 0:\n            box = self.tensor[angle_mask]\n            x1 = box[:, 0] - box[:, 2] / 2\n            y1 = box[:, 1] - box[:, 3] / 2\n            x2 = box[:, 0] + box[:, 2] / 2\n            y2 = box[:, 1] + box[:, 3] / 2\n\n            x1 = torch.clamp(x1, min=0, max=box_size[1])\n            y1 = torch.clamp(y1, min=0, max=box_size[0])\n            x2 = torch.clamp(x2, min=0, max=box_size[1])\n            y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n            width = x2 - x1\n            height = y2 - y1\n\n            x1 = box[:, 0] - width / 2\n            y1 = box[:, 1] - height / 2\n            x2 = box[:, 0] + width / 2\n            y2 = box[:, 1] + height / 2\n\n            self.tensor[angle_mask, 0] = (x1 + x2) / 2\n            self.tensor[angle_mask, 1] = (y1 + y2) / 2\n            self.tensor[angle_mask, 2] = width\n            self.tensor[angle_mask, 3] = height\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within the range (-180, 180]\n        self.normalize_angles()\n\n        # Identify indices of boxes that are nearly horizontal\n        nearly_horizontal_indices = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n\n        # Convert boxes to (x1, y1, x2, y2) representation\n        boxes = self.tensor.clone()\n        boxes[:, 0] -= boxes[:, 2] / 2.0\n        boxes[:, 1] -= boxes[:, 3] / 2.0\n        boxes[:, 2] += boxes[:, 0]\n        boxes[:, 3] += boxes[:, 1]\n\n        # Clamp x and y coordinates to fit within the box_size\n        boxes[:, 0].clamp_(min=0, max=box_size[1])\n        boxes[:, 1].clamp_(min=0, max=box_size[0])\n        boxes[:, 2].clamp_(min=0, max=box_size[1])\n        boxes[:, 3].clamp_(min=0, max=box_size[0])\n\n        # Convert boxes back to (center x, center y, width, height, angle) representation\n        boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0\n        boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n\n        # Update tensor attribute with clipped boxes\n        self.tensor[nearly_horizontal_indices] = boxes[nearly_horizontal_indices]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to [-180, 180)\n        self.normalize_angles()\n\n        # Identify nearly horizontal boxes based on angle threshold\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        is_near_horizontal = torch.abs(self.tensor[:, 4]) < angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n\n        # Clamp x and y coordinates to fit within box_size\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0] = (x1 + x2) / 2\n        self.tensor[:, 1] = (y1 + y2) / 2\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Convert angles to [-180, 180)\n        self.normalize_angles()\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to the range [-180, 180) degrees\n        self.normalize_angles()\n\n        # Identify indices of boxes that are nearly horizontal\n        angle_threshold = math.radians(clip_angle_threshold)\n        horizontal_indices = torch.abs(self.tensor[:, 4]) < angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1y1 = self.tensor[:, :2] - self.tensor[:, 2:4] / 2.0\n        x2y2 = self.tensor[:, :2] + self.tensor[:, 2:4] / 2.0\n        boxes = torch.cat([x1y1, x2y2], dim=1)\n\n        # Clamp x and y coordinates to ensure they do not exceed the box_size limits\n        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(min=0, max=box_size[1])\n        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        x1y1 = (boxes[:, :2] + boxes[:, 2:4]) / 2.0\n        x2y2 = (boxes[:, 2:4] - boxes[:, :2])\n        self.tensor = torch.cat([x1y1, x2y2, self.tensor[:, 4:]], dim=1)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to [-180, 180)\n        self.normalize_angles()\n\n        # Identify indices of boxes that are nearly horizontal\n        nearly_horizontal_indices = torch.abs(self.tensor[:, 4]) <= clip_angle_threshold\n\n        # Convert boxes to (x1, y1, x2, y2) format\n        boxes = self.tensor.clone()\n        boxes[:, 0] -= boxes[:, 2] / 2.0\n        boxes[:, 1] -= boxes[:, 3] / 2.0\n        boxes[:, 2] += boxes[:, 0]\n        boxes[:, 3] += boxes[:, 1]\n\n        # Clamp x and y coordinates\n        boxes[:, 0].clamp_(min=0, max=box_size[1])\n        boxes[:, 1].clamp_(min=0, max=box_size[0])\n        boxes[:, 2].clamp_(min=0, max=box_size[1])\n        boxes[:, 3].clamp_(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) format\n        self.tensor[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0\n        self.tensor[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0\n        self.tensor[:, 2] = boxes[:, 2] - boxes[:, 0]\n        self.tensor[:, 3] = boxes[:, 3] - boxes[:, 1]\n\n        # Update angles for nearly horizontal boxes\n        self.tensor[nearly_horizontal_indices, 4] = 0.0\n\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify indices of nearly horizontal boxes based on the clip_angle_threshold\n        nearly_horizontal_indices = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n\n        # Convert to (x1, y1, x2, y2) format\n        x1y1x2y2 = self.convert_to_xyxy()\n\n        # Clamp the x and y coordinates to the box_size limits\n        x1y1x2y2[:, 0::2].clamp_(min=0, max=box_size[1])\n        x1y1x2y2[:, 1::2].clamp_(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) format\n        self.tensor[:, :2] = (x1y1x2y2[:, :2] + x1y1x2y2[:, 2:]) / 2.0\n        self.tensor[:, 2:4] = x1y1x2y2[:, 2:] - x1y1x2y2[:, :2]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify indices of nearly horizontal boxes based on angle threshold\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        horizontal_indices = torch.abs(self.tensor[:, 4]) < angle_threshold\n\n        # Convert boxes to (x1, y1, x2, y2) format for clipping\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n\n        # Clip coordinates to ensure they fit within the specified box size\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert boxes back to original representation\n        self.tensor[:, 0] = (x1 + x2) / 2\n        self.tensor[:, 1] = (y1 + y2) / 2\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Convert back to (center x, center y, width, height, angle) format\n        self.tensor[horizontal_indices, :4] = torch.stack(\n            [\n                (x1 + x2) / 2,\n                (y1 + y2) / 2,\n                x2 - x1,\n                y2 - y1,\n            ],\n            dim=1,\n        )"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles\n        self.normalize_angles()\n\n        # Identify indices of nearly horizontal boxes\n        angle_threshold = math.radians(clip_angle_threshold)\n        angle_mask = torch.abs(self.tensor[:, 4]) < angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        box_corners = self.get_corners()\n\n        # Clamp x and y coordinates\n        box_corners[:, 0::2] = box_corners[:, 0::2].clamp(min=0, max=box_size[1])\n        box_corners[:, 1::2] = box_corners[:, 1::2].clamp(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor = self.from_corners(box_corners)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range [-180, 180]\n        self.normalize_angles()\n\n        # Identify the indices of the boxes that are nearly horizontal\n        angle_threshold = math.radians(clip_angle_threshold)\n        horizontal_indices = torch.abs(self.tensor[:, 4]) < angle_threshold\n\n        # Convert the boxes to (x1, y1, x2, y2) representation\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n        boxes = torch.stack([x1, y1, x2, y2], dim=1)\n\n        # Clamp the x and y coordinates\n        boxes[:, 0::2].clamp_(min=0, max=box_size[1])\n        boxes[:, 1::2].clamp_(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        x1 = boxes[:, 0]\n        y1 = boxes[:, 1]\n        x2 = boxes[:, 2]\n        y2 = boxes[:, 3]\n        width = x2 - x1\n        height = y2 - y1\n        x_center = (x1 + x2) / 2\n        y_center = (y1 + y2) / 2\n        angle = self.tensor[horizontal_indices, 4]\n        boxes = torch.stack([x_center, y_center, width, height, angle], dim=1)\n\n        # Update the tensor attribute\n        self.tensor[horizontal_indices] = boxes[horizontal_indices]\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify indices of nearly horizontal boxes based on clip_angle_threshold\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        near_horizontal_indices = torch.abs(self.tensor[:, 4]) < angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation for clipping\n        x1y1x2y2 = self.get_vertices()\n\n        # Clip x and y coordinates\n        x1y1x2y2[:, 0::2] = x1y1x2y2[:, 0::2].clamp(min=0, max=box_size[1])\n        x1y1x2y2[:, 1::2] = x1y1x2y2[:, 1::2].clamp(min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor = self.from_vertices(x1y1x2y2)\n\n        # Ensure numerical stability by converting back to original representation\n        self.tensor = self.to_xywha()\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {}\n        for item in self.data:\n            if item['type'] not in statistics:\n                statistics[item['type']] = 0\n            statistics[item['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n\n        for item in self.data:\n            stats[item['type']] += 1\n\n        return stats\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for data in self.data:\n            statistics[data['type']] += 1\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        types = ['doc', 'gen', 'kno', 'num']\n        stats = {t: 0 for t in types}\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {}\n        for obj in self.data:\n            if obj['type'] not in statistics:\n                statistics[obj['type']] = 1\n            else:\n                statistics[obj['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for data in self.data:\n            statistics[data['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {}\n        for obj in self.data:\n            obj_type = obj['type']\n            if obj_type in statistics:\n                statistics[obj_type] += 1\n            else:\n                statistics[obj_type] = 1\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            stats[item['type']] += 1\n\n        return stats\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        warnings.warn('DeprecationWarning: mmseg.models.losses is deprecated, '\n                      'please use mmdet.models.losses instead.')\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        if cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n            return MMSEG_LOSSES.build(cfg)\n        else:\n            return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, train_cfg, test_cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, train_cfg, test_cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, train_cfg, test_cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field ' \\\n        f'{cfg.get(\"type\")}.'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field ' \\\n        f'{cfg.get(\"type\")}.'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, train_cfg, test_cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field ' \\\n        f'{cfg.get(\"type\")}.'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field ' \\\n        f'{cfg.get(\"type\")}.'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check your ' \\\n        'configs.'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check your ' \\\n        'configs.'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert 'train_cfg' not in cfg.keys()\n    assert 'test_cfg' not in cfg.keys()\n    if train_cfg is not None:\n        cfg['train_cfg'] = train_cfg\n    if test_cfg is not None:\n        cfg['test_cfg'] = test_cfg\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check your ' \\\n        'configs.'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check your ' \\\n        'configs.'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert (\n        cfg.get('train_cfg') is None or train_cfg is None\n    ), 'train_cfg specified in both outer field and model field '\n    assert (\n        cfg.get('test_cfg') is None or test_cfg is None\n    ), 'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert (\n        'train_cfg' not in cfg\n        or train_cfg is None\n    ), f'train_cfg specified in outer field and in {cfg[\"type\"]}'\n    assert (\n        'test_cfg' not in cfg\n        or test_cfg is None\n    ), f'test_cfg specified in outer field and in {cfg[\"type\"]}'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, train_cfg, test_cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert 'type' in cfg\n    if train_cfg is not None:\n        assert 'train_cfg' not in cfg, \\\n            \"train_cfg specified in both outer field and as a model cfg option\"\n        cfg['model']['train_cfg'] = train_cfg\n    if test_cfg is not None:\n        assert 'test_cfg' not in cfg, \\\n            \"test_cfg specified in both outer field and as a model cfg option\"\n        cfg['model']['test_cfg'] = test_cfg\n\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check your ' \\\n        'configs.'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check your ' \\\n        'configs.'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, train_cfg, test_cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, train_cfg, test_cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert (\n        cfg.get('train_cfg') is None or train_cfg is None\n    ), 'train_cfg specified in both outer field and model field '\n    assert (\n        cfg.get('test_cfg') is None or test_cfg is None\n    ), 'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert (\n        'train_cfg' not in cfg\n        or train_cfg is None\n    ), 'train_cfg specified in outer field and in model field '\n    assert (\n        'test_cfg' not in cfg\n        or test_cfg is None\n    ), 'test_cfg specified in outer field and in model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert 'train_cfg' not in cfg, 'train_cfg shouldn\\'t be specified ' \\\n                                   'in the model field '\n    assert 'test_cfg' not in cfg, 'test_cfg shouldn\\'t be specified ' \\\n                                  'in the model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert (\n            cfg.get('train_cfg') is None or train_cfg is None\n    ), 'train_cfg specified in both outer field and model field ' + \\\n        'is deprecated, please check your configs.'\n    assert (\n            cfg.get('test_cfg') is None or test_cfg is None\n    ), 'test_cfg specified in both outer field and model field ' + \\\n        'is deprecated, please check your configs.'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert (\n            cfg.get('train_cfg') is None or train_cfg is None\n    ), 'train_cfg specified in both outer field and model field '\n    assert (\n            cfg.get('test_cfg') is None or test_cfg is None\n    ), 'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if box_type_3d is not None:\n        from mmdet3d.core.bbox import box_np_ops\n        from mmdet3d.core.bbox.structures import get_box_type\n        from mmdet3d.core.bbox.structures.utils import box_np_mapping\n        box_type_3d = get_box_type(box_type_3d)\n\n    def _eval_map_recall(gt_annos,\n                         dt_annos,\n                         metric,\n                         label2cat=None,\n                         box_type_3d=None,\n                         box_mode_3d=None):\n\n        \"\"\"\n        This function evaluates the detection results by comparing the detection annotations with the ground truth annotations. It calculates the mean Average Precision (mAP) and mean Average Recall (mAR) for different IoU thresholds.\n\n        Input-Output Arguments\n        :param gt_annos: list[dict]. Ground truth annotations containing information about the true bounding boxes and their labels.\n        :param dt_annos: list[dict]. Detection annotations containing detected bounding boxes, their labels, and scores.\n        :param metric: list[float]. A list of IoU thresholds used for computing average precisions and recalls.\n        :param label2cat: dict, optional. A mapping from numerical labels to category names.\n        :param box_type_3d: class, optional. The type of 3D bounding box used, which determines how the boxes are converted and processed.\n        :param box_mode_3d: str, optional. The mode or format in which the 3D bounding boxes are represented.\n        :return: dict[str, float]. A dictionary containing the evaluation results, including class-wise AP and AR for each IoU threshold, as well as overall mAP and mAR.\n\n        \"\"\"\n\n        if box_type_3d is not None:\n            from mmdet3d.core.bbox import box_np_ops\n            from mmdet3d.core.bbox.structures import get_box_type"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import get_official_eval_result\n    gt_annos = [\n        ann for ann in gt_annos\n        if ann.get('name', 'indoor') == 'indoor'\n    ]\n    dt_annos = [\n        ann for ann in dt_annos\n        if ann.get('name', 'indoor') == 'indoor'\n    ]\n    gt_annos = [\n        ann for ann in gt_annos\n        if ann.get('ann', dict()).get('ignore', False) == False\n    ]\n    recall, precision, ap = eval_map_recall(dt_annos, gt_annos, metric)\n    eval_results = dict()\n    eval_results['mAP'] = sum([ap[i][1] for i in range(len(metric))]) / len(\n        metric)\n    eval_results['mAR'] = sum([recall[i][1] for i in range(len(metric))]) / len(\n        metric)\n    for i in range(len(metric)):\n        eval_results['AR@{}'.format(metric[i])] = recall[i][1]\n        eval_results['AP@{}'.format(metric[i])] = ap[i][1]\n    eval_results['mAP_50'] = sum([ap[i][0] for i in range(len(metric))]) / len(\n        metric)\n    eval_results['mAR_50'] = sum([recall[i][0] for i in range(len(metric))]) / len(\n        metric)\n    eval_results['mAP_75'] = sum([ap[i][2] for i in range(len(metric))]) / len(\n        metric)\n    eval_results['mAR_75'] = sum([recall[i][2] for i in range(len(metric))]) / len(\n        metric)\n    eval_results['mAP_50_95'] = sum([ap[i][3] for i in"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # get gt and dt\n    gt_annos = [\n        x for x in gt_annos\n        if x['annos']['name'] in label2cat.keys()\n    ]\n    dt_annos = [\n        x for x in dt_annos if x['name'] in label2cat.keys()\n    ]\n\n    # eval\n    eval_results = eval_map_recall(dt_annos, gt_annos, metric)\n    eval_results = eval_results[0]\n\n    # calculate mAP\n    mAP = {}\n    for iou_idx, thresh in enumerate(metric):\n        for label in eval_results[iou_idx].keys():\n            if label in mAP:\n                mAP[label] += eval_results[iou_idx][label][-1]\n            else:\n                mAP[label] = eval_results[iou_idx][label][-1]\n\n    mAP = {k: v / len(dt_annos) for k, v in mAP.items()}\n\n    # calculate mAR\n    mAR = {}\n    for iou_idx, thresh in enumerate(metric):\n        for label in eval_results[iou_idx].keys():\n            if label in mAR:\n                mAR[label] += eval_results[iou_idx][label][0]\n            else:\n                mAR[label] = eval_results[iou_idx][label][0]\n\n    mAR = {k: v / len(dt_annos) for k, v in mAR.items()}\n\n    # calculate mAP_50\n    mAP_50 = {}\n    for iou_idx, thresh in enumerate(metric):\n        for label in eval_results[iou_idx].keys():\n            if label in mAP_50:\n                mAP_50[label] += eval_results[iou_idx][label][-1]\n            else:\n                mAP_50[label] = eval_"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from . import eval_det_cls\n    from . import eval_map_recall\n\n    # check gt and dt\n    if len(gt_annos) == 0:\n        raise ValueError('No ground truth detected.')\n    if len(dt_annos) == 0:\n        raise ValueError('No detection result for evaluation.')\n\n    # get gt and dt\n    gt_annos = eval_det_cls.get_annos(gt_annos, box_type_3d, box_mode_3d)\n    dt_annos = eval_det_cls.get_annos(dt_annos, box_type_3d, box_mode_3d)\n\n    # eval map and recall\n    ret_dict = eval_map_recall(dt_annos, gt_annos, metric)\n    recall, precision, ap = ret_dict\n\n    # eval summary\n    summary = eval_det_cls.print_map_summary(\n        recall,\n        precision,\n        ap,\n        metric,\n        label2cat,\n        logger=logger,\n        mode='3d')\n\n    return summary"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if not isinstance(dt_annos, list):\n        dt_annos = [dt_annos]\n    if not isinstance(gt_annos, list):\n        gt_annos = [gt_annos]\n\n    recalls = {}\n    precisions = {}\n    aps = {}\n    for iou_thr in metric:\n        recalls[iou_thr], precisions[iou_thr], aps[iou_thr] = eval_map_recall(\n            dt_annos, gt_annos, [iou_thr])\n\n    mAP = sum([aps[thr][0] for thr in metric]) / len(metric)\n    mAR = sum([recalls[thr][0][-1] for thr in metric]) / len(metric)\n\n    eval_results = {\n        'mAP': mAP,\n        'mAR': mAR,\n        'AP': aps,\n        'AR': recalls,\n        'precision': precisions\n    }\n\n    # format results\n    summary_table = [['category', 'iou_thr', 'mAP', 'mAR']]\n    for iou_thr in metric:\n        for cat in label2cat.keys():\n            summary_table.append([\n                label2cat[cat], iou_thr, aps[iou_thr][cat][0],\n                recalls[iou_thr][cat][0][-1]\n            ])\n    summary_table.append(['mAP', '', mAP, mAR])\n\n    if logger is None:\n        print_log(\n            '\\n' + AsciiTable(summary_table).table, logger=logger, level='info')\n    else:\n        print_log(\n            '\\n' + AsciiTable(summary_table).table, logger=logger, level='INFO')\n\n    return eval_results"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # convert gt and dt to standard format\n    gt_annos = [\n        x.to_dict() if isinstance(x, box_type_3d) else x\n        for x in gt_annos\n    ]\n    dt_annos = [\n        x.to_dict() if isinstance(x, box_type_3d) else x\n        for x in dt_annos\n    ]\n\n    gt_annos = [\n        x.to_dict(box_mode_3d) if isinstance(x, box_type_3d) else x\n        for x in gt_annos\n    ]\n    dt_annos = [\n        x.to_dict(box_mode_3d) if isinstance(x, box_type_3d) else x\n        for x in dt_annos\n    ]\n\n    # get gt and dt\n    gt_annos = [x for x in gt_annos if x['name'] in label2cat]\n    dt_annos = [x for x in dt_annos if x['name'] in label2cat]\n\n    gt_annos_class = {}\n    dt_annos_class = {}\n    for x in gt_annos:\n        gt_annos_class.setdefault(x['name'], []).append(x)\n    for x in dt_annos:\n        dt_annos_class.setdefault(x['name'], []).append(x)\n\n    # eval\n    eval_results = {}\n    for classname in label2cat.values():\n        if classname in gt_annos_class:\n            gt_annos_class_ = gt_annos_class[classname]\n        else:\n            gt_annos_class_ = []\n        if classname in dt_annos_class:\n            dt_annos_class_ = dt_annos_class[classname]\n        else:\n            dt_annos_class_ = []\n        eval_results[classname] = eval_map_recall(\n           "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # Get the number of classes\n    num_classes = len(label2cat)\n\n    # Initialize the evaluation results\n    eval_results = dict()\n\n    # Calculate the mAP and mAR for each class\n    for i in range(num_classes):\n        # Get the ground truth and detection annotations for the current class\n        gt_annos_curr_class = [\n            x for x in gt_annos if x['name'] == label2cat[i]\n        ]\n        dt_annos_curr_class = [\n            x for x in dt_annos if x['name'] == label2cat[i]\n        ]\n\n        # Convert the 3D bounding boxes to the desired format\n        gt_boxes_curr_class = [\n            x['boxes_3d'].tensor.numpy() for x in gt_annos_curr_class\n        ]\n        dt_boxes_curr_class = [\n            x['boxes_3d'].tensor.numpy() for x in dt_annos_curr_class\n        ]\n        gt_labels_curr_class = [\n            x['boxes_3d'].label for x in gt_annos_curr_class\n        ]\n        dt_labels_curr_class = [\n            x['boxes_3d'].label for x in dt_annos_curr_class\n        ]\n        gt_scores_curr_class = [\n            x['boxes_3d'].score for x in gt_annos_curr_class\n        ]\n        dt_scores_curr_class = [\n            x['boxes_3d'].score for x in dt_annos_curr_class\n        ]\n\n        # Compute the mAP and mAR for the current class\n        rec, prec, ap = eval_map_recall(\n            {\n                i: [(dt_boxes_curr_class[j], dt_scores_curr_class[j])\n                    for j in range(len(dt_boxes"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from . import indoor_eval_utils\n    from .indoor_eval_utils import _prepare_data\n\n    if box_type_3d is not None and box_mode_3d is not None:\n        from .indoor_eval_utils import get_box_type\n        box_type_3d = get_box_type(box_type_3d)\n\n    # convert gt and dt to standard format\n    gt_annos = _prepare_data(gt_annos,\n                             label2cat,\n                             box_type_3d=box_type_3d,\n                             box_mode_3d=box_mode_3d)\n    dt_annos = _prepare_data(dt_annos,\n                             label2cat,\n                             box_type_3d=box_type_3d,\n                             box_mode_3d=box_mode_3d)\n\n    # eval\n    ret_dict = indoor_eval_utils.indoor_eval(gt_annos,\n                                             dt_annos,\n                                             metric,\n                                             label2cat=label2cat)\n\n    # print the results\n    if logger is None:\n        print_log(\n            '\\n' + AsciiTable(\n                [list(ret_dict.values())],\n                list(ret_dict.keys())).table,\n            logger=logger)\n    else:\n        logger.info(\n            '\\n' + AsciiTable(\n                [list(ret_dict.values())],\n                list(ret_dict.keys())).table)\n\n    return ret_dict"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from . import indoor_eval_utils\n    if box_type_3d is None:\n        from mmdet3d.core.bbox import get_box_type\n        box_type_3d = get_box_type()\n    from mmdet.core.bbox import get_box_type\n    box_type_2d = get_box_type()\n    from mmdet.datasets.pipelines import Compose\n\n    if isinstance(logger, str):\n        import logging\n        level_dict = {\n            'debug': logging.DEBUG,\n            'info': logging.INFO,\n            'warning': logging.WARNING,\n            'error': logging.ERROR,\n            'critical': logging.CRITICAL\n        }\n        logger = logging.getLogger()\n        logger.setLevel(level_dict[logger])\n    elif logger is not None:\n        logger.info(f'Evaluate annotation type: {box_type_3d}')\n\n    # eval\n    gt_annos = [\n        box_type_3d(anno['annos'], box_dim=3)\n        for anno in gt_annos\n    ]\n    dt_annos = [\n        box_type_2d(anno['annos'], box_dim=4)\n        for anno in dt_annos\n    ]\n    eval_result = indoor_eval_utils.indoor_eval(\n        gt_annos,\n        dt_annos,\n        metric,\n        label2cat=label2cat,\n        logger=logger,\n        box_mode_3d=box_mode_3d)\n    return eval_result\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from . import indoor_eval_utils as utils\n\n    if box_type_3d is None:\n        from mmdet3d.core.bbox import box_np_ops\n        box_type_3d = box_np_ops.RotateIoU3DBoxes\n    if box_mode_3d is None:\n        box_mode_3d = 'xyzwhlr'\n\n    # convert gt and dt to box_type_3d\n    gt_bboxes_list = []\n    gt_labels_list = []\n    for ann in gt_annos:\n        gt_bboxes_list.append(box_type_3d(ann['boxes_3d'], box_dim=7,\n                                          box_mode=box_mode_3d))\n        gt_labels_list.append(ann['labels_3d'])\n\n    dt_bboxes_list = []\n    dt_scores_list = []\n    dt_labels_list = []\n    for ann in dt_annos:\n        dt_bboxes_list.append(box_type_3d(ann['boxes_3d'], box_dim=7,\n                                          box_mode=box_mode_3d))\n        dt_scores_list.append(ann['scores_3d'])\n        dt_labels_list.append(ann['labels_3d'])\n\n    # eval\n    result_strings, result_dict = utils.eval(\n        gt_bboxes_list,\n        gt_labels_list,\n        dt_bboxes_list,\n        dt_scores_list,\n        dt_labels_list,\n        metric=metric,\n        label2cat=label2cat)\n\n    # log\n    if logger is None:\n        print_log('\\n'.join(result_strings), logger=logger)\n    else:\n        for result_string in result_strings:\n            logger.info(result_string)\n\n    return result_dict\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from . import kitti_common as kitti\n    from . import eval as det_eval\n\n    def _do_python_eval(dt_annos,\n                        gt_annos,\n                        metric,\n                        logger=None,\n                        label2cat=None,\n                        box_type_3d=None,\n                        box_mode_3d=None):\n        \"\"\"\n        This function performs the evaluation of detection results using the Python implementation of the official KITTI evaluation script. It calculates class-wise AP and AR for different IoU thresholds and returns the overall mAP and mAR.\n\n        Input-Output Arguments\n        :param dt_annos: list[dict]. Detection annotations containing detected bounding boxes, their labels, and scores.\n        :param gt_annos: list[dict]. Ground truth annotations containing information about the true bounding boxes and their labels.\n        :param metric: list[float]. A list of IoU thresholds used for computing average precisions and recalls.\n        :param logger: logging.Logger or str, optional. Specifies how the mAP summary is printed. If None, the default logging method is used.\n        :param label2cat: dict. A mapping from numerical labels to category names.\n        :param box_type_3d: class, optional. The type of 3D bounding box used, which determines how the boxes are converted and processed.\n        :param box_mode_3d: str, optional. The mode or format in which the 3D bounding boxes are represented.\n        :return: dict[str, float]. A dictionary containing the evaluation results, including class-wise AP and AR for each IoU threshold, as well as overall mAP and mAR.\n\n        \"\"\"\n\n        def _voc_eval_func(\n                class_recs,\n                npos,\n                class_name,\n                ovthresh=0.5,\n                use_07_metric=True,\n        ):\n            \"\"\"\n            This function evaluates the detection results for a single class using the Python implementation of the official KITTI evaluation script. It calculates the average precision (AP) and average recall (AR"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    gt_annos = [\n        gt_anno.copy() for gt_anno in gt_annos\n    ]\n    dt_annos = [\n        dt_anno.copy() for dt_anno in dt_annos\n    ]\n    metric = np.array(metric)\n    recalls, precisions, aps = eval_map_recall(dt_annos, gt_annos, metric)\n    eval_results = {}\n    print_log(\n        f'Evaluation results of metric {metric}', logger=logger, level='INFO')\n    for i, (recall, precision, ap) in enumerate(zip(recalls, precisions, aps)):\n        print_log(\n            f'{metric[i]:.2f}: {ap.mean():.4f}', logger=logger, level='INFO')\n        eval_results[f'{metric[i]:.2f}'] = {\n            'recall': recall,\n            'precision': precision,\n            'AP': ap\n        }\n    eval_results['mAP'] = ap.mean()\n    eval_results['mAR'] = recall.mean()\n\n    # format results to table\n    table_data = [['mAP', 'mAR'] + [f'{m:.2f}' for m in metric]]\n    for i, (recall, precision, ap) in enumerate(zip(recalls, precisions, aps)):\n        row = [f'{recall.mean():.4f}', f'{precision.mean():.4f}']\n        row.extend([f'{m:.4f}' for m in ap])\n        table_data.append([f'{metric[i]:.2f}'] + row)\n    table = AsciiTable(table_data)\n    print_log('\\n' + table.table, logger=logger, level='INFO')\n    return eval_results"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if isinstance(logger, str):\n        import logging\n        level_dict = {\n            'debug': logging.DEBUG,\n            'info': logging.INFO,\n            'warning': logging.WARNING\n        }\n        logger = logging.getLogger(logger)\n        logger.setLevel(level_dict[logger])\n    else:\n        logger = print_log\n\n    if not isinstance(dt_annos, list):\n        dt_annos = [dt_annos]\n    if not isinstance(gt_annos, list):\n        gt_annos = [gt_annos]\n\n    recalls = {}\n    precisions = {}\n    aps = {}\n    for iou_thr in metric:\n        recalls[iou_thr], precisions[iou_thr], aps[iou_thr] = eval_map_recall(\n            dt_annos, gt_annos, [iou_thr])\n\n    # compute mAP\n    ap = sum([aps[thr][metric_name] for thr in metric]) / len(metric)\n\n    # compute mAR\n    ar = sum([recalls[thr][metric_name] for thr in metric]) / len(metric)\n\n    # print the results\n    header = ['category', 'AP']\n    for iou_thr in metric:\n        header.append('AR@' + str(iou_thr))\n    row_data = []\n    for i, class_name in enumerate(aps[metric[0]]):\n        if class_name not in gt_annos[0]:\n            continue\n        if i == 0:\n            logger('mAP: {}'.format(ap))\n        row = [class_name]\n        row.append('{:.4f}'.format(aps[metric[0]][class_name]))\n        for iou_thr in metric:\n            row.append('{:.4f}'.format(recalls[iou_thr][class_name]))\n        row_data.append(row)\n    table = AsciiTable([header] + row_data)\n    logger('\\n' +"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from . import eval_utils\n\n    if logger is None:\n        logger = print_log\n    else:\n        logger = mmcv.utils.get_logger(logger)\n\n    # filter out ignored gt\n    gt_annos = [ann for ann in gt_annos if ann.get('ignore', False) == False]\n\n    gt_annos = eval_utils.get_official_eval_result(gt_annos, box_type_3d, box_mode_3d)\n    dt_annos = eval_utils.get_official_eval_result(dt_annos, box_type_3d, box_mode_3d)\n\n    # eval\n    eval_result = eval_utils.indoor_eval(gt_annos, dt_annos, metric, label2cat)\n    logger.info('\\n' + AsciiTable(\n        [['class', 'iou_thr', 'AP', 'AR']] +\n        [['all', 'all', '%.4f' % eval_result['mAP'], '%.4f' % eval_result['mAR']]] +\n        [['{}'.format(k), '{}'.format(v['iou_thr']), '%.4f' % v['AP'], '%.4f' % v['AR']]\n         for k, v in eval_result['results'].items()]).table)\n\n    return eval_result\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import get_official_eval_result\n\n    if logger is None:\n        logger = print_log\n\n    if box_type_3d is None:\n        from mmdet3d.core.bbox import Box3DMode, limit_period\n        box_type_3d = Box3DMode\n    if box_mode_3d is None:\n        box_mode_3d = 'lidar'\n\n    if not isinstance(metric, list):\n        metric = [metric]\n\n    class_names = list(label2cat.keys())\n    num_classes = len(class_names)\n    mAP_modes = ['bbox', 'beam']\n\n    recalls = {}\n    precisions = {}\n    aps = {}\n    for iou_thr in metric:\n        recalls[iou_thr], precisions[iou_thr], aps[iou_thr] = \\\n            eval_map_recall(dt_annos, gt_annos, [iou_thr])\n\n    eval_results = {}\n    for iou_thr in metric:\n        eval_results[f'AP-{iou_thr}'] = aps[iou_thr]\n        eval_results[f'AR-{iou_thr}'] = recalls[iou_thr]\n\n    eval_results['mAP'] = aps\n    eval_results['mAR'] = recalls\n\n    if num_classes > 1:\n        for iou_thr in metric:\n            for cls_name in class_names:\n                eval_results[f'AP-{iou_thr}-{cls_name}'] = aps[iou_thr][cls_name]\n                eval_results[f'AR-{iou_thr}-{cls_name}'] = recalls[iou_thr][cls_name]\n\n    for mAP_mode in mAP_modes:\n        eval_results[f'mAP-{mAP_mode}'] = get_official_eval_"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if logger is None:\n        logger = print_log\n\n    class_names = list(label2cat.values())\n    num_classes = len(class_names)\n    # filter out difficult examples\n    if 'difficulty' in gt_annos[0]:\n        for i in range(len(gt_annos)):\n            difficult = gt_annos[i]['difficulty']\n            gt_annos[i] = [\n                o for o in gt_annos[i]['annos']\n                if o['gt_names'][0] not in difficult\n            ]\n\n    # filter out difficult examples\n    if 'difficulty' in dt_annos[0]:\n        for i in range(len(dt_annos)):\n            difficult = dt_annos[i]['difficulty']\n            dt_annos[i] = [\n                o for o in dt_annos[i]['annos']\n                if o['name'] not in difficult\n            ]\n\n    # create gt and dt dict\n    gt_dict = {}\n    dt_dict = {}\n    for i in range(num_classes):\n        gt_dict[i] = []\n        dt_dict[i] = []\n\n    for i in range(len(gt_annos)):\n        gt_anno = gt_annos[i]\n        for j in range(len(gt_anno)):\n            gt_bbox = gt_anno[j]['gt_bboxes_3d']\n            gt_name = gt_anno[j]['gt_names'][0]\n            gt_difficult = gt_anno[j]['gt_names'][1]\n            if gt_name not in class_names:\n                continue\n            difficult = False\n            if gt_difficult == 'easy':\n                difficult = False\n            elif gt_difficult == 'hard':\n                difficult = True\n            else:\n                difficult = False\n            gt_bbox = box_mode_3d.convert_box(gt_bbox,"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .eval_detection_3d import eval_map_recall\n\n    if box_type_3d is None:\n        from mmdet3d.core.bbox.box_3d_mode import Box3DMode\n        box_type_3d = Box3DMode.LIDAR\n    if box_mode_3d is None:\n        from mmdet3d.core.bbox.box_3d_mode import Box3DMode\n        box_mode_3d = Box3DMode.LIDAR\n\n    # convert gt and dt to the same type\n    if box_mode_3d == 'lidar':\n        gt_annos = [\n            ann.convert_to(box_type_3d) for ann in gt_annos\n        ]\n        dt_annos = [\n            ann.convert_to(box_type_3d) for ann in dt_annos\n        ]\n    elif box_mode_3d == 'camera':\n        gt_annos = [\n            ann.convert_to(box_type_3d, box_mode_3d) for ann in gt_annos\n        ]\n        dt_annos = [\n            ann.convert_to(box_type_3d, box_mode_3d) for ann in dt_annos\n        ]\n    else:\n        raise NotImplementedError\n\n    # eval\n    eval_results = {}\n    for iou_thr in metric:\n        rec, prec, ap = eval_map_recall(dt_annos, gt_annos, [iou_thr])\n        for i, class_name in enumerate(label2cat.keys()):\n            eval_results[f'AP-{class_name}-{iou_thr}'] = ap[i][class_name]\n            eval_results[f'AR-{class_name}-{iou_thr}'] = rec[i][class_name]\n\n    # calculate mAP and mAR\n    mAP = np.mean([eval_results[f'AP-{"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if isinstance(logger, str):\n        import logging\n        level_dict = {\n            'debug': logging.DEBUG,\n            'info': logging.INFO,\n            'warning': logging.WARNING\n        }\n        logger = logging.getLogger()\n        logger.setLevel(level_dict[logger])\n    elif logger is None:\n        import logging\n        logger = logging.getLogger()\n\n    # filter gt and det with score < 0\n    gt_annos = [anno for anno in gt_annos if anno['score'] > 0]\n    dt_annos = [anno for anno in dt_annos if anno['score'] > 0]\n\n    recalls, precisions, aps = eval_map_recall(\n        dt_annos, gt_annos, ovthresh=metric)\n\n    eval_results = {}\n    for i, iou_thr in enumerate(metric):\n        eval_results[f'AP-{iou_thr}'] = aps[i]\n        eval_results[f'AR-{iou_thr}'] = recalls[i]\n\n    # calculate mAP and mAR\n    mAP = sum([v for k, v in eval_results.items() if k[:2] == 'AP']) / len(\n        eval_results)\n    mAR = sum([v for k, v in eval_results.items() if k[:2] == 'AR']) / len(\n        eval_results)\n    eval_results['mAP'] = mAP\n    eval_results['mAR'] = mAR\n\n    # log mAP and mAR\n    table_header = ['mAP', 'mAR'] + [f'AP-{thr}' for thr in metric] + [\n        f'AR-{thr}' for thr in metric\n    ]\n    table_data = [[mAP, mAR] + [v for k, v in eval_results.items() if k[:2] == 'AP'] +\n                  [v for k, v in eval_results.items() if k[:"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import get_official_eval_result\n\n    # 1. convert gt and dt to kitti format\n    kitti_gt, kitti_dt = get_official_eval_result(\n        gt_annos, dt_annos, label2cat, box_type_3d, box_mode_3d)\n\n    # 2. evaluation\n    eval_results = {}\n    print_log('\\n')\n    print_log('*' * 50, logger=logger)\n    print_log('Evaluating indoor results', logger=logger)\n    print_log('*' * 50, logger=logger)\n    for iou_thr in metric:\n        print_log(f'iou_thr: {iou_thr}', logger=logger)\n        eval_result = eval_map_recall(\n            kitti_dt, kitti_gt, ovthresh=[iou_thr])\n        eval_results[f'mAP_{iou_thr}'] = eval_result[2][0]['Car']\n        eval_results[f'mAR_{iou_thr}'] = eval_result[0][0]['Car']\n\n    print_log(f'mAP: {eval_results[\"mAP_0.7\"]}', logger=logger)\n    print_log(f'mAR: {eval_results[\"mAR_0.7\"]}', logger=logger)\n    print_log('*' * 50, logger=logger)\n\n    # 3. format summary table\n    summary_table = [['class', 'mAP', 'mAR']]\n    for iou_thr in metric:\n        summary_table.append([\n            f'{iou_thr}',\n            f'{eval_results[f\"mAP_{iou_thr:.2f}\"]}',\n            f'{eval_results[f\"mAR_{iou_thr:.2f}\"]}'\n        ])\n    summary_table = AsciiTable(summary"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if box_type_3d is None:\n        from mmdet3d.core.bbox import get_box_type\n        box_type_3d = get_box_type()\n    if box_mode_3d is None:\n        from mmdet3d.core.bbox import get_box_mode\n        box_mode_3d = get_box_mode()\n\n    from mmdet3d.core.bbox import box_np_ops\n    from mmdet3d.core.evaluation.eval_detection_3d import get_official_eval_result\n\n    class_names = list(label2cat.values())\n    num_classes = len(class_names)\n    num_dts = len(dt_annos)\n    num_gts = len(gt_annos)\n    num_imgs = len(gt_annos)\n\n    print_log(f'Starting evaluation with {num_dts} detections and {num_gts} '\n              f'ground truths.', logger=logger)\n\n    if num_dts == 0:\n        print_log('No detections were made.', logger=logger)\n        return {}\n\n    if num_gts == 0:\n        print_log('No ground truths were found.', logger=logger)\n        return {}\n\n    if num_imgs == 0:\n        print_log('No ground truths were found.', logger=logger)\n        return {}\n\n    # convert gt_annos to eval format\n    eval_gt = {\n        class_names[i]: [\n            {\n                'name': class_names[i],\n                'bbox': box_type_3d(box_mode_3d, box.tensor, box.dim,\n                                    origin=box.origin),\n                'score': 1\n            } for box in gt_annos[i]\n        ] for i in range(num_imgs)\n    }\n\n    # convert dt_annos to eval format\n    eval_dt = {\n        class_names[i]: ["}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls_cuda = LiDARInstance3DBoxes\n        box_mode_cuda = 'lidar'\n    elif box_type == 'Camera':\n        box_cls_cuda = CameraInstance3DBoxes\n        box_mode_cuda = 'camera'\n    elif box_type == 'Depth':\n        box_cls_cuda = DepthInstance3DBoxes\n        box_mode_cuda = 'depth'\n    else:\n        raise ValueError(f'box type must be LiDAR, Camera, or Depth, but got {box_type}')\n\n    return box_cls_cuda, box_mode_cuda\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = None\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = None\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = None\n        mode = 'depth'\n    else:\n        raise NotImplementedError\n    return box_cls, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'Unsupported box type: {box_type}')\n\n    return box_cls, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_class = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_class = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_class = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box type {box_type} not supported')\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_class = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_class = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_class = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box_type must be LiDAR, Camera, or Depth, got {box_type}')\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = None\n        mode = 'xyzwhl'\n    elif box_type == 'Camera':\n        box_cls = None\n        mode = 'xyzwhl'\n    elif box_type == 'Depth':\n        box_cls = None\n        mode = 'xyzwhl'\n    else:\n        raise ValueError(f'Unsupported box type: {box_type}')\n\n    return box_cls, mode\n\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = 'LiDARBox3D'\n        box_mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = 'CameraBox3D'\n        box_mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = 'DepthBox3D'\n        box_mode = 'depth'\n    else:\n        raise ValueError(f'Unsupported box type: {box_type}')\n\n    return box_cls, box_mode\n\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Map the box type to the corresponding 3D box class and mode\n    box_type_mapping = {\n        'LiDAR': (LiDARBox3D, 'lidar'),\n        'Camera': (CameraBox3D, 'camera'),\n        'Depth': (DepthBox3D, 'depth')\n    }\n\n    # Get the 3D box class and mode based on the input box type\n    box_class, box_mode = box_type_mapping.get(box_type, (None, None))\n\n    # Raise an error if the box type is not recognized\n    if box_class is None:\n        raise ValueError(f\"Invalid box type '{box_type}'. \"\n                         f\"Expected one of 'LiDAR', 'Camera', or 'Depth'.\")\n\n    return box_class, box_mode\n\n\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_class_dict = {\n        'LiDAR': (LiDARBox3D, 'lidar'),\n        'Camera': (CameraBox3D, 'camera'),\n        'Depth': (DepthBox3D, 'depth')\n    }\n\n    if box_type not in box_class_dict:\n        raise ValueError('Invalid box type: {}'.format(box_type))\n\n    return box_class_dict[box_type]\n\n\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_class = 'LiDAR'\n        box_mode = 'xyzwhl'\n    elif box_type_lower == 'camera':\n        box_class = 'Camera'\n        box_mode = 'xyzwhl'\n    elif box_type_lower == 'depth':\n        box_class = 'Depth'\n        box_mode = 'xyzwhl'\n    else:\n        raise ValueError(f'Unsupported box type: {box_type}.')\n    return box_class, box_mode\n\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Map the box type to a specific 3D box class\n    box_type_to_class = {\n        'LiDAR': 'LiDARBox3D',\n        'Camera': 'CameraBox3D',\n        'Depth': 'DepthBox3D'\n    }\n\n    # Map the box type to a specific mode\n    box_type_to_mode = {\n        'LiDAR': 'lidar',\n        'Camera': 'camera',\n        'Depth': 'depth'\n    }\n\n    # Get the 3D box class and mode based on the input box type\n    box_class = box_type_to_class.get(box_type, None)\n    mode = box_type_to_mode.get(box_type, None)\n\n    # Raise an error if the box type is not recognized\n    if box_class is None:\n        raise ValueError(f\"Invalid box type: {box_type}. \"\n                         f\"Supported box types are: {list(box_type_to_class.keys())}\")\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Mapping from box type to class and mode\n    box_type_mapping = {\n        'LiDAR': (LiDARBox3D, 'lidar'),\n        'Camera': (CameraBox3D, 'camera'),\n        'Depth': (DepthBox3D, 'depth')\n    }\n\n    # Check if the box type is supported\n    if box_type not in box_type_mapping:\n        raise ValueError(f\"Unsupported box type: {box_type}. \"\n                         f\"Supported box types: {list(box_type_mapping.keys())}\")\n\n    # Return the corresponding class and mode\n    return box_type_mapping[box_type]\n\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Map the box type to the corresponding 3D box class\n    box_type_dict = {\n        'LiDAR': 'LiDARBox3D',\n        'Camera': 'CameraBox3D',\n        'Depth': 'DepthBox3D',\n    }\n\n    # Map the box type to the corresponding mode\n    mode_dict = {\n        'LiDAR': 'lidar',\n        'Camera': 'camera',\n        'Depth': 'depth',\n    }\n\n    # Check if the box type is recognized\n    if box_type not in box_type_dict:\n        raise ValueError(f'Unsupported box type: {box_type}. '\n                         f'Supported types are {list(box_type_dict.keys())}')\n\n    # Get the 3D box class and mode\n    box_class = box_type_dict[box_type]\n    mode = mode_dict[box_type]\n\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box_type must be LiDAR, Camera or Depth, got '\n                         f'{box_type}')\n    return box_cls, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = None\n        box_mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = None\n        box_mode = 'img'\n    elif box_type == 'Depth':\n        box_cls = None\n        box_mode = 'depth'\n    else:\n        raise ValueError(f'Unsupported box type: {box_type}')\n\n    return box_cls, box_mode\n\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Map the box type to the corresponding 3D box class and mode\n    box_type_map = {\n        'LiDAR': (LiDARBox3D, 'lidar'),\n        'Camera': (CameraBox3D, 'camera'),\n        'Depth': (DepthBox3D, 'depth')\n    }\n\n    # If the box type is not recognized, raise an error\n    if box_type not in box_type_map:\n        raise ValueError(f'Unknown box type: {box_type}')\n\n    # Return the corresponding 3D box class and mode\n    return box_type_map[box_type]\n\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARBox3D\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraBox3D\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthBox3D\n        mode = 'depth'\n    else:\n        raise ValueError(f'box type must be LiDAR, Camera, or Depth, but got {box_type}')\n    return box_cls, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_class = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_class = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_class = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box_type must be \"LiDAR\", \"Camera\", or \"Depth\", '\n                         f'but got {box_type}')\n    return box_class, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Map the box type to a 3D box class and mode\n    box_type_mapping = {\n        \"LiDAR\": (LiDARBox3D, 'lidar'),\n        \"Camera\": (CameraBox3D, 'camera'),\n        \"Depth\": (DepthBox3D, 'depth'),\n    }\n\n    # Check if the box type is recognized\n    if box_type not in box_type_mapping:\n        raise ValueError(f\"Unsupported box type: {box_type}\")\n\n    return box_type_mapping[box_type]\n\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Map the box type to a specific 3D box class and mode\n    if box_type == 'LiDAR':\n        box_cls = LiDARBox3D\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraBox3D\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthBox3D\n        mode = 'depth'\n    else:\n        raise ValueError(f'Unsupported box type: {box_type}')\n\n    return box_cls, mode\n\n\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      raise RequestError('must provide messages')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, Message):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message.role,\n            'content': message.content,\n            'images': [_encode_image(image) for image in message.images or []],\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message['role'],\n            'content': message['content'],\n            'image': _encode_image(message.get('image')),\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages:\n      if not isinstance(messages, list):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      for message in messages:\n        if not isinstance(message, Message):\n          raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages or [],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, Message):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [message.dict() for message in messages],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message['role'],\n            'content': message['content'],\n            'image': _encode_image(message.get('image')),\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      raise RequestError('must provide at least one message')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message.get('role', 'user'),\n            'content': message.get('content', ''),\n            'images': [_encode_image(image) for image in message.get('images', [])],\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages:\n      for message in messages:\n        if not isinstance(message, Message):\n          raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message['role'],\n            'content': message['content'],\n            'images': [_encode_image(image) for image in message.get('images', [])],\n          }\n          for message in messages or []\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      messages = []\n    elif not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message.get('role', 'user'),\n            'content': message['content'],\n            'images': [\n              _encode_image(image)\n              for image in message.get('images', [])\n            ],\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      raise RequestError('must provide messages')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, (Message, dict)) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message.get('role', ''),\n            'content': message.get('content', ''),\n            'name': message.get('name', ''),\n            'metadata': message.get('metadata', {}),\n            'images': [_encode_image(image) for image in message.get('images', [])],\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is not None and not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message.get('role', 'user'),\n            'content': message['content'],\n            'images': [_encode_image(image) for image in message.get('images', [])],\n          }\n          for message in messages or []\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list) or not all(isinstance(message, (Message, dict)) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      raise RequestError('must provide messages')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, dict) or isinstance(message, Message) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message['role'] if isinstance(message, dict) else message.role,\n            'content': message['content'] if isinstance(message, dict) else message.content,\n            'images': _encode_image(message['images']) if isinstance(message, dict) and message.get('images') else None,\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages:\n      if not isinstance(messages, list):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      for message in messages:\n        if not isinstance(message, Message):\n          raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [message.dict() for message in messages or []],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      raise RequestError('must provide messages')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list')\n\n    if not all(isinstance(message, Message) or isinstance(message, dict) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message.role,\n            'content': message.content,\n            'images': [_encode_image(image) for image in message.images or []],\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      raise RequestError('must provide messages')\n\n    if not isinstance(messages, list) or not all(isinstance(m, (Message, dict)) for m in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    messages = [Message(**m) for m in messages]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [{**m, 'images': [_encode_image(image) for image in m.get('images', [])]} for m in messages],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, Message) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if any(message.get('role') not in ('system', 'user', 'assistant') for message in messages):\n      raise RequestError('all messages must have a role of \"system\", \"user\", or \"assistant\"')\n\n    if any(not message.get('content') for message in messages):\n      raise RequestError('all messages must have a content')\n\n    if any(isinstance(message, dict) and not all(key in message for key in ('role', 'content')) for message in messages):\n      raise RequestError('all messages must have a role and content')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      raise RequestError('must provide at least one message')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, dict) or not all(key in message for key in ('role', 'content')):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      raise RequestError('must provide messages')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, dict) and not isinstance(message, Message):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message['role'],\n            'content': message['content'],\n            'images': [_encode_image(message['images']) for message in messages if 'images' in message],\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, Message):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message['role'],\n            'content': message['content'],\n            'images': _encode_image(message.get('images')),\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      raise RequestError('must provide messages')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, Message):\n        if not isinstance(message, dict):\n          raise TypeError('messages must be a list of Message or dict-like objects')\n        if 'role' not in message or 'content' not in message:\n          raise RequestError('messages must have a \"role\" and \"content\"')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message.get('role', 'user'),\n            'content': message.get('content', ''),\n            'images': _encode_image(message.get('images')),\n          } for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      f'/api/pull/{model}',\n      stream=stream,\n      params={'insecure': insecure},\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      params={'model': model, 'insecure': insecure},\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model == '':\n      raise RequestError('model is required')\n\n    params = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    return self._request_stream('POST', '/generate', json=params, stream=stream)\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model == '':\n      raise RequestError('model is required')\n\n    if not stream:\n      return self._request_stream(\n        'POST',\n        '/generate',\n        json={\n          'model': model,\n          'prompt': prompt,\n          'system': system,\n          'template': template,\n          'context': context,\n          'raw': raw,\n          'format': format,\n          'images': images,\n          'options': options,\n          'keep_alive': keep_alive,\n        },\n        stream=stream,\n      )\n\n    return self._stream(\n      'POST',\n      '/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n        'raw': raw,\n        'format': format,\n        'images': images,\n        'options': options,\n        'keep_alive': keep_alive,\n      },\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    return self._request_stream(\n      'POST',\n      '/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'format': format,\n        'images': images or [],\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    return self._request_stream(\n      'POST',\n      '/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'raw': raw,\n        'format': format,\n        'images': images or [],\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model == '':\n      raise RequestError('model is required')\n\n    params = {}\n    if system:\n      params['system'] = system\n    if template:\n      params['template'] = template\n    if context:\n      params['context'] = context\n    if raw:\n      params['raw'] = raw\n    if format:\n      params['format'] = format\n    if images:\n      params['images'] = images\n    if options:\n      params['options'] = options.dict()\n    if keep_alive:\n      params['keep_alive'] = keep_alive\n\n    return self._request_stream(\n      'POST',\n      f'/generate/{model}',\n      params=params,\n      data=prompt,\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    url = '/generate'\n\n    body = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    return self._request_stream('POST', url, json=body, stream=stream)\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('model must be provided')\n\n    if not prompt and not system:\n      raise RequestError('prompt or system must be provided')\n\n    if images is not None:\n      images = [b64encode(image).decode() for image in images]\n\n    if options is not None:\n      options = options.dict()\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, float):\n        keep_alive = str(keep_alive)\n      elif isinstance(keep_alive, str):\n        if not keep_alive.endswith('s'):\n          keep_alive += 's'\n      else:\n        keep_alive = None\n\n    return self._request_stream(\n      'POST',\n      '/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n        'raw': raw,\n        'format': format,\n        'images': images,\n        'options': options,\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    params = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options.dict() if options else None,\n      'keep_alive': keep_alive,\n    }\n\n    return self._request_stream(\n      'POST',\n      '/generate',\n      stream=stream,\n      params=params,\n      json={},\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if not prompt and not system:\n      raise RequestError('prompt or system is required')\n\n    if not format:\n      format = 'json'\n\n    if not options:\n      options = Options()\n\n    if keep_alive:\n      options.keep_alive = keep_alive\n\n    if images:\n      options.images = images\n\n    data = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'options': options.dict(),\n    }\n\n    return self._request_stream(\n      'POST',\n      'generate',\n      json=data,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model == '':\n      raise RequestError('model is required')\n\n    if format not in ['', 'json']:\n      raise RequestError('invalid format')\n\n    if images is not None:\n      images = [b64encode(image.encode()).decode() for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'format': format,\n        'images': images,\n        'options': options.dict() if options else None,\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    data = {\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    return self._request_stream(\n      'POST',\n      f'/{model}/generate',\n      json={k: v for k, v in data.items() if v is not None},\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if not prompt:\n      raise RequestError('prompt is required')\n\n    if images is not None:\n      images = [\n        b64encode(image).decode('ascii') if isinstance(image, bytes) else image\n        for image in images\n      ]\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    response = self._request_stream(\n      'POST',\n      '/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n        'raw': raw,\n        'format': format,\n        'images': images,\n        'options': options.dict(),\n      },\n      stream=stream,\n    )\n\n    if stream:\n      return response\n\n    return response\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    params = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context,\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    return self._request_stream(\n      'POST',\n      '/generate',\n      stream=stream,\n      params={k: v for k, v in params.items() if v is not None},\n      json={k: v for k, v in params.items() if v is None},\n    )\n\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if model == '':\n      raise RequestError('model is required')\n\n    if format not in ['', 'json']:\n      raise RequestError(f'invalid format: {format}')\n\n    if raw and format == '':\n      raise RequestError('raw and format cannot be used together')\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise RequestError('keep_alive must be a float or a string')\n\n    if images is not None:\n      images = [b64encode(image).decode('utf-8') for image in images]\n\n    params = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context or [],\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    return self._request_stream(\n      'POST',\n      '/generate',\n      json=params,\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if images:\n      if not isinstance(images, Sequence):\n        raise RequestError('images must be a sequence')\n\n      if not all(isinstance(image, str) for image in images):\n        raise RequestError('images must be a sequence of strings')\n\n      images = [image.encode('utf-8') for image in images]\n\n    if options:\n      options = options.dict(exclude_none=True)\n\n    if keep_alive:\n      if isinstance(keep_alive, str):\n        try:\n          keep_alive = float(keep_alive)\n        except ValueError:\n          raise RequestError(f'invalid keep_alive: {keep_alive}')\n      elif not isinstance(keep_alive, float):\n        raise RequestError(f'invalid keep_alive: {keep_alive}')\n\n    return self._request_stream(\n      'POST',\n      f'/v1/generate/{model}',\n      json={\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n        'raw': raw,\n        'format': format,\n        'images': images,\n        'options': options,\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    params = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context or [],\n      'raw': raw,\n      'format': format,\n      'images': images or [],\n      'options': options or {},\n      'keep_alive': keep_alive,\n    }\n\n    return self._request_stream(\n      'POST',\n      '/generate',\n      params=params,\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if not prompt and not system:\n      raise RequestError('prompt or system is required')\n\n    if context is None:\n      context = []\n\n    if images is None:\n      images = []\n\n    if options is None:\n      options = {}\n\n    if keep_alive is None:\n      keep_alive = ''\n\n    if isinstance(keep_alive, float):\n      keep_alive = str(keep_alive)\n\n    if isinstance(keep_alive, str):\n      keep_alive = f'{keep_alive}s'\n\n    if format not in ('', 'json'):\n      raise RequestError(f'invalid format: {format}')\n\n    if raw and format:\n      raise RequestError('raw and format are mutually exclusive')\n\n    if raw and keep_alive:\n      raise RequestError('raw and keep_alive are mutually exclusive')\n\n    if images and keep_alive:\n      raise RequestError('images and keep_alive are mutually exclusive')\n\n    if images and format:\n      raise RequestError('images and format are mutually exclusive')\n\n    if raw and images:\n      raise RequestError('raw and images are mutually exclusive')\n\n    if keep_alive and images:\n      raise RequestError('keep_alive and images are mutually exclusive')\n\n    if keep_alive and format:\n      raise RequestError('keep_alive and format are mutually exclusive')\n\n    if keep_alive and raw:\n      raise RequestError('keep_alive and raw are mutually exclusive')\n\n    if images and options:\n      raise RequestError('images and options are mutually exclusive')\n\n    if keep_alive and options:\n      raise RequestError('keep_alive and options are mutually exclusive')\n\n    if format and options:\n      raise RequestError('format and options are mutually exclusive')\n\n    if raw and options:\n      raise RequestError('raw and options are mutually exclusive')\n\n    if images and template:\n      raise RequestError('images"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('model must be provided')\n\n    if not prompt and not system:\n      raise RequestError('prompt or system must be provided')\n\n    if images:\n      if not isinstance(images, (list, tuple)):\n        raise RequestError('images must be a list or tuple')\n\n      if not all(isinstance(image, str) for image in images):\n        raise RequestError('images must be a list of strings')\n\n    if images and len(images) > 10:\n      raise RequestError('images must be less than 10')\n\n    if format and format not in ['', 'json']:\n      raise RequestError('format must be either an empty string or \"json\"')\n\n    if options:\n      if not isinstance(options, Options):\n        raise RequestError('options must be an instance of Options')\n\n    if keep_alive:\n      if isinstance(keep_alive, str):\n        if not keep_alive.isdigit():\n          raise RequestError('keep_alive must be a float or None')\n\n        keep_alive = float(keep_alive)\n\n      if not isinstance(keep_alive, (float, int)):\n        raise RequestError('keep_alive must be a float or None')\n\n    if keep_alive and keep_alive < 0:\n      raise RequestError('keep_alive must be a positive float or None')\n\n    if keep_alive and keep_alive > 10000:\n      raise RequestError('keep_alive must be less than 10000')\n\n    if context:\n      if not isinstance(context, (list, tuple)):\n        raise RequestError('context must be a list or tuple')\n\n      if not all(isinstance(c, int) for c in context):\n        raise RequestError('context must be a list of integers')\n\n    if context and len(context) > 10:\n      raise RequestError('context must be less than 10')\n\n    params = {\n      'model': model,\n      'prompt': prompt,"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('Model must be provided')\n\n    if format not in ['', 'json']:\n      raise RequestError('Invalid format')\n\n    if not prompt and not system and not template:\n      raise RequestError('Prompt, system, or template must be provided')\n\n    if images and not isinstance(images, (list, tuple)):\n      raise RequestError('Images must be a list')\n\n    if images:\n      images = [\n        b64encode(\n          sha256(\n            image.read() if isinstance(image, io.IOBase) else image.encode()\n          ).digest()\n        ).decode()\n        for image in images\n      ]\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n      options.keep_alive = keep_alive\n\n    return self._request_stream(\n      'POST',\n      f'/generate/{model}',\n      json={\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n        'raw': raw,\n        'format': format,\n        'images': images,\n        'options': options.dict(),\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if not prompt and not system:\n      raise RequestError('prompt or system is required')\n\n    if not format and raw:\n      raise RequestError('raw cannot be used without format')\n\n    if images:\n      if not isinstance(images, (list, tuple)):\n        raise RequestError('images must be a list or tuple')\n\n      if not all(isinstance(image, (bytes, bytearray, memoryview, str)) for image in images):\n        raise RequestError('images must be a list of bytes, bytearray, memoryview, or str')\n\n      if len(images) > 1:\n        raise RequestError('images must be a list of length 1')\n\n    if options:\n      if not isinstance(options, Options):\n        raise RequestError('options must be an instance of Options')\n\n    if keep_alive:\n      if not isinstance(keep_alive, (float, str)):\n        raise RequestError('keep_alive must be a float, str, or None')\n\n    if isinstance(keep_alive, float):\n      if keep_alive < 0:\n        raise RequestError('keep_alive must be a positive float')\n\n    if isinstance(keep_alive, str):\n      if not keep_alive.isdigit():\n        raise RequestError('keep_alive must be a positive integer')\n\n      if int(keep_alive) < 0:\n        raise RequestError('keep_alive must be a positive integer')\n\n    data = {\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context or [],\n      'raw': raw,\n      'format': format,\n      'images': [b64encode(image).decode('utf-8') for image in images] if images else None,\n      'options': options.to_dict() if options else None,\n      'keep_alive': keep_alive,\n    }\n\n    return self._request_stream(\n      'POST',\n      f'/{model}',"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n    \"\"\"\n    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide either a path or modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide either a path or a modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide either a path or a modelfile')\n\n    if not modelfile:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide either a path or a modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide either a path or a modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either a path or a modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': b64encode(modelfile).decode('utf-8'),\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide a path or modelfile')\n\n    if modelfile:\n      return self._request_stream(\n        'POST',\n        '/api/create',\n        json={\n          'name': model,\n          'modelfile': modelfile,\n          'stream': stream,\n        },\n        stream=stream,\n      )\n\n    with open(path, 'rb') as f:\n      return self._request_stream(\n        'POST',\n        '/api/create',\n        files={'modelfile': f},\n        data={'name': model},\n        stream=stream,\n      )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either a path or a modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'path': path,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide a path or modelfile')\n\n    if path:\n      path = Path(path).resolve()\n      if not path.exists():\n        raise RequestError(f'path {path} does not exist')\n      if not path.is_file():\n        raise RequestError(f'path {path} is not a file')\n      modelfile = path.read_text()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide a model file or path')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide a path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      data={\n        'model': model,\n        'path': path,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either a path or a modelfile')\n\n    if path:\n      path = Path(path)\n      if not path.exists():\n        raise RequestError(f'path {path} does not exist')\n      if not path.is_file():\n        raise RequestError(f'path {path} is not a file')\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide either a path or a modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'model': model,\n        'path': path,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide a path or modelfile')\n\n    if modelfile:\n      if not isinstance(modelfile, str):\n        raise TypeError('modelfile must be a string')\n    else:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either a path or a modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      data={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide either a path or modelfile')\n\n    if not path and modelfile:\n      path = io.BytesIO(modelfile.encode('utf-8'))\n\n    if isinstance(path, str):\n      path = Path(path)\n\n    if isinstance(path, Path):\n      path = path.open('rb')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'stream': stream,\n      },\n      data={\n        'file': path,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide either a path or modelfile')\n\n    if not path and modelfile:\n      path = io.BytesIO(modelfile.encode())\n\n    if isinstance(path, str):\n      path = Path(path)\n\n    if isinstance(path, Path):\n      with path.open('rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      files={\n        'modelfile': modelfile,\n      },\n      data={\n        'name': model,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide either a path or a modelfile')\n\n    if path:\n      path = str(path)\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'model': model,\n        'path': path,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either a path or a modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not modelfile and not path:\n      raise RequestError('must provide either a path or a modelfile')\n\n    if modelfile:\n      files = {'file': io.BytesIO(modelfile.encode('utf-8'))}\n    else:\n      files = {'file': open(path, 'rb')}\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      data={'name': model},\n      files=files,\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n\n    with path.open('rb') as f:\n      checksum = sha256(f.read()).hexdigest()\n\n    response = self._client.head(f'/api/blobs/{checksum}')\n    if response.status_code == 404:\n      with path.open('rb') as f:\n        response = self._client.post('/api/blobs', data=f)\n      if response.status_code != 201:\n        raise ResponseError(response.text, response.status_code)\n\n    return f'sha256:{checksum}'\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n    checksum = sha256(path.read_bytes()).hexdigest()\n    digest = f'sha256:{checksum}'\n\n    try:\n      self._client.head(f'/api/blob/{digest}')\n      return digest\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code != 404:\n        raise ResponseError(e.response.text, e.response.status_code) from None\n\n    with path.open('rb') as f:\n      response = self._client.post('/api/blob', files={'file': f})\n\n    try:\n      response.raise_for_status()\n    except httpx.HTTPStatusError as e:\n      raise ResponseError(e.response.text, e.response.status_code) from None\n\n    return response.json()['digest']\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      checksum = sha256(f.read()).hexdigest()\n\n    response = self._client.head(f'/api/blob/{checksum}')\n    if response.status_code != 404:\n      return f'sha256:{checksum}'\n\n    with open(path, 'rb') as f:\n      response = self._client.post(\n        '/api/blob',\n        data=f,\n        headers={'Content-Type': 'application/octet-stream'},\n      )\n\n    return response.json()['digest']\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n    with path.open('rb') as f:\n      digest = sha256(f.read()).hexdigest()\n\n    response = self._client.head(f'/api/blobs/{digest}')\n    if response.status_code == 404:\n      with path.open('rb') as f:\n        response = self._client.post('/api/blobs', data=f)\n      response.raise_for_status()\n\n    return response.headers['Digest']\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = _as_path(path)\n\n    with path.open('rb') as f:\n      digest = sha256(f.read()).hexdigest()\n\n    response = self._client.head(f'/api/blob/{digest}')\n\n    if response.status_code == 404:\n      response = self._client.post(\n        '/api/blob',\n        headers={'Content-Type': 'application/octet-stream'},\n        data=path.read_bytes(),\n      )\n      response.raise_for_status()\n\n    return response.text\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      data = f.read()\n\n    checksum = sha256(data).hexdigest()\n    digest = f'sha256:{checksum}'\n\n    try:\n      self._client.head(f'/api/blobs/{digest}')\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code == 404:\n        self._client.post(\n          f'/api/blobs/{digest}',\n          content=data,\n        )\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      checksum = sha256(f.read()).hexdigest()\n\n    response = self._client.head(f'/api/blob/{checksum}')\n    if response.status_code == 404:\n      with open(path, 'rb') as f:\n        response = self._client.post('/api/blob', data=f)\n      if response.status_code != 201:\n        raise ResponseError(response.text, response.status_code)\n\n    return response.headers['Digest']\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n    digest = sha256(path.read_bytes()).hexdigest()\n    if (response := self._client.head(f'/api/blobs/{digest}')).status_code == 404:\n      response = self._client.post(\n        '/api/blobs',\n        data=path.read_bytes(),\n        headers={'Content-Type': 'application/octet-stream', 'Digest': f'sha256:{digest}'},\n      )\n      response.raise_for_status()\n    return response.headers['Digest']\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n\n    if not path.exists():\n      raise FileNotFoundError(f'{path} does not exist')\n\n    checksum = sha256(path.read_bytes()).hexdigest()\n    digest = f'sha256:{checksum}'\n\n    response = self._client.head(f'/api/blob/{digest}')\n    if response.status_code == 404:\n      response = self._client.post(\n        '/api/blob',\n        files={'file': path.open('rb')},\n      )\n      response.raise_for_status()\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = _as_path(path)\n    if not path.exists():\n      raise FileNotFoundError(f'file not found: {path}')\n\n    checksum = sha256(path.read_bytes()).hexdigest()\n    digest = f'sha256:{checksum}'\n\n    try:\n      self._client.head(f'/api/blob/{digest}')\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code != 404:\n        raise ResponseError(e.response.text, e.response.status_code) from None\n\n      with path.open('rb') as f:\n        self._client.post(f'/api/blob/{digest}', content=f)\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n    checksum = sha256(path.read_bytes()).hexdigest()\n\n    if (response := self._client.head(f'/api/blobs/{checksum}')) and response.status_code == 404:\n      response = self._client.post(\n        '/api/blobs',\n        data=path.read_bytes(),\n        headers={'Digest': f'sha256:{checksum}'},\n      )\n      response.raise_for_status()\n\n    return response.headers['Digest']\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      digest = sha256(f.read()).hexdigest()\n\n    try:\n      self._client.head(f'/api/blob/{digest}')\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code != 404:\n        raise ResponseError(e.response.text, e.response.status_code) from None\n\n      with open(path, 'rb') as f:\n        response = self._client.post(f'/api/blob/{digest}', data=f)\n        response.raise_for_status()\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path).expanduser()\n    if not path.exists():\n      raise FileNotFoundError(f'{path} does not exist')\n\n    with path.open('rb') as f:\n      checksum = sha256(f.read()).hexdigest()\n\n    try:\n      self._client.head(f'/api/blobs/{checksum}')\n      return f'sha256:{checksum}'\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code != 404:\n        raise\n\n    with path.open('rb') as f:\n      response = self._client.post('/api/blobs', data=f)\n      response.raise_for_status()\n\n    return f'sha256:{checksum}'\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    checksum = sha256()\n    with open(path, 'rb') as f:\n      for chunk in iter(lambda: f.read(4096), b''):\n        checksum.update(chunk)\n    digest = checksum.hexdigest()\n\n    # Check if the blob already exists on the server\n    response = self._client.head(f'/api/blob/{digest}')\n    if response.status_code == 404:\n      # Upload the file as a new blob\n      with open(path, 'rb') as f:\n        response = self._client.post('/api/blob', data=f)\n      response.raise_for_status()\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Open the file and calculate its SHA-256 checksum\n    with open(path, 'rb') as f:\n      checksum = sha256(f.read()).hexdigest()\n\n    # Check if the blob already exists on the server\n    response = self._client.head(f'/api/blobs/{checksum}')\n    if response.status_code == 200:\n      return checksum\n\n    # Upload the file as a new blob\n    with open(path, 'rb') as f:\n      response = self._client.post(f'/api/blobs/{checksum}', content=f.read())\n\n    # Raise an error if the upload failed\n    response.raise_for_status()\n\n    # Return the digest of the file\n    return checksum\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path).expanduser()\n    if not path.exists():\n      raise FileNotFoundError(f'file not found: {path}')\n\n    with path.open('rb') as f:\n      checksum = sha256(f.read()).hexdigest()\n\n    if response := self._client.head(f'/api/blobs/{checksum}'):\n      if response.status_code == 404:\n        with path.open('rb') as f:\n          response = self._client.post('/api/blobs', files={'file': f})\n          response.raise_for_status()\n      else:\n        response.raise_for_status()\n\n    return checksum\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n\n    with path.open('rb') as f:\n      checksum = sha256(f.read()).hexdigest()\n\n    url = f'/api/blobs/{checksum}'\n\n    try:\n      self._client.head(url)\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code != 404:\n        raise ResponseError(e.response.text, e.response.status_code) from None\n\n      with path.open('rb') as f:\n        response = self._client.post(url, content=f.read())\n\n      try:\n        response.raise_for_status()\n      except httpx.HTTPStatusError as e:\n        raise ResponseError(e.response.text, e.response.status_code) from None\n\n    return checksum\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n    if not path.is_file():\n      raise RequestError(f'{path} is not a file')\n\n    checksum = sha256(path.read_bytes()).hexdigest()\n    digest = f'sha256:{checksum}'\n\n    response = self._client.head(f'/api/blob/{digest}')\n    if response.status_code == 404:\n      response = self._client.post(\n        f'/api/blob/{digest}',\n        content=path.read_bytes(),\n      )\n      response.raise_for_status()\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = _as_path(path)\n    checksum = sha256(path.read_bytes()).hexdigest()\n\n    try:\n      self._client.head(f'/api/blob/{checksum}')\n      return checksum\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code != 404:\n        raise ResponseError(e.response.text, e.response.status_code) from None\n\n    with path.open('rb') as f:\n      response = self._client.post(\n        '/api/blob',\n        files={\n          'file': (path.name, f),\n        },\n      )\n\n    try:\n      response.raise_for_status()\n    except httpx.HTTPStatusError as e:\n      raise ResponseError(e.response.text, e.response.status_code) from None\n\n    return response.json()['digest']\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      data = f.read()\n\n    digest = sha256(data).hexdigest()\n\n    try:\n      self._client.head(f'/api/blobs/{digest}')\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code != 404:\n        raise ResponseError(e.response.text, e.response.status_code) from None\n\n      response = self._client.post(f'/api/blobs/{digest}', content=data)\n      response.raise_for_status()\n\n    return digest\n\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns an asynchronous `ProgressResponse` generator.\n    \"\"\"\n    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns an asynchronous `ProgressResponse` generator.\n    \"\"\"\n    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns an asynchronous `ProgressResponse` generator.\n    \"\"\"\n    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n    \"\"\"\n    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    \"\"\"\n    Create a chat response using the requested model.\n\n    Raises `RequestError` if a model is not provided.\n\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ChatResponse` if `stream` is `False`, otherwise returns an asynchronous `ChatResponse` generator.\n    \"\"\"\n\n    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    \"\"\"\n    Create a chat response using the requested model.\n\n    Raises `RequestError` if a model is not provided.\n\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ChatResponse` if `stream` is `False`, otherwise returns an asynchronous `ChatResponse` generator.\n    \"\"\"\n\n    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n    \"\"\"\n    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n    \"\"\"\n    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n    \"\"\"\n    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n    \"\"\"\n    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n    \"\"\"\n    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + cls.CODE_SPLITTER + test_code\n\n        # Write code to a temporary file\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            f.seek(0)\n            file_path = f.name\n\n        # Run Pyright with the temporary file\n        try:\n            output = subprocess.check_output(\n                [\"pyright\", file_path],\n                stderr=subprocess.STDOUT,\n            )\n        except subprocess.CalledProcessError as e:\n            output = e.output\n\n        # Parse the Pyright output\n        lines = output.decode(\"utf-8\").splitlines()\n        errors = []\n        for line in lines:\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                errors.append((line_no, message))\n\n        # Check if there are any expected errors\n        expected_errors = []\n        for line_no, message in errors:\n            if cls.EXPECT_ERROR_COMMENT in message:\n                expected_errors.append(line_no)\n\n        # Check if there are any unexpected errors\n        unexpected_errors = []\n        for line_no, message in errors:\n            if cls.EXPECT_ERROR_COMMENT not in message:\n                unexpected_errors.append(line_no)\n\n        # Check if there are any errors\n        if unexpected_errors:\n            return TypeCheckResult(\n                message=f\"Unexpected errors on lines {unexpected_errors}\",\n                passed=False,\n                debug_info={\"errors\": errors},\n            )\n\n        # Check if there are any expected errors\n        if expected_errors:\n            return TypeCheckResult(\n                message=f\"Expected errors on lines {expected_errors}\",\n                passed=False,\n                debug_info={\"errors"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", encoding=\"utf-8\") as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n\n            # Run Pyright on the file\n            output = subprocess.check_output(\n                [\"pyright\", f.name],\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n            )\n\n        # Parse the output to extract expected error messages and line numbers\n        expected_errors = {}\n        for line in output.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors[line_no] = message\n\n        # Check if the type check passed\n        passed = True\n        if expected_errors:\n            passed = False\n\n        return TypeCheckResult(\n            message=output,\n            passed=passed,\n            debug_info={\"expected_errors\": expected_errors},\n        )\n\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir = Path(temp_dir)\n            user_code_file = temp_dir / \"user_code.py\"\n            test_code_file = temp_dir / \"test_code.py\"\n            user_code_file.write_text(user_code, encoding=\"utf-8\")\n            test_code_file.write_text(test_code, encoding=\"utf-8\")\n\n            # Run pyright\n            pyright_output = subprocess.run(\n                [\"pyright\", \"--outputjson\", str(temp_dir / \"pyright_output.json\")],\n                capture_output=True,\n                text=True,\n                cwd=temp_dir,\n            )\n\n            if pyright_output.returncode != 0:\n                return TypeCheckResult(\n                    message=f\"Pyright exited with code {pyright_output.returncode}.\\n\"\n                    f\"Stderr:\\n{pyright_output.stderr}\",\n                    passed=False,\n                )\n\n            # Parse pyright output\n            pyright_output_json = temp_dir / \"pyright_output.json\"\n            if not pyright_output_json.exists():\n                return TypeCheckResult(\n                    message=f\"Pyright output file {pyright_output_json} does not exist.\",\n                    passed=False,\n                )\n\n            # Extract expected error lines\n            expected_error_lines = set()\n            with io.StringIO(pyright_output_json.read_text(encoding=\"utf-8\")) as f:\n                for token in tokenize.generate_tokens(f.readline):\n                    if token.string == cls.EXPECT_ERROR_COMMENT:\n                        expected_error_lines.add(token.start[0])\n\n            # Extract actual error lines\n            actual_error_lines = set()\n            for line in pyright_output.stdout.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Write user code and test code to temporary files\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            user_file = f.name\n\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(test_code)\n            test_file = f.name\n\n        # Run Pyright on the combined file\n        try:\n            output = subprocess.check_output(\n                [\"pyright\", \"--outputjson\", user_file, test_file],\n                stderr=subprocess.STDOUT,\n                encoding=\"utf-8\",\n            )\n        except subprocess.CalledProcessError as e:\n            output = e.output\n\n        # Parse Pyright output to extract expected error messages and line numbers\n        expected_errors = []\n        for line in output.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                expected_errors.append((line_no, message))\n\n        # Read the user code to extract the expected error lines\n        with open(user_file, \"r\", encoding=\"utf-8\") as f:\n            user_code_lines = f.readlines()\n\n        # Determine which lines have expected errors\n        expected_error_lines = set()\n        for line_no, message in expected_errors:\n            if cls.EXPECT_ERROR_COMMENT in message:\n                expected_error_lines.add(line_no)\n\n        # Check if the type check passed\n        passed = True\n        if expected_error_lines:\n            passed = False\n\n        # Remove temporary files\n        Path(user_file).unlink()\n        Path(test_file).unlink()\n\n        return TypeCheckResult(\n            message=\"Type check passed\" if passed else \"Type check failed\",\n            passed=passed,"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", encoding=\"utf-8\", delete=False\n        ) as f:\n            f.write(user_code)\n            user_code_path = Path(f.name)\n\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", encoding=\"utf-8\", delete=False\n        ) as f:\n            f.write(test_code)\n            test_code_path = Path(f.name)\n\n        try:\n            # Run pyright with the user code and test code\n            output = subprocess.check_output(\n                [\"pyright\", user_code_path, test_code_path],\n                stderr=subprocess.STDOUT,\n                encoding=\"utf-8\",\n            )\n        except subprocess.CalledProcessError as e:\n            # If pyright fails, return a failed result with the error message\n            return TypeCheckResult(\n                message=f\"Pyright failed with exit code {e.returncode}\",\n                passed=False,\n                debug_info={\"output\": e.output},\n            )\n\n        # Parse the output of pyright to extract the expected type errors\n        expected_errors = cls._parse_expected_errors(output)\n\n        # If there are no expected errors, return a passed result\n        if not expected_errors:\n            return TypeCheckResult(message=\"No expected type errors found\", passed=True)\n\n        # Otherwise, return a failed result with the expected errors\n        return TypeCheckResult(\n            message=\"Type check failed\",\n            passed=False,\n            debug_info={\"expected_errors\": expected_errors},\n        )\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as temp_dir:\n            # Write user code to a temporary file\n            user_code_file = Path(temp_dir) / \"user_code.py\"\n            user_code_file.write_text(user_code, encoding=\"utf-8\")\n\n            # Write test code to a temporary file\n            test_code_file = Path(temp_dir) / \"test_code.py\"\n            test_code_file.write_text(test_code, encoding=\"utf-8\")\n\n            # Run Pyright to perform type checking\n            process = subprocess.Popen(\n                [\n                    \"pyright\",\n                    \"--outputjson\",\n                    \"--pythonversion\",\n                    \"3.10\",\n                    \"--pythonsearchpath\",\n                    temp_dir,\n                    str(user_code_file),\n                    str(test_code_file),\n                ],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n            )\n            stdout, stderr = process.communicate()\n\n            # Parse Pyright output to extract error messages\n            error_messages = []\n            for line in stdout.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no = int(match.group(1))\n                    message = match.group(2)\n                    error_messages.append((line_no, message))\n\n        # Check if there are any expected errors in the user code\n        expected_error_lines = set()\n        for line_no, line in tokenize.tokenize(io.BytesIO(user_code.encode(\"utf-8\")).readline):\n            if line.startswith(cls.EXPECT_ERROR_COMMENT):\n                expected_error_lines.add(line_no)\n\n        # Determine if the type check passed or failed based on the expected errors and actual errors\n        passed = expected_error_lines == set(line_no for line_no, _ in error_messages)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as temp_dir:\n            # Write the user code to a file\n            user_code_file = Path(temp_dir) / \"user_code.py\"\n            user_code_file.write_text(user_code, encoding=\"utf-8\")\n\n            # Write the test code to a file\n            test_code_file = Path(temp_dir) / \"test_code.py\"\n            test_code_file.write_text(test_code, encoding=\"utf-8\")\n\n            # Run the type check using Pyright\n            try:\n                output = subprocess.check_output(\n                    [\n                        \"pyright\",\n                        \"--outputjson\",\n                        \"--pythonversion\",\n                        \"3.11\",\n                        \"--pythonpath\",\n                        temp_dir,\n                        user_code_file,\n                        test_code_file,\n                    ],\n                    stderr=subprocess.STDOUT,\n                )\n            except subprocess.CalledProcessError as e:\n                # Pyright returns non-zero exit code when there are type errors\n                # We need to handle this case\n                output = e.output\n\n            # Parse the output to extract the line numbers of expected type errors\n            expected_error_lines = set()\n            for line in output.decode(\"utf-8\").splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_number = int(match.group(1))\n                    message = match.group(2)\n                    if cls.EXPECT_ERROR_COMMENT in message:\n                        expected_error_lines.add(line_number)\n\n            # Check if the type check passed or failed\n            if expected_error_lines:\n                passed = False\n                message = f\"Type check failed on lines {expected_error_lines}\"\n            else:\n                passed = True\n                message = \"Type check passed\"\n\n            return TypeCheckResult(message=message, passed=passed)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as tmpdir:\n            tmpdir = Path(tmpdir)\n            user_file = tmpdir / \"user.py\"\n            test_file = tmpdir / \"test.py\"\n            user_file.write_text(user_code)\n            test_file.write_text(test_code)\n\n            # Run pyright with the user file and test file\n            pyright_output = subprocess.check_output(\n                [\"pyright\", \"--outputjson\", user_file, test_file],\n                encoding=\"utf-8\",\n                stderr=subprocess.STDOUT,\n            )\n\n            # Parse the pyright output\n            errors = []\n            for line in pyright_output.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no = int(match.group(1))\n                    message = match.group(2)\n                    errors.append((line_no, message))\n\n            # Check if there are any expected type errors\n            expected_errors = [\n                (line_no, message)\n                for line_no, message in errors\n                if cls.EXPECT_ERROR_COMMENT in message\n            ]\n\n            # Check if there are any unexpected type errors\n            unexpected_errors = [\n                (line_no, message)\n                for line_no, message in errors\n                if cls.EXPECT_ERROR_COMMENT not in message\n            ]\n\n            # If there are any expected type errors, check if they match the expected errors\n            if expected_errors:\n                passed = len(expected_errors) == len(\n                    [\n                        (line_no, message)\n                        for line_no, message in expected_errors\n                        if message.startswith(\"error:\")\n                    ]\n                )\n            else:\n                passed = True\n\n            # If there are no expected type errors, check if there are any unexpected type errors\n            if not expected_errors and unexpected_errors:\n                passed = False\n\n            # If there are no expected type errors and no unexpected type errors,"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        combined_code = user_code + test_code\n\n        # Create a temporary file to write the combined code to\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(combined_code)\n            f.flush()\n            combined_file_path = f.name\n\n        # Run Pyright to perform the type check\n        result = subprocess.run(\n            [\"pyright\", combined_file_path],\n            capture_output=True,\n            text=True,\n            check=False,\n        )\n\n        # Parse the Pyright output to extract the expected error messages\n        expected_error_lines = set()\n        expected_error_messages = []\n        for line in result.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_error_lines.add(line_no)\n                    expected_error_messages.append(message)\n\n        # Determine if the type check passed or failed\n        passed = not expected_error_lines\n        if passed:\n            message = \"All tests passed!\"\n        else:\n            message = \"Type check failed on the following lines:\\n\" + \"\\n\".join(\n                expected_error_messages\n            )\n\n        # Return the result\n        return TypeCheckResult(message=message, passed=passed)\n\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir = Path(temp_dir)\n            user_code_file = temp_dir / \"user_code.py\"\n            test_code_file = temp_dir / \"test_code.py\"\n            user_code_file.write_text(user_code, encoding=\"utf-8\")\n            test_code_file.write_text(test_code, encoding=\"utf-8\")\n\n            # Run Pyright on the combined code\n            pyright_output = subprocess.run(\n                [\"pyright\", \"--outputjson\", str(temp_dir / \"pyright.json\")],\n                capture_output=True,\n                text=True,\n                check=True,\n            )\n\n            # Parse the output to find expected type errors\n            expected_error_lines = set()\n            for line in pyright_output.stdout.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no = int(match.group(1))\n                    message = match.group(2)\n                    if cls.EXPECT_ERROR_COMMENT in message:\n                        expected_error_lines.add(line_no)\n\n            # Read the combined code to get the expected error messages\n            expected_error_messages = []\n            with io.StringIO() as combined_code:\n                for file in (user_code_file, test_code_file):\n                    with open(file, encoding=\"utf-8\") as f:\n                        for line in f:\n                            combined_code.write(line)\n                combined_code.seek(0)\n                for i, token in enumerate(tokenize.generate_tokens(combined_code.readline)):\n                    if i + 1 in expected_error_lines:\n                        expected_error_messages.append(token.string)\n\n            # Return the result of the type check\n            return TypeCheckResult(\n                message=\"\\n\".join(expected_error_messages),\n                passed=not expected_error_messages,"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir = Path(temp_dir)\n            user_code_file = temp_dir / \"user_code.py\"\n            test_code_file = temp_dir / \"test_code.py\"\n\n            # Write the user code and test code to separate files\n            user_code_file.write_text(user_code, encoding=\"utf-8\")\n            test_code_file.write_text(test_code, encoding=\"utf-8\")\n\n            # Run Pyright with the user code and test code files\n            pyright_output = subprocess.run(\n                [\"pyright\", \"--outputjson\", user_code_file, test_code_file],\n                capture_output=True,\n                text=True,\n            ).stdout\n\n            # Parse the Pyright output to extract error messages and line numbers\n            errors = []\n            for line in pyright_output.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no = int(match.group(1))\n                    message = match.group(2)\n                    errors.append((line_no, message))\n\n            # Determine if the type check passed or failed based on the presence of expected type errors\n            passed = not any(cls.EXPECT_ERROR_COMMENT in msg for _, msg in errors)\n\n            # Return a TypeCheckResult object with the result of the type check\n            return TypeCheckResult(\n                message=\"Type check passed\" if passed else \"Type check failed\",\n                passed=passed,\n                debug_info={\"errors\": errors},\n            )\n\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n            temp_file_path = Path(f.name)\n\n        # Run Pyright with the temporary file as input\n        pyright_output = subprocess.run(\n            [\"pyright\", \"--outputjson\", temp_file_path],\n            capture_output=True,\n            text=True,\n        )\n\n        # Remove the temporary file\n        temp_file_path.unlink()\n\n        # Parse the Pyright output and identify lines with expected type errors\n        expected_errors = set()\n        error_lines = set()\n        for line in pyright_output.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors.add(line_no)\n                else:\n                    error_lines.add(line_no)\n\n        # Determine if the type check passed or failed\n        passed = not error_lines and not expected_errors\n        if passed:\n            message = \"Congratulations, your code is correct!\"\n        else:\n            error_lines = sorted(error_lines)\n            expected_errors = sorted(expected_errors)\n            message = (\n                \"Your code has type errors, please fix them before submitting.\"\n                if error_lines\n                else \"Your code has unexpected type errors, please check your code.\"\n            )\n\n        # Return the result as a TypeCheckResult object\n        return TypeCheckResult(\n            message=message,\n            passed=passed,\n            debug_info={\"error_lines\": error_lines, \"expected_errors\": expected_errors},\n        )\n\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.seek(0)\n            filename = f.name\n\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.write(f\"# {cls.EXPECT_ERROR_COMMENT}\\n\")\n            f.seek(0)\n            filename_with_expect_error = f.name\n\n        try:\n            # Run pyright with the combined code and capture the output\n            output = subprocess.check_output(\n                [\"pyright\", \"--noEmit\", filename],\n                stderr=subprocess.STDOUT,\n            )\n            output_with_expect_error = subprocess.check_output(\n                [\"pyright\", \"--noEmit\", filename_with_expect_error],\n                stderr=subprocess.STDOUT,\n            )\n        except subprocess.CalledProcessError as e:\n            # If pyright exits with a non-zero exit code, return a failure result with the error message\n            return TypeCheckResult(\n                message=f\"pyright exited with code {e.returncode}\",\n                passed=False,\n                debug_info={\"stdout\": e.stdout, \"stderr\": e.stderr},\n            )\n\n        # Parse the output to extract the line numbers of expected errors\n        expected_error_lines = set()\n        for line in output_with_expect_error.decode(\"utf-8\").split(\"\\n\"):\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match and cls.EXPECT_ERROR_COMMENT in match.group(2):\n                expected_error_lines.add(int(match.group(1)))\n\n        # Parse the output to extract the line numbers of actual errors\n        actual_error_lines = set"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            file_path = f.name\n\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(f\"# @{cls.EXPECT_ERROR_COMMENT}\\n\")\n            f.write(test_code)\n            f.write(f\"# @{cls.EXPECT_ERROR_COMMENT}\\n\")\n            f.write(user_code)\n            f.write(test_code)\n            f.write(f\"# @{cls.EXPECT_ERROR_COMMENT}\\n\")\n            f.write(user_code)\n            error_file_path = f.name\n\n        try:\n            output = subprocess.check_output(\n                [\n                    \"pyright\",\n                    \"--outputjson\",\n                    \"--pythonversion\",\n                    \"3.11\",\n                    \"--files\",\n                    file_path,\n                    \"--files\",\n                    error_file_path,\n                ],\n                stderr=subprocess.STDOUT,\n            )\n        except subprocess.CalledProcessError as e:\n            output = e.output\n\n        # Remove temporary files\n        Path(file_path).unlink()\n        Path(error_file_path).unlink()\n\n        errors = []\n        for line in output.decode(\"utf-8\").splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                error = match.group(2)\n                errors.append((line_no, error))\n\n        # Sort errors by line number\n        errors.sort(key=lambda x: x[0])\n\n        # Find the first error that is not in the user code\n        first_error = None\n        for line_no, error in errors:\n            if line_no < len(user_code"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.flush()\n\n            # Run Pyright\n            pyright_output = subprocess.run(\n                [\"pyright\", \"--outputjson\", f.name],\n                capture_output=True,\n                text=True,\n                check=True,\n            )\n\n            # Parse Pyright output\n            errors = []\n            for line in pyright_output.stdout.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_number, error_message = match.groups()\n                    errors.append((int(line_number), error_message))\n\n            # Identify lines with expected type errors\n            expected_error_lines = set()\n            for line_number, error_message in errors:\n                if cls.EXPECT_ERROR_COMMENT in error_message:\n                    expected_error_lines.add(line_number)\n\n            # Check if type check passed\n            passed = True\n            for line_number, error_message in errors:\n                if line_number not in expected_error_lines:\n                    passed = False\n                    break\n\n        # Return result\n        return TypeCheckResult(\n            message=\"Type check passed\", passed=passed, debug_info={\"errors\": errors}\n        )\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Write the user code and test code to temporary files\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            user_code_file = f.name\n\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(test_code)\n            test_code_file = f.name\n\n        # Run Pyright to perform the type check\n        command = [\n            \"pyright\",\n            \"--pythonversion\",\n            \"3.11\",\n            \"--ignoreExternalModules\",\n            \"--noEmit\",\n            \"--noImplicitAny\",\n            \"--strict\",\n            \"--output\",\n            \"json\",\n            user_code_file,\n            test_code_file,\n        ]\n        result = subprocess.run(\n            command, capture_output=True, text=True, check=False\n        )\n\n        # Check if the type check passed\n        passed = result.returncode == 0\n        message = \"\"\n\n        # If the type check failed, parse the error messages\n        if not passed:\n            # Parse the Pyright output to extract the expected error messages and line numbers\n            expected_errors = defaultdict(list)\n            for line in result.stderr.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no = int(match.group(1))\n                    error_message = match.group(2)\n                    if cls.EXPECT_ERROR_COMMENT in error_message:\n                        expected_errors[line_no].append(error_message)\n\n            # If there are expected errors, check if the actual errors match\n            if expected_errors:\n                # Parse the Pyright output to extract the actual error messages and line numbers\n                actual_errors = defaultdict(list)\n                for line in result.stdout.splitlines():\n                    match = re.match(cls.PYRIGHT_MESSAGE_"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n\n            result = subprocess.run(\n                [\"pyright\", \"--outputjson\", f.name],\n                capture_output=True,\n                encoding=\"utf-8\",\n            )\n\n        # The output of pyright is a JSON string.\n        # We need to parse it to get the error messages.\n        # Here we use a regex to capture the error messages and line numbers.\n        # The regex matches the error message and line number, and returns them as a tuple.\n        # The regex is defined as a class attribute, and is used in the _type_check_with_pyright method.\n        # The result of the regex is then stored in a dictionary, with the line number as the key and the error message as the value.\n        # The dictionary is then used to create a TypeCheckResult object, which is returned by the function.\n\n        # The regex is defined as a class attribute, and is used in the _type_check_with_pyright method.\n        # The result of the regex is then stored in a dictionary, with the line number as the key and the error message as the value.\n        # The dictionary is then used to create a TypeCheckResult object, which is returned by the function.\n        # The regex is defined as a class attribute, and is used in the _type_check_with_pyright method.\n        # The result of the regex is then stored in a dictionary, with the line number as the key and the error message as the value.\n        # The dictionary is then used to create a TypeCheckResult object, which is returned by the function.\n        # The regex is defined as a class attribute, and is used in the _type_check_with_pyright method.\n        # The result of the regex is then stored in a dictionary, with the line number as the key and the error message as the value.\n        # The dictionary is then used to create a TypeCheckResult object, which is returned by the function.\n        # The regex is defined as a class attribute"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", encoding=\"utf-8\", delete=False\n        ) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n            # Run pyright\n            result = subprocess.run(\n                [\"pyright\", f.name],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                encoding=\"utf-8\",\n            )\n\n        # Parse the output\n        errors = []\n        expected_errors = set()\n        for line in result.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors.add(line_no)\n                else:\n                    errors.append(line)\n\n        # Check if there are any expected errors\n        passed = len(expected_errors) == 0\n        if not passed:\n            # If there are expected errors, check if all expected errors are present\n            for line_no in expected_errors:\n                if line_no not in [int(line.split(\":\")[1]) for line in errors]:\n                    passed = False\n                    break\n\n        return TypeCheckResult(\n            message=\"\\n\".join(errors),\n            passed=passed,\n            debug_info={\"pyright_output\": result.stdout},\n        )\n\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code into a single file\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            file_path = f.name\n\n        # Run Pyright type checker\n        try:\n            output = subprocess.check_output(\n                [\"pyright\", \"--noEmit\", file_path],\n                stderr=subprocess.STDOUT,\n                text=True,\n            )\n        except subprocess.CalledProcessError as e:\n            # Pyright returns 1 if there are errors\n            return TypeCheckResult(\n                message=f\"Pyright error: {e.output}\",\n                passed=False,\n            )\n\n        # Parse Pyright output to find expected type errors\n        expected_errors = []\n        for line in output.splitlines():\n            match = re.search(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no, message = match.groups()\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors.append(int(line_no))\n\n        # Check if all expected errors are present\n        passed = len(expected_errors) == 0\n        if not passed:\n            message = f\"Expected type errors on lines {expected_errors}\"\n        else:\n            message = \"Type check passed\"\n\n        return TypeCheckResult(\n            message=message,\n            passed=passed,\n            debug_info={\"pyright_output\": output},\n        )\n\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", encoding=\"utf-8\") as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n            process = subprocess.run(\n                [\"pyright\", f.name],\n                capture_output=True,\n                text=True,\n            )\n\n        # Extract the expected error lines from the output\n        expected_error_lines = set()\n        for line in process.stderr.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_number = int(match.group(1))\n                if cls.EXPECT_ERROR_COMMENT in line:\n                    expected_error_lines.add(line_number)\n\n        # Check if the type check passed or failed\n        passed = True\n        if expected_error_lines:\n            passed = False\n\n        # Create a TypeCheckResult object with the result and error messages\n        return TypeCheckResult(\n            message=process.stdout,\n            passed=passed,\n            debug_info={\"expected_error_lines\": expected_error_lines},\n        )\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn())\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn())\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_creator=get_compiler_fn())\n    else:\n        return aot_function(fn, compiler_creator=get_compiler_fn())"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, forward_compiler=get_compiler_fn(\"Forward Compiler\"),\n                          backward_compiler=get_compiler_fn(\"Backward Compiler\"))\n    else:\n        return aot_function(fn, forward_compiler=get_compiler_fn(\"Forward Compiler\"),\n                            backward_compiler=get_compiler_fn(\"Backward Compiler\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_creator=get_compiler_fn())\n    else:\n        return aot_function(fn, compiler_creator=get_compiler_fn())\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, get_compiler_fn(title=\"Function\"))\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn())\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn())\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, get_compiler_fn(\"Module Compiled\"))\n    else:\n        return aot_function(fn, get_compiler_fn(\"Function Compiled\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, forward_compiler=get_compiler_fn(\"Forward Compiler\"),\n                          backward_compiler=get_compiler_fn(\"Backward Compiler\"))\n    else:\n        return aot_function(fn, forward_compiler=get_compiler_fn(\"Forward Compiler\"),\n                            backward_compiler=get_compiler_fn(\"Backward Compiler\"))\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_creator=get_compiler_fn(\"Module\"))\n    else:\n        return aot_function(fn, compiler_creator=get_compiler_fn(\"Function\"))\n\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_creator=get_compiler_fn(title=\"Module\"))\n    elif callable(fn):\n        return aot_function(fn, compiler_creator=get_compiler_fn(title=\"Function\"))\n    else:\n        raise ValueError(\"Input argument is not a module or function.\")\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, example_inputs=[torch.randn(1, 3, 224, 224)],\n                          compiler_fn=get_compiler_fn(\"Module\"))\n    else:\n        return aot_function(fn, example_inputs=[torch.randn(1, 3, 224, 224)],\n                            compiler_fn=get_compiler_fn(\"Function\"))\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    elif callable(fn):\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))\n    else:\n        raise ValueError(\"Input argument must be a torch.nn.Module or function\")\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn())\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn())\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(\"Module\"))\n    elif callable(fn):\n        return aot_function(fn, compiler_fn=get_compiler_fn(\"Function\"))\n    else:\n        raise ValueError(\"Input argument must be a torch.nn.Module or a function.\")\n\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, example_inputs=(torch.randn(1, 3, 224, 224),), compiler_fn=get_compiler_fn(\"Module\"))\n    else:\n        return aot_function(fn, example_inputs=(torch.randn(1, 3, 224, 224),), compiler_fn=get_compiler_fn(\"Function\"))\n\n\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    best_config_df = summary_df[summary_df['best_execution_time'] == summary_df['best_execution_time'].min()]\n    best_config_df = best_config_df.reset_index(drop=True)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml')))\n    best_config_dict = summary_df_to_yaml(best_config_df, config_dict)\n\n    if output_path:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config_dict, f)\n\n    return best_config_dict\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load summary file\n    summary_df = load_summary_file(trial_path)\n\n    # Load config file\n    config_path = os.path.join(trial_path, 'config.yaml')\n    with open(config_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    # Convert summary to yaml\n    yaml_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    # Save yaml file\n    if output_path:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            output_path += '.yaml'\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(yaml_dict, f)\n\n    return yaml_dict\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml')))\n\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    best_config = summary_df.iloc[0]\n    best_config_dict = best_config.to_dict()\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml')))\n    best_config_dict['strategy'] = config_dict['strategy']\n    best_config_dict['node_lines'] = summary_df_to_yaml(summary_df, config_dict)\n    if output_path:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config_dict, f)\n    return best_config_dict\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    summary_df = summary_df.sort_values(by=['best_execution_time'])\n    best_config = summary_df.iloc[0].to_dict()\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('The output file path must end with .yaml or .yml if provided.')\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml'), 'r'))\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path:\n        output_path = output_path.replace('.yaml', '')\n        output_path = output_path.replace('.yml', '')\n        output_path = f'{output_path}.yaml'\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read summary file\n    summary_df = load_summary_file(trial_path)\n\n    # Read config file\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    # Convert summary dataframe to yaml\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    # Save to file\n    if output_path:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load summary file\n    summary_df = load_summary_file(os.path.join(trial_path, \"summary.csv\"))\n\n    # Load config file\n    with open(os.path.join(trial_path, \"config.yaml\"), \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    # Convert summary dataframe to yaml\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    # Save best config to yaml file\n    if output_path is not None:\n        if not output_path.endswith((\".yaml\", \".yml\")):\n            raise ValueError(\"Output path must end with .yaml or .yml\")\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config_dict, f)\n\n    return best_config_dict\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml')))\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_path = os.path.join(trial_path, 'config.yaml')\n    with open(config_path) as f:\n        config_dict = yaml.safe_load(f)\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary.csv file from the trial directory\n    summary_df = load_summary_file(trial_path)\n\n    # Load the config.yaml file from the trial directory\n    config_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    # Extract the optimal pipeline configuration from the summary and config files\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    # Save the optimal pipeline configuration to a YAML file if the output_path is specified\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config_dict, f)\n\n    return best_config_dict\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_file_path = os.path.join(trial_path, 'summary.csv')\n    config_file_path = os.path.join(trial_path, 'config.yaml')\n\n    summary_df = load_summary_file(summary_file_path)\n    config_dict = yaml.safe_load(open(config_file_path))\n\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            output_path += '.yaml'\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    best_config_df = summary_df[summary_df['best_execution_time'] == summary_df['execution_time']]\n    if len(best_config_df) == 0:\n        raise ValueError(f'No best configuration found in {trial_path}')\n    best_config_df = best_config_df.drop_duplicates(subset=['node_line_name', 'node_type'])\n    best_config_df = best_config_df[['node_line_name', 'node_type', 'best_module_filename',\n                                     'best_module_name', 'best_module_params', 'best_execution_time']]\n\n    config_file = os.path.join(trial_path, 'config.yaml')\n    with open(config_file, 'r') as f:\n        config_dict = yaml.safe_load(f)\n    best_config_dict = summary_df_to_yaml(best_config_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('Output path must have .yaml or .yml extension')\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config_dict, f)\n\n    return best_config_dict\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    best_config = summary_df.iloc[0]\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml')))\n    best_config_dict = summary_df_to_yaml(best_config.to_frame().T, config_dict)\n\n    if output_path is not None:\n        if output_path.endswith('.yaml') or output_path.endswith('.yml'):\n            with open(output_path, 'w') as f:\n                yaml.dump(best_config_dict, f)\n        else:\n            raise ValueError(f'Invalid file extension for output_path: {output_path}. '\n                             f'Please provide a file path with .yaml or .yml extension.')\n\n    return best_config_dict\n\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary file\n    summary_file_path = os.path.join(trial_path, 'summary.csv')\n    summary_df = load_summary_file(summary_file_path)\n\n    # Load the config file\n    config_file_path = os.path.join(trial_path, 'config.yaml')\n    with open(config_file_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    # Extract the best pipeline configuration from the summary file\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    # Save the best pipeline configuration to a YAML file if an output path is provided\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config_dict, f)\n\n    return best_config_dict\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    config_dict = yaml.load(open(os.path.join(trial_path, 'config.yaml')), Loader=yaml.SafeLoader)\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            output_path += '.yaml'\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f, sort_keys=False)\n\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary file and get the best module for each node line\n    summary_df = load_summary_file(trial_path)\n    best_modules = summary_df.groupby('node_line_name')['best_module_filename'].first().to_dict()\n\n    # Load the config file and replace the best module for each node line\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config = yaml.safe_load(f)\n\n    for node_line in config['node_lines']:\n        node_line_name = node_line['node_line_name']\n        if node_line_name in best_modules:\n            node_line['nodes'][0]['modules'][0]['module_type'] = best_modules[node_line_name]\n\n    # Save the updated config file to a YAML file if output_path is specified\n    if output_path:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(config, f)\n\n    return config\n\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    best_summary = summary_df.iloc[0]\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml')))\n    best_config = summary_df_to_yaml(summary_df=best_summary, config_dict=config_dict)\n    if output_path is not None:\n        output_path = os.path.abspath(output_path)\n        if output_path.endswith(('.yaml', '.yml')):\n            with open(output_path, 'w') as f:\n                yaml.dump(best_config, f)\n        else:\n            raise ValueError('output_path must end with .yaml or .yml')\n    return best_config\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary file\n    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n\n    # Load the config file\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    # Extract the best config\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    # Save the best config to a file\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config_dict, f)\n\n    return best_config_dict\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary file\n    summary_df = load_summary_file(trial_path)\n\n    # Load the config file\n    with open(os.path.join(trial_path, \"config.yaml\"), \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    # Convert summary_df to yaml\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    # Save the best config to a file\n    if output_path is not None:\n        if not output_path.endswith(('.yaml', '.yml')):\n            raise ValueError(\"Invalid file extension for output_path. Expected .yaml or .yml\")\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config_dict, f)\n\n    return best_config_dict\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with lock:\n            if key not in cache:\n                traced_module, compiler = trace_with_kwargs(func, *args,\n                                                            **kwargs)\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module)\n                cache[key] = traced_module\n            return cache[key](*args, **kwargs)\n\n    cache = {}\n    lock = threading.Lock()\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with lock:\n            if key not in cache:\n                cache[key] = trace_with_kwargs(func, *args, **kwargs)\n            traced_module, wrapper_fn = cache[key]\n        return wrapper_fn(traced_module)\n\n    cache = {}\n    lock = threading.Lock()\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with lock:\n            if key in cache:\n                return cache[key]\n            else:\n                traced_module, traced_call = trace_with_kwargs(\n                    func, *args, **kwargs)\n                if ts_compiler:\n                    traced_module = ts_compiler(traced_module)\n                cache[key] = traced_call(traced_module)\n                return cache[key]\n\n    cache = {}\n    lock = threading.Lock()\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def traced_func(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with traced_func.lock:\n            if key not in traced_func.cache:\n                traced_func.cache[key] = trace_with_kwargs(func, *args, **kwargs)\n            traced_module, helper = traced_func.cache[key]\n            if ts_compiler is not None:\n                traced_module = ts_compiler(traced_module)\n            return helper(traced_module)(*args, **kwargs)\n\n    traced_func.lock = threading.Lock()\n    traced_func.cache = {}\n    return traced_func\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, tuple(sorted(kwargs.items())))\n        with lock:\n            if key not in cache:\n                traced_module, _ = trace_with_kwargs(\n                    func, args, kwargs, **kwargs_)\n                cache[key] = traced_module\n            traced_module = cache[key]\n        return traced_module(*args, **kwargs)\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if isinstance(func, torch.nn.Module):\n        func = func.forward\n\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with threading.Lock():\n            if key not in wrapped._cache:\n                wrapped._cache[key] = trace_with_kwargs(func, *args, **kwargs)\n        traced_module, traced_call_helper = wrapped._cache[key]\n        return traced_call_helper(func(*args, **kwargs))\n\n    wrapped._cache = {}\n    wrapped._ts_compiler = ts_compiler\n    return wrapped\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache_lock = threading.Lock()\n    cache = {}\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        key = (args, tuple(kwargs.items()))\n        with cache_lock:\n            if key in cache:\n                return cache[key]\n\n        if isinstance(func, torch.nn.Module):\n            traced_module, traced_call_helper = trace_with_kwargs(\n                func.forward, *args, **kwargs)\n        else:\n            traced_module, traced_call_helper = trace_with_kwargs(\n                func, *args, **kwargs)\n\n        if ts_compiler is not None:\n            traced_module = ts_compiler(traced_module)\n\n        with cache_lock:\n            cache[key] = traced_call_helper(traced_module)\n        return cache[key]\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with lock:\n            if key in cache:\n                return cache[key]\n            traced_module, traced_module_factory = trace_with_kwargs(\n                func, *args, **kwargs, **kwargs_)\n            if ts_compiler is not None:\n                traced_module = ts_compiler(traced_module)\n            traced_module_factory = functools.partial(\n                traced_module_factory, func=func)\n            cache[key] = (traced_module, traced_module_factory)\n            return traced_module\n\n    cache = {}\n    lock = threading.Lock()\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache_key = (func, tuple(sorted(kwargs_.items())))\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        with lock:\n            if cache_key not in cache:\n                traced_module, compiler = trace_with_kwargs(func, *args, **kwargs_)\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module)\n                cache[cache_key] = traced_module\n            return cache[cache_key](*args, **kwargs)\n\n    lock = threading.Lock()\n    cache = {}\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with _lazy_trace_lock:\n            if key not in _lazy_trace_cache:\n                if inspect.isclass(func):\n                    func = func.__call__\n                traced_module, compiler = trace_with_kwargs(func, *args,\n                                                            **kwargs_)\n                if ts_compiler:\n                    traced_module = ts_compiler(traced_module, *args, **kwargs_)\n                _lazy_trace_cache[key] = traced_module\n            traced_module = _lazy_trace_cache[key]\n        return traced_module(*args, **kwargs)\n\n    return wrapped\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if not callable(func):\n        raise TypeError(f\"{func} is not callable\")\n\n    if not isinstance(func, torch.nn.Module):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            return func(*args, **kwargs)\n\n        return wrapped\n\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with lock:\n            if key not in cache:\n                cache[key] = trace_with_kwargs(func, *args, **kwargs, **kwargs_)\n            return cache[key]\n\n    cache = {}\n    lock = threading.Lock()\n    return wrapped\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def traced_func(*args, **kwargs):\n        with lock:\n            key = (func, tuple(args), tuple(kwargs.items()))\n            if key not in cache:\n                traced_module, compile_func = trace_with_kwargs(\n                    func, *args, **kwargs)\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module, *args, **kwargs)\n                cache[key] = traced_func = compile_func(traced_module)\n            return cache[key](*args, **kwargs)\n\n    cache = {}\n    lock = threading.Lock()\n    return traced_func\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, frozenset(kwargs.items()))\n        with threading.Lock():\n            if key in wrapper.cache:\n                return wrapper.cache[key]\n            traced_module, traced_call = trace_with_kwargs(\n                func, example_inputs=args, example_kwarg_inputs=kwargs, **kwargs_)\n            if ts_compiler:\n                traced_module = ts_compiler(traced_module)\n            wrapper.cache[key] = traced_call(traced_module)\n            return wrapper.cache[key]\n\n    wrapper.cache = {}\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def traced_func(*args, **kwargs):\n        traced_func.lock.acquire()\n        traced_func.traced_func = traced_func.traced_func or better_trace(\n            func, args, **kwargs_)\n        traced_func.lock.release()\n        return traced_func.traced_func(*args, **kwargs)\n\n    traced_func.lock = threading.Lock()\n    traced_func.traced_func = None\n\n    if ts_compiler is not None:\n        traced_func = ts_compiler(traced_func)\n\n    return traced_func\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if isinstance(func, torch.nn.Module):\n        func = func.forward\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, tuple(sorted(kwargs.items())))\n        with threading.Lock():\n            if key not in wrapper.cache:\n                wrapper.cache[key] = trace_with_kwargs(func, *args, **kwargs_)\n            traced_module, traced_module_wrapper = wrapper.cache[key]\n        return traced_module_wrapper(traced_module(*args, **kwargs))\n\n    wrapper.cache = {}\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Create a unique key based on the function and arguments\n        key = (func, tuple(args), tuple(kwargs.items()))\n\n        # Acquire the lock to ensure thread safety\n        with lock:\n            # Check if the key exists in the cache\n            if key in cache:\n                # Return the cached result\n                return cache[key]\n\n        # Trace the function or module's forward method\n        traced_module, traced_call = trace_with_kwargs(func, *args, **kwargs)\n\n        # Apply the compiler function if provided\n        if ts_compiler is not None:\n            traced_module = ts_compiler(traced_module)\n\n        # Store the traced module in the cache\n        cache[key] = traced_module\n\n        # Return the traced module\n        return traced_module\n\n    # Create a lock to ensure thread safety\n    lock = threading.Lock()\n\n    # Create a cache to store traced modules\n    cache = {}\n\n    # Return the wrapper function\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if not isinstance(func, torch.nn.Module):\n        func = func.forward\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, kwargs)\n        with _lazy_trace_lock:\n            if key in _lazy_trace_cache:\n                return _lazy_trace_cache[key]\n\n        traced_module, traced_call = trace_with_kwargs(func, *args, **kwargs)\n        if ts_compiler is not None:\n            traced_module = ts_compiler(traced_module)\n        traced_call = functools.partial(traced_call, func)\n        _lazy_trace_cache[key] = traced_call\n        return traced_call(*args, **kwargs)\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if not isinstance(func, torch.nn.Module):\n        return functools.partial(lazy_trace, func, ts_compiler=ts_compiler,\n                                 **kwargs_)\n\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        if key not in cache:\n            with lock:\n                if key not in cache:\n                    traced_module, compile_fn = trace_with_kwargs(\n                        func, *args, **kwargs_)\n                    cache[key] = compile_fn(traced_module)\n        return cache[key](*args, **kwargs)\n\n    cache = {}\n    lock = threading.Lock()\n\n    if ts_compiler is not None:\n        apply_to_all_modules(func, lambda m: apply_to_module(\n            m, lambda m: m.forward, lambda m: m.forward))\n        return wrapped\n    return func\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, frozenset(kwargs.items()))\n        with lock:\n            if key not in cache:\n                if isinstance(func, torch.nn.Module):\n                    traced_module, traced_call = trace_with_kwargs(\n                        func.forward, *args, **kwargs)\n                else:\n                    traced_module, traced_call = trace_with_kwargs(\n                        func, *args, **kwargs)\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module, *args, **kwargs)\n                cache[key] = traced_module, traced_call\n            traced_module, traced_call = cache[key]\n        return traced_call(traced_module)\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def wrapped_func(*args, **kwargs):\n        # Generate a unique key based on the function and its arguments\n        key = (func, tuple(args), tuple(kwargs.items()))\n\n        # Acquire the lock to ensure thread safety\n        lock.acquire()\n\n        # Check if the traced module is already cached\n        if key in cache:\n            # Return the cached traced module\n            traced_module, traced_call = cache[key]\n        else:\n            # Trace the function or module's forward method\n            traced_module, traced_call = trace_with_kwargs(func,\n                                                           *args, **kwargs)\n\n            # Apply the compiler function to the traced module\n            if ts_compiler is not None:\n                traced_module = ts_compiler(traced_module)\n\n            # Cache the traced module\n            cache[key] = (traced_module, traced_call)\n\n        # Release the lock\n        lock.release()\n\n        # Call the traced module\n        return traced_call(traced_module)\n\n    # Create a lock to synchronize access to the cache\n    lock = threading.Lock()\n\n    # Create a cache to store the traced modules\n    cache = {}\n\n    # Return the wrapped function\n    return wrapped_func\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_path = os.path.join(trial_path, 'best_config.yaml')\n        project_dir = os.path.dirname(trial_path)\n        return cls.from_yaml(yaml_path, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        return cls(config, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_path = os.path.join(trial_path, 'best_config.yaml')\n        project_dir = os.path.dirname(trial_path)\n        return cls.from_yaml(yaml_path, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_dict = extract_best_config(trial_path)\n        return cls(yaml_dict)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_dict = extract_best_config(trial_path)\n        return cls(yaml_dict, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_path = os.path.join(trial_path, 'best_config.yaml')\n        return cls.from_yaml(yaml_path, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.abspath(os.path.join(trial_path, '..'))\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        return cls(config, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_path = os.path.join(trial_path, 'best_config.yaml')\n        return cls.from_yaml(yaml_path, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module among retrieval node results\n    best_result = select_best_average(modules, module_params, previous_result, node_line_dir, strategies)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best retrieval node result\n    best_result = evaluate_retrieval(\n        modules,\n        module_params,\n        previous_result,\n        node_line_dir,\n        strategies\n    )\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best result among all the modules\n    best_result = select_best_average(\n        modules=modules,\n        module_params=module_params,\n        previous_result=previous_result,\n        strategies=strategies,\n        node_line_dir=node_line_dir\n    )\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(\"Running Retrieval Node\")\n\n    # Create directory for this node line\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module among retrieval node results\n    best_result, best_module_name = evaluate_retrieval(modules, module_params, previous_result, strategies)\n\n    # Save the best result to disk\n    best_result_file = os.path.join(node_line_dir, \"best_result.csv\")\n    best_result.to_csv(best_result_file, index=False)\n\n    # Save the summary of the execution times and evaluation metrics to disk\n    summary_file = os.path.join(node_line_dir, \"summary.csv\")\n    best_result[[\"module\", \"execution_time\", \"metrics\"]].to_csv(summary_file, index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module among retrieval node results\n    result_list = []\n    for module, params in zip(modules, module_params):\n        result = module(**params)\n        result_list.append(result)\n\n    # Measure the execution times of each module\n    execution_times = measure_speed(result_list)\n\n    # Save the execution times to a file\n    execution_times_path = os.path.join(node_line_dir, \"execution_times.json\")\n    execution_times.to_json(execution_times_path, indent=4)\n\n    # Evaluate the results\n    evaluation_results = evaluate_retrieval(result_list, previous_result, strategies)\n\n    # Save the evaluation results to a file\n    evaluation_results_path = os.path.join(node_line_dir, \"evaluation_results.json\")\n    evaluation_results.to_json(evaluation_results_path, indent=4)\n\n    # Filter the results by the specified speed thresholds\n    filtered_results = filter_by_threshold(evaluation_results, strategies)\n\n    # Select the best result based on the specified evaluation metrics\n    best_result = select_best_average(filtered_results, strategies)\n\n    # Save the best result to a file\n    best_result_path = os.path.join(node_line_dir, \"best_result.json\")\n    best_result.to_json(best_result_path, indent=4)\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create directory for this node line\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module with given parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        results.append(module(**params))\n\n    # Measure execution times of each module\n    execution_times = measure_speed(results)\n\n    # Save execution times to disk\n    execution_times.to_csv(os.path.join(node_line_dir, \"execution_times.csv\"), index=False)\n\n    # Evaluate each module\n    evaluation_results = evaluate_retrieval(results, previous_result)\n\n    # Save evaluation results to disk\n    evaluation_results.to_csv(os.path.join(node_line_dir, \"evaluation_results.csv\"), index=False)\n\n    # Apply strategies to select the best module\n    best_result = select_best_average(evaluation_results, strategies)\n\n    # Save the best module to disk\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Combine the best module with the previous result\n    combined_result = pd.merge(previous_result, best_result, on=\"query_id\", how=\"inner\")\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best retrieval node result\n    best_result = evaluate_retrieval(\n        modules=modules,\n        module_params=module_params,\n        previous_result=previous_result,\n        strategies=strategies,\n        node_line_dir=node_line_dir,\n    )\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize the best result dataframe with the columns from the previous result\n    best_result = previous_result.copy()\n\n    # Initialize the summary dataframe to store the execution times and evaluation metrics\n    summary = pd.DataFrame(columns=[\"module_name\", \"execution_time\", \"evaluation_metrics\"])\n\n    # Iterate over the retrieval modules and their parameters\n    for module, module_param in zip(modules, module_params):\n\n        # Execute the retrieval module with the given parameters\n        result = module(**module_param)\n\n        # Measure the execution time of the retrieval module\n        execution_time = measure_speed(result)\n\n        # Evaluate the retrieval module's result using the evaluation context and specified metrics\n        evaluation_metrics = evaluate_retrieval(result, previous_result, strategies[\"evaluation_metrics\"])\n\n        # Add the execution time and evaluation metrics to the summary dataframe\n        summary = pd.concat([summary, pd.DataFrame([[module.__name__, execution_time, evaluation_metrics]],\n                                                   columns=[\"module_name\", \"execution_time\", \"evaluation_metrics\"])])\n\n        # Apply the specified strategies to filter and select the best retrieval module result\n        if strategies[\"filter_by_threshold\"]:\n            result = filter_by_threshold(result, strategies[\"filter_by_threshold\"])\n\n        if strategies[\"select_best_average\"]:\n            result = select_best_average(result, strategies[\"select_best_average\"])\n\n        # Combine the previous result columns with the selected retrieval module's result columns\n        result = pd.concat([previous_result, result], axis=1)\n\n        # Update the best result dataframe with the selected retrieval module's result\n        best_result = result\n\n    # Save the best result dataframe to disk\n    best_result.to_csv"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(f\"Running retrieval node at {node_line_dir}\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module among retrieval node results\n    result_dict = {}\n    for i, module in enumerate(modules):\n        logger.info(f\"Running module {i}\")\n        module_result = module(**module_params[i])\n        result_dict[i] = module_result\n\n    # Measure execution times and evaluate the results\n    execution_times, evaluation_results = evaluate_retrieval(result_dict, previous_result, strategies)\n\n    # Apply strategies to select the best module\n    best_module_index = select_best_average(evaluation_results)\n    best_result = result_dict[best_module_index]\n\n    # Save the results and a summary of the execution times and evaluation metrics\n    best_result.to_csv(os.path.join(node_line_dir, \"result.csv\"), index=False)\n    execution_times.to_csv(os.path.join(node_line_dir, \"execution_times.csv\"), index=False)\n    evaluation_results.to_csv(os.path.join(node_line_dir, \"evaluation_results.csv\"), index=False)\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    best_result_with_previous = pd.concat([previous_result, best_result], axis=1)\n\n    return best_result_with_previous\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the output directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize the best result dataframe with the previous result\n    best_result = previous_result.copy()\n\n    # Initialize the summary dataframe\n    summary = pd.DataFrame()\n\n    # Loop through each retrieval module and its parameters\n    for module, params in zip(modules, module_params):\n\n        # Run the retrieval module with the given parameters\n        result = module(**params)\n\n        # Measure the execution time of the retrieval module\n        result = measure_speed(result, module.__name__)\n\n        # Evaluate the retrieval module's results using the evaluation context\n        result = evaluate_retrieval(result, previous_result)\n\n        # Apply the specified strategies to filter and select the best result\n        result = filter_by_threshold(result, strategies)\n        result = select_best_average(result, strategies)\n\n        # Append the result to the summary dataframe\n        summary = pd.concat([summary, result], ignore_index=True)\n\n        # Update the best result dataframe with the selected result\n        best_result = pd.merge(best_result, result, on='query_id', how='left')\n\n    # Save the best result dataframe to disk\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    # Save the summary dataframe to disk\n    summary.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    # Return the best result dataframe\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module among retrieval node results\n    best_result = None\n    for module, params in zip(modules, module_params):\n        # Run the retrieval module\n        result = module(**params)\n\n        # Measure the execution time of the retrieval module\n        speed = measure_speed(result)\n\n        # Evaluate the retrieval module\n        evaluation_context = {\"previous_result\": previous_result}\n        evaluation_result = evaluate_retrieval(result, evaluation_context)\n\n        # Filter the retrieval module by speed and evaluation metrics\n        result, speed, evaluation_result = filter_by_threshold(result, speed, evaluation_result, strategies)\n\n        # Save the result to disk\n        result_path = os.path.join(node_line_dir, f\"{module.__name__}.csv\")\n        result.to_csv(result_path, index=False)\n\n        # Save the evaluation result to disk\n        evaluation_result_path = os.path.join(node_line_dir, f\"{module.__name__}_evaluation.csv\")\n        evaluation_result.to_csv(evaluation_result_path, index=False)\n\n        # Save the execution time to disk\n        speed_path = os.path.join(node_line_dir, f\"{module.__name__}_speed.csv\")\n        speed.to_csv(speed_path, index=False)\n\n        # Save the summary of the execution time and evaluation metrics to disk\n        summary_path = os.path.join(node_line_dir, f\"{module.__name__}_summary.csv\")\n        summary = pd.concat([speed, evaluation_result], axis=1)\n        summary.to_csv(summary_path, index=False)\n\n        # Select the best module among retrieval node results\n        if best_result is None:\n            best_result = result\n            best_speed = speed\n            best_evaluation_result"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it does not exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize the result dataframe\n    result_df = pd.DataFrame()\n\n    # Iterate over each retrieval module and its parameters\n    for module, params in zip(modules, module_params):\n        # Initialize the module result dataframe\n        module_result_df = pd.DataFrame()\n\n        # Run the retrieval module with the given parameters\n        module_result_df = module(**params)\n\n        # Measure the execution time of the module\n        module_result_df = measure_speed(module_result_df)\n\n        # Evaluate the module result using the evaluation context and the evaluation metrics\n        module_result_df = evaluate_retrieval(module_result_df, previous_result, strategies[\"evaluation_metrics\"])\n\n        # Apply the filtering strategies\n        module_result_df = filter_by_threshold(module_result_df, strategies[\"filtering_strategies\"])\n\n        # Select the best result based on the evaluation metrics\n        module_result_df = select_best_average(module_result_df, strategies[\"evaluation_metrics\"])\n\n        # Concatenate the module result with the previous result\n        result_df = pd.concat([result_df, module_result_df], axis=1)\n\n        # Save the module result to disk\n        module_result_df.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n    # Save the result dataframe to disk\n    result_df.to_csv(os.path.join(node_line_dir, \"result.csv\"), index=False)\n\n    # Save the summary of the execution times and evaluation metrics to disk\n    summary_df = pd.DataFrame(\n        {\n            \"module\": [module.__name__ for module in modules],\n            \"execution_time\": result_df[\"execution_time\"],\n            \"evaluation"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    logger.info(\"Running retrieval nodes...\")\n\n    result_dfs = []\n    summary_dfs = []\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__.split(\".\")[-1]\n        module_dir = os.path.join(node_line_dir, module_name)\n        if not os.path.exists(module_dir):\n            os.makedirs(module_dir)\n        logger.info(f\"Running module: {module_name}\")\n        result_df, summary_df = module(**params)\n        result_dfs.append(result_df)\n        summary_dfs.append(summary_df)\n        result_df.to_csv(os.path.join(module_dir, \"result.csv\"), index=False)\n        summary_df.to_csv(os.path.join(module_dir, \"summary.csv\"), index=False)\n\n    logger.info(\"Evaluating and selecting the best retrieval node result...\")\n\n    if \"speed\" in strategies:\n        speed_threshold = strategies[\"speed\"]\n        logger.info(f\"Speed threshold: {speed_threshold}\")\n        result_dfs = measure_speed(result_dfs, speed_threshold)\n\n    if \"evaluation\" in strategies:\n        evaluation_metrics = strategies[\"evaluation\"]\n        logger.info(f\"Evaluation metrics: {evaluation_metrics}\")\n        result_dfs, summary_dfs = evaluate_retrieval(result_dfs, summary_dfs, previous_result, evaluation_metrics)\n\n    if \"threshold\" in strategies:\n        threshold = strategies[\"threshold\"]\n        logger.info(f\"Threshold: {threshold}\")\n        result_dfs = filter_by_threshold(result_dfs, threshold)\n\n    if \"average\" in strategies:\n        average_strategy = strategies[\"average\"]\n        logger"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it does not exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best retrieval node result by running each module with given parameters, measuring their execution times, and applying specified strategies.\n    best_result = None\n    for module, module_param in zip(modules, module_params):\n        module_name = module.__name__\n        logger.info(f\"Running retrieval node with {module_name}\")\n        result = module(**module_param)\n        result = measure_speed(result, strategies[\"speed_threshold\"])\n        result = evaluate_retrieval(result, previous_result, strategies[\"metrics\"])\n        result = filter_by_threshold(result, strategies[\"speed_threshold\"])\n        best_result = select_best_average(best_result, result)\n\n    # Save the results and a summary of the execution times and evaluation metrics to disk\n    if best_result is not None:\n        best_result.to_csv(os.path.join(node_line_dir, \"result.csv\"), index=False)\n        result_summary = load_summary_file(node_line_dir)\n        result_summary.to_csv(os.path.join(node_line_dir, \"result_summary.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(f\"Starting retrieval node with {len(modules)} modules\")\n    # Evaluate and select the best module among retrieval node results\n    results = []\n    execution_times = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running module {module.__name__}\")\n        result, execution_time = module(**params)\n        results.append(result)\n        execution_times.append(execution_time)\n\n    logger.info(f\"Evaluating and selecting the best module among {len(modules)} modules\")\n    # Evaluate and select the best module among retrieval node results\n    best_result, best_result_idx = evaluate_retrieval(results, previous_result, strategies)\n    execution_time_summary = pd.DataFrame(execution_times, index=[m.__name__ for m in modules])\n    execution_time_summary.to_csv(os.path.join(node_line_dir, \"execution_time_summary.csv\"))\n\n    # Save the best result to disk\n    logger.info(f\"Saving the best result to disk\")\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Save the selected module's execution time to disk\n    logger.info(f\"Saving the selected module's execution time to disk\")\n    execution_time_summary.loc[execution_time_summary.index[best_result_idx]].to_csv(os.path.join(node_line_dir, \"execution_time.csv\"))\n\n    # Save the selected module's result to disk\n    logger.info(f\"Saving the selected module's result to disk\")\n    results[best_result_idx].to_csv(os.path.join(node_line_dir, \"result.csv\"), index=False)\n\n    # Return the best result\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(\"Running retrieval node\")\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Run each module with given parameters and save the results to a file\n    results = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running module {module.__name__} with params {params}\")\n        result = module(**params)\n        results.append(result)\n        result.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n    # Measure execution times for each module\n    execution_times = measure_speed(results)\n\n    # Evaluate each module and save the evaluation metrics to a file\n    eval_metrics = evaluate_retrieval(results, previous_result, strategies[\"evaluation_metrics\"])\n    eval_metrics.to_csv(os.path.join(node_line_dir, \"evaluation_metrics.csv\"), index=False)\n\n    # Apply filtering strategies\n    if strategies[\"filtering_strategies\"]:\n        for strategy_name, strategy_params in strategies[\"filtering_strategies\"].items():\n            if strategy_name == \"threshold\":\n                results, execution_times = filter_by_threshold(results, execution_times, strategy_params)\n            elif strategy_name == \"average\":\n                results, execution_times = select_best_average(results, execution_times, strategy_params)\n            else:\n                raise ValueError(f\"Unknown filtering strategy: {strategy_name}\")\n\n    # Save the execution times and evaluation metrics to a file\n    execution_times.to_csv(os.path.join(node_line_dir, \"execution_times.csv\"), index=False)\n\n    # Select the best result and save it to a file\n    best_result = results[0]\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Combine the previous result columns with"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module among retrieval node results\n    best_result = None\n    best_result_name = None\n    best_result_summary = None\n    best_result_speed = None\n    for i, module in enumerate(modules):\n        result = module(**module_params[i])\n        result_name = module.__name__\n\n        # Measure the speed of the retrieval module\n        result_speed = measure_speed(result)\n\n        # Evaluate the retrieval module\n        result_summary = evaluate_retrieval(result, previous_result, strategies[\"evaluation_metrics\"])\n\n        # Filter the result by speed threshold\n        if strategies[\"speed_threshold\"] is not None:\n            result_summary = filter_by_threshold(result_summary, strategies[\"speed_threshold\"], \"speed\")\n\n        # Select the best result\n        if best_result is None or result_summary[\"mean_score\"] > best_result_summary[\"mean_score\"]:\n            best_result = result\n            best_result_name = result_name\n            best_result_summary = result_summary\n            best_result_speed = result_speed\n\n    # Save the best result and summary\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n    best_result_summary.to_csv(os.path.join(node_line_dir, \"best_result_summary.csv\"), index=False)\n\n    # Save the best result speed\n    best_result_speed.to_csv(os.path.join(node_line_dir, \"best_result_speed.csv\"), index=False)\n\n    # Select the best result among the retrieval node results\n    best_result_df = select_best_average(previous_result, best_result, strategies[\"evaluation_metrics\"])\n\n    return best_result_"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create directory for this node line\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module\n    result_dfs = []\n    for module, params in zip(modules, module_params):\n        # Run the module with the given parameters\n        result_df = module(**params)\n\n        # Measure the execution time of the module\n        result_df = measure_speed(result_df)\n\n        # Evaluate the module's result using the evaluation context\n        result_df = evaluate_retrieval(result_df, previous_result, strategies[\"evaluation_context\"])\n\n        # Filter the results based on the speed threshold\n        result_df = filter_by_threshold(result_df, strategies[\"speed_threshold\"])\n\n        # Save the result and summary to disk\n        result_df.to_csv(os.path.join(node_line_dir, \"result.csv\"), index=False)\n        result_df.to_csv(os.path.join(node_line_dir, \"result.json\"), index=False)\n        result_df.to_csv(os.path.join(node_line_dir, \"result.txt\"), index=False)\n        result_df[[\"module\", \"execution_time\"]].to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n        # Append the result dataframe to the list\n        result_dfs.append(result_df)\n\n    # Select the best module based on the specified metrics\n    best_result_df = select_best_average(result_dfs, strategies[\"metrics\"])\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    combined_result_df = pd.concat([previous_result, best_result_df], axis=1)\n\n    return combined_result_df\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    result_df = pd.DataFrame()\n    summary_df = pd.DataFrame()\n\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running retrieval module: {module.__name__}\")\n\n        result, speed = measure_speed(module, params)\n\n        if strategies[\"speed_threshold\"] is not None and speed > strategies[\"speed_threshold\"]:\n            logger.info(f\"Module {module.__name__} is too slow. Skipping.\")\n            continue\n\n        result = evaluate_retrieval(result, previous_result, strategies[\"evaluation_metrics\"])\n        result[\"module\"] = module.__name__\n        result[\"params\"] = str(params)\n        result[\"speed\"] = speed\n\n        result_df = pd.concat([result_df, result])\n\n        summary_df = pd.concat([summary_df, result[[\"module\", \"params\", \"speed\", \"evaluation_score\"]]])\n\n    summary_df = summary_df.groupby([\"module\", \"params\"]).mean().reset_index()\n    summary_df = summary_df.sort_values(\"evaluation_score\", ascending=False)\n\n    best_module, best_params = summary_df.iloc[0][\"module\"], summary_df.iloc[0][\"params\"]\n\n    best_result = result_df[result_df[\"module\"] == best_module]\n    best_result = best_result[best_result[\"params\"] == best_params]\n\n    best_result.to_csv(os.path.join(node_line_dir, \"retrieval_node_result.csv\"), index=False)\n    summary_df.to_csv(os.path.join(node_line_dir, \"retrieval_node_summary.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module among retrieval node results\n    result_list = []\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        logger.info(f\"Executing {module.__name__} with params {params}\")\n        # Execute the retrieval module\n        result = module(**params)\n        # Measure the execution time of the module\n        time_taken = measure_speed(module, params)\n        # Evaluate the result\n        evaluation_result = evaluate_retrieval(result, previous_result, strategies[\"evaluation_metrics\"])\n        # Combine the result, time taken, and evaluation metrics into a single DataFrame\n        combined_result = pd.concat([result, time_taken, evaluation_result], axis=1)\n        # Save the combined result to a CSV file\n        combined_result.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n        # Add the combined result to the result list\n        result_list.append(combined_result)\n\n    # Combine all the results into a single DataFrame\n    combined_result = pd.concat(result_list, axis=0)\n\n    # Filter the results by the specified speed thresholds\n    if \"speed_thresholds\" in strategies:\n        combined_result = filter_by_threshold(combined_result, strategies[\"speed_thresholds\"])\n\n    # Select the best result by evaluating and selecting the best result among the remaining results\n    best_result = select_best_average(combined_result, strategies[\"evaluation_metrics\"])\n\n    # Save the best result to a CSV file\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Save the combined result to a CSV file\n    combined_"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # create directory for the node\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # create a dictionary to store the results\n    results = {}\n\n    # create a dictionary to store the summaries\n    summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}\n\n    # create a dictionary to store the best summaries\n    best_summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}\n\n    # create a dictionary to store the best summaries\n    best_summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}\n\n    # create a dictionary to store the best summaries\n    best_summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}\n\n    # create a dictionary to store the best summaries\n    best_summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}\n\n    # create a dictionary to store the best summaries\n    best_summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}\n\n    # create a dictionary to store the best summaries\n    best_summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}\n\n    # create a dictionary to store the best summaries\n    best_summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}\n\n    # create a dictionary to store the best summaries\n    best_summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}\n\n    # create a dictionary to store the best summaries\n    best_summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}\n\n    # create a dictionary to store the best summaries\n    best_summaries = {}\n\n    # create a dictionary to store the best results\n    best_results = {}"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running query expansion node\")\n\n    # Create directory for query expansion results\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create directory for query expansion summaries\n    summary_dir = os.path.join(node_dir, \"summaries\")\n    pathlib.Path(summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create directory for query expansion best results\n    best_dir = os.path.join(node_dir, \"best\")\n    pathlib.Path(best_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create directory for query expansion speed results\n    speed_dir = os.path.join(node_dir, \"speed\")\n    pathlib.Path(speed_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create directory for query expansion speed summaries\n    speed_summary_dir = os.path.join(node_dir, \"speed_summaries\")\n    pathlib.Path(speed_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create directory for query expansion speed best results\n    speed_best_dir = os.path.join(node_dir, \"speed_best\")\n    pathlib.Path(speed_best_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create directory for query expansion speed best summaries\n    speed_best_summary_dir = os.path.join(node_dir, \"speed_best_summaries\")\n    pathlib.Path(speed_best_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create directory for query expansion speed best results\n    speed_best_results_dir = os.path.join(node_dir, \"speed_best_results\")\n    pathlib.Path(speed_best_results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running query expansion node\")\n    # Save the previous result as a csv file\n    previous_result.to_csv(os.path.join(node_line_dir, \"previous_result.csv\"), index=False)\n\n    # Create a directory for the results of each module\n    module_results_dir = os.path.join(node_line_dir, \"module_results\")\n    pathlib.Path(module_results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the summaries of each module\n    module_summaries_dir = os.path.join(node_line_dir, \"module_summaries\")\n    pathlib.Path(module_summaries_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the best result of each module\n    module_best_dir = os.path.join(node_line_dir, \"module_best\")\n    pathlib.Path(module_best_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the best result of each module\n    module_best_dir = os.path.join(node_line_dir, \"module_best\")\n    pathlib.Path(module_best_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the best result of each module\n    module_best_dir = os.path.join(node_line_dir, \"module_best\")\n    pathlib.Path(module_best_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the best result of each module\n    module_best_dir = os.path.join(node_line_dir, \"module_best\")\n    pathlib.Path(module_best_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the best result of each module\n    module_best_dir = os.path.join(node_line_dir, \"module_best\")\n    pathlib.Path(module_best"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory for the query expansion node if it does not exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a list of dictionaries containing the parameters for each module\n    module_params_list = make_combinations(module_params)\n\n    # Create a list of dictionaries containing the parameters for each module and the previous result\n    module_params_list = [{**module_params, **{\"previous_result\": previous_result}} for module_params in module_params_list]\n\n    # Create a list of dictionaries containing the parameters for each module, the previous result, and the node directory\n    module_params_list = [{**module_params, **{\"node_line_dir\": node_line_dir}} for module_params in module_params_list]\n\n    # Create a list of dictionaries containing the parameters for each module, the previous result, the node directory, and the strategies\n    module_params_list = [{**module_params, **{\"strategies\": strategies}} for module_params in module_params_list]\n\n    # Create a list of dictionaries containing the parameters for each module, the previous result, the node directory, the strategies, and the modules\n    module_params_list = [{**module_params, **{\"modules\": modules}} for module_params in module_params_list]\n\n    # Create a list of dictionaries containing the parameters for each module, the previous result, the node directory, the strategies, the modules, and the node name\n    module_params_list = [{**module_params, **{\"node_name\": \"query_expansion\"}} for module_params in module_params_list]\n\n    # Create a list of dictionaries containing the parameters for each module, the previous result, the node directory, the strategies, the modules, the node name, and the node line directory\n    module_params_list = [{**module_params, **{\"node_line_dir\": node_line_dir}} for module_params in module_params_list]\n\n    # Create a list of dictionaries containing the parameters for each module, the previous result, the node"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory for the current node line\n    node_line_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the current node line's results\n    results_dir = os.path.join(node_line_dir, \"results\")\n    pathlib.Path(results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the current node line's summaries\n    summaries_dir = os.path.join(node_line_dir, \"summaries\")\n    pathlib.Path(summaries_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the current node line's best results\n    best_results_dir = os.path.join(node_line_dir, \"best_results\")\n    pathlib.Path(best_results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the current node line's best summaries\n    best_summaries_dir = os.path.join(node_line_dir, \"best_summaries\")\n    pathlib.Path(best_summaries_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a dictionary to store the best results for each metric\n    best_results = {}\n\n    # Create a dictionary to store the best summaries for each metric\n    best_summaries = {}\n\n    # Create a dictionary to store the best result for each metric\n    best_result = {}\n\n    # Create a dictionary to store the best summary for each metric\n    best_summary = {}\n\n    # Create a dictionary to store the best result for each metric\n    best_result_summary = {}\n\n    # Create a dictionary to store the best summary for each metric\n    best_summary_summary = {}\n\n    # Create a dictionary to store the best result for each metric\n    best_result_summary_summary = {}\n\n    #"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the node directory\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module with its parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(previous_result, **params)\n        results.append(result)\n\n    # Measure the execution times of each module\n    execution_times = measure_speed(results)\n\n    # Evaluate each module based on the specified strategies\n    evaluation_results = []\n    for module, result, execution_time in zip(modules, results, execution_times):\n        evaluation_result = evaluate_query_expansion_node(module, result, execution_time, strategies)\n        evaluation_results.append(evaluation_result)\n\n    # Save the evaluation results\n    evaluation_results_df = pd.DataFrame(evaluation_results)\n    evaluation_results_df.to_csv(os.path.join(node_dir, \"evaluation_results.csv\"), index=False)\n\n    # Save the execution times\n    execution_times_df = pd.DataFrame({\"module\": [module.__name__ for module in modules], \"execution_time\": execution_times})\n    execution_times_df.to_csv(os.path.join(node_dir, \"execution_times.csv\"), index=False)\n\n    # Select the best result based on the evaluation\n    best_result = select_best_average(evaluation_results, strategies[\"metrics\"])\n    best_result_df = pd.DataFrame(best_result)\n    best_result_df.to_csv(os.path.join(node_dir, \"best_result.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the node results\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate each module with its parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        result = evaluate_query_expansion_node(module, params, previous_result, node_line_dir, strategies)\n        results.append(result)\n\n    # Select the best result based on the specified strategies\n    best_result = select_best_query_expansion_node(results, strategies)\n\n    # Save the best result to a file\n    best_result_file_path = os.path.join(node_line_dir, \"best_result.csv\")\n    best_result.to_csv(best_result_file_path, index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module with the given parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running query expansion module {module.__name__} with parameters {params}\")\n        result = module(previous_result, **params)\n        results.append(result)\n\n    # Save the results\n    for i, result in enumerate(results):\n        result.to_csv(os.path.join(node_line_dir, f\"{modules[i].__name__}.csv\"), index=False)\n\n    # Measure the execution time of each module\n    execution_times = measure_speed(results)\n\n    # Evaluate each module using the specified strategies\n    metrics = strategies[\"metrics\"]\n    evaluation_results = []\n    for i, result in enumerate(results):\n        evaluation_results.append(evaluate_retrieval_node(result, metrics))\n\n    # Save the evaluation results\n    evaluation_results_df = pd.DataFrame(evaluation_results, index=[module.__name__ for module in modules])\n    evaluation_results_df.to_csv(os.path.join(node_line_dir, \"evaluation_results.csv\"))\n\n    # Save the execution times\n    execution_times_df = pd.DataFrame(execution_times, index=[module.__name__ for module in modules])\n    execution_times_df.to_csv(os.path.join(node_line_dir, \"execution_times.csv\"))\n\n    # Filter the results by speed thresholds\n    filtered_results = filter_by_threshold(results, strategies[\"speed_thresholds\"])\n\n    # Select the best result based on the specified strategies\n    best_result = select_best_average(filtered_results, strategies[\"metrics\"], strategies[\"weight\"])\n\n    # Save the best result\n    best_result.to_csv(os.path.join(node_"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a list of dictionaries with module name and parameters\n    module_params_list = make_combinations(module_params)\n\n    # Evaluate each module and save the results to the directory\n    results = []\n    for module_name, module_param in module_params_list:\n        module = get_support_modules(module_name, modules)\n        result = evaluate_query_expansion_node(module, module_param, previous_result, node_line_dir, strategies)\n        results.append(result)\n\n    # Explode the results list of dictionaries into a dataframe\n    results = explode(results)\n\n    # Save the results to a CSV file\n    results.to_csv(os.path.join(node_line_dir, \"results.csv\"), index=False)\n\n    # Select the best result based on the strategies\n    best_result = select_best_query_expansion_result(results, strategies)\n\n    # Save the best result to a CSV file\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Return the best result\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory for the current node line\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Run each module with the given parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(previous_result, **params)\n        results.append(result)\n\n    # Save the results of each module\n    for i, result in enumerate(results):\n        result.to_csv(f\"{node_line_dir}/result_{i}.csv\", index=False)\n\n    # Measure the execution time of each module\n    execution_times = measure_speed(results)\n\n    # Evaluate the performance of each module based on the specified strategies\n    evaluation_results = evaluate_query_expansion_node(results, strategies)\n\n    # Save the execution times and evaluation results to a summary file\n    summary = pd.DataFrame({\"module\": [str(m) for m in modules],\n                            \"execution_time\": execution_times,\n                            \"evaluation_results\": evaluation_results})\n    summary.to_csv(f\"{node_line_dir}/summary.csv\", index=False)\n\n    # Select the best module based on the evaluation results\n    best_module_index = select_best_average(evaluation_results)\n    best_result = results[best_module_index]\n\n    # Save the best result to a file\n    best_result.to_csv(f\"{node_line_dir}/best_result.csv\", index=False)\n\n    # Return the best result\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Check if the node_line_dir exists, if not, create it\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a copy of the previous result to avoid modifying the original dataframe\n    previous_result = deepcopy(previous_result)\n\n    # Evaluate each module with its parameters\n    module_results = evaluate_query_expansion_node(modules, module_params, previous_result, node_line_dir)\n\n    # Select the best module based on the specified strategies\n    best_result = select_best_query_expansion_module(module_results, strategies)\n\n    # Save the best result to a file\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Save a summary of the results to a file\n    summary = pd.DataFrame(columns=[\"module\", \"execution_time\", \"speed_threshold\", \"metrics\"])\n    for module_name, module_result in module_results.items():\n        module_summary = pd.DataFrame(columns=[\"module\", \"execution_time\", \"speed_threshold\", \"metrics\"])\n        module_summary[\"module\"] = [module_name]\n        module_summary[\"execution_time\"] = [module_result[\"execution_time\"]]\n        module_summary[\"speed_threshold\"] = [module_result[\"speed_threshold\"]]\n        module_summary[\"metrics\"] = [module_result[\"metrics\"]]\n        summary = pd.concat([summary, module_summary], ignore_index=True)\n    summary.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create directory for node results\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run modules with parameters and save results\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(previous_result, **params)\n        results.append(result)\n        result.to_csv(os.path.join(node_line_dir, f\"{module.__name__}.csv\"), index=False)\n\n    # Measure execution times and save to results\n    execution_times = measure_speed(results)\n    execution_times.to_csv(os.path.join(node_line_dir, \"execution_times.csv\"), index=False)\n\n    # Evaluate results based on strategies\n    evaluation_metrics = evaluate_query_expansion_node(results, strategies)\n    evaluation_metrics.to_csv(os.path.join(node_line_dir, \"evaluation_metrics.csv\"), index=False)\n\n    # Select the best result based on evaluation metrics\n    best_result = select_best_average(evaluation_metrics, strategies[\"evaluation_metric\"])\n\n    # Save the best result to a CSV file\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory for the node line if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Create a list of parameter combinations\n    params_combinations = make_combinations(module_params)\n\n    # Create a list of modules with their corresponding parameter combinations\n    modules_with_params = [(module, params) for module in modules for params in params_combinations]\n\n    # Evaluate each module with its parameters\n    results = []\n    for module, params in modules_with_params:\n        result = evaluate_query_expansion_node(module, params, previous_result, node_line_dir, strategies)\n        results.append(result)\n\n    # Explode the results list to create a dataframe with a row for each module-parameter combination\n    results_df = explode(results)\n\n    # Save the results dataframe to a CSV file\n    results_df.to_csv(os.path.join(node_line_dir, \"results.csv\"), index=False)\n\n    # Select the best result based on the evaluation\n    best_result = select_best_result(results_df, strategies)\n\n    # Save the best result to a CSV file\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Return the best result\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the output directory for the query expansion node\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Get the list of supported modules\n    supported_modules = get_support_modules()\n\n    # Create a dictionary to store the results of each module\n    results = {}\n\n    # Create a list to store the execution times of each module\n    execution_times = []\n\n    # Create a list to store the evaluation metrics of each module\n    evaluation_metrics = []\n\n    # Loop through each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Check if the module is supported\n        if module.__name__ not in supported_modules:\n            logger.warning(f\"Module {module.__name__} is not supported.\")\n            continue\n\n        # Create a directory for the module\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Run the module with the given parameters\n        result, execution_time, evaluation_metric = evaluate_query_expansion_node(module, params, previous_result, module_dir)\n\n        # Store the result, execution time, and evaluation metric in the dictionary\n        results[module.__name__] = result\n        execution_times.append(execution_time)\n        evaluation_metrics.append(evaluation_metric)\n\n    # Save the results of each module to a separate file in the node directory\n    for module_name, result in results.items():\n        result_path = os.path.join(node_line_dir, module_name + \".csv\")\n        result.to_csv(result_path, index=False)\n\n    # Create a dataframe from the execution times and evaluation metrics\n    summary = pd.DataFrame({\"module\": list(results.keys()), \"execution_time\": execution_times, \"evaluation_metric\": evaluation_metrics})\n\n    # Save the summary to a file in the node directory\n    summary_path = os.path.join(node_line_dir, \"summary."}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Query Expansion Node\")\n\n    # Create directory for node results\n    node_dir = os.path.join(node_line_dir, \"QueryExpansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a dictionary to store the results for each module\n    results = {}\n\n    # Run each module with the given parameters and save the results\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running module: {module.__name__}\")\n        results[module.__name__] = module(previous_result, **params)\n        results[module.__name__].to_csv(os.path.join(node_dir, f\"{module.__name__}.csv\"), index=False)\n\n    # Measure the execution time of each module\n    execution_times = measure_speed(results)\n\n    # Evaluate the performance of each module based on the specified strategies\n    evaluation_results = evaluate_query_expansion_node(results, strategies)\n\n    # Save the execution times and evaluation results to a summary file\n    summary = pd.DataFrame({\"Module\": list(results.keys()), \"Execution Time\": execution_times, **evaluation_results})\n    summary.to_csv(os.path.join(node_dir, \"Summary.csv\"), index=False)\n\n    # Select the best module based on the evaluation results\n    best_module = select_best_average(summary, strategies)\n\n    # Save the best result to a file\n    best_result = results[best_module]\n    best_result.to_csv(os.path.join(node_dir, \"Best.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the current node line\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the current node line\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Get the support modules\n    support_modules = get_support_modules(modules)\n\n    # Get the parameters for each module\n    module_params = [deepcopy(param) for param in module_params]\n\n    # Create a list to store the results of each module\n    module_results = []\n\n    # Create a list to store the execution times of each module\n    module_times = []\n\n    # Create a list to store the summaries of each module\n    module_summaries = []\n\n    # Create a list to store the parameters of each module\n    module_params_list = []\n\n    # Loop through each module and its corresponding parameters\n    for module, param in zip(modules, module_params):\n\n        # Get the name of the module\n        module_name = module.__name__\n\n        # Create a directory for the current module\n        module_dir = os.path.join(node_line_dir, module_name)\n        pathlib.Path(module_dir).mkdir(parents=True, exist_ok=True)\n\n        # Run the module with the given parameters\n        result, time = support_modules[module_name](previous_result, param)\n\n        # Save the result to a file\n        result.to_csv(os.path.join(module_dir, \"result.csv\"), index=False)\n\n        # Add the result to the list of results\n        module_results.append(result)\n\n        # Add the execution time to the list of execution times\n        module_times.append(time)\n\n        # Create a summary of the module's performance\n        summary = pd.DataFrame({\"module\": module_name, \"time\": time}, index=[0])\n\n        # Add the summary to the list of summaries\n        module_summaries.append(summary"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running query expansion node\")\n\n    # Create directory for this node\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create directory for this node\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Get combinations of modules and params\n    module_param_combinations = make_combinations(modules, module_params)\n\n    # Run each module with its params\n    results = []\n    for module, module_params in module_param_combinations:\n\n        # Run module\n        result = module(previous_result, **module_params)\n\n        # Save results\n        results.append(result)\n\n        # Save results\n        result.to_csv(f\"{node_line_dir}/{module.__name__}.csv\", index=False)\n\n    # Evaluate each module\n    module_results = []\n    for result in results:\n\n        # Measure execution time\n        result = measure_speed(result, \"query_expansion\")\n\n        # Evaluate performance\n        for strategy, params in strategies.items():\n            result = strategy(result, **params)\n\n        # Save results\n        module_results.append(result)\n\n    # Save summary\n    summary = pd.concat(module_results)\n    summary.to_csv(f\"{node_line_dir}/summary.csv\", index=False)\n\n    # Select best result\n    best_result = select_best_average(summary, \"query_expansion\")\n\n    # Save best result\n    best_result.to_csv(f\"{node_line_dir}/best.csv\", index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the current node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Get the support modules\n    support_modules = get_support_modules()\n\n    # Create a directory for the current node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a list of combinations of modules and parameters\n    module_param_combinations = make_combinations(modules, module_params)\n\n    # Create a list to store the results of each module\n    module_results = []\n\n    # Iterate over each combination of modules and parameters\n    for i, (module, params) in enumerate(module_param_combinations):\n\n        # Create a directory for the current module if it doesn't exist\n        module_dir = os.path.join(node_line_dir, f\"module_{i + 1}\")\n        pathlib.Path(module_dir).mkdir(parents=True, exist_ok=True)\n\n        # Run the module with the given parameters\n        logger.info(f\"Running module {i + 1}\")\n        result = module(previous_result, **params)\n\n        # Measure the execution time of the module\n        execution_time = measure_speed(result)\n\n        # Save the execution time to a file\n        with open(os.path.join(module_dir, \"execution_time.txt\"), \"w\") as f:\n            f.write(f\"Execution time: {execution_time} seconds\")\n\n        # Evaluate the performance of the module using the specified strategies\n        strategy_results = {}\n\n        # Evaluate the performance using the specified strategies\n        for strategy_name, strategy_params in strategies.items():\n            logger.info(f\"Evaluating strategy {strategy_name}\")\n            strategy_result = strategy_params[\"function\"](result, **strategy_params[\"params\"])\n            strategy_results[strategy_name] = strategy_result\n\n       "}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create directory for node results\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Get the name of the module and save the results\n    for module_name, module, params in zip(get_support_modules(modules), modules, module_params):\n        logger.info(f\"Running {module_name} with params {params}\")\n\n        # Run module and get results\n        result = module(previous_result, **params)\n\n        # Save results\n        result.to_csv(f\"{node_line_dir}/{module_name}.csv\", index=False)\n\n    # Get the results from the node line directory\n    results = [pd.read_csv(f\"{node_line_dir}/{module_name}.csv\") for module_name in get_support_modules(modules)]\n\n    # Get the execution times for each module\n    execution_times = measure_speed(results)\n\n    # Save execution times\n    execution_times.to_csv(f\"{node_line_dir}/execution_times.csv\", index=False)\n\n    # Get the evaluation metrics for each module\n    evaluation_metrics = evaluate_query_expansion_node(results, strategies)\n\n    # Save evaluation metrics\n    evaluation_metrics.to_csv(f\"{node_line_dir}/evaluation_metrics.csv\", index=False)\n\n    # Get the best result\n    best_result = select_best_average(evaluation_metrics, strategies)\n\n    # Save the best result\n    best_result.to_csv(f\"{node_line_dir}/best_result.csv\", index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running query expansion node\")\n    # Create the directory for the current node if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Get the support modules\n    support_modules = get_support_modules()\n\n    # Create a list of parameter combinations\n    combinations = make_combinations(module_params)\n\n    # Run the query expansion modules with different parameters\n    results = []\n    for i, params in enumerate(combinations):\n        logger.info(f\"Running module {i + 1} with parameters {params}\")\n\n        # Run the query expansion module\n        result = modules[i](previous_result, params)\n\n        # Measure the execution time of the query expansion module\n        execution_time = measure_speed(result)\n\n        # Evaluate the query expansion module based on the specified strategies\n        evaluation = evaluate_query_expansion_node(result, strategies, support_modules)\n\n        # Save the results and a summary to the specified directory\n        result_path = os.path.join(node_line_dir, f\"module_{i + 1}_result.csv\")\n        summary_path = os.path.join(node_line_dir, f\"module_{i + 1}_summary.csv\")\n        result.to_csv(result_path, index=False)\n        summary = pd.DataFrame({\n            \"execution_time\": [execution_time],\n            \"evaluation\": [evaluation]\n        })\n        summary.to_csv(summary_path, index=False)\n\n        # Save the results and summaries to the results list\n        results.append({\n            \"module\": i + 1,\n            \"params\": params,\n            \"result\": result,\n            \"summary\": summary\n        })\n\n    # Select the best result based on the evaluation\n    best_result = select_best_average(results, \"evaluation\")\n\n    # Save the best result to the specified directory\n    best_result_path = os.path.join(node_line_dir, \"best_result.csv\")\n   "}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Validate the input data\n    validate_qa_dataset(previous_result, \"previous_result\")\n\n    # Create the output directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # create node directory\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # create node summary directory\n    node_summary_dir = os.path.join(node_dir, 'summary')\n    pathlib.Path(node_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # create node output directory\n    node_output_dir = os.path.join(node_dir, 'output')\n    pathlib.Path(node_output_dir).mkdir(parents=True, exist_ok=True)\n\n    # create node output summary directory\n    node_output_summary_dir = os.path.join(node_output_dir, 'summary')\n    pathlib.Path(node_output_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # create node output output directory\n    node_output_output_dir = os.path.join(node_output_dir, 'output')\n    pathlib.Path(node_output_output_dir).mkdir(parents=True, exist_ok=True)\n\n    # create node output output summary directory\n    node_output_output_summary_dir = os.path.join(node_output_output_dir, 'summary')\n    pathlib.Path(node_output_output_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # create node output output output directory\n    node_output_output_output_dir = os.path.join(node_output_output_dir, 'output')\n    pathlib.Path(node_output_output_output_dir).mkdir(parents=True, exist_ok=True)\n\n    # create node output output output summary directory\n    node_output_output_output_summary_dir = os.path.join(node_output_output_output_dir, 'summary')\n    pathlib.Path(node_output_output_output_summary_dir).mkdir(parents=True, exist_ok"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for the current node if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    node_output_dir = os.path.join(node_line_dir, \"output\")\n    os.makedirs(node_output_dir, exist_ok=True)\n\n    # Create a directory for the node's summary\n    node_summary_dir = os.path.join(node_line_dir, \"summary\")\n    os.makedirs(node_summary_dir, exist_ok=True)\n\n    # Create a directory for the node's execution times\n    node_execution_dir = os.path.join(node_line_dir, \"execution_time\")\n    os.makedirs(node_execution_dir, exist_ok=True)\n\n    # Create a directory for the node's evaluation metrics\n    node_evaluation_dir = os.path.join(node_line_dir, \"evaluation_metrics\")\n    os.makedirs(node_evaluation_dir, exist_ok=True)\n\n    # Create a directory for the node's evaluation metrics\n    node_evaluation_dir = os.path.join(node_line_dir, \"evaluation_metrics\")\n    os.makedirs(node_evaluation_dir, exist_ok=True)\n\n    # Create a directory for the node's evaluation metrics\n    node_evaluation_dir = os.path.join(node_line_dir, \"evaluation_metrics\")\n    os.makedirs(node_evaluation_dir, exist_ok=True)\n\n    # Create a directory for the node's evaluation metrics\n    node_evaluation_dir = os.path.join(node_line_dir, \"evaluation_metrics\")\n    os.makedirs(node_evaluation_dir, exist_ok=True)\n\n    # Create a directory for the node's evaluation metrics\n    node_evaluation_dir = os.path.join(node_line_dir, \"evaluation_met"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, \"prompt_maker\"), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, \"prompt_maker\", \"output\"), exist_ok=True)\n\n    # Run prompt maker modules and evaluate their performance\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(**params)\n        result[\"module_name\"] = module.__name__\n        results.append(result)\n\n    # Select the best prompt maker module based on specified strategies\n    best_prompt_maker = select_best_average(results, strategies[\"metrics\"])\n\n    # Save the best prompt maker's output\n    best_prompt_maker.to_csv(os.path.join(node_line_dir, \"prompt_maker\", \"output\", \"best_prompt_maker.csv\"), index=False)\n\n    # Combine the results of the previous operation and the best prompt maker's output\n    combined_result = pd.concat([previous_result, best_prompt_maker], ignore_index=True)\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Validate the input parameters\n    validate_qa_dataset(previous_result)\n\n    # Get the generator module from the strategies dictionary\n    generator_module = strategies.get(\"generator_module\", None)\n\n    # Create the node_line_dir if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Get the generator module from the strategies dictionary\n    generator_module = strategies.get(\"generator_module\", None)\n\n    # Create the node_line_dir if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a subdirectory for the current node\n    node_dir = os.path.join(node_line_dir, \"prompt_maker\")\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create a subdirectory for the current node\n    node_dir = os.path.join(node_line_dir, \"prompt_maker\")\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create a subdirectory for the current node\n    node_dir = os.path.join(node_line_dir, \"prompt_maker\")\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create a subdirectory for the current node\n    node_dir = os.path.join(node_line_dir, \"prompt_maker\")\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create a subdirectory for the current node\n    node_dir = os.path.join(node_line_dir, \"prompt_maker\")\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create a subdirectory for the current node\n    node_dir = os.path.join(node_line_dir, \"prompt_maker\")\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create a subdirectory for the current node\n    node_dir = os.path.join("}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run prompt maker modules\n    result_list = []\n    for module, params in zip(modules, module_params):\n        result = module(**params)\n        result['module'] = module.__name__\n        result['params'] = params\n        result_list.append(result)\n\n    # Combine results and evaluate\n    result_df = pd.concat(result_list, ignore_index=True)\n    result_df = pd.concat([previous_result, result_df], ignore_index=True)\n    result_df = evaluate_generation(result_df, strategies['evaluation_metrics'])\n\n    # Select best prompt maker\n    best_prompt_maker = select_best_average(result_df, strategies['evaluation_metrics'])\n\n    # Save results\n    result_df.to_csv(os.path.join(node_line_dir, 'result.csv'), index=False)\n    best_prompt_maker.to_csv(os.path.join(node_line_dir, 'best_prompt_maker.csv'), index=False)\n\n    # Save summary\n    summary_df = cast_metrics(result_df, strategies['evaluation_metrics'])\n    summary_df.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    # Return combined results\n    return result_df\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Validate and prepare the input data\n    validate_qa_dataset(previous_result)\n\n    # Run prompt maker modules with given parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(previous_result, **params)\n        results.append(result)\n\n    # Evaluate the performance of each prompt maker module\n    evaluation_results = evaluate_generation(results, strategies['generator'])\n\n    # Select the best prompt maker module based on specified strategies\n    best_module = select_best_average(evaluation_results, strategies['metrics'], strategies['threshold'])\n\n    # Save the results and a summary, including execution times and evaluation metrics, to the specified directory\n    best_module.to_csv(os.path.join(node_line_dir, 'best_module_result.csv'), index=False)\n    evaluation_results.to_csv(os.path.join(node_line_dir, 'evaluation_results.csv'), index=False)\n\n    # Combine the results of the previous operation and the best prompt maker's output\n    combined_result = pd.concat([previous_result, best_module])\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Validate input parameters\n    validate_qa_dataset(modules, \"prompt_maker\")\n    validate_qa_dataset(module_params, \"prompt_maker\")\n    validate_qa_dataset(previous_result, \"prompt_maker\")\n    validate_qa_dataset(strategies, \"prompt_maker\")\n\n    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    summary_dir = os.path.join(node_line_dir, \"summary\")\n    os.makedirs(summary_dir, exist_ok=True)\n\n    # Create a list of prompt maker module names\n    module_names = [module.__name__ for module in modules]\n\n    # Create a list of combinations of prompt maker module parameters\n    module_param_combinations = make_combinations(module_params)\n\n    # Create a list of prompt maker module names and parameter combinations\n    module_param_combinations = [(module_name, param_comb) for module_name, param_comb in\n                                 zip(module_names, module_param_combinations)]\n\n    # Create a list of prompt maker module names and parameter combinations\n    module_param_combinations = [(module_name, param_comb) for module_name, param_comb in\n                                 zip(module_names, module_param_combinations)]\n\n    # Create a list of prompt maker module names and parameter combinations\n    module_param_combinations = [(module_name, param_comb) for module_name, param_comb in\n                                 zip(module_names, module_param_combinations)]\n\n    # Create a list of prompt maker module names and parameter combinations\n    module_param_combinations = [(module_name, param_comb) for module_name, param_comb in\n                                 zip(module_names, module_param_combinations)]\n\n    # Create a list of prompt maker module names and parameter combinations\n    module_param_combinations = [(module_name, param_comb) for module_name, param_comb in\n                                 zip(module_"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker'), exist_ok=True)\n\n    # Execute prompt maker modules\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(**params)\n        results.append(result)\n\n    # Combine results\n    result_df = pd.concat(results, axis=0, ignore_index=True)\n    result_df = pd.concat([previous_result, result_df], axis=0, ignore_index=True)\n\n    # Select best prompt maker\n    if strategies.get('speed_threshold', None) is not None:\n        result_df = filter_by_threshold(result_df, strategies['speed_threshold'], 'speed')\n\n    if strategies.get('metrics', None) is not None:\n        result_df = cast_metrics(result_df, strategies['metrics'])\n        result_df = select_best_average(result_df, strategies['metrics'])\n\n    # Save results\n    result_df.to_csv(os.path.join(node_line_dir, 'prompt_maker', 'result.csv'), index=False)\n\n    # Save summary\n    summary = pd.DataFrame(columns=['module', 'speed', 'metrics'])\n    for i, module in enumerate(modules):\n        summary.loc[i] = [module.__name__, result_df.loc[i, 'speed'], result_df.loc[i, 'metrics']]\n    summary.to_csv(os.path.join(node_line_dir, 'prompt_maker', 'summary.csv'), index=False)\n\n    return result_df\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Validate the input parameters\n    validate_qa_dataset(previous_result)\n\n    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a list of all possible combinations of modules and parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of dataframes for each combination\n    result_list = []\n    for i, (module, params) in enumerate(combinations):\n        # Create a directory for the current combination\n        combination_dir = os.path.join(node_line_dir, f\"combination_{i}\")\n        os.makedirs(combination_dir, exist_ok=True)\n\n        # Execute the current combination of modules and parameters\n        result = module(previous_result, **params)\n\n        # Save the result to a CSV file\n        result.to_csv(os.path.join(combination_dir, \"result.csv\"), index=False)\n\n        # Save the parameters to a JSON file\n        with open(os.path.join(combination_dir, \"params.json\"), \"w\") as f:\n            json.dump(params, f)\n\n        # Add the result to the list\n        result_list.append(result)\n\n    # Combine the results from all combinations into a single dataframe\n    combined_result = pd.concat(result_list, ignore_index=True)\n\n    # Save the combined result to a CSV file\n    combined_result.to_csv(os.path.join(node_line_dir, \"combined_result.csv\"), index=False)\n\n    # Evaluate the performance of each combination using the specified evaluation metrics\n    evaluation_results = evaluate_generation(combined_result, strategies[\"evaluation_metrics\"])\n\n    # Save the evaluation results to a CSV file\n    evaluation_results.to_csv(os.path.join(node_line_dir, \"evaluation_results.csv\"), index=False)\n\n    # Select the best prompt maker module based on the specified strategies\n    best_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for the current node if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Get the generator module from the strategies\n    generator_module = strategies.get(\"generator_module\", None)\n    if generator_module is None:\n        raise ValueError(\"Generator module not specified in strategies.\")\n\n    # Get the generator module's parameters\n    generator_module_params = strategies.get(\"generator_module_params\", {})\n\n    # Get the evaluation metrics from the strategies\n    evaluation_metrics = strategies.get(\"evaluation_metrics\", None)\n    if evaluation_metrics is None:\n        raise ValueError(\"Evaluation metrics not specified in strategies.\")\n\n    # Get the speed thresholds from the strategies\n    speed_thresholds = strategies.get(\"speed_thresholds\", None)\n    if speed_thresholds is None:\n        raise ValueError(\"Speed thresholds not specified in strategies.\")\n\n    # Get the evaluation metrics for speed thresholds\n    speed_thresholds_metrics = strategies.get(\"speed_thresholds_metrics\", None)\n    if speed_thresholds_metrics is None:\n        raise ValueError(\"Speed thresholds metrics not specified in strategies.\")\n\n    # Create a list of combinations of module and its parameters\n    module_combinations = make_combinations(modules, module_params)\n\n    # Initialize a list to store the results of each module\n    results = []\n\n    # Iterate over the combinations of module and its parameters\n    for module_combination in module_combinations:\n        # Extract the module and its parameters\n        module = module_combination[0]\n        module_params = module_combination[1]\n\n        # Run the module with the given parameters\n        result = module(**module_params)\n\n        # Add the module name to the result\n        result[\"module\"] = module.__name__\n\n        # Add the module parameters to the result\n        result[\"module_params\"] = module_params\n\n        # Append the result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Check if the directory exists, if not, create it\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize a list to store the results of each prompt maker module\n    results = []\n\n    # Iterate through each prompt maker module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Execute the prompt maker module with the given parameters\n        result = module(**params)\n\n        # Append the result to the list of results\n        results.append(result)\n\n    # Combine the results of all prompt maker modules into a single dataframe\n    result = pd.concat(results, ignore_index=True)\n\n    # Combine the results of the previous operation and the best prompt maker's result\n    result = pd.concat([previous_result, result], ignore_index=True)\n\n    # Save the combined result to a CSV file\n    result.to_csv(os.path.join(node_line_dir, \"result.csv\"), index=False)\n\n    # Save a summary of the execution times and evaluation metrics to a CSV file\n    summary = result.groupby([\"prompt_maker_name\", \"prompt_maker_params\"]).agg(\n        execution_time=(\"execution_time\", \"mean\"),\n        **cast_metrics(strategies[\"metrics\"])\n    )\n    summary.to_csv(os.path.join(node_line_dir, \"summary.csv\"))\n\n    # Select the best prompt maker module based on the specified strategies\n    best_prompt_maker = select_best_average(summary, strategies[\"metrics\"])\n\n    # Filter the result dataframe by the best prompt maker module\n    result = result[result[\"prompt_maker_name\"] == best_prompt_maker[\"prompt_maker_name\"]]\n\n    # Return the combined result of the previous operation and the best prompt maker's result\n    return result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Validate input arguments\n    validate_qa_dataset(previous_result)\n\n    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Run all prompt maker modules\n    results = []\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        result = module(**params)\n        result['module_name'] = module.__name__\n        results.append(result)\n\n    # Combine results from all prompt maker modules\n    combined_result = pd.concat(results)\n\n    # Evaluate the best prompt maker module\n    best_prompt_maker_module = strategies['best_prompt_maker_module']\n    best_prompt_maker_params = strategies['best_prompt_maker_params']\n    best_prompt_maker_result = best_prompt_maker_module(**best_prompt_maker_params)\n    best_prompt_maker_result['module_name'] = best_prompt_maker_module.__name__\n\n    # Combine the best prompt maker module's result with the previous result\n    combined_result = pd.concat([combined_result, best_prompt_maker_result])\n\n    # Save the combined result\n    combined_result.to_csv(os.path.join(node_line_dir, 'result.csv'), index=False)\n\n    # Save the summary of the node's execution\n    summary = {'execution_time': combined_result['execution_time'].sum(),\n               'metrics': combined_result['metrics'].iloc[0]}\n    pd.DataFrame(summary, index=[0]).to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Validate input arguments\n    validate_qa_dataset(previous_result)\n    assert isinstance(modules, list) and len(modules) > 0, \"modules should be a non-empty list of callable functions\"\n    assert isinstance(module_params, list) and len(module_params) > 0, \"module_params should be a non-empty list of dictionaries\"\n    assert isinstance(node_line_dir, str) and node_line_dir != \"\", \"node_line_dir should be a non-empty string\"\n    assert isinstance(strategies, dict) and len(strategies) > 0, \"strategies should be a non-empty dictionary\"\n\n    # Create necessary directories\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a list of all combinations of modules and module_params\n    combinations = make_combinations(modules, module_params)\n\n    # Run each combination and evaluate its performance\n    results = []\n    for combination in combinations:\n        module = combination[0]\n        module_param = combination[1]\n        result = module(**module_param)\n        result = cast_metrics(result)\n        results.append(result)\n\n    # Combine results from previous operations with the best prompt maker's result\n    results = pd.concat([previous_result, pd.concat(results)])\n\n    # Select the best prompt maker module based on specified strategies\n    if \"speed\" in strategies:\n        results = measure_speed(results, strategies[\"speed\"])\n\n    if \"threshold\" in strategies:\n        results = filter_by_threshold(results, strategies[\"threshold\"])\n\n    if \"generator_module\" in strategies:\n        results = evaluate_generation(results, strategies[\"generator_module\"])\n\n    if \"best\" in strategies:\n        results = select_best_average(results, strategies[\"best\"])\n\n    # Save the results and a summary, including execution times and evaluation metrics, to the specified directory\n    results.to_csv(os.path.join(node_"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Get the generator module and its parameters\n    generator_module = strategies['generator_module']\n    generator_params = strategies['generator_params']\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Create a dataframe to store the results\n    results = pd.DataFrame()\n\n    # Create a dataframe to store the execution times\n    execution_times = pd.DataFrame()\n\n    # Create a dataframe to store the evaluation metrics\n    evaluation_metrics = pd.DataFrame()\n\n    # Create a dataframe to store the evaluation metrics\n    evaluation_metrics = pd.DataFrame()\n\n    # Create a dataframe to store the evaluation metrics\n    evaluation_metrics = pd.DataFrame()\n\n    # Iterate over the prompt maker modules and their parameters\n    for module, module_param in zip(modules, module_params):\n\n        # Run the prompt maker module\n        result, execution_time = module(module_param)\n\n        # Evaluate the prompt maker module's output\n        evaluation_metric = evaluate_generation(result, generator_module, generator_params)\n\n        # Add the execution time and evaluation metric to the dataframes\n        execution_times = execution_times.append(\n            pd.DataFrame({'execution_time': [execution_time]}),\n            ignore_index=True\n        )\n        evaluation_metrics = evaluation_metrics.append(\n            pd.DataFrame({'evaluation_metric': [evaluation_metric]}),\n            ignore_index=True\n        )\n\n        # Combine the results with the previous result\n        result = pd.concat([result, previous_result], axis=0)\n\n        # Add the result to the results dataframe\n        results = results.append(result, ignore_index=True)\n\n    # Save the results to the output directory\n    results.to_csv(os.path.join(node_line_dir, 'results.csv'), index=False)\n\n    # Save the execution"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Generate combinations of prompt maker modules and their parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Run prompt maker modules\n    results = []\n    for combination in combinations:\n        module = combination[0]\n        params = combination[1]\n        # Run the prompt maker module with the specified parameters\n        result = module(**params)\n        # Add the module name and parameters to the result\n        result['module'] = module.__name__\n        result['params'] = params\n        # Append the result to the list of results\n        results.append(result)\n\n    # Convert the results to a dataframe\n    results = pd.DataFrame(results)\n\n    # Combine the results with the previous result\n    results = pd.concat([previous_result, results], ignore_index=True)\n\n    # Get the best prompt maker module based on the specified strategies\n    best_module = select_best_average(results, strategies)\n\n    # Get the best prompt maker module's parameters\n    best_params = best_module['params']\n\n    # Get the best prompt maker module's name\n    best_module_name = best_module['module']\n\n    # Get the best prompt maker module's output\n    best_output = best_module[['prompt', 'completion']]\n\n    # Get the best prompt maker module's execution time\n    best_execution_time = best_module['execution_time']\n\n    # Get the best prompt maker module's evaluation metrics\n    best_metrics = best_module.drop(['prompt', 'completion', 'execution_time', 'module', 'params'], axis=1)\n\n    # Cast the evaluation metrics to the specified data type\n    best_metrics = cast_metrics(best_metrics, strategies['metrics'])\n\n    # Save the best prompt maker module's output to a file\n    best_output.to_csv(os.path.join(node_line_dir, f\"{best_module_name}_output."}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for the current node\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Combine the parameters for each module into a list of dictionaries\n    params_list = make_combinations(module_params)\n\n    # Create a list of dictionaries to store the results of each module\n    results = []\n\n    # Iterate over each module and its parameters\n    for module, params in zip(modules, params_list):\n        # Create a directory for the current module\n        os.makedirs(os.path.join(node_line_dir, module.__name__), exist_ok=True)\n\n        # Run the module with the specified parameters\n        result = module(**params)\n\n        # Store the results of the module\n        results.append(result)\n\n        # Save the results to a file\n        result.to_csv(os.path.join(node_line_dir, module.__name__, f\"{module.__name__}.csv\"), index=False)\n\n    # Combine the results into a single dataframe\n    result = pd.concat(results, ignore_index=True)\n\n    # Save the combined results to a file\n    result.to_csv(os.path.join(node_line_dir, \"result.csv\"), index=False)\n\n    # Save the summary of the node's execution to a file\n    with open(os.path.join(node_line_dir, \"summary.txt\"), \"w\") as f:\n        f.write(f\"{result.shape[0]} prompts generated.\\n\")\n        f.write(f\"Execution time: {result['execution_time'].sum()} seconds.\\n\")\n        f.write(f\"Prompts per second: {result['execution_time'].sum() / result.shape[0]} seconds.\\n\")\n\n    # Select the best prompt maker module based on the specified strategies\n    best_result = select_best_average(result, strategies)\n\n    # Save the best result to a file\n    best_result.to_csv(os.path.join(node_line_dir,"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Validate the input parameters\n    validate_qa_dataset(modules, module_params)\n\n    # Create the output directory if it does not exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a list of all combinations of prompt maker modules and their parameters\n    combinations = make_combinations(modules, module_params)\n\n    # Create a list of all possible generator modules\n    generator_modules = get_support_modules(\"generator\")\n\n    # Initialize an empty list to store the results of each combination\n    results = []\n\n    # Iterate over all combinations and run each prompt maker module with its parameters\n    for combination in combinations:\n        # Extract the prompt maker module and its parameters from the combination\n        prompt_maker_module = combination[0]\n        prompt_maker_params = combination[1]\n\n        # Run the prompt maker module with the specified parameters\n        result = prompt_maker_module(**prompt_maker_params)\n\n        # Add the combination and the result to the list of results\n        results.append((combination, result))\n\n    # Extract the best prompt maker module from the results based on the specified strategies\n    best_prompt_maker_module, best_prompt_maker_params, best_result = select_best_average(results, strategies)\n\n    # Create a directory for the best prompt maker module and its parameters\n    best_prompt_maker_dir = os.path.join(node_line_dir, f\"{best_prompt_maker_module.__name__}\")\n    pathlib.Path(best_prompt_maker_dir).mkdir(parents=True, exist_ok=True)\n\n    # Save the best prompt maker module and its parameters to a file\n    best_prompt_maker_file = os.path.join(best_prompt_maker_dir, \"best_prompt_maker.txt\")\n    with open(best_prompt_maker_file, \"w\") as f:\n        f.write(f\"{best_prompt_maker_module.__name__} {best_prompt"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Get the default generator module\n    generator_module = strategies.get('generator_module', 'default')\n\n    # Get the evaluation metrics\n    evaluation_metrics = strategies.get('evaluation_metrics', ['rouge1', 'rouge2', 'rougeL'])\n\n    # Get the speed thresholds\n    speed_thresholds = strategies.get('speed_thresholds', {'fast': 10, 'medium': 30, 'slow': 60})\n\n    # Get the generator module parameters\n    generator_module_params = strategies.get('generator_module_params', {})\n\n    # Validate the evaluation metrics\n    evaluation_metrics = cast_metrics(evaluation_metrics)\n\n    # Create a copy of the previous result\n    previous_result = deepcopy(previous_result)\n\n    # Combine the module parameters into a list of dictionaries\n    module_params = make_combinations(module_params)\n\n    # Create a list of dictionaries to store the results\n    results = []\n\n    # Iterate over the modules and parameters\n    for module, params in zip(modules, module_params):\n        # Run the module with the given parameters\n        result = module(**params)\n\n        # Add the module name and parameters to the result\n        result['module_name'] = module.__name__\n        result['module_params'] = params\n\n        # Add the result to the list of results\n        results.append(result)\n\n    # Evaluate the results\n    evaluation_results = evaluate_generation(results,\n                                             generator_module=generator_module,\n                                             generator_module_params=generator_module_params,\n                                             evaluation_metrics=evaluation_metrics)\n\n    # Add the evaluation results to the results list\n    results = [dict(x, **y) for x, y in zip(results, evaluation_results)]\n\n    # Select the best result based on the specified strategies\n    best_result = select_best_average(results, strategies)"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Convert ground truth strings to embeddings\n    ground_truth_embeddings = embedding_model.embed_documents(generation_gt)\n\n    # Convert predicted string to embedding\n    predicted_embedding = embedding_model.embed_query(pred)\n\n    # Calculate cosine similarity between predicted embedding and ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(predicted_embedding, ground_truth_embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Convert ground truth strings to embeddings\n    gt_embeddings = embedding_model.get_embeddings(generation_gt)\n\n    # Convert predicted string to embedding\n    pred_embedding = embedding_model.get_embeddings([pred])\n\n    # Calculate cosine similarity between predicted embedding and ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    # Convert the ground truth strings into embeddings\n    gt_embeddings = embedding_model.get_query_embedding(generation_gt)\n\n    # Convert the predicted string into an embedding\n    pred_embedding = embedding_model.get_query_embedding(pred)\n\n    # Calculate the cosine similarity between the predicted embedding and the ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Convert the input strings into embeddings\n    gt_embeddings = embedding_model.embed_documents(generation_gt)\n    pred_embedding = embedding_model.embed_query(pred)\n\n    # Calculate the cosine similarity between the predicted string and each ground truth string\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    # Convert the input strings into embeddings\n    gt_embeddings = embedding_model.get_embedding(generation_gt)\n    pred_embedding = embedding_model.get_embedding(pred)\n\n    # Compute the cosine similarity between the predicted string and each ground truth string\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Convert ground truth strings into embeddings\n    gt_embeddings = embedding_model.embed_documents(generation_gt)\n    # Convert prediction string into an embedding\n    pred_embedding = embedding_model.embed_query(pred)\n\n    # Calculate cosine similarity between prediction and ground truth embeddings\n    similarity_scores = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n    return max(similarity_scores)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Convert ground truth strings to embeddings\n    gt_embeddings = embedding_model.embed_documents(generation_gt)\n\n    # Convert predicted string to embedding\n    pred_embedding = embedding_model.embed_query(pred)\n\n    # Calculate cosine similarity between predicted string and ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n    pred_embedding = embedding_model.get_text_embedding(pred)\n    gt_embeddings = embedding_model.get_text_embeddings(generation_gt)\n    return calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model(model_name='all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.get_agg_embedding(pred)\n    gt_embeddings = [embedding_model.get_agg_embedding(gt) for gt in generation_gt]\n\n    return calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.load_or_init_embedding_model('all-mpnet-base-v2')\n\n    # Convert the ground truth strings into embeddings\n    gt_embeddings = embedding_model.get_query_embeddings(generation_gt)\n\n    # Convert the prediction string into an embedding\n    pred_embedding = embedding_model.get_query_embeddings([pred])[0]\n\n    # Calculate the cosine similarity between the prediction and each ground truth string\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n\n    # Convert the input strings into embeddings\n    embeddings = embedding_model.get_agg_embedding(generation_gt)\n    pred_embedding = embedding_model.get_agg_embedding([pred])\n\n    # Calculate the cosine similarity between the predicted string and each ground truth string\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model(\n            model_name='all-mpnet-base-v2')\n    gt_embeddings = embedding_model.embed_documents(generation_gt)\n    pred_embedding = embedding_model.embed_query(pred)\n    return calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # Load the embedding model if not provided\n    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Convert the input strings into embeddings\n    gt_embeddings = embedding_model.get_embeddings(generation_gt)\n    pred_embeddings = embedding_model.get_embeddings([pred])\n\n    # Calculate the cosine similarity between the predicted string and each ground truth string\n    cosine_similarities = calculate_cosine_similarity(pred_embeddings, gt_embeddings)\n\n    # Return the maximum cosine similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model(\n            os.environ.get('EMBEDDING_MODEL', 'all-mpnet-base-v2'))\n\n    # Convert the input strings into embeddings\n    embeddings = embedding_model.get_text_embedding(generation_gt)\n\n    # Compute the cosine similarity between the predicted string and the ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(embedding_model.get_text_embedding([pred]), embeddings)\n\n    # Return the maximum cosine similarity\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    # Convert the input strings into embeddings\n    embeddings = embedding_model.get_text_embedding(generation_gt + [pred])\n\n    # Compute cosine similarity between the prediction and each ground truth string\n    scores = calculate_cosine_similarity(embeddings[:-1], embeddings[-1])\n\n    # Return the maximum cosine similarity score\n    return max(scores)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model(\n            os.environ.get(\"EMBEDDING_MODEL\", \"all-mpnet-base-v2\"))\n    embedding_model.set_embedding_model_type(\"text\")\n    # Convert ground truth strings to embeddings\n    ground_truth_embeddings = embedding_model.get_query_embeddings(generation_gt)\n    # Convert predicted string to embedding\n    prediction_embedding = embedding_model.get_query_embeddings([pred])\n    # Calculate cosine similarity between predicted embedding and ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(\n        prediction_embedding, ground_truth_embeddings)\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    pred_embedding = embedding_model.get_query_embedding(pred)\n    gt_embeddings = embedding_model.get_doc_embeddings(generation_gt)\n    return calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    embedding_model.set_embedding_model_name('all-mpnet-base-v2')\n    embedding_model.set_embedding_model_type('sentence-transformers')\n\n    embedding_model.set_model_path(os.path.join(embedding_model.get_model_path(), 'all-mpnet-base-v2'))\n    embedding_model.load_model()\n\n    embedding_model.set_model_path(os.path.join(embedding_model.get_model_path(), 'all-mpnet-base-v2'))\n    embedding_model.load_model()\n\n    # Convert ground truth strings to embeddings\n    embedding_gt = embedding_model.get_embeddings(generation_gt)\n\n    # Convert predicted string to embedding\n    embedding_pred = embedding_model.get_embeddings([pred])\n\n    # Calculate cosine similarity between predicted embedding and ground truth embeddings\n    cosine_similarity = calculate_cosine_similarity(embedding_pred, embedding_gt)\n\n    # Return maximum cosine similarity\n    return max(cosine_similarity)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # Load the embedding model if not provided\n    if embedding_model is None:\n        embedding_model = embedding_models.load_embedding_model('all-mpnet-base-v2')\n\n    # Convert the ground truth strings into embeddings\n    gt_embeddings = embedding_model.get_query_embeddings(generation_gt)\n\n    # Convert the predicted string into an embedding\n    pred_embedding = embedding_model.get_query_embeddings([pred])[0]\n\n    # Compute the cosine similarity between the predicted embedding and the ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Compute the embeddings for the ground truth strings and the prediction\n    embedding_gt = embedding_model.get_query_embedding(generation_gt)\n    embedding_pred = embedding_model.get_query_embedding([pred])\n\n    # Compute the cosine similarity between the prediction and the ground truth strings\n    return calculate_cosine_similarity(embedding_gt, embedding_pred)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up, skipping face restoration\")\n        return np_image\n    return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except errors.ModelError as e:\n        logger.warning(f\"GFPGAN face restorer failed: {e}\")\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n    if gfpgan_face_restorer is None:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(shared.args.gfpgan_model_path)\n    if gfpgan_face_restorer.is_loaded():\n        return gfpgan_face_restorer.restore(np_image)\n    else:\n        logger.warning(\"GFPGAN face restorer is not set up. Skipping.\")\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if shared.opts.gfpgan_face_restore_strength != 1:\n        logger.warning(\"GFPGAN face restoration is disabled because the strength is not 1\")\n        return np_image\n\n    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restoration is disabled because it is not set up\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except errors.ModelError:\n        logger.warning(\"GFPGAN face restoration is disabled because the model is not set up\")\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up, skipping face restoration\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(f\"{gfpgan_face_restorer.name()} face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is not None:\n        return gfpgan_face_restorer.restore(np_image)\n    else:\n        logger.warning(\"GFPGAN face restorer not set up, skipping face restoration\")\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up. Skipping face restoration.\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except errors.FaceRestorationError as e:\n        logger.warning(f\"GFPGAN face restoration failed: {e}\")\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up, skipping face restoration\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except Exception:\n        logger.warning(\"GFPGAN face restoration failed, skipping\", exc_info=True)\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up, skipping face restoration\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(f\"GFPGAN face restorer not set up\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except errors.ModelError:\n        logger.warning(f\"GFPGAN face restorer model error\")\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up, skipping face restoration.\")\n        return np_image\n    else:\n        return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except errors.FaceRestorationError as e:\n        logger.warning(f\"GFPGAN face restorer error: {e}\")\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is not None:\n        return gfpgan_face_restorer.restore(np_image)\n    else:\n        logger.warning(\n            \"GFPGAN face restorer not set up. Skipping face restoration. To set up the GFPGAN face restorer, set the 'gfpgan_model_path' option in the config.yaml file to the path of the GFPGAN model.\"\n        )\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n\n    if gfpgan_face_restorer is None:\n        if shared.cmd_opts.no_half_gfpgan:\n            gfpgan_face_restorer = FaceRestorerGFPGAN(\n                model_path=None,\n                model_url=None,\n                model_name=None,\n                model_hash=None,\n                use_half=False,\n            )\n        else:\n            gfpgan_face_restorer = FaceRestorerGFPGAN(\n                model_path=None,\n                model_url=None,\n                model_name=None,\n                model_hash=None,\n                use_half=True,\n            )\n\n    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up, skipping face restoration\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except errors.ModelError:\n        logger.warning(\"GFPGAN face restorer not set up, skipping face restoration\")\n        return np_image\n    except Exception:\n        logger.exception(\"GFPGAN face restorer failed, skipping face restoration\")\n        return np_image\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up\")\n        return np_image\n    return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\n            f\"GFPGAN face restorer is not set up. Skipping face restoration.\"\n        )\n        return np_image\n    return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(face_restorer)\n    except errors.ModelNotFoundError as e:\n        logger.error(e)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        logger.info(f\"CodeFormer restored initialized\")\n    except Exception as e:\n        logger.error(f\"CodeFormer setup failed: {e}\")\n        raise errors.ModelError(f\"CodeFormer setup failed: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except errors.ModelError as e:\n        logger.warning(f\"Error setting up CodeFormer: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        logger.error(f\"CodeFormer setup error: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except errors.ModelNotFoundError:\n        logger.warning(\"CodeFormer model not found\")\n    except Exception as e:\n        logger.warning(f\"Error setting up CodeFormer model: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except errors.ModelNotFoundError as e:\n        logger.warning(f\"Error setting up CodeFormer: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except errors.ModelError as e:\n        logger.error(f\"CodeFormer setup error: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer face restorer: {e}\")\n\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer face restoration: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n        logger.info(\"CodeFormer model loaded successfully\")\n    except errors.ModelError as e:\n        logger.warning(f\"Error loading CodeFormer model: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except Exception as e:\n        logger.error(f\"Failed to setup codeformer: {e}\")\n        raise errors.ModelError(f\"Failed to setup codeformer: {e}\") from e\n\n    face_restoration.face_restorers.append(codeformer)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        logger.error(f'Error setting up CodeFormer: {e}')\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        logger.error(f'CodeFormer setup error: {e}')\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer face restoration: {e}\")\n        raise errors.ModelError(\"Error setting up CodeFormer face restoration\") from e\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")\n        raise errors.ModelError(f\"Error setting up CodeFormer: {e}\") from e\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer face restoration: {e}\")\n        raise errors.FaceRestorationError(f\"Error setting up CodeFormer face restoration: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except errors.ModelNotFoundError:\n        logger.warning(\"CodeFormer model not found\")\n    except errors.ModelError as e:\n        logger.warning(f\"Error loading CodeFormer: {e}\")\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n        logger.info(f\"CodeFormer loaded successfully from {dirname}\")\n    except Exception as e:\n        logger.error(f\"CodeFormer failed to load from {dirname}\")\n        raise errors.FaceRestorationError(f\"CodeFormer failed to load from {dirname}\") from e\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer face restoration: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        modelloader.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        errors.report(e, \"GFPGAN face restorer setup\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        shared.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        face_restoration_utils.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        gfpgan_face_restorer.load_model()\n    except Exception as e:\n        logger.warning(f\"Error setting up GFPGAN face restorer: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        face_restoration.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        shared.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        logger.info(\"GFPGAN face restorer set up\")\n    except Exception as e:\n        logger.error(f\"GFPGAN face restorer setup failed: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.error(f\"GFPGAN face restorer setup failed: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        face_restoration.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        shared.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.error(f\"Failed to set up GFPGAN face restorer: {e}\")\n        raise errors.SetupError(\"GFPGAN face restorer\") from e\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    # patch facexlib\n    try:\n        from facexlib.utils import set_facexlib_path\n        set_facexlib_path(dirname)\n    except Exception as e:\n        logger.warning(f\"Failed to patch facexlib: {e}\")\n\n    # initialize GFPGAN face restorer\n    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"Failed to initialize GFPGAN face restorer: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        face_restoration.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except errors.ModelNotFoundError as e:\n        logger.warning(f\"GFPGAN model not found: {e}\")\n    except errors.ModelError as e:\n        logger.warning(f\"GFPGAN model error: {e}\")\n    except Exception as e:\n        logger.warning(f\"GFPGAN setup error: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        if not gfpgan_face_restorer:\n            gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n            logger.info(\"GFPGAN face restorer set up\")\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        # Patch facexlib\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        if not os.path.exists(facexlib_path):\n            raise errors.FaceXLibNotFoundError(f\"FaceXLib not found in {facexlib_path}\")\n        facexlib_path = os.path.join(facexlib_path, \"facexlib\")\n        facexlib_path = os.path.abspath(facexlib_path)\n        shared.patch_facexlib(facexlib_path)\n\n        # Initialize GFPGAN face restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        gfpgan_face_restorer.load_model()\n\n    except Exception as e:\n        logger.error(f\"Error setting up GFPGAN face restorer: {e}\")\n        raise e\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        face_restoration.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except errors.ModelNotFoundError:\n        logger.warning(\"GFPGAN model not found, skipping setup\")\n    except Exception as e:\n        logger.warning(f\"Error setting up GFPGAN face restorer: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        if not os.path.exists(facexlib_path):\n            os.makedirs(facexlib_path)\n        shared.patch_facexlib(dirname)\n\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        logger.info(\"GFPGAN face restorer set up\")\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        gfpgan_face_restorer.setup_model()\n    except errors.ModelNotFoundError:\n        logger.warning(\"GFPGAN model not found, skipping setup\")\n    except Exception as e:\n        logger.warning(f\"Error setting up GFPGAN: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    # Patch facexlib\n    if not shared.args.no_half:\n        face_restoration_utils.patch_facexlib_for_gfpgan()\n\n    # Init face restorer\n    global gfpgan_face_restorer\n    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except errors.ModelNotFoundError:\n        logger.warning(\n            \"GFPGAN model not found, falling back to face_restore.py\"\n        )\n        gfpgan_face_restorer = None\n    except Exception as e:\n        logger.warning(f\"GFPGAN model setup failed: {e}\")\n        gfpgan_face_restorer = None\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    # Patch facexlib\n    try:\n        facexlib_dir = os.path.join(dirname, \"facexlib\")\n        facexlib_path = os.path.join(facexlib_dir, \"facexlib\")\n        if os.path.exists(facexlib_path):\n            facexlib_path = facexlib_dir\n        facexlib_path = os.path.realpath(facexlib_path)\n        facexlib_path = os.path.abspath(facexlib_path)\n        facexlib_path = os.path.normpath(facexlib_path)\n        if facexlib_path not in shared.facexlib_paths:\n            shared.facexlib_paths.append(facexlib_path)\n        logger.info(f\"Patched facexlib with {facexlib_path}\")\n    except Exception as e:\n        logger.warning(f\"Error patching facexlib: {e}\")\n\n    # Initialize GFPGAN face restorer\n    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        logger.info(\"GFPGAN face restorer initialized\")\n    except Exception as e:\n        logger.warning(f\"Error initializing GFPGAN face restorer: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        logger.info(\"GFPGAN face restorer set up\")\n    except Exception as e:\n        logger.error(f\"GFPGAN face restorer setup failed: {e}\")\n        raise errors.FaceRestorationError(f\"GFPGAN face restorer setup failed: {e}\") from e\n\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  return multiply(multiply(q, qv), conjugate(q))[..., 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v_q = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, v_q), conjugate(q))[Ellipsis, 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v_q = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  return multiply(multiply(q, v_q), conjugate(q))[..., 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  return multiply(multiply(q, qv), conjugate(q))[..., 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  return multiply(multiply(q, q_v), conjugate(q))[..., 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  qv = multiply(q, multiply(qv, conjugate(q)))\n  return qv[..., 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  return multiply(multiply(q, q_v), conjugate(q))[..., 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  return multiply(multiply(q, qv), conjugate(q))[..., 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, qv), conjugate(q))[Ellipsis, 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[..., :1])], axis=-1)\n  return multiply(multiply(q, v), conjugate(q))[..., :3]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, qv), conjugate(q))[Ellipsis, 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  q_v = jnp.concatenate([jnp.zeros_like(v), v], axis=-1)\n\n  # Apply the rotation using the quaternion multiplication\n  q_rotated = multiply(multiply(q, q_v), conjugate(q))\n\n  # Extract the rotated vector from the quaternion format\n  v_rotated = im(q_rotated)\n\n  return v_rotated\n\n"}
{"namespace": "quaternion.rotate", "completion": "  q_vec = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  q_rot = multiply(multiply(q, q_vec), conjugate(q))\n  return q_rot[Ellipsis, 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  qv = multiply(multiply(q, qv), conjugate(q))\n  return im(qv)\n\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  return multiply(multiply(q, qv), conjugate(q))[..., 1:]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(re(q)), im(q)], axis=-1)\n  qv = multiply(multiply(q, qv), conjugate(q))\n  return im(qv)\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format.\n  q_v = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  # Apply the rotation using the quaternion multiplication.\n  q_v_rotated = multiply(multiply(q, q_v), conjugate(q))\n  # Extract the rotated vector from the quaternion format.\n  return q_v_rotated[..., 1:]\n\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  q_v = jnp.concatenate([jnp.zeros_like(v), v], axis=-1)\n\n  # Apply the rotation using the quaternion multiplication\n  q_v_rotated = multiply(multiply(q, q_v), conjugate(q))\n\n  # Extract the rotated vector from the quaternion format\n  v_rotated = q_v_rotated[Ellipsis, 1:]\n\n  return v_rotated\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  q_v = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n\n  # Apply the rotation by multiplying the quaternion with the vector quaternion\n  q_rotated = multiply(multiply(q, q_v), conjugate(q))\n\n  # Return the rotated vector\n  return im(q_rotated)\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  v_quat = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n\n  # Apply the rotation using the quaternion multiplication\n  result_quat = multiply(multiply(q, v_quat), conjugate(q))\n\n  # Extract the rotated vector from the quaternion format\n  result = result_quat[..., 1:]\n\n  return result\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  if angle < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n  axis = axis_angle / angle\n  return jnp.concatenate([jnp.sin(angle / 2.0) * axis, jnp.cos(angle / 2.0)])\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = axis_angle[..., :3]\n  angle = axis_angle[..., 3:]\n  axis_norm = jnp.linalg.norm(axis, axis=-1, keepdims=True)\n  axis = axis / jnp.maximum(axis_norm, eps * jnp.ones_like(axis_norm))\n  angle = angle / 2.0\n  return jnp.concatenate([jnp.sin(angle) * axis, jnp.cos(angle)], axis=-1)\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle_norm = jnp.linalg.norm(axis_angle)\n  if axis_angle_norm < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n  axis_angle = axis_angle / axis_angle_norm\n  return jnp.concatenate([jnp.sin(axis_angle_norm / 2.0) * axis_angle,\n                          jnp.cos(axis_angle_norm / 2.0)])\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = jnp.array(axis_angle[:3])\n  angle = jnp.array(axis_angle[3])\n  axis_norm = jnp.linalg.norm(axis)\n  if axis_norm < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n  axis = axis / axis_norm\n  angle = angle * 0.5\n  return jnp.concatenate([jnp.sin(angle) * axis, jnp.cos(angle)])\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis_angle_norm = jnp.linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis_angle_norm = jnp.where(axis_angle_norm < eps, eps, axis_angle_norm)\n  axis_angle_normed = axis_angle / axis_angle_norm\n  axis_angle_normed_scaled = axis_angle_normed * jnp.sin(axis_angle_norm / 2.0)\n  return jnp.concatenate(\n      [axis_angle_normed_scaled, jnp.cos(axis_angle_norm / 2.0)], axis=-1\n  )\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  angle = jnp.linalg.norm(axis_angle)\n  if angle < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n  axis = axis_angle / angle\n  return jnp.array([\n      jnp.sin(angle / 2.0) * axis[0],\n      jnp.sin(angle / 2.0) * axis[1],\n      jnp.sin(angle / 2.0) * axis[2],\n      jnp.cos(angle / 2.0)\n  ])\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n  axis_norm = jnp.linalg.norm(axis)\n  if axis_norm < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n  axis = axis / axis_norm\n  angle = angle / 2.0\n  s = jnp.sin(angle)\n  return jnp.concatenate([axis * s, jnp.cos(angle)])\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  if angle < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n  axis = axis_angle / angle\n  return jnp.concatenate([jnp.sin(angle / 2.0) * axis, jnp.cos(angle / 2.0)], axis=-1)\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n  axis = axis / jnp.linalg.norm(axis)\n  x, y, z = axis * jnp.sin(angle / 2.0)\n  w = jnp.cos(angle / 2.0)\n  return jnp.array([x, y, z, w])\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  axis_angle = axis_angle / (angle + eps)\n  return jnp.concatenate((axis_angle * jnp.sin(angle / 2.0), [jnp.cos(angle / 2.0)]))\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  axis_angle = axis_angle / jnp.maximum(angle, eps)\n  q = jnp.concatenate([jnp.sin(angle / 2.0) * axis_angle, jnp.cos(angle / 2.0)])\n  return q\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  if angle < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n  else:\n    axis_angle = axis_angle / angle\n    return jnp.concatenate(\n        [jnp.sin(angle / 2.0) * axis_angle, jnp.cos(angle / 2.0)], axis=-1\n    )\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = axis_angle / linalg.norm(axis_angle, axis=-1, keepdims=True)\n  angle = linalg.norm(axis_angle, axis=-1)\n  angle = jnp.clip(angle, -jnp.pi, jnp.pi)\n  angle = jnp.where(angle < eps, eps, angle)\n  angle = jnp.where(angle > jnp.pi - eps, jnp.pi - eps, angle)\n  w = jnp.cos(angle / 2.0)\n  xyz = jnp.sin(angle / 2.0) * axis\n  return jnp.concatenate([xyz, w[Ellipsis, None]], axis=-1)\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis = axis_angle[..., :3]\n  angle = axis_angle[..., 3:]\n  axis_norm = jnp.linalg.norm(axis, axis=-1, keepdims=True)\n  axis = axis / axis_norm\n  angle = angle / 2\n  sin_angle = jnp.sin(angle)\n  cos_angle = jnp.cos(angle)\n  return jnp.concatenate([sin_angle * axis, cos_angle], axis=-1)\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n  angle = jnp.linalg.norm(axis_angle)\n  if angle < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n  return jnp.concatenate(\n      [jnp.sin(angle / 2.0) * axis_angle, jnp.cos(angle / 2.0)], axis=-1\n  )\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  angle = linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis = axis_angle / jnp.maximum(angle, eps * jnp.ones_like(angle))\n  return jnp.concatenate((jnp.sin(0.5 * angle) * axis, jnp.cos(0.5 * angle)), axis=-1)\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n  s = jnp.sin(angle / 2.0)\n  return jnp.concatenate([axis * s, jnp.cos(angle / 2.0)], axis=-1)\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = axis_angle[Ellipsis, :3]\n  angle = axis_angle[Ellipsis, 3:]\n  axis = axis / jnp.linalg.norm(axis, axis=-1, keepdims=True)\n  s = jnp.sin(0.5 * angle)\n  c = jnp.cos(0.5 * angle)\n  return jnp.concatenate([s * axis, c], axis=-1)\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n\n  # Normalize the axis vector\n  axis_norm = jnp.linalg.norm(axis)\n  axis = jnp.where(axis_norm > 0, axis / axis_norm, axis)\n\n  # Compute the quaternion components\n  half_angle = 0.5 * angle\n  real_part = jnp.cos(half_angle)\n  imag_part = jnp.sin(half_angle) * axis\n\n  # Construct the quaternion\n  quat = jnp.concatenate([imag_part, real_part])\n\n  # Ensure numerical stability for small angles\n  quat = jnp.where(jnp.abs(angle) < eps, jnp.array([0.0, 0.0, 0.0, 1.0]), quat)\n\n  return quat\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  if axis_angle.ndim == 1:\n    axis_angle = axis_angle[None, :]\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle, axis=1)[:, None]\n  angle = jnp.linalg.norm(axis_angle, axis=1)\n  angle = jnp.where(angle < eps, eps, angle)\n  axis_angle = axis_angle / angle[:, None]\n  angle = angle / 2.0\n  return jnp.concatenate(\n      [\n          jnp.sin(angle)[:, None] * axis_angle,\n          jnp.cos(angle)[:, None],\n      ],\n      axis=1,\n  )\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = high / 2\n    while high >= low + 1e-8:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = high / 2\n    while high >= low:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = 0\n    mid = (high + low) / 2\n    while high >= low + 1e-8:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    num_calls = k\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        high *= 2\n        num_calls += k\n    return -high, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = high / 2\n    while high > low:\n        mid = (high + low) / 2\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    logit_bias[idx] = high\n    logprobs = model.logprobs(prefix, logit_bias)\n    logprobs = np.array(logprobs)\n    logprobs[idx] = logprobs[idx] - high\n    logprobs = logprobs - logprobs.max()\n    logprobs = np.exp(logprobs)\n    logprobs = logprobs / logprobs.sum()\n    return -math.log(logprobs[idx]), num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = high / 2\n    while high >= low:\n        mid = (high + low) / 2\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = 1\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += 1\n    high = logit_bias[idx]\n\n    # get raw topk\n    topk_words = model.topk(prefix, logit_bias)\n    topk_idxs = list(topk_words.keys())\n    topk_logprobs = list(topk_words.values())\n\n    # get logprob of idx\n    idx_logprob = topk_logprobs[topk_idxs.index(idx)]\n\n    # get logprob of topk\n    topk_logprobs = np.array(topk_logprobs)\n    topk_logprobs = topk_logprobs - logsumexp(topk_logprobs)\n\n    # get logprob of topk - idx\n    topk_logprobs = topk_logprobs[:-1]\n    topk_logprobs = topk_logprobs - logsumexp(topk_logprobs)\n\n    # get logprob of topk - idx\n    topk_logprobs = topk_logprobs[:-1]\n    topk_logprobs = topk_logprobs - logsumexp(topk_logprobs)\n\n    # get logprob of topk - idx\n    topk_logprobs = topk_logprobs[:-1]\n    topk_logprobs = topk_logprobs - logsumexp(topk_logprobs)\n\n    # get logprob of topk - idx\n    topk_logprobs = topk_logprobs[:-1]\n    topk_logprobs = topk_logprobs - logsumexp(topk_logprobs)"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    logit_bias[idx] = high / 2\n    while high >= 1:\n        logit_bias[idx] = high / 2\n        if model.argmax(prefix, logit_bias) == idx:\n            high = logit_bias[idx]\n        else:\n            high /= 2\n        num_calls += k\n    return -logit_bias[idx], num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    if model.argmax(prefix) == idx:\n        return 0, 1\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n\n    # improve estimate\n    num_calls = 1\n    while True:\n        logit_bias[idx] /= 2\n        if model.argmax(prefix, logit_bias) == idx:\n            break\n        num_calls += 1\n    return -logit_bias[idx], num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    num_calls = 0\n    logit_bias = {idx: high}\n    topk_words, topk_idxs, topk_logprobs = model.topk(prefix, logit_bias, k=k)\n    while topk_idxs[0] != idx:\n        logit_bias[idx] *= 2\n        topk_words, topk_idxs, topk_logprobs = model.topk(prefix, logit_bias, k=k)\n        num_calls += k\n    return topk_logprobs[0], num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx not in topk_words:\n        raise TypeError(\n            f\"Tokens {idx} not in top-k with bias {high}.\"\n            \"Either increase bias or provide top unbiased logprob (top_logprob)\"\n        )\n\n    log_biased_prob = topk_words[idx]\n    return log_biased_prob, k\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = 0\n    mid = (high + low) / 2\n    while high >= low:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    topk_idxs = [i for i in topk_words]\n    topk_logprobs = [topk_words[i] for i in topk_idxs]\n    num_calls = k\n\n    if idx not in topk_idxs:\n        raise TypeError(\n            f\"Tokens {idx} not in top-k with bias {high}.\"\n            \"Either increase bias or provide top unbiased logprob (top_logprob)\"\n        )\n\n    log_topk_prob = logsumexp(topk_logprobs)\n    log_topk_prob_idx = topk_logprobs.index(log_topk_prob)\n    log_topk_prob_idx_logprob = topk_logprobs[log_topk_prob_idx]\n    log_prob = log_topk_prob_idx_logprob - np.logaddexp(\n        high + np.log1p(-np.exp(log_topk_prob)), log_topk_prob\n    )\n    return log_prob, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = 0\n    mid = (high + low) / 2\n    while high >= low:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # get topk\n    topk_words = model.topk(prefix, logit_bias)\n    topk_idxs = list(topk_words.keys())\n    topk_logprobs = list(topk_words.values())\n\n    # find idx in topk\n    try:\n        idx_idx = topk_idxs.index(idx)\n    except ValueError:\n        return 0, num_calls\n\n    # get logprob\n    logprob = topk_logprobs[idx_idx]\n\n    return logprob, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = high / 2\n    while high > low:\n        logit_bias[idx] = (high + low) / 2\n        if model.argmax(prefix, logit_bias) == idx:\n            high = logit_bias[idx]\n        else:\n            low = logit_bias[idx]\n        num_calls += k\n    return -logit_bias[idx], num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = 0\n    mid = (high + low) / 2\n    while high >= low + 1e-8:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx in topk_words:\n        return topk_words[idx], 1\n    else:\n        raise TypeError(\n            f\"Tokens {idx} not in top-k with bias {high}.\"\n            \"Either increase bias or provide top unbiased logprob (top_logprob)\"\n        )\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx not in topk_words:\n        raise TypeError(f\"Tokens {idx} not in top-k with bias {high}.\")\n    log_prob = topk_words[idx]\n    num_calls = k\n    return log_prob, num_calls\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if method == 'TRILINEAR':\n    return resample_3d_trilinear(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center,\n    )\n  elif method == 'NEAREST':\n    return resample_3d_nearest(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center,\n    )\n  else:\n    raise ValueError(f'Unknown method: {method}')\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if method == 'TRILINEAR':\n    return resample_3d_trilinear(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center,\n    )\n  elif method == 'NEAREST':\n    return resample_3d_nearest(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center,\n    )\n  else:\n    raise ValueError(f'Unknown resampling method {method}')\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data is a tensor.\n  if not isinstance(data, jnp.ndarray):\n    raise ValueError('Input data must be a tensor.')\n\n  # Check if the input locations are a tensor.\n  if not isinstance(locations, jnp.ndarray):\n    raise ValueError('Input locations must be a tensor.')\n\n  # Check if the input locations have the correct shape.\n  if locations.shape[-1] != 3:\n    raise ValueError('Input locations must have shape [D, ..., 3].')\n\n  # Check if the input edge_behavior is valid.\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Invalid edge_behavior. Must be either '\n                     '\"CONSTANT_OUTSIDE\" or \"CLAMP\".')\n\n  # Check if the input coordinate_order is valid.\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Invalid coordinate_order. Must be either \"xyz\" or \"zyx\".')\n\n  # Check if the input method is valid.\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Invalid method. Must be either \"TRILINEAR\" or \"NEAREST\".')\n\n  # Check if the input half_pixel_center is valid.\n  if not isinstance(half_pixel_center, bool):\n    raise ValueError('Invalid half_pixel_center. Must be a bool.')\n\n  # Check if the input constant_values is valid.\n  if not isinstance(constant_values, float):\n    raise ValueError('Invalid constant_values. Must be a float.')\n\n  # Check if the input data has the correct shape.\n  if len(data.shape) != 4:\n    raise ValueError('Input data must have shape [D, H, W, C].')\n\n  # Check if the input locations have the correct shape.\n  if len(locations.shape) != len(data.shape) - 1:\n    raise ValueError('"}
{"namespace": "resample.resample_3d", "completion": "  if half_pixel_center:\n    locations = locations + 0.5\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((0, 0), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n    locations = locations + 1\n  elif edge_behavior == 'CLAMP':\n    locations = jnp.clip(locations, 0, data.shape[0] - 1)\n  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n  if method == 'TRILINEAR':\n    x_floor = jnp.floor(x_coordinate).astype(jnp.int32)\n    y_floor = jnp.floor(y_coordinate).astype(jnp.int32)\n    z_floor = jnp.floor(z_coordinate).astype(jnp.int32)\n    x_ceil = x_floor + 1\n    y_ceil = y_floor + 1\n    z_ceil = z_floor + 1\n    x_weight = x_coordinate - x_floor\n    y_weight = y_coordinate - y_floor\n    z_weight = z_coordinate - z_floor\n    x_weight_flip = 1.0 - x_weight\n    y_weight_flip = 1.0 - y_weight\n    z_weight_flip = 1.0 - z_weight\n    x_weight_flip = jnp.clip(x_weight_flip, 0.0, 1.0)"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  # Pad the data with constant_values if necessary.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n\n  # Adjust the sample locations for half-pixel centering if necessary.\n  if half_pixel_center:\n    x_coordinate += 0.5\n    y_coordinate += 0.5\n    z_coordinate += 0.5\n\n  # Clamp the sample locations to the volume if necessary.\n  if edge_behavior == 'CLAMP':\n    x_coordinate = jnp.clip(x_coordinate, 0, data.shape[2] - 1)\n    y_coordinate = jnp.clip(y_coordinate, 0, data.shape[1] - 1)\n    z_coordinate = jnp.clip(z_coordinate, 0, data.shape[0] - 1)\n\n  # Compute the interpolation weights for trilinear interpolation.\n  if method == 'TRILINEAR':\n    x_floor = jnp.floor(x_coordinate).astype(jnp.int32)\n    x_ceil = jnp.ceil(x_coordinate).astype(jnp.int32)\n    y_floor = jnp.floor(y_coordinate).astype(jnp.int32)\n    y_ceil = jnp."}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((0, 0), (1, 1), (1, 1), (0, 0)), mode='constant', constant_values=constant_values)\n  elif edge_behavior == 'CLAMP':\n    data = jnp.clip(data, a_min=0, a_max=1)\n  else:\n    raise ValueError(f'Unknown edge behavior: {edge_behavior}')\n\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  if method == 'TRILINEAR':\n    return trilinear_interpolate(data, locations, coordinate_order)\n  elif method == 'NEAREST':\n    return nearest_neighbor_interpolate(data, locations, coordinate_order)\n  else:\n    raise ValueError(f'Unknown interpolation method: {method}')\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Pad the input data with a constant value if edge_behavior is set to 'CONSTANT_OUTSIDE'.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((0, 0), (1, 1), (1, 1), (0, 0)), mode='constant', constant_values=constant_values)\n\n  # Calculate the sample locations.\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Extract the coordinates from the locations tensor.\n  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  # Clamp the sample locations to the input volume if edge_behavior is set to 'CLAMP'.\n  if edge_behavior == 'CLAMP':\n    x_coordinate = jnp.clip(x_coordinate, 0, data.shape[2] - 1)\n    y_coordinate = jnp.clip(y_coordinate, 0, data.shape[1] - 1)\n    z_coordinate = jnp.clip(z_coordinate, 0, data.shape[0] - 1)\n\n  # Extract the values at the sample locations.\n  x0 = jnp.floor(x_coordinate).astype(jnp.int32)\n  y0 = jnp.floor(y_coordinate).astype(jnp.int32)\n  z0 = jnp.floor(z_coordinate).astype(jnp.int32)\n  x1 = x0 + 1\n  y1 = y0 + 1\n  z1 = z0"}
{"namespace": "resample.resample_3d", "completion": "  # Handle half-pixel centering.\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Handle edge behavior.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((0, 0), (1, 1), (1, 1), (0, 0)), constant_values=constant_values)\n    locations = jnp.clip(locations, 0, data.shape[1] - 2)\n  elif edge_behavior == 'CLAMP':\n    locations = jnp.clip(locations, 0, data.shape[1] - 1)\n  else:\n    raise ValueError(f'Unsupported edge behavior: {edge_behavior}')\n\n  # Gather data at locations.\n  gathered_data = gather_volume(data, locations, coordinate_order)\n\n  # Handle interpolation method.\n  if method == 'TRILINEAR':\n    return trilinear_interpolate(gathered_data, locations, coordinate_order)\n  elif method == 'NEAREST':\n    return nearest_neighbor_interpolate(gathered_data, locations, coordinate_order)\n  else:\n    raise ValueError(f'Unsupported interpolation method: {method}')\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Convert locations to float32 for numerical stability.\n  locations = locations.astype(jnp.float32)\n\n  # Pad the input data with zeros if edge_behavior is 'CONSTANT_OUTSIDE'.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n\n  # Calculate sample locations for trilinear interpolation.\n  if half_pixel_center:\n    if coordinate_order == 'xyz':\n      x_coordinate = locations[Ellipsis, 0] + 0.5\n      y_coordinate = locations[Ellipsis, 1] + 0.5\n      z_coordinate = locations[Ellipsis, 2] + 0.5\n    elif coordinate_order == 'zyx':\n      z_coordinate = locations[Ellipsis, 0] + 0.5\n      y_coordinate = locations[Ellipsis, 1] + 0.5\n      x_coordinate = locations[Ellipsis, 2] + 0.5\n  else:\n    if coordinate_order == 'xyz':\n      x_coordinate = locations[Ellipsis, 0]\n      y_coordinate = locations[Ellipsis, 1]\n      z_coordinate = locations[Ellipsis, 2]\n    elif coordinate_order == 'zyx':\n      z_coordinate = locations[Ellipsis, 0]\n      y_coordinate = locations[Ellipsis, 1]\n      x_coordinate = locations[Ellipsis, 2]\n\n  # Calculate the fractional part of the sample locations.\n  x_fraction = x_coordinate - jnp.floor(x_coordinate)\n  y_fraction = y_coordinate - jnp.floor(y_coordinate)\n  z_fraction = z_coordinate - jnp"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior == 'CONSTANT_OUTSIDE':\n    if half_pixel_center:\n      locations = locations - 0.5\n    data = pad_volume(data, constant_values)\n\n  if method == 'TRILINEAR':\n    return trilinear_interpolation(data, locations, coordinate_order)\n  elif method == 'NEAREST':\n    return nearest_neighbor_interpolation(data, locations, coordinate_order)\n  else:\n    raise ValueError('Invalid interpolation method')\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Pad the input volume if necessary to handle edge behavior.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n\n  # Calculate sample locations.\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Gather the data at the sample locations.\n  gathered_data = gather_volume(data, locations, coordinate_order)\n\n  # If nearest neighbor interpolation is used, return the gathered data.\n  if method == 'NEAREST':\n    return gathered_data\n\n  # If trilinear interpolation is used, perform trilinear interpolation.\n  if method == 'TRILINEAR':\n    # Calculate the fractional part of the sample locations.\n    fractional_locations = locations - jnp.floor(locations)\n\n    # Calculate the lower and upper indices for each dimension.\n    lower_indices = jnp.floor(locations).astype(jnp.int32)\n    upper_indices = lower_indices + 1\n\n    # Clamp the indices to the valid range.\n    lower_indices = jnp.clip(lower_indices, 0, data.shape[0] - 1)\n    upper_indices = jnp.clip(upper_indices, 0, data.shape[0] - 1)\n\n    # Gather the data at the lower and upper indices.\n    lower_data = gather_volume(data, lower_indices, coordinate_order)\n    upper_data = gather_volume(data, upper_indices, coordinate_order)\n\n    # Interpolate between the lower and upper data using the fractional part.\n    interpolated_data = (1 - fractional_locations) * lower_data + fractional_locations * upper_data\n\n    return interpolated_data\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  # Use Advanced indexing to gather data data.\n  data_gather = gather_volume(data, locations, coordinate_order)\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the input volume with constant values.\n    data_pad = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                       constant_values=constant_values)\n\n    # Calculate the sample locations adjusted for half-pixel centering.\n    if half_pixel_center:\n      x_coordinate += 0.5\n      y_coordinate += 0.5\n      z_coordinate += 0.5\n\n    # Gather data from the padded volume.\n    data_gather_pad = gather_volume(data_pad, locations, coordinate_order)\n\n    # Calculate the interpolation weights.\n    weights = jnp.ones_like(data_gather)\n\n  elif edge_behavior == 'CLAMP':\n    # Clamp the sample locations to the input volume dimensions.\n    x_coordinate = jnp.clip(x_coordinate, 0, data.shape[2] - 1)\n    y_coordinate = jnp.clip(y_coordinate, 0, data.shape[1] - 1)\n    z_coordinate = jnp.clip(z_coordinate, 0, data.shape[0] - 1)\n\n    # Gather data from the clamped locations.\n    data_gather = gather_volume(data, locations, coordinate_order)"}
{"namespace": "resample.resample_3d", "completion": "  # Pad the input volume to handle edge behavior.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n    locations = locations + 1\n  elif edge_behavior == 'CLAMP':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='edge')\n    locations = jnp.clip(locations, 1, data.shape[0] - 2)\n\n  # Calculate the sample locations.\n  if half_pixel_center:\n    locations = locations + 0.5\n  else:\n    locations = locations\n\n  # Gather the input data at the sample locations.\n  gathered_data = gather_volume(data, locations, coordinate_order)\n\n  # If nearest neighbor interpolation is selected, return the gathered data.\n  if method == 'NEAREST':\n    return gathered_data\n\n  # If trilinear interpolation is selected, calculate the interpolation weights.\n  if method == 'TRILINEAR':\n    # Calculate the fractional part of the sample locations.\n    frac_x = locations[Ellipsis, 0] - jnp.floor(locations[Ellipsis, 0])\n    frac_y = locations[Ellipsis, 1] - jnp.floor(locations[Ellipsis, 1])\n    frac_z = locations[Ellipsis, 2] - jnp.floor(locations[Ellipsis, 2])\n\n    # Calculate the interpolation weights.\n    w_x0 = frac_x\n    w_x1 = frac_x - 1\n    w_y0 = frac_y\n    w_y1 = frac_y - 1\n    w_z0 = frac_z\n    w_z1 = frac_z"}
{"namespace": "resample.resample_3d", "completion": "  if half_pixel_center:\n    # Adjust locations for half-pixel centering.\n    locations = locations + 0.5\n\n  # Pad data with constant values if necessary.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n\n  # Gather data at the locations.\n  gathered_data = gather_volume(data, locations, coordinate_order)\n\n  # If nearest neighbor interpolation is requested, return the gathered data.\n  if method == 'NEAREST':\n    return gathered_data\n\n  # Compute interpolation weights.\n  weights = jnp.zeros_like(gathered_data)\n  if method == 'TRILINEAR':\n    weights = trilinear_interpolation_weights(locations, coordinate_order)\n\n  # Compute the resampled values.\n  resampled_values = jnp.sum(weights * gathered_data, axis=-1)\n\n  return resampled_values\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Pad the data with constant values if needed.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(\n        data,\n        ((1, 1), (1, 1), (1, 1), (0, 0)),\n        mode='constant',\n        constant_values=constant_values)\n\n  # Calculate the sample locations based on the coordinate_order and half_pixel_center flag.\n  if coordinate_order == 'xyz':\n    if half_pixel_center:\n      locations = locations + 0.5\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    if half_pixel_center:\n      locations = jnp.flip(locations, axis=-1)\n      locations = locations + 0.5\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  # Calculate the trilinear weights.\n  x_floor = jnp.floor(x_coordinate).astype(jnp.int32)\n  x_ceil = x_floor + 1\n  y_floor = jnp.floor(y_coordinate).astype(jnp.int32)\n  y_ceil = y_floor + 1\n  z_floor = jnp.floor(z_coordinate).astype(jnp.int32)\n  z_ceil = z_floor + 1\n  x_weight = x_coordinate - x_floor\n  y_weight = y_coordinate - y_floor\n  z_weight = z_coordinate - z_floor\n\n  # Gather the data at the four corners of the voxel.\n  x_floor_y_floor_z_floor = gather_volume(data, jnp.stack([x"}
{"namespace": "resample.resample_3d", "completion": "  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Pad the data if necessary.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(\n        data,\n        ((0, 0), (1, 1), (1, 1), (0, 0)),\n        constant_values=constant_values,\n    )\n\n  # Gather data at the specified locations.\n  gathered_data = gather_volume(data, locations, coordinate_order)\n\n  if method == 'TRILINEAR':\n    # Compute the fractional offsets from the locations.\n    fractional_offsets = locations - jnp.floor(locations)\n\n    # Compute the trilinear weights.\n    trilinear_weights = (1.0 - fractional_offsets) * (\n        1.0 - fractional_offsets) * (1.0 - fractional_offsets)\n\n    # Compute the trilinear weights for the neighboring voxels.\n    trilinear_weights_neighbor = (\n        fractional_offsets * (1.0 - fractional_offsets) * (1.0 - fractional_offsets))\n\n    # Compute the trilinear weights for the second neighbor voxels.\n    trilinear_weights_neighbor_2 = (\n        (1.0 - fractional_offsets) * fractional_offsets * fractional_offsets)\n\n    # Compute the trilinear weights for the third neighbor voxels.\n    trilinear_weights_neighbor_3 = (\n        fractional_offsets * fractional_offsets * (1.0 - fractional_offsets))\n\n    # Compute the trilinear weights for the fourth neighbor voxels.\n    trilinear_weights_neighbor_4 = (\n        fractional_offsets * fractional_offsets * fractional_offsets)\n\n    # Compute the trilinear weights for the fifth neighbor voxels.\n    triline"}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input data.\n  data_shape = data.shape\n\n  # Get the shape of the sample locations.\n  locations_shape = locations.shape\n\n  # Check if the sample locations are in the correct shape.\n  if locations_shape[-1] != 3:\n    raise ValueError(\n        'Sample locations must be in the shape [D, ..., 3], but got shape: {}'.\n        format(locations_shape))\n\n  # Check if the sample locations are in the correct order.\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\n        'Invalid coordinate order. Must be either \"xyz\" or \"zyx\", but got: {}'.\n        format(coordinate_order))\n\n  # Check if the interpolation method is valid.\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\n        'Invalid interpolation method. Must be either \"TRILINEAR\" or \"NEAREST\", but got: {}'.\n        format(method))\n\n  # Check if the sample locations are in the correct range.\n  if np.any(locations < 0) or np.any(locations >= data_shape[:3]):\n    raise ValueError(\n        'Sample locations must be in the range [0, D) x [0, H) x [0, W)')\n\n  # Check if the sample locations are floating point.\n  if not np.issubdtype(locations.dtype, np.floating):\n    raise ValueError(\n        'Sample locations must be floating point, but got dtype: {}'.format(\n            locations.dtype))\n\n  # Check if the sample locations are in the correct order.\n  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n   "}
{"namespace": "resample.resample_3d", "completion": "  # Compute the dimensions of the input volume.\n  d, h, w, c = data.shape\n\n  # Compute the dimensions of the output volume.\n  d_out, h_out, w_out, c_out = locations.shape[:-1]\n\n  # Compute the sample locations.\n  if half_pixel_center:\n    # Compute the sample locations assuming half-pixel centers.\n    sample_locations = (locations + 0.5)\n  else:\n    # Compute the sample locations assuming voxel centers.\n    sample_locations = locations\n\n  # Handle edge behavior.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the input volume with constant values.\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n    # Compute the sample locations adjusted for the padding.\n    sample_locations = sample_locations + 1\n  elif edge_behavior == 'CLAMP':\n    # Clamp the sample locations to the volume.\n    sample_locations = jnp.clip(sample_locations, 0, d - 1)\n\n  # Compute the fractional part of the sample locations.\n  fractional_locations = sample_locations - jnp.floor(sample_locations)\n\n  # Compute the integer part of the sample locations.\n  integer_locations = jnp.floor(sample_locations).astype(jnp.int32)\n\n  # Compute the interpolation weights.\n  if method == 'TRILINEAR':\n    # Compute the interpolation weights for trilinear interpolation.\n    w_000 = (1 - fractional_locations[Ellipsis, 0]) * (\n        1 - fractional_locations[Ellipsis, 1]) * (\n            1 - fractional_locations[Ellipsis, 2])\n    w_100 = fractional_locations["}
{"namespace": "resample.resample_3d", "completion": "  # Add a batch dimension to the input data if it is not present.\n  if len(data.shape) == 3:\n    data = data[None, ...]\n\n  # Add a batch dimension to the locations if it is not present.\n  if len(locations.shape) == 2:\n    locations = locations[None, Ellipsis]\n\n  # Determine the shape of the input data and locations.\n  batch_size, depth, height, width, channels = data.shape\n  num_locations = locations.shape[1:-1]\n\n  # Convert locations to voxel coordinates if half_pixel_center is True.\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Pad the input data if edge_behavior is set to 'CONSTANT_OUTSIDE'.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((0, 0), (1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n\n  # Clamp the sample locations to the input volume if edge_behavior is set to 'CLAMP'.\n  if edge_behavior == 'CLAMP':\n    locations = jnp.clip(locations, 0, depth - 1)\n\n  # Gather the input data at the sample locations.\n  gathered_data = gather_volume(data, locations, coordinate_order)\n\n  # Perform trilinear interpolation if method is set to 'TRILINEAR'.\n  if method == 'TRILINEAR':\n    # Calculate the fractional part of the sample locations.\n    fractional_locations = locations - jnp.floor(locations)\n\n    # Calculate the indices of the surrounding voxels.\n    z_indices = jnp.floor(locations[Ellipsis, 0]).astype(jnp.int32)\n    y_indices = jnp.floor(locations[Ellipsis, 1]).astype("}
{"namespace": "resample.resample_3d", "completion": "  # Check the input data and locations shape.\n  data_shape = data.shape\n  locations_shape = locations.shape\n  assert len(data_shape) == 4, 'Input data must be a 4D tensor.'\n  assert len(locations_shape) >= 3, 'Locations must be at least 3D.'\n  assert data_shape[3] == locations_shape[-1], 'Input data and locations must have the same channel dimension.'\n\n  # Check the edge behavior.\n  assert edge_behavior in ['CONSTANT_OUTSIDE', 'CLAMP'], 'Invalid edge behavior.'\n\n  # Check the interpolation method.\n  assert method in ['TRILINEAR', 'NEAREST'], 'Invalid interpolation method.'\n\n  # Check the coordinate order.\n  assert coordinate_order in ['xyz', 'zyx'], 'Invalid coordinate order.'\n\n  # Check the half-pixel centering flag.\n  assert isinstance(half_pixel_center, bool), 'Half-pixel centering flag must be a bool.'\n\n  # Pad the input data if necessary.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = pad_volume(data, constant_values)\n\n  # Adjust the sample locations for half-pixel centering.\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Convert the sample locations to integers.\n  locations = jnp.round(locations).astype(jnp.int32)\n\n  # Clamp the sample locations to the input volume bounds.\n  if edge_behavior == 'CLAMP':\n    locations = jnp.clip(locations, 0, data_shape[0] - 1)\n\n  # Gather the data at the sample locations.\n  if method == 'TRILINEAR':\n    gathered_data = gather_volume(data, locations, coordinate_order)\n  elif method == 'NEAREST':\n    gathered_data = gather_volume(data, locations, coordinate_order)\n\n  return gathered_data\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.nextafter(x, jnp.finfo(x.dtype).max)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.nextafter(x, jnp.inf)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, max_val))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.nextafter(x, jnp.inf)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(x < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, max_val))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, max_val))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(x < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.nextafter(x, min_val)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, max_val))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.nextafter(x, min_val)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(x > tiny_val, x, tiny_val)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.nextafter(x, jnp.finfo(x.dtype).max)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.nextafter(x, min_val)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(x < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, max_val))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.nextafter(x, max_val)\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, max_val))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(x < tiny_val, tiny_val, nextafter(x, jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.nextafter(x, jnp.finfo(x.dtype).max)\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: x_dot * y,\n      (min_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: x_dot * y,\n      (min_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: x_dot * jnp.exp(x),\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: x_dot * jnp.exp(x),\n      (-max_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: x_dot * safe_exp(x),\n      (min_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: x_dot * y,\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: x_dot * y,\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: x_dot * y,\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: x_dot * jnp.exp(x),\n      (-max_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, 100),\n  )(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: x_dot * jnp.exp(x),\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: x_dot * jnp.exp(x),\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / x, (tiny_val, max_val))(\n      x\n  )\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / x, (tiny_val, max_val))(x)\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / x, (tiny_val, max_val))(\n      x\n  )\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / (x * y),\n      (tiny_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, dy: dy / y, (tiny_val, max_val))(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / x, (tiny_val, max_val))(\n      x\n  )\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / x, (tiny_val, max_val))(x)\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / x, (tiny_val, max_val))(x)\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, dy: dy / y, (tiny_val, max_val))(\n      x\n  )\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / y, (tiny_val, max_val))(\n      x\n  )\n\n"}
{"namespace": "math.safe_log", "completion": "  def grad_fn(x, y, x_dot):\n    return x_dot / (x * y)\n\n  return generate_safe_fn(jnp.log, grad_fn, (tiny_val, max_val))(x)\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, dy: dy / x, (tiny_val, max_val))(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / x, (tiny_val, max_val))(\n      x\n  )\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, dy: dy / x, (tiny_val, max_val))(x)\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, dy: dy / y, (tiny_val, max_val))(x)\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, dy: dy / y, (-jnp.inf, max_val))(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, dy: dy / x, (-max_val, max_val))(x)\n\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, dy: dy / x, (-jnp.inf, 0.0))(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, dy: dy / (y + tiny_val),\n                          (tiny_val, max_val))(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / (remove_zero(x) * y),\n      (-jnp.inf, max_val),\n  )(x)\n\n\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (0, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (tiny_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: x_dot / (2 * safe_sqrt(x)),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: x_dot / (2 * safe_sqrt(x)),\n      (0, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (0.0, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: x_dot / (2 * jnp.sqrt(x)),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / (y + tiny_val),\n      (tiny_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: x_dot / (2 * jnp.sqrt(jnp.clip(x, 0, max_val))),\n      (0, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(jnp.clip(x, 0, max_val)),\n      (0, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (0, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / safe_sqrt(x),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda _, y, x_dot: x_dot / (2 * y),\n      (0, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (0, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda _, y, x_dot: x_dot / (2 * y),\n      (0, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return max_val\n  elif p == 2:\n    return max_val\n  elif p == 3:\n    return max_val\n  elif p == 4:\n    return max_val\n  elif p == 5:\n    return max_val\n  elif p == 6:\n    return max_val\n  elif p == 7:\n    return max_val\n  elif p == 8:\n    return max_val\n  elif p == 9:\n    return max_val\n  elif p == 10:\n    return max_val\n  elif p == 11:\n    return max_val\n  elif p == 12:\n    return max_val\n  elif p == 13:\n    return max_val\n  elif p == 14:\n    return max_val\n  elif p == 15:\n    return max_val\n  elif p == 16:\n    return max_val\n  elif p == 17:\n    return max_val\n  elif p == 18:\n    return max_val\n  elif p == 19:\n    return max_val\n  elif p == 20:\n    return max_val\n  elif p == 21:\n    return max_val\n  elif p == 22:\n    return max_val\n  elif p == 23:\n    return max_val\n  elif p == 24:\n    return max_val\n  elif p == 25:\n    return max_val\n  elif p == 26:\n    return max_val\n  elif p == 27:\n    return max_val\n  elif p == 28:\n    return max_val\n  elif p == 29:\n    return max_val\n  elif p == 30:\n    return max_val\n  elif p == 31:\n    return max_val\n  elif p == 32:\n    return max_val\n  elif p == 33:\n    return max_val\n  elif p == 34:\n    return max_val\n  elif p == 35:\n    return max_val\n  elif p == 36:\n    return max_val\n  elif p"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p == 1:\n    return np.inf\n  elif p == 2:\n    return np.inf\n  elif p == 3:\n    return np.inf\n  elif p == 4:\n    return np.inf\n  elif p == 5:\n    return np.inf\n  elif p == 6:\n    return np.inf\n  elif p == 7:\n    return np.inf\n  elif p == 8:\n    return np.inf\n  elif p == 9:\n    return np.inf\n  elif p == 10:\n    return np.inf\n  elif p == 11:\n    return np.inf\n  elif p == 12:\n    return np.inf\n  elif p == 13:\n    return np.inf\n  elif p == 14:\n    return np.inf\n  elif p == 15:\n    return np.inf\n  elif p == 16:\n    return np.inf\n  elif p == 17:\n    return np.inf\n  elif p == 18:\n    return np.inf\n  elif p == 19:\n    return np.inf\n  elif p == 20:\n    return np.inf\n  elif p == 21:\n    return np.inf\n  elif p == 22:\n    return np.inf\n  elif p == 23:\n    return np.inf\n  elif p == 24:\n    return np.inf\n  elif p == 25:\n    return np.inf\n  elif p == 26:\n    return np.inf\n  elif p == 27:\n    return np.inf\n  elif p == 28:\n    return np.inf\n  elif p == 29:\n    return np.inf\n  elif p == 30:\n    return np.inf\n  elif p == 31:\n    return np.inf\n  elif p == 32:\n    return np.inf\n  elif p == 33:\n    return np.inf\n  elif p == 34:\n    return np.inf\n  elif p == 35:\n    return np.inf\n  elif p == 36"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p <= 0:\n    return 0\n  elif p == 1:\n    return np.inf\n  elif p == 2:\n    return np.inf\n  elif p == 3:\n    return np.inf\n  elif p == 4:\n    return np.inf\n  elif p == 5:\n    return np.inf\n  elif p == 6:\n    return np.inf\n  elif p == 7:\n    return np.inf\n  elif p == 8:\n    return np.inf\n  elif p == 9:\n    return np.inf\n  elif p == 10:\n    return np.inf\n  elif p == 11:\n    return np.inf\n  elif p == 12:\n    return np.inf\n  elif p == 13:\n    return np.inf\n  elif p == 14:\n    return np.inf\n  elif p == 15:\n    return np.inf\n  elif p == 16:\n    return np.inf\n  elif p == 17:\n    return np.inf\n  elif p == 18:\n    return np.inf\n  elif p == 19:\n    return np.inf\n  elif p == 20:\n    return np.inf\n  elif p == 21:\n    return np.inf\n  elif p == 22:\n    return np.inf\n  elif p == 23:\n    return np.inf\n  elif p == 24:\n    return np.inf\n  elif p == 25:\n    return np.inf\n  elif p == 26:\n    return np.inf\n  elif p == 27:\n    return np.inf\n  elif p == 28:\n    return np.inf\n  elif p == 29:\n    return np.inf\n  elif p == 30:\n    return np.inf\n  elif p == 31:\n    return np.inf\n  elif p == 32:\n    return np.inf\n  elif p == 33:\n    return np.inf\n  elif p == 34:\n    return np.inf\n  elif p == 35:\n    return np.inf\n  elif p == 3"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return 0\n  elif p == 0:\n    return 1\n  elif p < 1:\n    return 2**p\n  elif p == 1:\n    return np.inf\n  else:\n    return np.inf\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p < 0:\n    return 1\n  elif p == 1:\n    return 1\n  elif p > 1:\n    return np.inf\n\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 0:\n    return 0\n  elif p > 1:\n    return 1\n  elif p < 0:\n    return 0\n  else:\n    return 1\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return 0\n  elif p == 0:\n    return 1\n  elif p == 1:\n    return np.inf\n  elif p > 1:\n    return np.inf\n  else:\n    return np.inf\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p < 1:\n    return 0\n  elif p > 1:\n    return np.inf\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p == 1:\n    return np.inf\n  elif p == -1:\n    return 0\n  elif p < 0:\n    return 0\n  else:\n    return np.inf\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p > 1, 1),\n          (p == 1, 1),\n          (p < 1, 1 / (1 - p)),\n      ],\n      np.inf,\n  )\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p < 0:\n    return np.inf\n  elif p == 1:\n    return np.inf\n  elif p == 2:\n    return np.inf\n  elif p > 2:\n    return np.inf\n  elif p == 0.5:\n    return np.inf\n  elif p == 1.5:\n    return np.inf\n  elif p == 2.5:\n    return np.inf\n  else:\n    return np.inf\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1.0\n  elif p == 2:\n    return 1.0\n  elif p == 4:\n    return 1.0\n  elif p == np.inf:\n    return 0.0\n  else:\n    raise ValueError(\"Invalid value for p.\")\n\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p < 0:\n    return 0\n  elif p == 1:\n    return np.inf\n  elif p > 1:\n    return np.inf\n  elif p == 0.5:\n    return 2\n  elif p < 0.5:\n    return 0\n  elif p == 0.25:\n    return 1.5\n  elif p < 0.25:\n    return 0\n  elif p == 0.125:\n    return 1.25\n  elif p < 0.125:\n    return 0\n  else:\n    raise ValueError(f\"Unsupported power value: {p}\")\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  return jnp.where(p > 0, max_val, jnp.where(p < 0, 0, 1))\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 1:\n    return 1\n  elif p == 1:\n    return 0\n  elif p < 1:\n    return -1\n  else:\n    return None\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 1:\n    return 0.0\n  elif p == 1:\n    return np.inf\n  else:\n    return np.power(np.e, np.power(np.e, p - 1))\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p <= 0:\n    return 1.0\n  elif p == 1:\n    return np.inf\n  elif p > 1:\n    return np.inf\n  elif p < 0:\n    return 0.0\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return np.inf\n  elif p == 1:\n    return np.inf\n  elif p == 2:\n    return np.inf\n  elif p == np.inf:\n    return np.inf\n  elif p == -np.inf:\n    return 0\n  elif p == np.nan:\n    return np.nan\n  elif p < 0:\n    return 0\n  elif p > 0:\n    return np.inf\n  else:\n    return np.nan\n\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p == np.inf:\n    return np.inf\n  elif p == -np.inf:\n    return 0\n  elif p < 0:\n    return 0\n  elif p > 1:\n    return np.inf\n  else:\n    return np.exp(np.power(np.e, 1 - p)) - 1\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p < 0:\n    return np.inf\n  elif p == 0:\n    return 1.0\n  elif p == 1:\n    return np.inf\n  elif p > 1:\n    return np.inf\n  elif p == 0.5:\n    return 2.0\n  else:\n    return np.inf\n\n"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [1, -1, -1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 0.942809, 0.333333],\n        [0, -0.471405, 0.866025],\n        [0, -0.942809, -0.333333],\n        [0.816497, 0, 0.57735],\n        [0.408248, 0.707107, 0.57735],\n        [0.408248, -0.707107, 0.57735],\n        [-0.408248, 0.707107, 0.57735],\n        [-0.408248, -0.707107, 0.57735],\n        [-0.816497, 0, 0.57735],\n        [0.57735, 0.57735, 0.57735],\n        [0.57735, -0.57735, 0.57735],\n        [-0.57735, 0.57735, 0.57735],"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n      [1, 1, 1],\n      [-1, 1, -1],\n      [-1, -1, 1],\n      [1, -1, -1],\n    ])\n    base_faces = np.array([\n      [0, 1, 2],\n      [0, 1, 3],\n      [0, 2, 3],\n      [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n      [0, 0, 1],\n      [0, 1, 0],\n      [1, 0, 0],\n      [0, -1, 0],\n      [-1, 0, 0],\n      [0, 0, -1],\n      [0.5, 0.5, 0.5],\n      [-0.5, 0.5, 0.5],\n      [-0.5, -0.5, 0.5],\n      [0.5, -0.5, 0.5],\n      [0.5, 0.5, -0.5],\n      [-0.5, 0.5, -0.5],\n      [-0.5, -0.5, -0.5],\n      [0.5, -0.5, -0.5],\n    ])\n    base_faces = np.array([\n      [0, 1, 2],\n      [0, 2, 3],\n      [0, 3, 4],\n      [0, 4, 1],\n      [1, 4, 5],\n      [1, 5, 2],\n      [2, 5, 6],\n      [2, 6, 3],\n      [3, 6, 7],\n      [3, 7, 4],\n      [4, 7, 8],\n      [4, 8"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array(\n        [\n            [0, 0, 1],\n            [0, 0.942809, 0.333333],\n            [0, -0.942809, 0.333333],\n            [0.816497, 0, 0.57735],\n            [-0.816497, 0, 0.57735],\n            [0.422618, 0.903989, 0.0],\n            [-0.422618, 0.903989, 0.0],\n            [0.422618, -0.903989, 0.0],\n            [-0.422618, -0.903989, 0.0],\n            [0.723607, 0.525731, 0.447214],\n            [-0.723607, 0.525731, 0.447214],\n            [0.723607, -0.525731, 0.447214],\n            [-0.723607, -0.525731, 0.447214],\n            [0.276385, 0.5257"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1.0, 1.0, 1.0],\n        [-1.0, 1.0, -1.0],\n        [-1.0, -1.0, 1.0],\n        [1.0, -1.0, -1.0],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0.0, 0.0, 1.0],\n        [0.89442719, 0.0, 0.4472136],\n        [0.2763932, 0.8506508, 0.4472136],\n        [-0.72360679, 0.52573111, 0.4472136],\n        [-0.72360679, -0.52573111, 0.4472136],\n        [0.2763932, -0.8506508, 0.4472136],\n        [0.0, -1.0, 0.0],\n        [-0.89442719, 0.0, -0.4472136],\n        [-0.2763932, 0.8506508, -0.4472136],\n        [0.72360679, 0.52573111, -0.4472136],\n        [0.723606"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n        [1, -1, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 0.942809, -0.333333],\n        [0, -0.471405, -0.816497],\n        [0, -0.942809, 0.333333],\n        [0.816497, 0, 0.55557],\n        [0.333333, 0.942809, 0],\n        [-0.333333, 0.942809, 0],\n        [-0.816497, 0, -0.55557],\n        [-0.333333, -0.942809, 0],\n        [0.333333, -0.942809, 0],\n        [0.816497, 0, -0.55557],\n        [0.55557, 0.816497, 0],\n        [0.55557, -0.816497, 0],\n        [-0.55557, 0.816497, 0],\n        [-0.55557, -0.8"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [-1, 1, 1],\n        [-1, -1, 1],\n        [1, -1, 1],\n        [1, 1, -1],\n        [-1, 1, -1],\n        [-1, -1, -1],\n        [1, -1, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 3],\n        [1, 2, 3],\n        [2, 0, 3],\n        [0, 2, 1],\n        [4, 5, 7],\n        [5, 6, 7],\n        [6, 4, 7],\n        [4, 6, 5],\n        [0, 4, 7],\n        [1, 5, 4],\n        [2, 6, 5],\n        [3, 7, 6],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 0.942809, 0.333333],\n        [0, -0.471405, 0.881921],\n        [0, -0.942809, -0.333333],\n        [0, 0.471405, -0.881921],\n        [0.816497, 0, 0.57735],\n        [0.408248, 0.707107, 0.57735],\n        [-0.408248, 0.707107, 0.57735],\n        [-0.816497, 0, "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, 1, -1], [-1, -1, 1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    # https://en.wikipedia.org/wiki/Regular_icosahedron\n    phi = (1 + 5**0.5) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, 1, -phi],\n            [0, -1, phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [1, -phi, 0],\n            [-1, phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [9, 4, 5],\n            [4, 8, 5],\n            [4, 1, 8],\n            [8, 1, 6],\n            [8, 6, 5],\n            [2, 11, 3],\n            [3, 11, 7],\n            [3, 7, 10],\n            [10, 7, 6],\n            [7, 11, 0],\n            [0, 7, 1],\n            [10, 6, 1],\n            [6, 2, 1],\n            [6"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n        [1, -1, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 1, 0.5],\n        [0, -1, 0.5],\n        [-1, 0, 0.5],\n        [1, 0, 0.5],\n        [0.5, 0.5, 0],\n        [-0.5, 0.5, 0],\n        [0.5, -0.5, 0],\n        [-0.5, -0.5, 0],\n        [0, 0.5, 0.5],\n        [0, -0.5, 0.5],\n        [0.5, 0, 0.5],\n        [-0.5, 0, 0.5],\n        [0.5, 0, -0.5],\n        [-0.5, 0, -0.5],\n        [0, 0.5, -0.5],\n        [0, -0.5, -0.5],\n        [0.5, 0.5, 0],\n        [-0.5, 0.5, 0],\n        [0.5, -0.5, 0],\n        [-0.5, -0.5, 0],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 2, 3],\n        [0,"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [1, -1, -1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 0.942809, -0.333333],\n        [0, -0.471405, -0.816497],\n        [0.816497, 0, -0.57735],\n        [-0.471405, 0, -0.816497],\n        [0.471405, 0.816497, 0],\n        [0.471405, -0.816497, 0],\n        [-0.816497, 0, 0.57735],\n        [-0.942809, 0.333333, 0],\n        [0.942809, -0.333333, 0],\n        [0.333333, 0.942809, 0],\n        [-0.333333, 0.942809, 0],\n        [-0.333333, -0.942809, 0],\n        [0.333333, -0.942809, 0],\n        [0, -0.471"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [-1, 1, 1],\n        [-1, -1, 1],\n        [1, -1, 1],\n        [1, 1, -1],\n        [-1, 1, -1],\n        [-1, -1, -1],\n        [1, -1, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 2, 3],\n        [0, 3, 4],\n        [0, 4, 1],\n        [5, 1, 4],\n        [5, 4, 7],\n        [5, 7, 6],\n        [5, 6, 2],\n        [5, 2, 1],\n        [6, 7, 3],\n        [6, 3, 2],\n        [3, 7, 4],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 1, 0.5],\n        [0, -1, 0.5],\n        [0, 1, -0.5],\n        [0, -1, -0.5],\n        [0.5, 0, 1],\n        [-0.5, 0, 1],\n        [-0.5, 0, -1],\n        [0.5, 0, -1],\n        [1, 0.5, 0],\n        [-1, 0.5, 0],\n        [-1, -0.5, 0],\n        [1, -0.5, 0],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 2, 3],\n        [0, 3, 4],\n        [0, 4, "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    # Generate icosahedron vertices and faces.\n    t = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [-1, t, 0],\n            [1, t, 0],\n            [-1, -t, 0],\n            [1, -t, 0],\n            [0, -1, t],\n            [0, 1, t],\n            [0, -1, -t],\n            [0, 1, -t],\n            [t, 0, -1],\n            [t, 0, 1],\n            [-t, 0, -1],\n            [-t, 0, 1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [9, 4, 5],\n            [4, 8, 5],\n            [4, 1, 8],\n            [8, 1, 11],\n            [8, 11, 3],\n            [11, 10, 3],\n            [8, 3, 3],\n            [11, 7, 3],\n            [3, 10, 7],\n            [7, 10, 2],\n            [7, 6, 10],\n            [7, 11, 6],\n            [11, 0, 6],\n            [0, 2, 6],"}
{"namespace": "geopoly.generate_basis", "completion": "  # Define the vertices and faces of the starting polyhedron\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    # Define the vertices and faces of the starting polyhedron\n    t = (1 + np.sqrt(5)) / 2\n    base_verts = np.array([\n        [-1, t, 0], [1, t, 0], [-1, -t, 0], [1, -t, 0], [0, -1, t], [0, 1, t],\n        [0, -1, -t], [0, 1, -t], [t, 0, -1], [t, 0, 1], [-t, 0, -1],\n        [-t, 0, 1]\n    ])\n    base_faces = np.array([\n        [0, 1, 4], [0, 4, 9], [9, 4, 5], [4, 8, 5], [4, 1, 8], [8, 1, 11],\n        [8, 11, 3], [11, 1, 2], [11, 2, 6], [3, 11, 6], [3, 6, 10], [3, 10, 2],\n        [10, 7, 2], [7, 10, 6], [7, 0, 6], [7, 9, 6], [7, 5, 6], [7, 8, 9]\n    ])\n  elif base_shape == 'octahedron':\n    base_"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [1, 0, -1],\n        [-1, 0, -1],\n        [0, 1, -1],\n    ])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, -1, 0],\n        [-1, 0, 0],\n        [0, 0, -1],\n        [0.5, 0.5 * np.sqrt(5), 0],\n        [-0.5, 0.5 * np.sqrt(5), 0],\n        [0.5, -0.5 * np.sqrt(5), 0],\n        [-0.5, -0.5 * np.sqrt(5), 0],\n        [0, 0.5 * np.sqrt(5), 0.5],\n        [0, 0.5 * np.sqrt(5), -0.5],\n        [0, -0.5 * np.sqrt(5), 0.5],\n        [0, -0.5 * np.sqrt(5), -0.5],\n        [0.5 * np.sqrt(5), 0, 0.5],\n        [-0.5 * np.sqrt(5), 0, 0.5],\n        [0.5 * np.sqrt(5), 0, -0.5],\n        [-0.5 * np.sqrt(5), 0, -0.5],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 2, 3],\n       "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n      [0, 0, 1],\n      [1, 0, -1],\n      [-1, 0, -1],\n      [0, 1, -1],\n    ])\n    base_faces = np.array([\n      [0, 1, 2],\n      [0, 1, 3],\n      [0, 2, 3],\n      [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n      [0, 0, 1],\n      [0, 1, 0],\n      [1, 0, 0],\n      [0, -1, 0],\n      [-1, 0, 0],\n      [0, 0, -1],\n      [0.89442719, 0.4472136, 0.0],\n      [0.2763932, 0.4472136, 0.8506508],\n      [-0.72360679, 0.4472136, 0.52573111],\n      [-0.72360679, 0.4472136, -0.52573111],\n      [0.2763932, 0.4472136, -0.8506508],\n      [0.0, -0.4472136, 0.89442719],\n      [0.8506508, -0.4472136, 0.2763932],\n      [0.52573111, -0.4472136, -0.72360679],\n     "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [1, -1, -1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n    ])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 1, 0],\n        [0, -1, 0],\n        [1, 0, 0],\n        [-1, 0, 0],\n        [0.57735, 0.57735, 0.57735],\n        [-0.57735, 0.57735, -0.57735],\n        [-0.57735, -0.57735, 0.57735],\n        [0.57735, -0.57735, -0.57735],\n        [0.57735, -0.57735, 0.57735],\n        [-0.57735, 0.57735, 0.57735],\n        [0.57735, 0.57735, -0.57735],\n    ])\n    base_faces = np.array([\n        [0, 1, 5],\n        [0, 5, 10],\n        [0, 10, 7],\n        [0, 7, 11],\n        [0, 11, 4],\n        [0, 4, 1],\n        [1, 2, 6],\n        ["}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0],\n        [0.0, 0.0, -1.0],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0.0, 0.0, 1.0],\n        [0.0, 1.0, 0.92388],\n        [0.0, -1.0, 0.92388],\n        [1.0, 0.0, 0.92388],\n        [-1.0, 0.0, 0.92388],\n        [0.0, 1.0, -0.92388],\n        [0.0, -1.0, -0.92388],\n        [1.0, 0.0, -0.92388],\n        [-1.0, 0.0, -0.92388],\n        [0.0, 0.0, -1.0],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 2, 3],\n        [0, 3, 4],\n        [0, 4, 5],\n        [0, 5, 1],\n        [1, 6, 2],\n        [2, 6, 7],\n        [2, 7, 3],\n        [3, 7, 8],\n        [3,"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, -1, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [1, 3, 2],\n        [0, 3, 1],\n        [0, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array([\n        [0, 1, phi],\n        [0, 1, -phi],\n        [0, -1, phi],\n        [0, -1, -phi],\n        [1, phi, 0],\n        [1, -phi, 0],\n        [-1, phi, 0],\n        [-1, -phi, 0],\n        [phi, 0, 1],\n        [-phi, 0, 1],\n        [phi, 0, -1],\n        [-phi, 0, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 4],\n        [0, 4, 9],\n        [9, 4, 5],\n        [4, 8, 5],\n        [4, 1, 8],\n        [8, 1, 6],\n        [8, 6, 5],\n        [1, 2, 6],\n        [2, 7, 6],\n        [6, 7, 11],\n        [3, 2, 7],\n        [3, 7, 10],\n        [7, 11, 10],\n        [11, 0, 9],\n        [11, 10, 0],\n        [10, 6,"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 0.9428090415820634, -0.3333333333333333],\n        [0.8164965809277261, 0.4714045409886641, -0.3333333333333333],\n        [-0.8164965809277261, 0.4714045409886641, -0.3333333333333333],\n        [0.8164965809277261, -0.4714045409886641, -0.3333333333333333],\n        [-0.8164965809277261, -0.4714045409886641, -0.3333333333333333],\n        [0.3333333333333333, 0.9428090415820634, 0.0],\n        [-0.33333333333"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 1, 0.57735],\n        [0.85065, 0.52573, 0.0],\n        [0.27639, 0.92388, 0.27639],\n        [-0.27639, 0.92388, 0.27639],\n        [-0.85065, 0.52573, 0.0],\n        [0.27639, 0.92388, -0.27639],\n        [-0.27639, 0.92388, -0.27639],\n        [-0.85065, -0.52573, 0.0],\n        [-0.27639, -0.92388, -0.27639],\n        [0.27639, -0.92388, -0.27639],\n        [0.85065, -0.52573, 0.0],\n        [0.27639, -0.92388, 0.27639],\n        [-0.27639, -0.92388, 0.27639],\n        [0, -1, 0.57735],"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, -1, 0],\n        [-1, 0, 0],\n        [0, 0, -1],\n        [0.5, 0.5, 0.5],\n        [0.5, -0.5, 0.5],\n        [0.5, 0.5, -0.5],\n        [-0.5, 0.5, 0.5],\n        [-0.5, -0.5, 0.5],\n        [-0.5, 0.5, -0.5],\n        [0.5, -0.5, -0.5],\n        [-0.5, -0.5, -0.5],\n        [-0.5, -0.5, -0.5],\n        [-0.5, -0.5, -0.5],\n        [-0.5, 0.5, -0.5],\n        [0.5, -0.5, -0.5],\n        [0.5, 0.5, -0.5],\n        [0.5, -0.5, 0.5],\n        [0.5, 0.5, 0.5],\n        [-0.5, 0.5, 0.5],\n        [-0.5, 0.5, -0.5],\n        [-0.5, -0.5,"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda _, y, x_dot: x_dot / (y + 1),\n      (min_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda _, y, x_dot: x_dot / (y + 1),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.sign(x) * jnp.log1p(jnp.abs(x))\n  elif p == -jnp.inf:\n    y = jnp.sign(x) * jnp.expm1(jnp.abs(x))\n  elif p == jnp.inf:\n    y = jnp.sign(x) * jnp.exp(jnp.abs(x))\n  else:\n    y = jnp.sign(x) * jnp.power(jnp.abs(x), p)\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    return x\n  elif p == 0:\n    return jnp.sign(x) * jnp.log1p(jnp.abs(x))\n  elif p == -jnp.inf:\n    return jnp.sign(x) * jnp.expm1(jnp.abs(x))\n  elif p == jnp.inf:\n    return jnp.sign(x) * jnp.abs(x)\n  else:\n    return jnp.sign(x) * jnp.abs(x) ** p\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.sign(x)\n  elif p == -jnp.inf:\n    y = jnp.abs(x)\n  elif p == jnp.inf:\n    y = jnp.abs(x) ** 2\n  else:\n    y = jnp.abs(x) ** p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.sign(x) * jnp.log(jnp.abs(x) + 1)\n  elif p == -jnp.inf:\n    y = jnp.sign(x) * jnp.log1p(jnp.abs(x))\n  elif p == jnp.inf:\n    y = jnp.sign(x) * jnp.expm1(jnp.abs(x))\n  else:\n    y = jnp.sign(x) * jnp.power(jnp.abs(x) + 1, p) - 1\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    if postmult is not None:\n      x = x * postmult\n    return x\n\n  if p == 0:\n    if postmult is not None:\n      x = jnp.sign(x) * postmult\n    return x\n\n  if p == -jnp.inf:\n    if postmult is not None:\n      x = jnp.abs(x) * postmult\n    return x\n\n  if p == jnp.inf:\n    if postmult is not None:\n      x = jnp.sign(x) * jnp.log(jnp.abs(x) + 1) * postmult\n    return x\n\n  x = jnp.where(x < 0, -jnp.power(-x, p), jnp.power(x, p))\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.sign(x)\n  elif p == -jnp.inf:\n    y = jnp.abs(x)\n  elif p == jnp.inf:\n    y = jnp.abs(x) ** 2\n  else:\n    y = jnp.sign(x) * jnp.abs(x) ** p\n\n  if postmult is not None:\n    y *= postmult\n\n  return y\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  x = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_div(x, 1 + jnp.abs(x))),\n          (p == -jnp.inf, safe_div(1, 1 + jnp.abs(x))),\n          (p == jnp.inf, jnp.sign(x)),\n          (p == -1, -jnp.sign(x)),\n          (p == -0.5, -jnp.sqrt(jnp.abs(x))),\n          (p == 0.5, jnp.sqrt(jnp.abs(x))),\n          (p == -2, -jnp.sign(x) * jnp.sqrt(jnp.abs(x))),\n          (p == 2, jnp.sign(x) * jnp.sqrt(jnp.abs(x))),\n          (p == -3, -jnp.sign(x) * jnp.cbrt(jnp.abs(x))),\n          (p == 3, jnp.sign(x) * jnp.cbrt(jnp.abs(x))),\n          (p == -4, -jnp.sign(x) * jnp.power(jnp.abs(x), 1 / 4)),\n          (p == 4, jnp.sign(x) * jnp.power(jnp.abs(x), 1 / 4)),\n          (p == -5, -jnp.sign(x) * jnp.power(jnp.abs(x), 1 / 5)),\n          (p == 5, jnp.sign(x) * jnp.power(jnp.abs(x), 1 / 5)),\n          (p == -6, -jnp.sign(x) * jnp.power(jnp.abs(x), 1 / 6)),\n          (p == 6, jnp.sign(x) * jnp.power(jnp.abs(x), 1 / 6)),\n          (p == -7,"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.sign(x) * jnp.sqrt(jnp.abs(x))\n  elif p == -jnp.inf:\n    y = jnp.sign(x) * jnp.abs(x)\n  elif p == jnp.inf:\n    y = jnp.sign(x) * jnp.abs(x) ** 2\n  else:\n    y = x ** p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  x = select(\n      [\n          (p == 1, x),\n          (p == 0, jnp.sign(x) * jnp.sqrt(jnp.abs(x))),\n          (p == -jnp.inf, jnp.sign(x) * jnp.log(jnp.abs(x) + 1)),\n          (p == jnp.inf, jnp.sign(x) * jnp.exp(jnp.abs(x))),\n      ],\n      x,\n  )\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n\n"}
{"namespace": "math.power_ladder", "completion": "  # Handle special cases for p\n  if p == 1:\n    return x\n  elif p == 0:\n    return jnp.log(x)\n  elif p == -jnp.inf:\n    return jnp.log(x)\n  elif p == jnp.inf:\n    return jnp.exp(x)\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x = x * premult\n\n  # Apply Tukey's power ladder transformation\n  if p > 0:\n    x = jnp.sign(x) * jnp.abs(x) ** p\n  elif p < 0:\n    x = jnp.sign(x) * (1 - jnp.abs(x) ** (-p))\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  x = select(\n      [\n          (p == 1, x),\n          (p == 0, jnp.log(jnp.abs(x) + 1)),\n          (p == -jnp.inf, jnp.sign(x) * (jnp.abs(x) + 1) ** 2),\n          (p == jnp.inf, jnp.sign(x) * jnp.abs(x)),\n          (p == -1, jnp.sign(x) * (jnp.abs(x) + 1) ** 2),\n          (p == -2, jnp.sign(x) * (jnp.abs(x) + 1) ** 3),\n          (p == -3, jnp.sign(x) * (jnp.abs(x) + 1) ** 4),\n          (p == -4, jnp.sign(x) * (jnp.abs(x) + 1) ** 5),\n          (p == -5, jnp.sign(x) * (jnp.abs(x) + 1) ** 6),\n          (p == -6, jnp.sign(x) * (jnp.abs(x) + 1) ** 7),\n          (p == -7, jnp.sign(x) * (jnp.abs(x) + 1) ** 8),\n          (p == -8, jnp.sign(x) * (jnp.abs(x) + 1) ** 9),\n          (p == -9, jnp.sign(x) * (jnp.abs(x) + 1) ** 10),\n          (p == -10, jnp.sign(x) * (jnp.abs(x) + 1) ** 11),\n          (p == -11, jnp.sign(x) * (jnp.abs(x) + 1) ** 12),\n          (p == -12, jnp.sign(x) * (jnp.abs(x) + 1) ** 13),\n          ("}
{"namespace": "math.power_ladder", "completion": "  # Handle special cases for p\n  if p == 1:\n    return x\n  elif p == 0:\n    return jnp.sign(x)\n  elif p == -jnp.inf:\n    return jnp.abs(x)\n  elif p == jnp.inf:\n    return jnp.square(x)\n\n  # Apply pre-multiplication if provided\n  if premult is not None:\n    x *= premult\n\n  # Compute the transformed data\n  if p > 0:\n    transformed_data = jnp.sign(x) * jnp.power(jnp.abs(x), p)\n  else:\n    transformed_data = jnp.sign(x) * jnp.power(jnp.abs(x), 1 / p)\n\n  # Apply post-multiplication if provided\n  if postmult is not None:\n    transformed_data *= postmult\n\n  return transformed_data\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n\n  if p == 1:\n    # No transformation for p=1\n    pass\n  elif p == 0:\n    # Special case for p=0\n    x = jnp.sign(x) * jnp.sqrt(jnp.abs(x))\n  elif p == -jnp.inf:\n    # Special case for p=-inf\n    x = jnp.sign(x) * jnp.exp(jnp.abs(x))\n  elif p == jnp.inf:\n    # Special case for p=inf\n    x = jnp.sign(x) * jnp.log(jnp.abs(x))\n  else:\n    # General case for p not equal to 1, 0, -inf, or inf\n    x = jnp.sign(x) * jnp.abs(x) ** p\n\n  if postmult is not None:\n    x *= postmult\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    # Special case for p = 1\n    y = x\n  elif p == 0:\n    # Special case for p = 0\n    y = jnp.sign(x)\n  elif p == -jnp.inf:\n    # Special case for p = -inf\n    y = jnp.sign(x) * jnp.maximum(jnp.abs(x), 1)\n  elif p == jnp.inf:\n    # Special case for p = inf\n    y = jnp.sign(x) * jnp.minimum(jnp.abs(x), 1)\n  else:\n    # General case for p != 1, 0, -inf, or inf\n    y = jnp.sign(x) * jnp.power(jnp.abs(x), p)\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n  if postmult is not None:\n    postmult = postmult\n\n  x = jnp.where(p == 1, x, x ** p)\n  x = jnp.where(p == 0, jnp.sign(x) * jnp.log1p(jnp.abs(x)), x)\n  x = jnp.where(p == -jnp.inf, jnp.sign(x) * jnp.expm1(jnp.abs(x)), x)\n  x = jnp.where(p == jnp.inf, jnp.sign(x) * jnp.log1p(jnp.abs(x)), x)\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  # Handle special cases for p values of 1, 0, -inf, and inf.\n  if p == 1:\n    return x\n  if p == 0:\n    return jnp.sign(x) * jnp.log1p(jnp.abs(x))\n  if p == -jnp.inf:\n    return jnp.sign(x) * jnp.expm1(jnp.abs(x))\n  if p == jnp.inf:\n    return jnp.abs(x)\n\n  # Apply the Tukey's power ladder transformation.\n  x = jnp.where(x < 0, -jnp.expm1(p * jnp.log1p(-x)), jnp.expm1(p * jnp.log1p(x)))\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = safe_div(x, premult)\n\n  if p == 1:\n    # This is the identity transformation.\n    pass\n  elif p == 0:\n    # This is a log transformation.\n    x = safe_log(x)\n  elif p == -jnp.inf:\n    # This is a reciprocal transformation.\n    x = 1 / x\n  elif p == jnp.inf:\n    # This is a square root transformation.\n    x = safe_sqrt(x)\n  else:\n    # This is a power transformation.\n    x = safe_div(safe_pow(x, p), power_ladder_max_output(p))\n\n  if postmult is not None:\n    x = safe_div(x, postmult)\n\n  return x\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.log(x)\n  elif p == -jnp.inf:\n    y = jnp.sign(x) * jnp.log(-jnp.abs(x))\n  elif p == jnp.inf:\n    y = jnp.sign(x) * x\n  else:\n    y = x**p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.sign(x)\n  elif p == -jnp.inf:\n    y = jnp.abs(x)\n  else:\n    y = jnp.sign(x) * jnp.abs(x) ** p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = safe_div(x, premult)\n\n  if p == 1:\n    # If p == 1, the transformation is a simple power operation.\n    x = safe_pow(x, p)\n  elif p == 0:\n    # If p == 0, the transformation is a log operation.\n    x = safe_log(x)\n  elif p == -jnp.inf:\n    # If p == -inf, the transformation is a reciprocal operation.\n    x = safe_div(1, x)\n  else:\n    # For other p values, the transformation is a power ladder operation.\n    x = safe_pow(x, p)\n\n  if postmult is not None:\n    x = safe_mul(x, postmult)\n\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(x) * |p - 1|/p * ((|x|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          safe_sign(y) * jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(x) * |p - 1|/p * ((|x|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.sign(y) * jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(yp)),\n          (p == -jnp.inf, -safe_expm1(-yp)),\n          (p == jnp.inf, safe_expm1(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(yp)),\n          (p == -jnp.inf, -safe_expm1(-yp)),\n          (p == jnp.inf, safe_expm1(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return safe_sign(y) * x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(-yp)),\n          (p == -jnp.inf, -safe_expm1(yp)),\n          (p == jnp.inf, safe_expm1(-yp)),\n      ],\n      clip_finite_nograd(\n          jnp.sign(y) * jnp.abs(p_safe - 1) / p_safe * (1 - (1 - ys) ** p_safe)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.sign(y) * jnp.abs(p_safe - 1) * (yp + 1) ** (1 / p_safe) - 1\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(x) * |p - 1|/p * ((|x|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(yp)),\n          (p == -jnp.inf, -safe_expm1(-yp)),\n          (p == jnp.inf, safe_expm1(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * (|y|/|p-1| + 1)^(1/p) - 1\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** (1 / p_safe) - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * (((|y|/|p-1| + 1)^(1/p) - 1) * |p - 1|)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** (1 / p_safe) - 1)\n      )\n      * jnp.abs(p_safe - 1),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * (1 - (1 - |y|/|p-1|)^p)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(-yp)),\n          (p == -jnp.inf, -safe_expm1(yp)),\n          (p == jnp.inf, safe_expm1(-yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * (1 - (1 - ys) ** p_safe)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = jnp.abs(p - 1)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          safe_sign(y) * jnp.sign(p) * (ys * (yp + 1) ** (1 / p_safe) - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * (|p - 1|/p * (1 + |y|/|p - 1|)^(1/p) - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * (1 + ys) ** (1 / p_safe) - 1\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_safe = clip_finite_nograd(y)\n  x = select(\n      [\n          (p == 1, y_safe),\n          (p == 0, safe_expm1(y_safe)),\n          (p == -jnp.inf, -safe_log1p(-y_safe)),\n          (p == jnp.inf, safe_log1p(y_safe)),\n      ],\n      clip_finite_nograd(\n          select(\n              [\n                  (p >= 0, safe_div(y_safe, safe_div(p_safe - 1, p_safe))),\n                  (p < 0, safe_div(y_safe, safe_div(1 - p_safe, p_safe))),\n              ],\n              safe_div(y_safe, safe_div(p_safe - 1, p_safe)),\n          )\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  ys = jnp.abs(y)\n  yp = jnp.abs(p)\n  y_safe = clip_finite_nograd(remove_zero(y))\n  x = select(\n      [\n          (p == 1, ys),\n          (p == 0, safe_expm1(y)),\n          (p == -jnp.inf, -safe_log1p(-y)),\n          (p == jnp.inf, safe_log1p(y)),\n      ],\n      clip_finite_nograd(\n          jnp.sign(y) * safe_div(ys * (yp + 1), yp - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  ys = jnp.abs(y)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, ys),\n          (p == 0, safe_expm1(ys)),\n          (p == -jnp.inf, -safe_log1p(-ys)),\n          (p == jnp.inf, safe_log1p(ys)),\n      ],\n      clip_finite_nograd(\n          safe_sign(y) * (((ys + 1) ** (1 / p_safe) - 1) * jnp.abs(p_safe - 1))\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(-yp)),\n          (p == -jnp.inf, -safe_expm1(yp)),\n          (p == jnp.inf, safe_expm1(-yp)),\n      ],\n      clip_finite_nograd(\n          jnp.sign(y) * (jnp.abs(p_safe - 1) / p_safe) * (1 + ys ** p_safe)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(-yp)),\n          (p == -jnp.inf, -safe_expm1(yp)),\n          (p == jnp.inf, safe_expm1(-yp)),\n      ],\n      clip_finite_nograd(\n          safe_sign(y) * (yp / jnp.maximum(tiny_val, jnp.abs(p_safe - 1))) ** p_safe\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_safe = clip_finite_nograd(remove_zero(y))\n  x = select(\n      [\n          (p == 1, y_safe),\n          (p == 0, safe_expm1(y_safe)),\n          (p == -jnp.inf, -safe_log1p(-y_safe)),\n          (p == jnp.inf, safe_log1p(y_safe)),\n      ],\n      clip_finite_nograd(\n          safe_sign(y_safe)\n          * (jnp.abs(p_safe - 1) / p_safe)\n          * (1 + (jnp.abs(y_safe) / (p_safe - 1)) ** p_safe)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, y),\n          (p == 0, safe_expm1(y)),\n          (p == -jnp.inf, -safe_log1p(-y)),\n          (p == jnp.inf, safe_log1p(y)),\n      ],\n      clip_finite_nograd(\n          jnp.sign(y) * (jnp.abs(p_safe - 1) / p_safe) * (1 + safe_div(y, p_safe)) ** (1 / p_safe)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n\n\n\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  y = clip_finite_nograd(y)\n  p = clip_finite_nograd(p)\n  ys = jnp.abs(y)\n  yp = jnp.abs(p - 1)\n  yp_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, ys),\n          (p == 0, safe_expm1(ys)),\n          (p == -jnp.inf, -safe_log1p(-ys)),\n          (p == jnp.inf, safe_log1p(ys)),\n      ],\n      clip_finite_nograd(\n          safe_sign(y)\n          * (\n              jnp.sign(y)\n              * (\n                  jnp.abs(y)\n                  * safe_div(\n                      (yp_safe - 1) * yp,\n                      (yp_safe - 1) * yp + 1,\n                  )\n              )\n          )\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr_delay_mult = 1\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr_delay_mult = 1\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr_delay_mult = 1\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr_delay_mult = 1\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr_delay_mult = 1\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr_delay_mult = 1\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr_delay_mult = 1\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr_delay_mult = 1\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr_delay_mult = 1\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr_delay_mult = 1\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n  lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n\n  if step < lr_delay_steps:\n    lr = lr_init * (step / lr_delay_steps)\n  else:\n    lr = lr_final + (lr_init - lr_final) * (\n        1 - jnp.exp(-(step - lr_delay_steps) / max_steps)\n    )\n\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1.0 / lr_delay_mult\n    lr_delay_steps = max_steps - lr_delay_steps\n\n  if lr_delay_steps > 0:\n    lr = jnp.where(\n        step < lr_delay_steps,\n        lr_init * lr_delay_mult**(step / lr_delay_steps),\n        lr_init * lr_delay_mult**(lr_delay_steps / max_steps),\n    )\n  else:\n    lr = lr_init * lr_delay_mult**(step / max_steps)\n\n  return lr * lr_final / lr_init\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    delay_mult = jnp.minimum(\n        1.0, lr_delay_mult + (1.0 - lr_delay_mult) * step / lr_delay_steps\n    )\n    lr = lr_init * delay_mult\n  else:\n    lr = lr_init\n\n  lr = lr * jnp.maximum(\n      0.0, 1.0 - step / max_steps\n  )  # Linear decay to lr_final\n  lr = lr * jnp.maximum(\n      0.0, 1.0 - step / max_steps\n  )  # Linear decay to lr_final\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n\n  if step <= lr_delay_steps:\n    return lr_init\n  elif step > max_steps:\n    return lr_final\n  else:\n    return log_lerp(\n        (step - lr_delay_steps) / (max_steps - lr_delay_steps), lr_init, lr_final\n    )\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the decay progression\n  decay_progress = jnp.clip(step / max_steps, 0, 1)\n\n  # Apply the delay if specified\n  if lr_delay_steps > 0:\n    decay_progress = jnp.where(\n        step < lr_delay_steps,\n        lr_delay_mult * decay_progress,\n        decay_progress,\n    )\n\n  # Interpolate between the initial and final learning rates\n  learning_rate = lr_init + (lr_final - lr_init) * decay_progress\n\n  return learning_rate\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_steps = jnp.clip(lr_delay_steps, 0, max_steps)\n    lr_delay_mult = jnp.clip(lr_delay_mult, 0, 1)\n    lr = jnp.where(\n        step < lr_delay_steps,\n        lr_init * lr_delay_mult**(step / lr_delay_steps),\n        lr_init * lr_delay_mult**(lr_delay_steps / max_steps),\n    )\n  else:\n    lr = lr_init * lr_delay_mult**(step / max_steps)\n  lr = jnp.where(step > max_steps, lr_final, lr)\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n    lr_delay_steps = max(lr_delay_steps, 1)\n    lr_init = lr_init * (lr_delay_mult**(lr_delay_steps - step))\n    lr_final = lr_final * (lr_delay_mult**(max_steps - step))\n  return log_lerp(step / max_steps, lr_init, lr_final)\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1\n\n  if step < lr_delay_steps:\n    lr = lr_init * (step / lr_delay_steps)\n  elif step >= max_steps:\n    lr = lr_final\n  else:\n    lr = lr_final + (lr_init - lr_final) * (\n        1 - (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n    )\n\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n\n  lr_delay_mult = jnp.maximum(lr_delay_mult, 1e-8)\n  lr_init = jnp.maximum(lr_init, 1e-8)\n  lr_final = jnp.maximum(lr_final, 1e-8)\n  max_steps = jnp.maximum(max_steps, 1)\n\n  if lr_delay_steps > 0:\n    lr_init = jnp.where(\n        step < lr_delay_steps,\n        lr_init * (lr_delay_mult**(step / lr_delay_steps)),\n        lr_init,\n    )\n\n  return lr_final * (\n      1 - jnp.minimum(1, step / max_steps)\n  ) + lr_init * jnp.minimum(1, step / max_steps)\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = (lr_init / lr_final) ** (1.0 / lr_delay_steps)\n    lr_init = lr_init * lr_delay_mult\n\n  lr_delay_mult = jnp.ones_like(lr_init)\n\n  # Calculate the learning rate decay schedule\n  lr_schedule = lr_init * lr_delay_mult * (\n      lr_final / lr_init\n  ) ** (jnp.clip(step, 0, max_steps) / max_steps)\n\n  return lr_schedule\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the learning rate decay based on the progression of steps\n  lr_decay = jnp.clip(\n      (max_steps - step) / max(1, max_steps - lr_delay_steps), 0, 1\n  )\n\n  # Calculate the initial learning rate with the delay multiplier\n  lr_init_delayed = lr_init * lr_delay_mult\n\n  # Calculate the learning rate using log-linear interpolation\n  lr = lr_init_delayed + lr_decay * (lr_final - lr_init_delayed)\n\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n\n  if lr_delay_steps > 0:\n    lr_init *= jnp.minimum(1, step / lr_delay_steps)\n\n  lr_decay = (lr_init - lr_final) / max_steps\n  lr = lr_init - lr_decay * step\n  lr = jnp.maximum(lr, lr_final)\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Calculate the learning rate during the delay period\n    lr_delay = (\n        lr_init * lr_delay_mult\n    )  # Multiply the initial learning rate by the delay multiplier\n    lr_delay_end = (\n        lr_delay + (lr_init - lr_delay) * (lr_delay_steps / max_steps)\n    )  # Calculate the end learning rate of the delay period\n    lr_delay_start = lr_delay - (\n        lr_delay - lr_init\n    ) * (lr_delay_steps / max_steps)  # Calculate the start learning rate of the delay period\n    lr_delay_decay = (\n        lr_delay_end - lr_delay_start\n    ) / lr_delay_steps  # Calculate the decay rate of the delay period\n\n    # Calculate the learning rate during the delay period\n    lr_delay_step = jnp.clip(\n        step - lr_delay_steps, 0, lr_delay_steps\n    )  # Clip the step to be within the delay period\n    lr_delay_decayed = lr_delay_start - lr_delay_decay * lr_delay_step\n\n    # Calculate the learning rate during the main period\n    lr_main = (\n        lr_init - (lr_init - lr_final) * (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n    )\n\n    # Combine the learning rates during the delay and main periods\n    lr = jnp.where(\n        step < lr_delay_steps, lr_delay_decayed, lr_main\n    )  # Select the appropriate learning rate based on the step\n  else:\n    # Calculate the learning rate during the main period\n    lr = (\n        lr_init - (lr_init - lr_final) * step / max_steps\n    )  # Calculate the learning rate based on"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1\n\n  # Calculate the progression of the optimization steps\n  progression = jnp.clip(step / max_steps, 0, 1)\n\n  # Apply the learning rate decay using log-linear interpolation\n  learning_rate = lr_init * (lr_final / lr_init) ** progression\n\n  # Apply the delay if specified\n  if lr_delay_steps > 0:\n    learning_rate = jnp.where(\n        step < lr_delay_steps,\n        lr_init * lr_delay_mult * (1 - progression)\n        + lr_init * (1 - lr_delay_mult) * progression,\n        learning_rate,\n    )\n\n  return learning_rate\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the learning rate based on the progression\n  lr_progress = jnp.clip(\n      (step - lr_delay_steps) / max(1, max_steps - lr_delay_steps), 0, 1\n  )\n\n  # Calculate the learning rate with a log-linear interpolation\n  lr = lr_init * lr_progress + lr_final * (1 - lr_progress)\n\n  # Apply the delay if specified\n  if lr_delay_steps > 0:\n    lr = lr * lr_delay_mult ** (step / lr_delay_steps)\n\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_delayed = jnp.maximum(\n        lr_init * lr_delay_mult, lr_final\n    )  # Apply delay multiplier.\n    lr = jnp.where(\n        step < lr_delay_steps,\n        lr_delayed,\n        lr_final\n        + (lr_init - lr_final)\n        * (1 - jnp.exp(-(step - lr_delay_steps) / max_steps)),\n    )\n  else:\n    lr = lr_final + (lr_init - lr_final) * (1 - jnp.exp(-step / max_steps))\n  return lr\n\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  lr_delay_mult = jnp.clip(lr_delay_mult, 0, 1)\n  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n\n  lr_decay = jnp.log(lr_init / lr_final) / (max_steps - lr_delay_steps)\n  lr = lr_init * jnp.exp(lr_decay * (step - lr_delay_steps))\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  lr_delay_mult = jnp.clip(lr_delay_mult, 0, 1)\n  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_final = lr_final * lr_delay_mult\n  lr_init = jnp.clip(lr_init, 0, lr_final)\n  lr_final = jnp.clip(lr_final, lr_init, 1)\n  if max_steps > 0:\n    lr_init = lr_init * (lr_final / lr_init) ** (lr_delay_steps / max_steps)\n    lr_final = lr_final * (lr_final / lr_init) ** (lr_delay_steps / max_steps)\n  return jnp.where(\n      step < lr_delay_steps,\n      lr_init * (lr_final / lr_init) ** (step / lr_delay_steps),\n      lr_init * (lr_final / lr_init) ** (step / max_steps),\n  )\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n  lr_delay_mult = 1 - lr_delay_mult\n  lr_delay_steps = max(0, lr_delay_steps)\n  lr_final = max(lr_final, lr_init)\n  lr_init = min(lr_init, lr_final)\n  if lr_delay_steps > 0:\n    return jnp.where(\n        step < lr_delay_steps,\n        lr_init + (lr_final - lr_init) * step / lr_delay_steps,\n        lr_init * lr_delay_mult ** ((step - lr_delay_steps) / (max_steps - lr_delay_steps)),\n    )\n  else:\n    return lr_init * lr_delay_mult ** (step / (max_steps - lr_delay_steps))\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=0,\n      origin_hi=1,\n      radius_lo=0,\n      radius_hi=1,\n      near_lo=0,\n      near_hi=1,\n      far_lo=0,\n      far_hi=1,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  n = 1\n  origin_lo = -1.0\n  origin_hi = 1.0\n  radius_lo = 0.0\n  radius_hi = 1.0\n  near_lo = 0.0\n  near_hi = 1.0\n  far_lo = 1.0\n  far_hi = 2.0\n\n  return generate_random_rays(\n      rng,\n      n,\n      origin_lo,\n      origin_hi,\n      radius_lo,\n      radius_hi,\n      near_lo,\n      near_hi,\n      far_lo,\n      far_hi,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  rng, rng_input = random.split(rng)\n  random_rays = generate_random_rays(\n      rng_input,\n      1,\n      -1.0,\n      1.0,\n      0.0,\n      1.0,\n      0.1,\n      10.0,\n      10.0,\n      100.0,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )\n  return random_rays\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  random_rays = generate_random_rays(\n      rng,\n      1,\n      -1.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n  return random_rays\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=100.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  random_rays = generate_random_rays(\n      rng=rng,\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=10.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n  return random_rays\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  n = 100\n  origin_lo = -1.0\n  origin_hi = 1.0\n  radius_lo = 0.0\n  radius_hi = 1.0\n  near_lo = 0.0\n  near_hi = 1.0\n  far_lo = 1.0\n  far_hi = 2.0\n  return generate_random_rays(\n      rng,\n      n,\n      origin_lo,\n      origin_hi,\n      radius_lo,\n      radius_hi,\n      near_lo,\n      near_hi,\n      far_lo,\n      far_hi,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=10.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  n = 1\n  origin_lo = -1.0\n  origin_hi = 1.0\n  radius_lo = 0.0\n  radius_hi = 1.0\n  near_lo = 0.0\n  near_hi = 1.0\n  far_lo = 1.0\n  far_hi = 2.0\n  return generate_random_rays(\n      rng,\n      n,\n      origin_lo,\n      origin_hi,\n      radius_lo,\n      radius_hi,\n      near_lo,\n      near_hi,\n      far_lo,\n      far_hi,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1,\n      origin_hi=1,\n      radius_lo=0.1,\n      radius_hi=0.5,\n      near_lo=0.1,\n      near_hi=1,\n      far_lo=1,\n      far_hi=10,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  random_rays = generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n  return random_rays\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  random_rays = generate_random_rays(\n      rng,\n      1,\n      origin_lo=-1,\n      origin_hi=1,\n      radius_lo=0.1,\n      radius_hi=1,\n      near_lo=0.1,\n      near_hi=1,\n      far_lo=1,\n      far_hi=10,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n  return random_rays\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      random.PRNGKey(0),\n      1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  n = 1\n  origin_lo = -1.0\n  origin_hi = 1.0\n  radius_lo = 0.0\n  radius_hi = 1.0\n  near_lo = 0.0\n  near_hi = 1.0\n  far_lo = 1.0\n  far_hi = 2.0\n\n  return generate_random_rays(\n      rng,\n      n,\n      origin_lo,\n      origin_hi,\n      radius_lo,\n      radius_hi,\n      near_lo,\n      near_hi,\n      far_lo,\n      far_hi,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          xnp.array([0.5, 0.5, 1.0]),\n          xnp.array([1.5, 0.5, 1.0]),\n          xnp.array([0.5, 1.5, 1.0]),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    camera_dirs_stacked = xnp.stack(\n        [\n            camera_dirs_stacked[Ellips"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply camera rotation matrices.\n  points = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], points)\n  # Apply inverse intrinsic matrices.\n  points = mat_vec_mul(pixtocams, points)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        points[Ellipsis, 0],\n        points[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points = xnp.stack(\n        [\n            points[Ellipsis, 0] * sin_theta_over_theta,\n            points[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points[Ellipsis, 0]\n    phi = points[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    points = xnp.stack(\n        ["}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply camera rotation matrices.\n  points_cam = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], points - camtoworlds[Ellipsis, :3, -1])\n\n  # Apply inverse intrinsic matrices.\n  points_cam = mat_vec_mul(pixtocams, points_cam)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        points_cam[Ellipsis, 0],\n        points_cam[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_cam = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points_cam[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points_cam = xnp.stack(\n        [\n            points_cam[Ellipsis, 0] * sin_theta_over_theta,\n            points_cam[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points_cam[Ellipsis, 0]\n    phi ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply camera rotation matrices.\n  points = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], points)\n  # Apply camera translation.\n  points = points + camtoworlds[Ellipsis, :3, -1]\n\n  # Apply inverse intrinsic matrices.\n  points = mat_vec_mul(pixtocams, points)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        points[Ellipsis, 0],\n        points[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points = xnp.stack(\n        [\n            points[Ellipsis, 0] * sin_theta_over_theta,\n            points[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points[Ellipsis, 0]\n    phi = points[Ellipsis, 1"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n\n  # Apply camera rotation matrices.\n  points = matmul(camtoworlds[Ellipsis, :3, :3], points[Ellipsis, :3, None])[\n      Ellipsis, 0\n  ]\n\n  # Apply camera intrinsic matrices.\n  points = matmul(pixtocams, points)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        points[Ellipsis, 0], points[Ellipsis, 1], **distortion_params, xnp=xnp\n    )\n    points = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points = xnp.stack(\n        [\n            points[Ellipsis, 0] * sin_theta_over_theta,\n            points[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points[Ellipsis, 0]\n    phi = points[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    points = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi),\n           "}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n\n  # Apply camera rotation matrices.\n  points = matmul(camtoworlds[Ellipsis, :3, :3], points[Ellipsis, :3, None])[\n      Ellipsis, 0\n  ]\n  # Apply camera intrinsic matrices.\n  points = matmul(pixtocams, points)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        points[Ellipsis, 0], points[Ellipsis, 1], **distortion_params, xnp=xnp\n    )\n    points = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points = xnp.stack(\n        [\n            points[Ellipsis, 0] * sin_theta_over_theta,\n            points[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points[Ellipsis, 0]\n    phi = points[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    points = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi),\n            -"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply camera rotation matrices.\n  camera_coordinates = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], points)\n  camera_coordinates = camera_coordinates + camtoworlds[Ellipsis, :3, -1]\n\n  # Apply inverse intrinsic matrices.\n  pixel_coordinates = mat_vec_mul(pixtocams, camera_coordinates)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        pixel_coordinates[Ellipsis, 0],\n        pixel_coordinates[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    pixel_coordinates = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(pixel_coordinates[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    pixel_coordinates = xnp.stack(\n        [\n            pixel_coordinates[Ellipsis, 0] * sin_theta_over_theta,\n            pixel_coordinates[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply camera rotation matrices.\n  camera_points = mat_vec_mul(\n      camtoworlds[Ellipsis, :3, :3], points\n  ) + camtoworlds[Ellipsis, :3, -1]\n\n  # Apply inverse intrinsic matrices.\n  pixel_coords = mat_vec_mul(pixtocams, camera_points)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        pixel_coords[Ellipsis, 0],\n        pixel_coords[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    pixel_coords = xnp.stack([x, y], axis=-1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(pixel_coords[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    pixel_coords = xnp.stack(\n        [\n            pixel_coords[Ellipsis, 0] * sin_theta_over_theta,\n            pixel_coords[Ellipsis, 1] * sin_theta_over_theta,\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = pixel_coords[Ellipsis, 0]\n    phi = pixel_co"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  points = xnp.broadcast_to(points, pixtocams.shape[:-2] + points.shape)\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply camera rotation matrices.\n  points_camera = mat_vec_mul(\n      camtoworlds[Ellipsis, :3, :3], points[Ellipsis, :3]\n  ) + camtoworlds[Ellipsis, :3, -1][Ellipsis, None]\n\n  # Apply inverse intrinsic matrices.\n  points_camera = mat_vec_mul(pixtocams, points_camera)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        points_camera[Ellipsis, 0],\n        points_camera[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_camera = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points_camera[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points_camera = xnp.stack(\n        [\n            points_camera[Ellipsis, 0] * sin_theta_over_theta"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[:3], screw_axis[3:]\n  theta = jnp.linalg.norm(w)\n  w_hat = skew(w)\n  R = jnp.eye(3) + jnp.sin(theta) / theta * w_hat + (1 - jnp.cos(theta)) / theta**2 * jnp.matmul(w_hat, w_hat)\n  p = jnp.where(theta > eps, v / theta, v)\n  return rp_to_se3(R, p)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[0:3]\n  v = screw_axis[3:6]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = jnp.eye(4) + skew(w) @ (1.0 - jnp.cos(theta)) / theta**2 + (\n      1.0 - jnp.sin(theta) / theta\n  ) * (skew(w) @ skew(w))\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  w_hat = skew(w_safe)\n  X = (\n      jnp.eye(4)\n      + jnp.sin(theta_safe) / theta_safe * w_hat\n      + (1.0 - jnp.cos(theta_safe)) / theta_safe**2 * w_hat @ w_hat\n  )\n  v = jnp.reshape(v, (3, 1))\n  X = X @ jnp.block([[jnp.eye(3), v], [jnp.zeros((1, 3)), 1.0]])\n\n  return jnp.where(theta > eps, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[:3], screw_axis[3:]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = jnp.eye(4) + skew(w) * theta\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  W = skew(w_safe)\n  X = (\n      jnp.eye(4)\n      + W * theta_safe\n      + (1.0 - jnp.cos(theta_safe)) / theta_safe**2 * spin_math.matmul(W, W)\n  )\n\n  v = jnp.reshape(v, (3, 1))\n  X = spin_math.matmul(X, jnp.block([[jnp.eye(3), v], [jnp.zeros((1, 3)), 1.0]]))\n\n  return jnp.where(theta > eps, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the rotation and translation components from the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the rotation matrix from the rotation component of the screw axis\n  R = exp_so3(w, eps)\n\n  # Compute the translation component of the screw axis\n  theta = jnp.linalg.norm(w)\n  if theta < eps:\n    # If theta is very small, use the first-order Taylor expansion\n    v_hat = jnp.array([[0.0, -w[2], w[1]], [w[2], 0.0, -w[0]], [-w[1], w[0], 0.0]])\n    v_hat_squared = jnp.matmul(v_hat, v_hat)\n    v_hat_cubed = jnp.matmul(v_hat_squared, v_hat)\n    v_hat_to_theta = jnp.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, -1.0],\n        [0.0, 1.0, 0.0],\n    ])\n    v_hat_to_theta = jnp.matmul(v_hat_to_theta, v_hat_to_theta)\n    v_hat_to_theta = jnp.matmul(v_hat_to_theta, v_hat_to_theta)\n    v_hat_to_theta = jnp.matmul(v_hat_to_theta, v_hat_to_theta)\n    v_hat_to_theta = jnp.matmul(v_hat_to_theta, v_hat_to_theta)\n    v_hat_to_theta = jnp.matmul(v_hat_to_theta, v_hat_to_theta)\n    v_hat_to_theta = jnp.matmul(v_hat_to_theta, v_hat_to_theta)\n   "}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + skew(w)\n  p_taylor = v\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  v_safe = jnp.where(theta_squared > eps**2, v, 0.0)\n  w = w_safe / theta_safe\n  W = skew(w)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n  p = (\n      jnp.sin(theta_safe) * v_safe\n      + (1.0 - jnp.cos(theta_safe)) * jnp.matmul(W, v_safe)\n  )\n\n  return rp_to_se3(R, p)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = rp_to_se3(exp_so3(w, eps), v)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_hat = skew(w_safe)\n  v_hat = skew(v)\n  X = (\n      jnp.eye(4)\n      + jnp.sin(theta_safe) * w_hat\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(w_hat, w_hat)\n      + (theta_safe - jnp.sin(theta_safe)) * spin_math.matmul(w_hat, v_hat)\n  )\n\n  return jnp.where(theta_squared > eps**2, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[3:], axis=-1)\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = jnp.eye(4) + skew(w) * theta\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  v_safe = jnp.where(theta > eps, v, 0.0)\n  w_hat = skew(w_safe)\n  X = jnp.eye(4) + jnp.sin(theta_safe) / theta_safe * w_hat + (\n      1.0 - jnp.cos(theta_safe)\n  ) / theta_safe**2 * jnp.matmul(w_hat, w_hat)\n  X = X + jnp.matmul(jnp.matmul(jnp.matmul(jnp.matmul(jnp.eye(4), v_safe), w_hat),\n                                jnp.eye(4)), (1.0 - jnp.cos(theta_safe)) /\n                                              theta_safe**2)\n\n  return jnp.where(theta > eps, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[3:])\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = rp_to_se3(exp_so3(w, eps), v)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  v_safe = jnp.where(theta > eps, v, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n\n  w_hat = skew(w_safe)\n  X = rp_to_se3(\n      exp_so3(w_safe, eps),\n      v_safe\n      + jnp.dot(\n          w_hat,\n          jnp.dot(\n              w_hat,\n              v_safe,\n          ),\n      )\n      / theta_safe\n      + jnp.dot(\n          w_hat,\n          jnp.cross(\n              w_safe,\n              v_safe,\n          ),\n      )\n      * (1.0 - jnp.cos(theta_safe))\n      / theta_safe**2,\n  )\n\n  return jnp.where(theta > eps, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[:3]\n  v = screw_axis[3:]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n  W = skew(w)\n  R = exp_so3(w, eps)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = jnp.block([[R, v[:, None]], [jnp.array([[0.0, 0.0, 0.0, 1.0]])]])\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  W_safe = jnp.where(theta > eps, W, jnp.eye(3))\n  v_safe = jnp.where(theta > eps, v, 0.0)\n  R_safe = jnp.where(theta > eps, R, jnp.eye(3))\n\n  X = jnp.block([[R_safe, v_safe[:, None]], [jnp.array([[0.0, 0.0, 0.0, 1.0]])]])\n\n  return jnp.where(theta > eps, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[0:3]\n  v = screw_axis[3:6]\n  theta = jnp.linalg.norm(w)\n  if theta < eps:\n    # If theta is small, we can use the Taylor series approximation.\n    R = jnp.eye(3)\n    p = v\n  else:\n    # Otherwise, we use the exponential map formula.\n    R = exp_so3(w, eps)\n    p = jnp.dot(v, jnp.sin(theta) / theta) + jnp.dot(\n        jnp.dot(w, w), (1 - jnp.cos(theta)) / theta**2\n    )\n  return rp_to_se3(R, p)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = jnp.split(screw_axis, 2, axis=-1)\n  theta = jnp.linalg.norm(w, axis=-1)\n  w = w / jnp.maximum(theta, eps)\n  theta = jnp.expand_dims(theta, axis=-1)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + skew(w)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  W = skew(w_safe)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n\n  p = jnp.where(theta > eps, v, v / eps)\n\n  return rp_to_se3(R, p)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[0:3]\n  v = screw_axis[3:6]\n  theta = jnp.linalg.norm(w)\n  R = exp_so3(w, eps)\n  if theta < eps:\n    return rp_to_se3(R, v)\n  else:\n    W = skew(w)\n    V = jnp.block([[W, v[:, None]], [0.0, 0.0, 0.0, 0.0]])\n    return jnp.matmul(jnp.exp(theta), V)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[:3]\n  v = screw_axis[3:]\n  theta = jnp.linalg.norm(w)\n  if theta < eps:\n    return jnp.eye(4)\n  R = exp_so3(w, eps)\n  V = jnp.block([[R, v[:, None]], [jnp.array([[0.0, 0.0, 0.0, 0.0]])]])\n  return V\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[:3]\n  v = screw_axis[3:]\n  theta = jnp.linalg.norm(w)\n  theta_squared = theta**2\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = jnp.eye(4) + skew(w) * (theta / (2.0 * theta_squared)) + (\n      1.0 / theta_squared\n  ) * jnp.matmul(skew(w), skew(w))\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_hat = skew(w_safe)\n  X = jnp.eye(4) + jnp.sin(theta_safe) / theta_safe * w_hat + (\n      1.0 - jnp.cos(theta_safe)\n  ) / theta_squared * jnp.matmul(w_hat, w_hat)\n  X = X + jnp.matmul(jnp.matmul(jnp.matmul(skew(w_safe), v), jnp.eye(3)), X)\n\n  return jnp.where(theta_squared > eps**2, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the rotation axis and angle from the screw axis.\n  w = screw_axis[0:3]\n  v = screw_axis[3:6]\n\n  # Compute the exponential map of the rotation axis.\n  R = exp_so3(w, eps)\n\n  # Compute the exponential map of the translation.\n  theta = jnp.linalg.norm(w)\n  if theta < eps:\n    # If theta is small, use the first-order Taylor approximation.\n    T = jnp.eye(4)\n    T[0:3, 0:3] = R\n    T[0:3, 3] = v\n  else:\n    # Otherwise, use the full exponential map.\n    W = skew(w)\n    V = jnp.block([[W, v[:, jnp.newaxis]], [jnp.zeros((1, 4))]])\n    T = jnp.eye(4) + jnp.sin(theta) / theta * V + (1 - jnp.cos(theta)) / theta**2 * jnp.matmul(W, W)\n\n  return T\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[:3]\n  v = screw_axis[3:]\n  theta = jnp.linalg.norm(w)\n  R = exp_so3(w, eps)\n  if theta < eps:\n    return jnp.block([[R, v[:, None]], [jnp.zeros((1, 4))]])\n  else:\n    return jnp.block([[R, v[:, None] * jnp.sin(theta) / theta],\n                      [jnp.zeros((1, 3)), jnp.cos(theta)]])\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the rotation and translation components from the screw axis.\n  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n\n  # Compute the angle of rotation theta and the unit vector axis.\n  theta = jnp.linalg.norm(w)\n  axis = w / (theta + eps)\n\n  # Compute the skew-symmetric matrix W.\n  W = skew(axis)\n\n  # Compute the rotation matrix R.\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1 - jnp.cos(theta)) * spin_math.matmul(W, W)\n\n  # Compute the translation matrix T.\n  T = jnp.eye(3) + (1 - jnp.cos(theta)) * W + (theta - jnp.sin(theta)) * spin_math.matmul(W, W)\n\n  # Compute the homogeneous transformation matrix X.\n  X = jnp.block([[R, v[:, None]], [jnp.array([[0.0, 0.0, 0.0, 1.0]])]])\n\n  return X\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[0:3]\n  v = screw_axis[3:6]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = jnp.eye(4) + spin_math.skew(w) * theta\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  W = spin_math.skew(w_safe)\n  X = (\n      jnp.eye(4)\n      + W * jnp.sin(theta_safe) / theta_safe\n      + (1.0 - jnp.cos(theta_safe)) * W * W / theta_safe**2\n  )\n\n  X = X * jnp.expand_dims(jnp.concatenate([v, [0.0]], axis=-1), axis=-1)\n\n  return jnp.where(theta > eps, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the rotation and translation components from the screw axis\n  w = screw_axis[0:3]\n  v = screw_axis[3:6]\n\n  # Compute the exponential map of the rotation component\n  R = exp_so3(w, eps)\n\n  # Compute the exponential map of the translation component\n  theta = jnp.linalg.norm(w)\n  if theta < eps:\n    # If theta is very small, use the first-order Taylor expansion\n    V = jnp.eye(3)\n  else:\n    # Otherwise, use the full exponential map\n    W = skew(w)\n    V = jnp.eye(3) + W / theta + jnp.dot(W, W) / (2.0 * theta**2)\n\n  # Combine the rotation and translation components to form the homogeneous transformation matrix\n  X = jnp.eye(4)\n  X[0:3, 0:3] = R\n  X[0:3, 3] = jnp.dot(V, v)\n\n  return X\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n  W = skew(w)\n\n  # Modern Robotics Eqn 3.59\n  V_theta = (\n      jnp.eye(3) * jnp.cos(theta)\n      + (1 - jnp.cos(theta)) * W\n      + (theta - jnp.sin(theta)) * spin_math.matmul(W, W) / theta\n  )\n  p_theta = jnp.dot(V_theta, v)\n\n  return jnp.block([[V_theta, p_theta[:, None]], [jnp.zeros((1, 4))]])\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n    return jnp.eye(3)\n  else:\n    axis = axis_angle / theta\n    return jnp.eye(3) + jnp.sin(theta) * skew(axis) + (1 - jnp.cos(theta)) * (\n        skew(axis) @ skew(axis)\n    )\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle of rotation from the input axis-angle vector.\n  axis = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n  angle = jnp.linalg.norm(axis_angle)\n\n  # Compute the skew-symmetric matrix of the axis vector.\n  w = jnp.array([[0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula.\n  R = jnp.eye(3) + jnp.sin(angle) * w + (1 - jnp.cos(angle)) * jnp.matmul(w, w)\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (theta + eps)\n  theta = theta + eps\n\n  # Rodrigues' formula\n  kx = jnp.array([[0, -axis[2], axis[1]], [axis[2], 0, -axis[0]],\n                  [-axis[1], axis[0], 0]])\n  R = jnp.eye(3) + jnp.sin(theta) * kx + (1 - jnp.cos(theta)) * kx @ kx\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  axis_angle = axis_angle / (theta + eps)\n  axis = axis_angle\n  theta = theta + eps\n  R = jnp.eye(3) + jnp.sin(theta) * skew(axis) + (1 - jnp.cos(theta)) * jnp.matmul(skew(axis), skew(axis))\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3))\n  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n    return jnp.eye(3)\n  axis = axis_angle / theta\n  axis = jnp.reshape(axis, (3, 1))\n  axis_cross = jnp.cross(jnp.eye(3), axis)\n  R = jnp.eye(3) + jnp.sin(theta) * axis_cross + (1 - jnp.cos(theta)) * axis_cross @ axis_cross\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  axis_angle_unit = axis_angle / (theta + eps)\n\n  # Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * skew(axis_angle_unit) + (1 - jnp.cos(theta)) * jnp.linalg.matrix_power(skew(axis_angle_unit), 2)\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3))\n  theta = jnp.linalg.norm(axis_angle)\n  k = axis_angle / (theta + eps)\n  c = jnp.cos(theta)\n  s = jnp.sin(theta)\n  K = skew(k)\n  R = jnp.eye(3) + s * K + (1 - c) * K @ K\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Normalize the axis of rotation\n  axis = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  theta = jnp.linalg.norm(axis_angle)\n  k = jnp.array([[0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0]])\n  R = jnp.eye(3) + jnp.sin(theta) * k + (1 - jnp.cos(theta)) * jnp.matmul(k, k)\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  k = axis_angle / (theta + eps)\n  K = skew(k)\n  R = jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * K @ K\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Normalize the axis of rotation\n  axis_angle = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n\n  # Extract the angle of rotation\n  angle = jnp.linalg.norm(axis_angle)\n\n  # Compute the skew-symmetric matrix from the axis of rotation\n  w = jnp.array([[0, -axis_angle[2], axis_angle[1]],\n                 [axis_angle[2], 0, -axis_angle[0]],\n                 [-axis_angle[1], axis_angle[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(angle) * w + (1 - jnp.cos(angle)) * jnp.dot(w, w)\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the angle and axis from the input axis-angle representation\n  theta = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (theta + eps)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  K = jnp.array([[0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0]])\n  R = jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * jnp.dot(K, K)\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3))\n  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n  angle = angle + eps\n  c = jnp.cos(angle)\n  s = jnp.sin(angle)\n  t = 1.0 - c\n  x, y, z = axis[0], axis[1], axis[2]\n  return jnp.array(\n      [[t * x * x + c, t * x * y - s * z, t * x * z + s * y],\n       [t * x * y + s * z, t * y * y + c, t * y * z - s * x],\n       [t * x * z - s * y, t * y * z + s * x, t * z * z + c]])\n\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n    return jnp.eye(3)\n  else:\n    axis = axis_angle / theta\n    return jnp.eye(3) + jnp.sin(theta) * skew(axis) + (1 - jnp.cos(theta)) * skew(\n        axis\n    ) @ skew(axis)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (theta + eps)\n  theta = theta + eps\n  axis = jnp.reshape(axis, (3, 1))\n  axis_cross = jnp.cross(axis, axis)\n  return jnp.eye(3) + jnp.sin(theta) * axis_cross + (1.0 - jnp.cos(theta)) * axis_cross @ axis_cross\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Normalize the axis of rotation\n  axis = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n\n  # Compute the angle of rotation\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * skew(axis) + (1 - jnp.cos(theta)) * jnp.linalg.matrix_power(skew(axis), 2)\n\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  axis_angle_norm = axis_angle / (theta + eps)\n  axis = axis_angle_norm\n  theta = theta + eps\n  R = jnp.eye(3) + jnp.sin(theta) * skew(axis) + (1 - jnp.cos(theta)) * (\n      skew(axis) @ skew(axis)\n  )\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle of rotation from the input axis-angle vector\n  axis = axis_angle / jnp.linalg.norm(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  if angle < eps:\n    # For small angles, use the Taylor expansion of the exponential map\n    return jnp.eye(3) + skew(axis_angle)\n  else:\n    # For large angles, use Rodrigues' formula directly\n    return jnp.eye(3) + jnp.sin(angle) * skew(axis) + (1 - jnp.cos(angle)) * jnp.linalg.matrix_power(skew(axis), 2)\n\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3, 1))\n  theta = jnp.linalg.norm(axis_angle)\n\n  # If theta is very small, use Rodrigues' formula for small angles.\n  if theta < eps:\n    w = axis_angle / (2 * theta)\n    return jnp.eye(3) + skew(w)\n\n  w = axis_angle / theta\n  w_cross = skew(w)\n\n  # Rodrigues' formula for large angles.\n  return jnp.eye(3) + jnp.sin(theta) * w_cross + (1 - jnp.cos(theta)) * jnp.matmul(w_cross, w_cross)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (theta + eps)\n  theta = jnp.where(theta > eps, theta, eps)\n  theta = jnp.where(theta < eps, eps, theta)\n  K = jnp.array([[0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0]])\n  R = jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * K @ K\n  return R\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis of rotation and the angle of rotation from the input axis-angle representation\n  axis = axis_angle / jnp.linalg.norm(axis_angle)\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Compute the skew-symmetric matrix from the axis of rotation\n  w = jnp.array([[0, -axis[2], axis[1]],\n                 [axis[2], 0, -axis[0]],\n                 [-axis[1], axis[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * w + (1 - jnp.cos(theta)) * jnp.dot(w, w)\n\n  # Apply the numerical stability mechanism\n  if theta < eps:\n    # If the angle of rotation is very small, use the Taylor series expansion for the rotation matrix\n    R = jnp.eye(3) + (1 - theta / 2) * jnp.dot(w, w)\n  else:\n    # Otherwise, use the Rodrigues' formula\n    R = jnp.eye(3) + jnp.sin(theta) * w + (1 - jnp.cos(theta)) * jnp.dot(w, w)\n\n  return R\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var = r_var * base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r = base_radius * t_mean\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r = base_radius * t_mean\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r = base_radius * t_mean\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius**2 * r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r = base_radius * t_mean\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r = base_radius * t_mean\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r = base_radius * t_mean\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Compute the mean and variances of the frustum.\n  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n\n  # Compute the mean and covariance of the Gaussian distribution.\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n  # Scale the radius by the base radius.\n  cov *= base_radius**2\n\n  return mean, cov\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r = base_radius * t_mean\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r = base_radius * t_mean\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r = base_radius * t_mean\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Convert pixel coordinates to camera coordinates.\n    pix_x_cam = (pix_x_int - pixtocams[Ellipsis, 0, 2]) * pixtocams[Ellipsis, 0, 0]\n    pix_y_cam = (pix_y_int - pixtocams[Ellipsis, 1, 2]) * pixtocams[Ellipsis, 1, 1]\n\n    # Convert camera coordinates to world coordinates.\n    origins = xnp.stack(\n        [\n            pix_x_cam * camtoworlds[Ellipsis, 0, 0]\n            + pix_y_cam * camtoworlds[Ellipsis, 0, 1]\n            + camtoworlds[Ellipsis, 0, 2],\n            pix_x_cam * camtoworlds[Ellipsis, 1, 0]\n            + pix_y_cam * camtoworlds[Ellipsis, 1, 1]\n            + camtoworlds[Ellipsis, 1, 2],\n            pix_x_cam * camtoworlds[Ellipsis, 2, 0]\n            + pix_y_cam * camtoworlds[Ellipsis, 2, 1]\n            + camtoworlds[Ellipsis, 2, 2],\n        ],\n        axis=-1,\n    )\n    directions = xnp.stack(\n        [\n            pix_x_cam * camtoworlds[Ellipsis, 0, 3]\n            + pix_y_cam * camtoworlds[Ellipsis, 1, 3]\n            + camtoworlds[Ellipsis, 2, 3],\n            pix_x_cam * camtoworlds[Ellipsis, 3, 3]\n            + pix_y_cam * camtoworlds[Ellipsis, 4, 3]\n            + camtoworld"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Perspective camera model.\n    pix_x_cam = (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0]\n    pix_y_cam = (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n  elif camtype == ProjectionType.FISHEYE:\n    # Fisheye camera model.\n    pix_x_cam = (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0]\n    pix_y_cam = (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n    r2 = pix_x_cam * pix_x_cam + pix_y_cam * pix_y_cam\n    pix_x_cam = pix_x_cam * xnp.sqrt(1.0 + r2)\n    pix_y_cam = pix_y_cam * xnp.sqrt(1.0 + r2)\n  elif camtype == ProjectionType.PANORAMIC:\n    # Panoramic camera model.\n    pix_x_cam = (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0]\n    pix_y_cam = (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n    pix_x_cam = xnp.clip(pix_x_cam, -1.0, 1.0)\n    pix_y_cam = xnp.clip"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam, pix_y_cam, pix_z_cam = (\n      pixtocams[Ellipsis, 0, 0] * pix_x_int + pixtocams[Ellipsis, 0, 1] * pix_y_int + pixtocams[Ellipsis, 0, 2],\n      pixtocams[Ellipsis, 1, 1] * pix_y_int + pixtocams[Ellipsis, 1, 1] * pix_y_int + pixtocams[Ellipsis, 1, 2],\n      pixtocams[Ellipsis, 2, 2],\n  )\n\n  # Apply lens distortion if necessary.\n  if distortion_params is not None:\n    pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n        pix_x_cam,\n        pix_y_cam,\n        k1=distortion_params['k1'],\n        k2=distortion_params['k2'],\n        p1=distortion_params['p1'],\n        p2=distortion_params['p2'],\n        xnp=xnp,\n    )\n\n  # Compute ray directions.\n  directions = xnp.stack([pix_x_cam, pix_y_cam, pix_z_cam], axis=-1)\n\n  # Convert camera coordinates to world coordinates.\n  origins = xnp.einsum('...ij,...j->...i', camtoworlds[Ellipsis, :3, :3], directions)\n  origins = origins + camtoworlds[Ellipsis, :3, 3:]\n\n  # Compute normalized view directions.\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute differential radii.\n  radii = xnp.linalg.norm(directions, axis=-1"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Convert pixel coordinates to camera coordinates.\n    pix_x_cam = (pix_x_int + 0.5) / pixtocams[Ellipsis, 0, 2]\n    pix_y_cam = (pix_y_int + 0.5) / pixtocams[Ellipsis, 1, 2]\n    pix_z_cam = xnp.ones_like(pix_x_cam)\n    pix_cam = xnp.stack([pix_x_cam, pix_y_cam, pix_z_cam], axis=-1)\n\n    # Apply distortion correction if distortion_params are provided.\n    if distortion_params is not None:\n      pix_cam = _radial_and_tangential_undistort(\n          pix_cam[Ellipsis, 0],\n          pix_cam[Ellipsis, 1],\n          k1=distortion_params['k1'],\n          k2=distortion_params['k2'],\n          k3=distortion_params['k3'],\n          k4=distortion_params['k4'],\n          p1=distortion_params['p1'],\n          p2=distortion_params['p2'],\n          xnp=xnp,\n      )\n\n    # Convert camera coordinates to world coordinates.\n    pix_world = camtoworlds[Ellipsis, :3, :3] @ pix_cam[Ellipsis, :, None]\n    pix_world = pix_world[Ellipsis, :, 0]\n\n    # Compute ray origins and directions.\n    origins = camtoworlds[Ellipsis, :3, 3]\n    directions = pix_world - origins\n\n    # Compute normalized view directions.\n    viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n    # Compute differential radii.\n    radii = xnp.linalg.norm(directions"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam, pix_y_cam = pixel_coordinates(\n      pix_x_int.shape[1], pix_x_int.shape[0], xnp\n  )\n  pix_x_cam = pix_x_cam.reshape(1, -1)\n  pix_y_cam = pix_y_cam.reshape(1, -1)\n  pix_x_cam = pix_x_cam.repeat(pix_x_int.shape[0], axis=0)\n  pix_y_cam = pix_y_cam.repeat(pix_y_int.shape[0], axis=0)\n  pix_x_cam = pix_x_cam.reshape(-1)\n  pix_y_cam = pix_y_cam.reshape(-1)\n\n  # Apply distortion correction to camera coordinates.\n  if distortion_params is not None:\n    pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n        pix_x_cam,\n        pix_y_cam,\n        k1=distortion_params['k1'],\n        k2=distortion_params['k2'],\n        p1=distortion_params['p1'],\n        p2=distortion_params['p2'],\n        xnp=xnp,\n    )\n\n  # Transform camera coordinates to world coordinates.\n  pix_x_world = pix_x_cam * pixtocams[0, 0] + pix_y_cam * pixtocams[0, 1] + pixtocams[0, 2]\n  pix_y_world = pix_x_cam * pixtocams[1, 0] + pix_y_cam * pixtocams[1, 1] + pixtocams[1, 2]\n  pix_z_world = pix_x_cam * pixtocams[2, 0] + pix_y_cam * pixtocams[2, 1] + pixtocams[2, 2]"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam = (pix_x_int + 0.5) / pixtocams[:, 0, 0]\n  pix_y_cam = (pix_y_int + 0.5) / pixtocams[:, 1, 1]\n\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Compute ray origins and directions in camera space.\n    origins = xnp.stack([pix_x_cam, pix_y_cam, xnp.ones_like(pix_x_cam)], -1)\n    directions = xnp.stack([pix_x_cam, pix_y_cam, -xnp.ones_like(pix_x_cam)], -1)\n    directions = xnp.linalg.inv(pixtocams) @ directions[..., None]\n    directions = directions[..., 0]\n  elif camtype == ProjectionType.FISHEYE:\n    # Compute ray origins and directions in camera space for fisheye camera.\n    origins = xnp.stack([pix_x_cam, pix_y_cam, xnp.zeros_like(pix_x_cam)], -1)\n    directions = xnp.stack([pix_x_cam, pix_y_cam, xnp.ones_like(pix_x_cam)], -1)\n    directions = xnp.linalg.inv(pixtocams) @ directions[..., None]\n    directions = directions[..., 0]\n  elif camtype == ProjectionType.PANORAMIC:\n    # Compute ray origins and directions in camera space for panoramic camera.\n    origins = xnp.stack([pix_x_cam, pix_y_cam, xnp.zeros_like(pix_x_cam)], -1)\n    directions = xnp.stack([pix_x_cam, pix_y_cam, xnp.ones_like(pix_x_cam)], -1)\n    directions = x"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam, pix_y_cam, pix_z_cam = _pixels_to_camera_coordinates(\n      pix_x_int, pix_y_int, pixtocams, camtype, xnp\n  )\n\n  # Compute ray directions in camera coordinates.\n  ray_dirs_cam = xnp.stack([pix_x_cam, pix_y_cam, pix_z_cam], axis=-1)\n\n  # Apply lens distortion correction if distortion_params is provided.\n  if distortion_params is not None:\n    ray_dirs_cam = _correct_lens_distortion(\n        ray_dirs_cam, distortion_params, xnp\n    )\n\n  # Convert ray directions to world coordinates.\n  ray_dirs_world = xnp.einsum('...ij,...j->...i', camtoworlds[Ellipsis, :3, :3],\n                              ray_dirs_cam)\n\n  # Compute ray origins in world coordinates.\n  ray_origins_world = xnp.einsum('...ij,...j->...i', camtoworlds[Ellipsis, :3, :3],\n                                 camtoworlds[Ellipsis, :3, 3])\n\n  # Compute normalized view directions.\n  viewdirs = xnp.einsum('...ij,...j->...i', camtoworlds[Ellipsis, :3, :3],\n                        ray_dirs_cam)\n\n  # Compute differential radii.\n  radii = xnp.linalg.norm(viewdirs, axis=-1, keepdims=True)\n\n  # Compute image plane coordinates.\n  imageplane = xnp.stack([pix_x_int, pix_y_int], axis=-1)\n\n  # Convert ray origins and directions to NDC space if pixtocam_ndc is provided.\n  if pixtocam_ndc is not None:\n    ray_origins_world"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute ray origins.\n  origins = xnp.stack(\n      [\n          (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0],\n          (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1],\n          xnp.ones_like(pix_x_int),\n      ],\n      axis=-1,\n  )\n  origins = origins[Ellipsis, None] * camtoworlds[Ellipsis, :3, 3:]\n  origins = origins[Ellipsis, :3, 3:]\n\n  # Compute ray directions.\n  directions = xnp.stack(\n      [\n          (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0],\n          (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1],\n          xnp.ones_like(pix_x_int),\n      ],\n      axis=-1,\n  )\n  directions = directions[Ellipsis, None] * camtoworlds[Ellipsis, :3, :3]\n  directions = directions[Ellipsis, :3, :]\n\n  # Compute normalized view directions.\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute differential radii.\n  radii = xnp.linalg.norm(origins, axis=-1)\n\n  # Compute image plane coordinates.\n  imageplane = xnp.stack(\n      [\n          (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[E"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x = pix_x_int.astype(xnp.float32)\n  pix_y = pix_y_int.astype(xnp.float32)\n  cam_x = (pix_x - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0]\n  cam_y = (pix_y - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n  if distortion_params is not None:\n    # Apply lens distortion correction.\n    cam_x, cam_y = _radial_and_tangential_undistort(\n        cam_x,\n        cam_y,\n        k1=distortion_params['k1'],\n        k2=distortion_params['k2'],\n        p1=distortion_params['p1'],\n        p2=distortion_params['p2'],\n        xnp=xnp,\n    )\n  # Convert camera coordinates to world coordinates.\n  origins = xnp.stack(\n      [cam_x, cam_y, xnp.ones_like(cam_x)], axis=-1\n  ) @ camtoworlds[Ellipsis, :3, :3].T + camtoworlds[Ellipsis, :3, 3:]\n\n  # Compute ray directions.\n  directions = xnp.stack(\n      [cam_x, cam_y, xnp.ones_like(cam_x)], axis=-1\n  ) @ camtoworlds[Ellipsis, :3, :3].T\n  directions = directions - origins\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute view directions.\n  viewdirs = directions\n\n  # Compute differential radii.\n  if camtype == ProjectionType.FISHEYE:\n    # For fis"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute camera coordinates from pixel coordinates.\n  pix_x_cam, pix_y_cam, pix_z_cam = (\n      pixtocams[Ellipsis, 0, 0] * pix_x_int + pixtocams[Ellipsis, 0, 1] * pix_y_int + pixtocams[Ellipsis, 0, 2],\n      pixtocams[Ellipsis, 1, 1] * pix_y_int + pixtocams[Ellipsis, 1, 1] * pix_y_int + pixtocams[Ellipsis, 1, 2],\n      pixtocams[Ellipsis, 2, 2],\n  )\n\n  # Compute ray origins and directions.\n  origins = camtoworlds[Ellipsis, :3, 3]\n  directions = (\n      camtoworlds[Ellipsis, :3, 0] * pix_x_cam[Ellipsis, None]\n      + camtoworlds[Ellipsis, :3, 1] * pix_y_cam[Ellipsis, None]\n      + camtoworlds[Ellipsis, :3, 2] * pix_z_cam[Ellipsis, None]\n  )\n\n  # Normalize directions.\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute view directions.\n  viewdirs = -directions\n\n  # Compute ray differential radii.\n  radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute image plane coordinates.\n  imageplane = xnp.stack([pix_x_int, pix_y_int], axis=-1)\n\n  # Correct lens distortion if distortion_params are provided.\n  if distortion_params is not None:\n    # Apply lens distortion correction to camera coordinates.\n    pix_"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Perspective projection.\n    pix_x_cam = pix_x_int\n    pix_y_cam = pix_y_int\n  elif camtype == ProjectionType.FISHEYE:\n    # Fisheye projection.\n    pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n        pix_x_int,\n        pix_y_int,\n        k1=distortion_params['k1'],\n        k2=distortion_params['k2'],\n        p1=distortion_params['p1'],\n        p2=distortion_params['p2'],\n        xnp=xnp,\n    )\n  elif camtype == ProjectionType.PANORAMIC:\n    # Panoramic projection.\n    pix_x_cam = xnp.arctan2(pix_x_int, pix_y_int)\n    pix_y_cam = xnp.arctan2(pix_y_int, pix_x_int)\n  else:\n    raise ValueError(f'Unknown camera type: {camtype}')\n\n  # Convert camera coordinates to world coordinates.\n  camtoworld = camtoworlds\n  pixtocam = pixtocams\n  if pixtocam_ndc is not None:\n    # Convert to NDC space.\n    pixtocam = pixtocam_ndc\n    origins, directions = convert_to_ndc(\n        origins=xnp.zeros_like(camtoworld[:, :3, 3]),\n        directions=xnp.zeros_like(camtoworld[:, :3, 2]),\n        pixtocam=pixtocam,\n        xnp=xnp,\n    )\n    # Convert back to camera space.\n    pix_x_cam = origins[Ellipsis, 0] / directions[Ellipsis, 0]\n    pix_y_cam = origins[Ell"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x, pix_y = (\n      xnp.broadcast_to(pix_x_int, pixtocams.shape[:-2]) / pixtocams[Ellipsis, 0, 0],\n      xnp.broadcast_to(pix_y_int, pixtocams.shape[:-2]) / pixtocams[Ellipsis, 1, 1],\n  )\n\n  if camtype == ProjectionType.PERSPECTIVE:\n    # For perspective cameras, the camera coordinates are simply the pixel coordinates.\n    cam_x, cam_y = pix_x, pix_y\n  elif camtype == ProjectionType.FISHEYE:\n    # For fisheye cameras, the camera coordinates are computed using the fisheye model.\n    cam_x, cam_y = _radial_and_tangential_undistort(\n        pix_x,\n        pix_y,\n        k1=distortion_params['k1'],\n        k2=distortion_params['k2'],\n        k3=distortion_params['k3'],\n        k4=distortion_params['k4'],\n        p1=distortion_params['p1'],\n        p2=distortion_params['p2'],\n        xnp=xnp,\n    )\n  elif camtype == ProjectionType.PANORAMIC:\n    # For panoramic cameras, the camera coordinates are computed using the spherical model.\n    cam_x, cam_y = pix_x, pix_y\n  else:\n    raise ValueError(f'Unsupported camera type: {camtype}')\n\n  # Convert camera coordinates to world coordinates.\n  cam_z = xnp.ones_like(cam_x)\n  world_x, world_y, world_z = (\n      cam_x * camtoworlds[Ellipsis, 0, 0]\n      + cam_y * camtoworlds[Ellipsis, 0, 1]\n      + cam_z * camtoworlds["}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if distortion_params is not None:\n    k1, k2, k3, k4, p1, p2 = distortion_params\n  else:\n    k1, k2, k3, k4, p1, p2 = 0, 0, 0, 0, 0, 0\n\n  # Convert pixel coordinates to camera coordinates.\n  pix_x, pix_y = (\n      pix_x_int.astype(xnp.float32),\n      pix_y_int.astype(xnp.float32),\n  )\n  if pixtocam_ndc is not None:\n    # Convert pixel coordinates to NDC space.\n    pix_x, pix_y = convert_to_ndc(\n        pix_x,\n        pix_y,\n        pixtocam_ndc,\n        near = 1.0,\n        xnp = xnp,\n    )\n  else:\n    # Convert pixel coordinates to camera coordinates.\n    pix_x, pix_y = (\n        pix_x / pixtocams[:, 0, 0] - pixtocams[:, 0, 2],\n        pix_y / pixtocams[:, 1, 1] - pixtocams[:, 1, 2],\n    )\n\n  # Apply lens distortion to pixel coordinates.\n  if k1 != 0 or k2 != 0 or k3 != 0 or k4 != 0 or p1 != 0 or p2 != 0:\n    pix_x, pix_y = _radial_and_tangential_undistort(\n        pix_x,\n        pix_y,\n        k1,\n        k2,\n        k3,\n        k4,\n        p1,\n        p2,\n        eps = 1e-9,\n        max_iterations = 10,\n        xnp = xnp,\n    )\n\n  # Compute ray directions in camera coordinates.\n  if camtype == ProjectionType.PERSPECTIVE:\n    directions = xnp.stack([pix_x, pix"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Perspective camera model.\n    # Camera coordinates are in the camera coordinate system.\n    # Image plane is at z=1.\n    cam_x, cam_y = (\n        pix_x_int / pixtocams[:, 0, 0] - pixtocams[:, 0, 2],\n        pix_y_int / pixtocams[:, 1, 1] - pixtocams[:, 1, 2],\n    )\n  elif camtype == ProjectionType.FISHEYE:\n    # Fisheye camera model.\n    # Camera coordinates are in the camera coordinate system.\n    # Image plane is at z=1.\n    cam_x, cam_y = (\n        pix_x_int / pixtocams[:, 0, 0] - pixtocams[:, 0, 2],\n        pix_y_int / pixtocams[:, 1, 1] - pixtocams[:, 1, 2],\n    )\n    r2 = cam_x * cam_x + cam_y * cam_y\n    cam_x *= xnp.arctan(r2) / r2\n    cam_y *= xnp.arctan(r2) / r2\n  elif camtype == ProjectionType.PANORAMIC:\n    # Panoramic camera model.\n    # Camera coordinates are in the camera coordinate system.\n    # Image plane is at z=1.\n    cam_x, cam_y = (\n        pix_x_int / pixtocams[:, 0, 0] - pixtocams[:, 0, 2],\n        pix_y_int / pixtocams[:, 1, 1] - pixtocams[:, 1, 2],\n    )\n    cam_x *= 2 * np.pi\n    cam_y *= 2 * np.pi\n  else:\n    raise ValueError(f'Unknown camera type {camtype}')\n\n "}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute ray origins and directions in camera coordinates.\n  pix_x_cam, pix_y_cam, pix_z_cam = _pixels_to_camera_coordinates(\n      pix_x_int,\n      pix_y_int,\n      pixtocams,\n      camtype,\n      xnp,\n  )\n  origins = _camera_coordinates_to_world_coordinates(\n      pix_x_cam,\n      pix_y_cam,\n      pix_z_cam,\n      camtoworlds,\n      xnp,\n  )\n  directions = _camera_coordinates_to_world_coordinates(\n      pix_x_cam,\n      pix_y_cam,\n      pix_z_cam,\n      camtoworlds,\n      xnp,\n      normalize=False,\n  )\n\n  # Compute ray directions in world coordinates.\n  directions = directions - origins\n\n  # Compute ray view directions.\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute ray differential radii.\n  radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute image plane coordinates.\n  imageplane = xnp.stack([pix_x_int, pix_y_int], axis=-1)\n\n  # Apply distortion correction, if provided.\n  if distortion_params is not None:\n    origins, directions = _correct_distortion(\n        origins,\n        directions,\n        distortion_params,\n        xnp,\n    )\n\n  # Convert ray origins and directions to NDC space, if provided.\n  if pixtocam_ndc is not None:\n    origins, directions = convert_to_ndc(\n        origins,\n        directions,\n        pixtocam_ndc,\n        xnp=xnp,\n    )\n\n  return origins, directions, viewdirs, radii, imageplane"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam, pix_y_cam, pix_z_cam = _pixels_to_camera_coordinates(\n      pix_x_int, pix_y_int, pixtocams, camtype, xnp\n  )\n\n  # Convert camera coordinates to world coordinates.\n  origins = _camera_to_world_coordinates(pix_x_cam, pix_y_cam, pix_z_cam, camtoworlds, xnp)\n  directions = _camera_to_world_coordinates(\n      pix_x_cam, pix_y_cam, pix_z_cam, camtoworlds, xnp, normalize=False\n  )\n\n  # Normalize the directions to unit length.\n  directions /= xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute normalized view directions.\n  viewdirs = -directions\n\n  # Compute differential radii.\n  radii = xnp.linalg.norm(origins, axis=-1, keepdims=True)\n\n  # Compute image plane coordinates.\n  imageplane = xnp.stack([pix_x_int, pix_y_int], axis=-1)\n\n  # Apply optional distortion correction.\n  if distortion_params is not None:\n    x_distorted, y_distorted = _radial_and_tangential_distort(\n        x=pix_x_cam,\n        y=pix_y_cam,\n        k1=distortion_params['k1'],\n        k2=distortion_params['k2'],\n        k3=distortion_params['k3'],\n        k4=distortion_params['k4'],\n        p1=distortion_params['p1'],\n        p2=distortion_params['p2'],\n    )\n    origins, directions, viewdirs, radii, imageplane = _undistort_rays(\n        x_distorted,\n        y_distorted,"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  camtoworlds = xnp.broadcast_to(camtoworlds, pix_x_int.shape + [3, 4])\n  pixtocams = xnp.broadcast_to(pixtocams, pix_x_int.shape + [3, 3])\n  camtoworlds = unpad_poses(camtoworlds)\n  pixtocams = unpad_poses(pixtocams)\n\n  # Compute ray origins and directions\n  pix_x_cam, pix_y_cam = (\n      pix_x_int[..., None] - pixtocams[..., 0, 2],\n      pix_y_int[..., None] - pixtocams[..., 1, 2],\n  )\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Perspective projection\n    directions = xnp.stack(\n        [\n            pix_x_cam / pixtocams[..., 0, 0],\n            pix_y_cam / pixtocams[..., 1, 1],\n            xnp.ones_like(pix_x_cam),\n        ],\n        axis=-1,\n    )\n    origins = xnp.broadcast_to(camtoworlds[..., :3, 3], directions.shape)\n  elif camtype == ProjectionType.FISHEYE:\n    # Fisheye projection\n    directions = xnp.stack(\n        [\n            pix_x_cam / pixtocams[..., 0, 0],\n            pix_y_cam / pixtocams[..., 1, 1],\n            xnp.ones_like(pix_x_cam),\n        ],\n        axis=-1,\n    )\n    origins = xnp.broadcast_to(camtoworlds[..., :3, 3], directions.shape)\n    directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  elif cam"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x, pix_y = (\n      xnp.stack([pix_x_int, pix_y_int], axis=-1) + 0.5\n  )  # Add 0.5 to center pixel coordinates.\n  pix_x = pix_x / pixtocams[Ellipsis, 0, 0]\n  pix_y = pix_y / pixtocams[Ellipsis, 1, 1]\n  pix_x = pix_x * 2 - 1  # Scale to [-1, 1].\n  pix_y = pix_y * 2 - 1  # Scale to [-1, 1].\n  pix_z = xnp.ones_like(pix_x)\n  pix_cam = xnp.stack([pix_x, pix_y, pix_z], axis=-1)\n\n  # Correct lens distortion if distortion_params is provided.\n  if distortion_params is not None:\n    k1 = distortion_params['k1']\n    k2 = distortion_params['k2']\n    k3 = distortion_params['k3']\n    k4 = distortion_params['k4']\n    p1 = distortion_params['p1']\n    p2 = distortion_params['p2']\n    pix_cam = _radial_and_tangential_undistort(\n        pix_cam[Ellipsis, 0],\n        pix_cam[Ellipsis, 1],\n        k1=k1,\n        k2=k2,\n        k3=k3,\n        k4=k4,\n        p1=p1,\n        p2=p2,\n        xnp=xnp,\n    )\n\n  # Convert camera coordinates to world coordinates.\n  origins = pix_cam[Ellipsis, None, :] @ camtoworlds[Ellipsis, :3, :3].T\n  origins = origins[Ellipsis, 0]\n  directions = pix_cam"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam, pix_y_cam = xnp.meshgrid(\n      xnp.arange(pix_x_int.shape[0]), xnp.arange(pix_y_int.shape[1]), indexing='xy'\n  )\n  pix_x_cam = pix_x_cam.astype(xnp.float32)\n  pix_y_cam = pix_y_cam.astype(xnp.float32)\n\n  if camtype == ProjectionType.PERSPECTIVE:\n    pix_x_cam = pix_x_cam + xnp.random.randint(\n        -1, 2, size=pix_x_cam.shape, dtype=xnp.float32\n    )\n    pix_y_cam = pix_y_cam + xnp.random.randint(\n        -1, 2, size=pix_y_cam.shape, dtype=xnp.float32\n    )\n\n  pix_x_cam = pix_x_cam.flatten()\n  pix_y_cam = pix_y_cam.flatten()\n\n  pix_x_cam = pix_x_cam[pix_x_int.flatten()]\n  pix_y_cam = pix_y_cam[pix_y_int.flatten()]\n\n  pix_x_cam = pix_x_cam.reshape(pix_x_int.shape)\n  pix_y_cam = pix_y_cam.reshape(pix_y_int.shape)\n\n  # Apply distortion correction if necessary.\n  if distortion_params is not None:\n    pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n        pix_x_cam,\n        pix_y_cam,\n        k1=distortion_params['k1'],\n        k2=distortion_params['k2'],\n        k3=distortion_params['k3'],\n        k4=distortion"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  camtoworlds = xnp.reshape(camtoworlds, list(camtoworlds.shape[:-2]) + [3, 4])\n  pixtocams = xnp.reshape(pixtocams, list(pixtocams.shape[:-2]) + [3, 3])\n\n  pix_x_cam = pix_x_int * 1.0\n  pix_y_cam = pix_y_int * 1.0\n  if distortion_params is not None:\n    pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n        pix_x_cam,\n        pix_y_cam,\n        k1=distortion_params['k1'],\n        k2=distortion_params['k2'],\n        p1=distortion_params['p1'],\n        p2=distortion_params['p2'],\n        xnp=xnp,\n    )\n  elif camtype == ProjectionType.PANORAMIC:\n    pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n        pix_x_cam,\n        pix_y_cam,\n        k1=0,\n        k2=0,\n        p1=0,\n        p2=0,\n        xnp=xnp,\n    )\n  elif camtype == ProjectionType.FISHEYE:\n    pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n        pix_x_cam,\n        pix_y_cam,\n        k1=0,\n        k2=0,\n        p1=0,\n        p2=0,\n        xnp=xnp,\n    )\n  pix_x_cam = pix_x_cam[Ellipsis, None]\n  pix_y_cam = pix_y_cam[Ellipsis, None]\n  pix_z_cam = xnp.ones_like(pix_x_cam"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = jnp.abs(density[Ellipsis, 1:] - density[Ellipsis, :-1])\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the norm-adjusted distances between points\n  norm_adjusted_dist = jnp.linalg.norm(dirs, axis=-1) * tdist\n\n  # Compute the density delta\n  density_delta = density * norm_adjusted_dist\n\n  # Compute the alpha weights using the helper function\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * jnp.linalg.norm(dirs, axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * jnp.linalg.norm(dirs, axis=-1) * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the norm-adjusted distance between points\n  dir_norms = jnp.linalg.norm(dirs, axis=-1, keepdims=True)\n  norm_adjusted_distances = tdist / dir_norms\n\n  # Compute the adjusted density values\n  density_delta = density * norm_adjusted_distances\n\n  # Compute the alpha compositing weights\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * jnp.linalg.norm(dirs, axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the norm-adjusted distance between points along a path or direction.\n  norm_dirs = jnp.linalg.norm(dirs, axis=-1, keepdims=True)\n  norm_dirs = jnp.maximum(norm_dirs, 1e-10)\n  tdist_norm = tdist / norm_dirs\n\n  # Compute the product of density and the adjusted distance between points.\n  density_delta = density * tdist_norm\n\n  # Compute the alpha compositing weights using the helper function.\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the norm-adjusted distances between points\n  tdist_norm = jnp.sqrt(jnp.sum(dirs**2, axis=-1))\n  tdist_norm = jnp.maximum(1e-10, tdist_norm)\n  density_delta = density[Ellipsis, 1:] * tdist[Ellipsis, 1:] / tdist_norm[Ellipsis, 1:]\n\n  # Compute the alpha weights\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Deterministic sampling\n  if rng is None:\n    u = jnp.linspace(eps, 1 - eps, num_samples)\n    if deterministic_center:\n      u = u + (1 / num_samples) / 2\n    t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n    return t_new\n\n  # Random sampling\n  u = jax.random.uniform(rng, shape=[num_samples], minval=eps, maxval=1 - eps)\n  if single_jitter:\n    u = u + jax.random.uniform(rng, shape=[num_samples], minval=0, maxval=1 / num_samples)\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  return t_new\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Check input shapes and types\n  if not isinstance(rng, jax.random.PRNGKeyArray):\n    raise ValueError(\"rng must be a PRNGKeyArray\")\n  if not isinstance(t, jnp.ndarray):\n    raise ValueError(\"t must be a jnp.ndarray\")\n  if not isinstance(w_logits, jnp.ndarray):\n    raise ValueError(\"w_logits must be a jnp.ndarray\")\n  if not isinstance(num_samples, int):\n    raise ValueError(\"num_samples must be an int\")\n  if not isinstance(single_jitter, bool):\n    raise ValueError(\"single_jitter must be a bool\")\n  if not isinstance(deterministic_center, bool):\n    raise ValueError(\"deterministic_center must be a bool\")\n  if not isinstance(eps, float):\n    raise ValueError(\"eps must be a float\")\n\n  # Check input shapes and types\n  if t.ndim != 1:\n    raise ValueError(\"t must be a 1D array\")\n  if w_logits.ndim != 1:\n    raise ValueError(\"w_logits must be a 1D array\")\n  if t.shape != w_logits.shape:\n    raise ValueError(\"t and w_logits must have the same shape\")\n\n  # Check that t is sorted\n  if not jnp.all(jnp.diff(t) >= 0):\n    raise ValueError(\"t must be sorted\")\n\n  # Check that w_logits are valid logits\n  if not jnp.all(jnp.isfinite(w_logits)):\n    raise ValueError(\"w_logits must be finite\")\n\n  # Check that eps is positive\n  if eps <= 0:\n    raise ValueError(\"eps must be positive\")\n\n  # Compute the PDF and CDF from the logits\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Compute the bin"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Sample from the CDF.\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Sample from the center of each interval.\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n      u = jnp.tile(u, (t.shape[0], 1))\n    else:\n      # Sample uniformly across each interval.\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n      u = jnp.tile(u, (t.shape[0], 1))\n      u = u + jnp.random.uniform(u.shape, rng, dtype=u.dtype)\n    if single_jitter:\n      u = u + jnp.random.uniform(u.shape, rng, dtype=u.dtype)\n  else:\n    # Random sampling.\n    u = jnp.random.uniform(\n        (t.shape[0], num_samples), rng, dtype=jnp.float32)\n    if single_jitter:\n      u = u + jnp.random.uniform(u.shape, rng, dtype=u.dtype)\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  return t_new\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  if deterministic_center:\n    # Center the distribution around the bin centers.\n    t_new = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n    cw_new = (cw[Ellipsis, 1:] + cw[Ellipsis, :-1]) / 2\n    t_new = jnp.concatenate([t_new[Ellipsis, :1], t_new, t_new[Ellipsis, -1:]],\n                            axis=-1)\n    cw_new = jnp.concatenate(\n        [cw_new[Ellipsis, :1], cw_new, cw_new[Ellipsis, -1:]], axis=-1)\n    t = t_new\n    cw = cw_new\n  if rng is None:\n    # Deterministic sampling.\n    u = jnp.linspace(eps, 1 - eps, num_samples)\n    if single_jitter:\n      u = u + eps\n    u = jnp.broadcast_to(u, (t.shape[0], num_samples))\n    return math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  else:\n    # Random sampling.\n    u = jax.random.uniform(rng, (t.shape[0], num_samples))\n    if single_jitter:\n      u = u + eps\n    return math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Determine the range of uniform samples.\n  if deterministic_center:\n    u_min = jnp.maximum(0, cw[Ellipsis, :-1] - eps)\n    u_max = jnp.minimum(1, cw[Ellipsis, 1:] + eps)\n  else:\n    u_min = jnp.maximum(0, cw[Ellipsis, :-1] - eps)\n    u_max = jnp.minimum(1, cw[Ellipsis, 1:] + eps)\n\n  # Sample from the uniform distribution.\n  if rng is None:\n    u = jnp.linspace(0, 1, num_samples, endpoint=False)\n  else:\n    if single_jitter:\n      u = jax.random.uniform(rng, (num_samples,))\n    else:\n      u = jax.random.uniform(rng, (num_samples,))\n\n  # Interpolate into the CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  # Add jitter to the samples.\n  if single_jitter:\n    u_jitter = jax.random.uniform(rng, (num_samples,))\n    t_new += u_jitter * (u_max - u_min)\n  else:\n    u_jitter = jax.random.uniform(rng, (num_samples,))\n    t_new += u_jitter * (u_max - u_min)\n\n  return t_new\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Compute the PDF and CDF for each weight vector.\n  p = jnp.diff(cw, axis=-1)\n  # Sample from the PDF.\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Sample from the center of each interval.\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n    else:\n      # Sample from the entire PDF.\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n    u = jnp.broadcast_to(u, (w.shape[0], num_samples))\n  else:\n    # Random sampling.\n    if single_jitter:\n      # Jitter each sample.\n      u = jax.random.uniform(rng, (w.shape[0], num_samples))\n    else:\n      # Jitter each sample independently.\n      u = jax.random.uniform(rng, (w.shape[0], num_samples), minval=eps, maxval=1 - eps)\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  return t_new\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Check input arguments\n  assert rng is None or isinstance(rng, jax.random.PRNGKey), \"rng must be a PRNGKey or None\"\n  assert isinstance(t, jnp.ndarray) and t.ndim == 1, \"t must be a 1D tensor\"\n  assert isinstance(w_logits, jnp.ndarray) and w_logits.ndim == 1, \"w_logits must be a 1D tensor\"\n  assert isinstance(num_samples, int) and num_samples > 0, \"num_samples must be a positive integer\"\n  assert isinstance(single_jitter, bool), \"single_jitter must be a boolean\"\n  assert isinstance(deterministic_center, bool), \"deterministic_center must be a boolean\"\n  assert isinstance(eps, float) and eps > 0, \"eps must be a positive float\"\n\n  # Compute the PDF and CDF\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Deterministic sampling\n  if rng is None:\n    # Compute the bin indices based on the CDF\n    bin_indices = jnp.searchsorted(cw, jnp.linspace(eps, 1 - eps, num_samples), side=\"right\") - 1\n    bin_indices = jnp.clip(bin_indices, 0, cw.shape[-1] - 1)\n\n    # Compute the sample values based on the bin indices\n    if deterministic_center:\n      sample_values = t[bin_indices]\n    else:\n      sample_values = jnp.where(\n          bin_indices == 0,\n          t[0],\n          jnp.where(\n              bin_indices == cw.shape[-1] - 1,\n              t[-1],\n              t[bin_indices] + (jnp.random.uniform(bin_indices.shape) if single_jitter else 0.5),"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Deterministic sampling\n  if rng is None:\n    # Deterministically sample from the PDF.\n    u = jnp.linspace(eps, 1 - eps, num_samples + 1)\n    # Add jitter if specified.\n    if single_jitter:\n      u = u + jax.random.uniform(rng, u.shape, minval=-eps, maxval=eps)\n    else:\n      u = u + jax.random.uniform(rng, u.shape, minval=-eps, maxval=eps)\n    # Interpolate into the inverse CDF.\n    t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n    # Optionally center the samples in each interval.\n    if deterministic_center:\n      t_new = t_new + (jnp.diff(t) / 2) * jnp.sign(jnp.diff(cw))\n    return t_new\n\n  # Random sampling\n  else:\n    # Randomly sample from the PDF.\n    u = jax.random.uniform(rng, (num_samples,), minval=eps, maxval=1 - eps)\n    # Add jitter if specified.\n    if single_jitter:\n      u = u + jax.random.uniform(rng, u.shape, minval=-eps, maxval=eps)\n    else:\n      u = u + jax.random.uniform(rng, u.shape, minval=-eps, maxval=eps)\n    # Inter"}
{"namespace": "stepfun.sample", "completion": "  # Validate inputs\n  utils.assert_valid_stepfun(t, w_logits)\n  assert num_samples >= 1, \"num_samples must be >= 1\"\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Determine the number of intervals in the PDF.\n  num_intervals = cw.shape[-1] - 1\n\n  # Determine the sampling method based on rng.\n  if rng is None:\n    # Deterministic sampling\n    if deterministic_center:\n      # Center the sampling in each interval.\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n    else:\n      # Span the entire PDF.\n      u = jnp.linspace(eps, 1 - eps, num_samples * num_intervals)\n      u = u.reshape([num_samples, num_intervals])\n      u = u.mean(axis=-1)\n    if single_jitter:\n      u = u + jax.random.uniform(rng, u.shape) * eps\n    else:\n      u = u + jax.random.uniform(rng, u.shape) * eps\n  else:\n    # Random sampling\n    if deterministic_center:\n      # Center the sampling in each interval.\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n    else:\n      # Span the entire PDF.\n      u = jax.random.uniform(rng, [num_samples])\n    if single_jitter:\n      u = u + jax.random.uniform(rng, u.shape) * eps\n    else:\n      u = u + jax.random.uniform(rng, u.shape) * eps\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Deterministic sampling\n  if rng is None:\n    # If deterministic_center is True, sample uniformly from the center of each interval.\n    if deterministic_center:\n      u = (cw[Ellipsis, 1:] + cw[Ellipsis, :-1]) / 2\n    else:\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n\n    # If single_jitter is True, jitter each sample by the same amount.\n    if single_jitter:\n      u = u + jax.random.uniform(rng, u.shape, dtype=jnp.float32) * (1 / num_samples)\n\n    # Interpolate into the inverse CDF.\n    t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n    return t_new\n\n  # Random sampling\n  else:\n    # Sample uniformly from the PDF.\n    u = jax.random.uniform(rng, [num_samples], dtype=jnp.float32)\n\n    # If single_jitter is True, jitter each sample by the same amount.\n    if single_jitter:\n      u = u + jax.random.uniform(rng, u.shape, dtype=jnp.float32) * (1 / num_samples)\n\n    # Interpolate into the inverse CDF.\n    t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n    return t_new\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Check input shapes and values\n  utils.assert_valid_stepfun(t, w_logits)\n  batch_size = w_logits.shape[0]\n\n  # Compute the PDF and CDF for each weight vector\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Compute the bin centers from the endpoints\n  t_centers = (t[..., 1:] + t[..., :-1]) / 2\n\n  # Determine the range of uniform sampling\n  if deterministic_center:\n    u_min = jnp.zeros((batch_size,))\n    u_max = jnp.ones((batch_size,))\n  else:\n    u_min = jnp.zeros((batch_size,)) - eps\n    u_max = jnp.ones((batch_size,)) + eps\n\n  # Generate random samples or linspace samples\n  if rng is None:\n    u = jnp.linspace(u_min, u_max, num_samples)\n  else:\n    if single_jitter:\n      u = jax.random.uniform(rng, (batch_size, num_samples))\n      u = u * (u_max - u_min) + u_min\n    else:\n      u = jax.random.uniform(rng, (batch_size, num_samples))\n      u = jnp.broadcast_to(u, (batch_size, num_samples))\n      u = u * (u_max - u_min) + u_min\n\n  # Invert the CDF to get the corresponding bin centers\n  t_new = math.sorted_interp(u, cw, t_centers, utils.device_is_tpu())\n\n  return t_new\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Deterministic sampling\n  if rng is None:\n    # Compute the bin widths.\n    bin_widths = jnp.diff(t)\n\n    # Compute the bin centers.\n    bin_centers = t[..., :-1] + bin_widths / 2\n\n    # If deterministic_center is True, center the samples in each bin.\n    if deterministic_center:\n      u = jnp.linspace(0, 1, num_samples + 2)[1:-1]\n      u = u.reshape([1] * (u.ndim - 1) + [-1, 1])\n      u = u.repeat(num_samples, axis=-1)\n    else:\n      u = jnp.linspace(0, 1, num_samples + 2)[1:-1]\n      u = u.reshape([1] * (u.ndim - 1) + [-1, 1])\n      u = u.repeat(num_samples, axis=-1)\n\n    # Compute the bin indices for each sample.\n    bin_indices = jnp.sum(u > cw, axis=-1) - 1\n\n    # Compute the jitter values for each sample.\n    jitter = u - cw[Ellipsis, bin_indices]\n\n    # Compute the sample values.\n    samples = bin_centers[Ellipsis, bin_indices] + jitter * bin_widths[\n        Ellipsis, bin_indices\n    ]\n\n    # If single_jitter is True, apply the same jitter to all samples.\n    if single_jitter:\n      samples = samples + jitter * bin_widths[Ellipsis, bin_indices]\n\n    # Add eps to avoid numerical issues.\n    samples = samples + eps\n\n    return samples"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Deterministic center sampling.\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n    else:\n      # Deterministic uniform sampling.\n      u = jnp.linspace(eps, 1 - eps, num_samples + 1)[:-1]\n    if single_jitter:\n      u = u + eps\n    else:\n      u = u + jax.random.uniform(rng, shape=(num_samples,), minval=eps, maxval=eps)\n    t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  else:\n    # Random sampling.\n    u = jax.random.uniform(rng, shape=(num_samples,), minval=eps, maxval=1 - eps)\n    if single_jitter:\n      u = u + eps\n    else:\n      u = u + jax.random.uniform(rng, shape=(num_samples,), minval=eps, maxval=eps)\n    t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  return t_new\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Check input shapes and types\n  assert t.ndim == 1, 't must be a 1D array'\n  assert w_logits.ndim == 1, 'w_logits must be a 1D array'\n  assert t.shape == w_logits.shape, 't and w_logits must have the same shape'\n  assert num_samples > 0, 'num_samples must be positive'\n\n  # Compute the PDF and CDF from the weights\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Determine the number of intervals and the number of samples per interval\n  num_intervals = t.shape[0] - 1\n  samples_per_interval = jnp.ceil(num_samples / num_intervals).astype(jnp.int32)\n\n  # Generate uniform samples within each interval\n  if rng is None:\n    # Deterministic sampling based on linspace\n    u = jnp.linspace(eps, 1 - eps, samples_per_interval)\n    u = jnp.tile(u, num_intervals)\n  else:\n    # Random sampling\n    if single_jitter:\n      u = jax.random.uniform(rng, (num_samples,))\n    else:\n      u = jax.random.uniform(rng, (num_intervals, samples_per_interval))\n      u = jnp.reshape(u, (num_samples,))\n\n  # Invert the CDF to get the corresponding samples\n  t_new = invert_cdf(u, t, w_logits)\n\n  # If deterministic_center is True, shift the samples to the center of each interval\n  if deterministic_center:\n    t_new = t_new + (t[1:] - t[:-1]) / 2\n\n  return t_new\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Compute the inverse CDF of the PDF.\n  t_inv = invert_cdf(cw, t, w_logits)\n\n  # Compute the interval indices for each sample.\n  u = jnp.linspace(eps, 1 - eps, num_samples)\n  if rng is not None:\n    u = jax.random.uniform(rng, [num_samples])\n\n  # Determine the jitter amount for each sample.\n  if single_jitter:\n    jitter = jnp.ones_like(u) * eps\n  else:\n    jitter = jax.random.uniform(rng, [num_samples]) * eps\n\n  # Compute the sampled values using the inverse CDF.\n  t_new = t_inv + jitter\n\n  # Adjust the sampled values based on the center option.\n  if deterministic_center:\n    t_new = t_new - jnp.mean(jnp.diff(t_new), axis=-1, keepdims=True)\n\n  # Return the sampled values.\n  return t_new\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Compute the bin widths and the total mass of the PDF.\n  bin_widths = jnp.diff(t)\n  total_mass = jnp.sum(w * bin_widths)\n  # Compute the bin indices for each sample.\n  if deterministic_center:\n    # If deterministic_center is True, sample from the center of each bin.\n    bin_indices = jnp.sum(cw < jnp.expand_dims(jnp.linspace(eps, 1.0 - eps, num_samples), -1), axis=-1)\n  else:\n    # If deterministic_center is False, sample uniformly from the entire PDF.\n    bin_indices = jnp.sum(cw < jnp.expand_dims(jnp.linspace(eps, 1.0 - eps, num_samples), -1), axis=-1)\n    # If bin_indices is 0, set it to the last bin index.\n    bin_indices = jnp.where(bin_indices == 0, jnp.shape(cw)[-1] - 1, bin_indices)\n  # Compute the offsets for each sample.\n  if single_jitter:\n    # If single_jitter is True, jitter each sample by the same amount.\n    offsets = jnp.linspace(eps, 1.0 - eps, num_samples)\n  else:\n    # If single_jitter is False, jitter each sample independently.\n    offsets = jax.random.uniform(rng, [num_samples])\n  # Compute the final sample values.\n  if rng is None:\n    # If rng is None, use linspace to generate the samples.\n    samples = jnp.linspace(eps, 1.0"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Deterministic sampling:\n  if rng is None:\n    # Generate samples from the PDF using linspace.\n    u = jnp.linspace(eps, 1 - eps, num_samples, endpoint=False)\n    # Determine the interval for each sample.\n    i = jnp.searchsorted(cw, u, side=\"right\") - 1\n    # Compute the jitter for each sample.\n    if single_jitter:\n      j = jnp.full_like(u, eps)\n    else:\n      j = jnp.linspace(eps, eps, num_samples, endpoint=False)\n    # Compute the sampled values.\n    t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n    t_new = t_new + j * (t[Ellipsis, 1:] - t[Ellipsis, :-1])\n    # Adjust the center if needed.\n    if deterministic_center:\n      t_new = t_new - t[Ellipsis, i]\n    return t_new\n\n  # Random sampling:\n  else:\n    # Generate random samples.\n    u = jax.random.uniform(rng, (num_samples,), minval=eps, maxval=1 - eps)\n    # Determine the interval for each sample.\n    i = jnp.searchsorted(cw, u, side=\"right\") - 1\n    # Compute the jitter for each sample.\n    if single_jitter:\n      j = jnp.full_like(u, eps)\n    else:\n      j = jax.random.uniform(rng, (num_samples,), minval=eps, maxval=eps)\n    # Compute the sampled values.\n    t_"}
{"namespace": "stepfun.sample", "completion": "  # Compute the PDF and CDF from the weights.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Determine the number of bins.\n  num_bins = t.shape[-1] - 1\n\n  # Handle deterministic sampling.\n  if rng is None:\n    # Compute the bin indices for deterministic sampling.\n    if deterministic_center:\n      bin_indices = jnp.clip(jnp.round((cw[Ellipsis, 1:] + cw[Ellipsis, :-1]) / 2 * num_bins), 0, num_bins - 1)\n    else:\n      bin_indices = jnp.clip(jnp.round((cw[Ellipsis, 1:] - cw[Ellipsis, :-1]) * num_bins), 0, num_bins - 1)\n\n    # Compute the sample values for deterministic sampling.\n    bin_widths = t[Ellipsis, 1:] - t[Ellipsis, :-1]\n    bin_widths = jnp.where(bin_widths < eps, eps, bin_widths)\n    bin_widths = bin_widths[Ellipsis, bin_indices]\n    bin_centers = t[Ellipsis, 1:] - bin_widths / 2\n    bin_centers = bin_centers[Ellipsis, bin_indices]\n    sample_values = bin_centers + jnp.linspace(0, 1, num_samples, endpoint=False) * bin_widths\n\n    # Reshape the sample values to match the desired output shape.\n    sample_values = sample_values[Ellipsis, None]\n\n  # Handle random sampling.\n  else:\n    # Generate random numbers for random sampling.\n    if single_jitter:\n      rng, key = jax.random.split(rng)\n      u = j"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Determine the number of intervals in the step function.\n  num_intervals = t.shape[-1] - 1\n\n  # Determine the range of the PDF.\n  min_t = t[..., 0]\n  max_t = t[..., -1]\n\n  # If deterministic_center is True, center the sampling around the interval centers.\n  if deterministic_center:\n    t_center = (t[..., 1:] + t[..., :-1]) / 2.0\n    min_t = t_center[..., 0]\n    max_t = t_center[..., -1]\n\n  # Compute the range of the PDF.\n  pdf_range = max_t - min_t\n\n  # If deterministic_center is True, adjust the range of the PDF to ensure that the samples are centered in each interval.\n  if deterministic_center:\n    pdf_range = pdf_range / num_intervals\n\n  # Compute the step size for sampling.\n  step_size = pdf_range / num_samples\n\n  # If deterministic_center is True, adjust the step size to ensure that the samples are centered in each interval.\n  if deterministic_center:\n    step_size = step_size * num_intervals\n\n  # Compute the starting point of the sampling.\n  start_point = min_t + step_size / 2.0\n\n  # If deterministic_center is True, adjust the starting point to ensure that the samples are centered in each interval.\n  if deterministic_center:\n    start_point = start_point + step_size / 2.0\n\n  # Initialize an empty array to store the sampled values.\n  sampled_values = jnp.zeros((num_samples,), dtype=jnp.float32"}
{"namespace": "stepfun.sample", "completion": "  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Compute the bin widths and the corresponding bin centers.\n  td = jnp.diff(t)\n  t_centers = t[Ellipsis, :-1] + td / 2\n\n  # Determine the number of bins and the shape of the output.\n  num_bins = t.shape[-1] - 1\n  batch_size = w.shape[0]\n  shape = (batch_size, num_samples)\n\n  # Generate uniform samples if rng is None.\n  if rng is None:\n    if deterministic_center:\n      # Deterministic sampling with center.\n      # Generate samples from the bin centers.\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n    else:\n      # Deterministic sampling without center.\n      # Generate samples from the bin endpoints.\n      u = jnp.linspace(eps, 1 - eps, num_samples + 1)[:-1]\n  else:\n    # Random sampling.\n    # Generate samples from the uniform distribution.\n    u = jax.random.uniform(rng, shape)\n\n    if single_jitter:\n      # Jitter each sample with the same amount.\n      u = u + eps\n    else:\n      # Jitter each sample independently.\n      u = u + jax.random.uniform(rng, shape) * eps\n\n  # Invert the CDF to obtain the corresponding sample values.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  # If deterministic_center is True, add the bin widths to the sample values.\n  if deterministic_center:\n    t_new = t_new + t_centers\n\n  return t_new\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_midpoints = jnp.concatenate(\n      [\n          jnp.broadcast_to(domain[0], t_midpoints.shape[:-1] + (1,)),\n          t_midpoints,\n          jnp.broadcast_to(domain[1], t_midpoints.shape[:-1] + (1,)),\n      ],\n      axis=-1,\n  )\n\n  return t_midpoints\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate([t[Ellipsis, :1], t_midpoints, t[Ellipsis, -1:]], axis=-1)\n\n  # Ensure that the sampled intervals are within the specified domain.\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  minval, maxval = domain\n  t_samples = jnp.concatenate([jnp.expand_dims(minval, -1), midpoints, jnp.expand_dims(maxval, -1)], axis=-1)\n\n  return t_samples\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  t_midpoints = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  t_midpoints = jnp.concatenate([t_midpoints[Ellipsis, :1], t_midpoints], axis=-1)\n  t_midpoints = jnp.concatenate([t_midpoints, t_midpoints[Ellipsis, -1:]], axis=-1)\n\n  # Ensure that the first and last intervals are within the specified domain\n  t_midpoints = jnp.clip(t_midpoints, domain[0], domain[1])\n\n  return t_midpoints\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the domain.\n  t_samples = jnp.concatenate([t_samples[..., :1], t_midpoints, t_samples[..., -1:]], axis=-1)\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter=single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate([t_samples[Ellipsis, :1], t_midpoints, t_samples[Ellipsis, -1:]], axis=-1)\n\n  # Ensure the sampled intervals are within the specified domain.\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_midpoints = jnp.concatenate(\n      [\n          jnp.broadcast_to(domain[0], t_midpoints.shape[:-1] + (1,)),\n          t_midpoints,\n          jnp.broadcast_to(domain[1], t_midpoints.shape[:-1] + (1,)),\n      ],\n      axis=-1,\n  )\n\n  return t_midpoints\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  t_samples = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  t_samples = jnp.concatenate([jnp.array([domain[0]]), t_samples, jnp.array([domain[1]])], axis=-1)\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_samples = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [t[..., :1], t_samples, t[..., -1:]], axis=-1\n  )\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1.0 - jnp.finfo(jnp.float32).eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = jnp.finfo(jnp.float32).eps + (1 - jnp.finfo(jnp.float32).eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - jnp.finfo(jnp.float32).eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Invert the CDF to get samples.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Calculate midpoints between adjacent samples.\n  midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the domain.\n  t_samples = jnp.concatenate([t[Ellipsis, :1], midpoints, t[Ellipsis, -1:]], axis=-1)\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(\n      rng,\n      t,\n      w_logits,\n      num_samples,\n      single_jitter=single_jitter,\n      deterministic_center=True,\n  )\n\n  # Calculate midpoints between adjacent samples.\n  t_samples = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [t[Ellipsis, :1], t_samples, t[Ellipsis, -1:]], axis=-1\n  )\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  t_midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  t_midpoints = jnp.concatenate([t_samples[..., :1], t_midpoints, t_samples[..., -1:]], axis=-1)\n  t_midpoints = jnp.clip(t_midpoints, domain[0], domain[1])\n\n  return t_midpoints\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter, False)\n\n  # Calculate midpoints between adjacent samples.\n  t_samples_mid = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples_mid = jnp.concatenate(\n      [\n          jnp.broadcast_to(domain[0], t_samples_mid.shape[:-1] + (1,)),\n          t_samples_mid,\n          jnp.broadcast_to(domain[1], t_samples_mid.shape[:-1] + (1,)),\n      ],\n      axis=-1,\n  )\n\n  return t_samples_mid\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate([t_samples[Ellipsis, :1], t_midpoints, t_samples[Ellipsis, -1:]], axis=-1)\n\n  # Ensure that the first and last intervals are within the specified domain.\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the domain.\n  t_samples = jnp.concatenate(\n      [t[Ellipsis, :1], t_midpoints, t[Ellipsis, -1:]], axis=-1\n  )\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter, True)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate([t_samples[Ellipsis, :1], t_midpoints, t_samples[Ellipsis, -1:]], axis=-1)\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_midpoints = jnp.concatenate([t_midpoints[..., :1], t_midpoints, t_midpoints[..., -1:]], axis=-1)\n\n  # Return the adjusted midpoints as the output.\n  return t_midpoints\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate([t_samples[..., :1], t_midpoints, t_samples[..., -1:]], axis=-1)\n\n  # Ensure that the first interval starts at the beginning of the domain.\n  t_samples = jnp.where(t_samples[..., 0] < domain[0], domain[0] + jnp.zeros_like(t_samples[..., 0]), t_samples)\n\n  # Ensure that the last interval ends at the end of the domain.\n  t_samples = jnp.where(t_samples[..., -1] > domain[1], domain[1] + jnp.zeros_like(t_samples[..., -1]), t_samples)\n\n  return t_samples\n\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1.0 - jnp.finfo(jnp.float32).eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = jnp.finfo(jnp.float32).eps + (1 - jnp.finfo(jnp.float32).eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - jnp.finfo(jnp.float32).eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n  # Invert the CDF to get the bin edges.\n  t_samples = invert_cdf(u, t, w_logits)\n  # Calculate midpoints between adjacent samples.\n  t_samples = (t_samples[Ellipsis, :-1] + t_samples[Ellipsis, 1:]) / 2\n  # Adjust the first and last intervals to ensure they are within the domain.\n  t_samples = jnp.concatenate(\n      [jnp.full(t_samples.shape[:-1] + (1,), domain[0]), t_samples, jnp.full(t_samples.shape[:-1] + (1,), domain[1])],\n      axis=-1)\n "}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample from the step function.\n  t_samples = sample(\n      rng,\n      t,\n      w_logits,\n      num_samples,\n      single_jitter=single_jitter,\n      deterministic_center=False,\n  )\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, :-1] + t_samples[Ellipsis, 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.broadcast_to(domain[0], t_samples.shape[:-1] + (1,)),\n          t_midpoints,\n          jnp.broadcast_to(domain[1], t_samples.shape[:-1] + (1,)),\n      ],\n      axis=-1,\n  )\n\n  return t_samples\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(jnp.isinf(w), 0, w)\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(jnp.isinf(w), 0, w)\n  w = jnp.where(w > 1, 1, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(jnp.isinf(w), 0, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(w > 1, 1, w)\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(jnp.isinf(w), 0, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(w > 1, 1, w)\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(jnp.isinf(w), 0, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(w > 1, 1, w)\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(jnp.isinf(w), 0, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(w > 1, 1, w)\n  w = jnp.where(jnp.isnan(w), 0, w"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.minimum(1, jnp.cumsum(w[Ellipsis, :-1], axis=-1))\n  shape = w.shape[:-1] + (1,)\n  w = jnp.concatenate([jnp.zeros(shape), w, jnp.ones(shape)], axis=-1)\n  return math.sorted_interp(ps / 100, w, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.where(w < 0, 0, w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n  cw = integrate_weights(w)\n  return math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(jnp.isinf(w), 0, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(w > 1, 1, w)\n  w = jnp.cumsum(w, axis=-1)\n  w = jnp.concatenate([jnp.zeros_like(w[..., :1]), w], axis=-1)\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(jnp.isinf(w), 0, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(w > 1, 1, w)\n  w = jnp.where(jnp.diff(w, axis=-1) < 0, w, jnp.zeros_like(w))\n  w = jnp.where(jnp.diff(w, axis=-1) > 1, w, jnp.ones_like(w))\n  w = jnp.cumsum(w, axis=-1)\n  w = jnp.concatenate([jnp.zeros_like(w[..., :1]), w], axis=-1)\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(jnp.isinf(w), 0, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(w > 1, 1, w)\n  w = jnp.where(jnp.diff(w, axis=-1) < 0, w, jnp.zeros_like(w))\n  w = jnp.where(jnp.diff(w, axis=-1) > 1, w, jnp.ones_like"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Integrate the weights.\n  cw = integrate_weights(w)\n\n  # Interpolate into the integrated weights.\n  return math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Integrate the weights to get the CDF.\n  cw = integrate_weights(w)\n\n  # Interpolate into the CDF to get the percentiles.\n  return math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the integrated weights.\n  cw = integrate_weights(w)\n\n  # Interpolate into the integrated weights to find the corresponding percentiles.\n  return math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Normalize the weights and integrate them to obtain the CDF.\n  w = jnp.minimum(1, jnp.cumsum(w[Ellipsis, :-1], axis=-1))\n  shape = w.shape[:-1] + (1,)\n  w = jnp.concatenate([jnp.zeros(shape), w, jnp.ones(shape)], axis=-1)\n\n  # Interpolate the CDF to obtain the percentiles.\n  return math.sorted_interp(ps / 100, w, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = jnp.asarray(w)\n  w = w / jnp.sum(w)\n\n  # Integrate the weights to obtain the CDF.\n  cw = integrate_weights(w)\n\n  # Interpolate into the CDF to obtain the corresponding percentiles.\n  return math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.minimum(1, jnp.cumsum(w[Ellipsis, :-1], axis=-1))\n  shape = w.shape[:-1] + (1,)\n  w = jnp.concatenate([jnp.zeros(shape), w, jnp.ones(shape)], axis=-1)\n  ps = jnp.sort(ps)\n  t_ps = math.sorted_interp(ps / 100, w, t, utils.device_is_tpu())\n  return t_ps\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = jnp.asarray(w)\n  w = w / jnp.sum(w)\n\n  # Integrate the weights\n  cw = integrate_weights(w)\n\n  # Interpolate the percentiles\n  return math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Normalize the weights.\n  w = jnp.where(jnp.isclose(jnp.sum(w, axis=-1, keepdims=True), 0), 1 / w.shape[-1], w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Integrate the weights to obtain the CDF.\n  cdf = integrate_weights(w)\n\n  # Interpolate into the CDF to obtain the corresponding percentiles.\n  return math.sorted_interp(ps, cdf, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = jnp.asarray(w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Integrate the weights to obtain the CDF\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Normalize the CDF to ensure that it spans the range [0, 1]\n  cdf = cdf / cdf[Ellipsis, -1:]\n\n  # Interpolate the CDF at the given percentiles to obtain the corresponding x-values\n  x_values = jnp.interp(ps / 100.0, cdf, t)\n\n  return x_values\n\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the integrated weights.\n  cw = integrate_weights(w)\n  # Interpolate into the integrated weights.\n  return math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = jnp.asarray(w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Integrate the weights to obtain the cumulative distribution function (CDF).\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Interpolate the CDF to obtain the weighted percentiles.\n  return linspline.interpolate_lin(t, cdf, ps)\n\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Integrate the weights and normalize them to ensure they sum to 1.\n  w_int = integrate_weights(w)\n  w_int = w_int / jnp.sum(w_int, axis=-1, keepdims=True)\n\n  # Interpolate the percentiles using the integrated weights.\n  return linspline.interpolate(t, w_int, ps)\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = jnp.asarray(w)\n  w = w / jnp.sum(w)\n\n  # Integrate the weights to obtain the cumulative distribution function (CDF).\n  cdf = jnp.cumsum(w)\n\n  # Interpolate into the CDF to obtain the corresponding x-values for the given percentiles.\n  return linspline.linspline(cdf, t, ps)\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Integrate the weights to get the cumulative distribution function (CDF)\n  cdf = integrate_weights(w)\n\n  # Interpolate the CDF to find the corresponding x-values for the input percentiles\n  x_values = linspline.interpolate(cdf, t, ps)\n\n  # Return the interpolated x-values as the weighted percentiles\n  return x_values\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Integrate the weights and normalize to ensure they sum to 1.\n  cw = integrate_weights(w)\n  cw = cw / jnp.sum(cw, axis=-1, keepdims=True)\n\n  # Interpolate into the integrated weights to obtain the corresponding percentiles.\n  return math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Normalize the weights to sum to 1\n  w = jnp.asarray(w)\n  w_norm = jnp.cumsum(w, axis=-1)\n  w_norm = w_norm / jnp.max(w_norm, axis=-1, keepdims=True)\n\n  # Interpolate into the normalized weights to find the percentiles\n  ps_norm = jnp.asarray(ps) / 100\n  t_interp = jnp.interp(ps_norm, w_norm, t)\n\n  return t_interp\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  blurred_pdf = linspline.gaussian_kernel(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  resampled_pdf = resample(tq, t, blurred_pdf)\n\n  # Convert the blurred and resampled PDF back to weights\n  resampled_weights = pdf_to_weight(tq, resampled_pdf)\n\n  return resampled_weights\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel with the specified half-width\n  blurred_pdf = linspline.gaussian_kernel_1d(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  resampled_pdf = linspline.interpolate_1d(tq, t, blurred_pdf)\n\n  # Convert the blurred and resampled PDF back to weights\n  resampled_weights = pdf_to_weight(tq, resampled_pdf)\n\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  blurred_p = linspline.blur_pdf(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  blurred_w = resample(tq, t, blurred_p)\n\n  return blurred_w"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  blurred_pdf = linspline.gaussian_kernel_pdf(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  resampled_pdf = linspline.resample_pdf(tq, t, blurred_pdf)\n\n  # Convert the blurred and resampled PDF back to weights\n  w_blurred = pdf_to_weight(tq, resampled_pdf)\n\n  return w_blurred\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  blurred_pdf = linspline.blur_pdf(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  resampled_pdf = linspline.resample_pdf(tq, t, blurred_pdf)\n\n  # Convert the blurred and resampled PDF back to weights\n  resampled_weights = pdf_to_weight(tq, resampled_pdf)\n\n  return resampled_weights\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = jax.nn.softmax(w, axis=-1)\n\n  # Blur the PDF using a Gaussian kernel\n  p_blurred = linspline.gaussian_kernel_pdf(t, p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  p_resampled = resample(tq, t, p_blurred)\n\n  # Convert the resampled PDF back to weights\n  w_resampled = jax.nn.softmax(p_resampled, axis=-1)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel with the specified half-width\n  blurred_pdf = linspline.gaussian_kernel_pdf(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  resampled_pdf = linspline.pdf_to_stepfun(tq, blurred_pdf)\n\n  # Convert the blurred and resampled PDF back to weights\n  resampled_weights = pdf_to_weight(tq, resampled_pdf)\n\n  return resampled_weights\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian filter with the specified half-width\n  p_blurred = linspline.gaussian_filter1d(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  w_resampled = resample(tq, t, p_blurred)\n\n  return w_resampled\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel with the specified half-width\n  p_blurred = linspline.gaussian_kernel_convolve(t, p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  w_blurred = pdf_to_weight(t, p_blurred)\n  w_resampled = resample(tq, t, w_blurred)\n\n  return w_resampled\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  p_blurred = linspline.gaussian_filter_1d(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  w_blurred = pdf_to_weight(t, p_blurred)\n  w_resampled = resample(tq, t, w_blurred)\n\n  return w_resampled\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Compute the PDF from the weights\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  p_blurred = linspline.blur(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  w_blurred = resample(tq, t, p_blurred)\n\n  # Convert the blurred PDF back to weights\n  w_blurred_resampled = pdf_to_weight(tq, w_blurred)\n\n  return w_blurred_resampled\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian filter\n  p_blurred = linspline.gaussian_filter1d(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  w_blurred = resample(tq, t, p_blurred)\n\n  return w_blurred\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel with a specified half-width\n  blurred_pdf = linspline.gaussian_kernel(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  resampled_pdf = resample(tq, t, blurred_pdf)\n\n  # Convert the resampled PDF back to weights\n  resampled_weights = pdf_to_weight(tq, resampled_pdf)\n\n  return resampled_weights\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  pdf_blurred = linspline.blur_pdf(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  w_resampled = resample(tq, t, pdf_blurred)\n\n  return w_resampled\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  kernel = jnp.exp(-0.5 * (tq[Ellipsis, None] - t[Ellipsis, None]) ** 2 / blur_halfwidth ** 2)\n  pdf_blurred = jnp.sum(pdf[Ellipsis, None] * kernel, axis=-2)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  resampled_pdf = jnp.interp(tq, t, pdf_blurred)\n\n  # Convert the blurred and resampled PDF back to a histogram\n  resampled_w = pdf_to_weight(tq, resampled_pdf)\n\n  return resampled_w\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Compute the PDF from the weights\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  p_blurred = linspline.gaussian_kernel_1d(\n      t, p, blur_halfwidth, num_samples=t.shape[-1]\n  )\n\n  # Resample the blurred PDF to match the new time points\n  w_blurred = pdf_to_weight(tq, p_blurred)\n\n  return w_blurred\n\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel with the specified half-width\n  blurred_p = linspline.gaussian_blur(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  resampled_p = linspline.resample(tq, t, blurred_p)\n\n  # Convert the blurred and resampled PDF back to weights\n  resampled_w = pdf_to_weight(tq, resampled_p)\n\n  return resampled_w\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian kernel with the specified half-width\n  p_blurred = linspline.blur_pdf(t, p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  w_blurred = resample(tq, t, p_blurred)\n\n  return w_blurred\n\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the weights to a probability density function (PDF)\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian kernel with the specified half-width\n  p_blurred = linspline.blur_pdf(t, p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  w_resampled = resample(tq, t, p_blurred)\n\n  return w_resampled\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel with the specified half-width\n  kernel = jnp.exp(-(tq - tq[:, None]) ** 2 / (2 * blur_halfwidth ** 2))\n  blurred_pdf = jnp.sum(pdf[:, None] * kernel, axis=0)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  resampled_pdf = jnp.interp(tq, t, blurred_pdf)\n\n  # Convert the resampled PDF back to weights\n  w_resampled = pdf_to_weight(tq, resampled_pdf)\n\n  return w_resampled\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform.T))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  transformed_vectors = matmul(transform, to_homogeneous(vectors))\n  return from_homogeneous(transformed_vectors)\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  vectors_homogeneous = to_homogeneous(vectors)\n  transformed_homogeneous = matmul(transform, vectors_homogeneous[..., None])[..., 0]\n  return from_homogeneous(transformed_homogeneous)\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform.T))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the 3D points to homogeneous coordinates\n  homogeneous_vectors = to_homogeneous(vectors)\n\n  # Apply the homogeneous transformation to the points\n  transformed_homogeneous_vectors = matmul(transform, homogeneous_vectors[..., None])[..., 0]\n\n  # Convert the transformed points back to 3D coordinates\n  transformed_vectors = from_homogeneous(transformed_homogeneous_vectors)\n\n  return transformed_vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform.T))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform.T))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform.T))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform.T))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Add a 1 to the end of each vector\n  vectors = to_homogeneous(vectors)\n  # Apply the transformation\n  vectors = matmul(transform, vectors[..., None])[..., 0]\n  # Remove the 1 from the end of each vector\n  vectors = from_homogeneous(vectors)\n  return vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the input points to homogeneous coordinates\n  homogeneous_vectors = to_homogeneous(vectors)\n\n  # Apply the homogeneous transformation to the points\n  transformed_homogeneous_vectors = matmul(transform, homogeneous_vectors[..., None])[..., 0]\n\n  # Convert the transformed points back to non-homogeneous coordinates\n  transformed_vectors = from_homogeneous(transformed_homogeneous_vectors)\n\n  return transformed_vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))\n\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Add a homogeneous coordinate of ones to the end of each vector\n  vectors_homogeneous = jnp.concatenate([vectors, jnp.ones_like(vectors[..., :1])], axis=-1)\n\n  # Perform the matrix multiplication\n  transformed_vectors_homogeneous = matmul(transform, vectors_homogeneous.T).T\n\n  # Split the transformed vectors back into their components\n  transformed_vectors = transformed_vectors_homogeneous[..., :-1]\n\n  return transformed_vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # pylint: disable=g-explicit-length-test\n  if len(vectors.shape) < 2:\n    raise ValueError(\n        f\"Input vectors must have at least 2 dimensions, got shape {vectors.shape}\"\n    )\n  if vectors.shape[-1] != transform.shape[0] - 1:\n    raise ValueError(\n        f\"The last dimension of the input vectors must be equal to the first dimension of the transform matrix minus 1, got {vectors.shape[-1]} and {transform.shape[0] - 1}\"\n    )\n\n  # Add a dimension to the vectors for the homogeneous coordinate\n  vectors = jnp.concatenate([vectors, jnp.ones_like(vectors[..., :1])], axis=-1)\n\n  # Apply the transformation\n  transformed_vectors = matmul(transform, vectors[..., None])[..., 0]\n\n  # Remove the homogeneous coordinate\n  transformed_vectors = transformed_vectors[..., :-1]\n\n  return transformed_vectors\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Add an extra dimension to the vectors, and add an extra dimension to the\n  # transform matrix, and then do a matrix multiplication.\n  vectors_homogeneous = to_homogeneous(vectors)\n  transformed_homogeneous = matmul(transform, vectors_homogeneous[..., None])\n  return from_homogeneous(transformed_homogeneous[..., 0])\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors have the same shape\n  if t.shape != tp.shape:\n    raise ValueError(\"Input tensors 't' and 'tp' must have the same shape.\")\n\n  # Check if the input tensors are sorted in ascending order\n  if not jnp.all(jnp.diff(t) >= 0):\n    raise ValueError(\"Input tensor 't' must be sorted in ascending order.\")\n\n  # Check if the input tensors have the same shape\n  if t.shape != vp.shape:\n    raise ValueError(\"Input tensors 't' and 'vp' must have the same shape.\")\n\n  # Check if the input tensors are sorted in ascending order\n  if not jnp.all(jnp.diff(t) >= 0):\n    raise ValueError(\"Input tensor 't' must be sorted in ascending order.\")\n\n  # Check if the input tensors have the same shape\n  if t.shape != vp.shape:\n    raise ValueError(\"Input tensors 't' and 'vp' must have the same shape.\")\n\n  # Check if the input tensors are sorted in ascending order\n  if not jnp.all(jnp.diff(t) >= 0):\n    raise ValueError(\"Input tensor 't' must be sorted in ascending order.\")\n\n  # Check if the input tensors have the same shape\n  if t.shape != vp.shape:\n    raise ValueError(\"Input tensors 't' and 'vp' must have the same shape.\")\n\n  # Check if the input tensors are sorted in ascending order\n  if not jnp.all(jnp.diff(t) >= 0):\n    raise ValueError(\"Input tensor 't' must be sorted in ascending order.\")\n\n  # Check if the input tensors have the same shape\n  if t.shape != vp.shape:\n    raise ValueError(\"Input tensors 't' and 'vp' must have the same shape.\")\n\n  # Check if the input tensors are sorted in ascending order\n  if not jnp.all(jnp.diff(t) >= 0):"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in `t`.\n  width = jnp.diff(t)\n\n  # Compute the indices of the intervals in `t` that each value in `tp` belongs to.\n  indices = jnp.searchsorted(t, tp, side='right') - 1\n\n  # Compute the start and end indices of each interval in `t`.\n  start_indices = jnp.maximum(0, indices - 1)\n  end_indices = jnp.minimum(len(t) - 1, indices)\n\n  # Compute the start and end values of each interval in `t`.\n  start_values = t[start_indices]\n  end_values = t[end_indices]\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end_values - start_values\n\n  # Compute the width of each interval in `t`.\n  width = end"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors have the same shape\n  assert t.shape == tp.shape, \"Input tensors must have the same shape.\"\n\n  # Check if the input tensors are sorted\n  assert jnp.all(jnp.diff(tp) >= 0), \"Input tensors must be sorted.\"\n\n  # Check if the input tensors have at least 2 elements\n  assert tp.shape[-1] >= 2, \"Input tensors must have at least 2 elements.\"\n\n  # Check if the input tensors have at least 1 element in t\n  assert t.shape[-1] >= 1, \"Input tensors must have at least 1 element in t.\"\n\n  # Check if the input tensors have at least 1 element in vp\n  assert vp.shape[-1] >= 1, \"Input tensors must have at least 1 element in vp.\"\n\n  # Check if the input tensors have the same leading dimensions\n  assert t.shape[:-1] == tp.shape[:-1], \"Input tensors must have the same leading dimensions.\"\n\n  # Check if the input tensors have the same leading dimensions\n  assert t.shape[:-1] == vp.shape[:-1], \"Input tensors must have the same leading dimensions.\"\n\n  # Check if the input tensors have the same leading dimensions\n  assert t.shape[:-1] == tp.shape[:-1], \"Input tensors must have the same leading dimensions.\"\n\n  # Check if the input tensors have the same leading dimensions\n  assert t.shape[:-1] == vp.shape[:-1], \"Input tensors must have the same leading dimensions.\"\n\n  # Check if the input tensors have the same leading dimensions\n  assert t.shape[:-1] == tp.shape[:-1], \"Input tensors must have the same leading dimensions.\"\n\n  # Check if the input tensors have the same leading dimensions\n  assert t.shape[:-1] == vp.shape[:-1], \"Input tensors must have the same leading dimensions.\"\n\n  # Check if the input tensors have the same leading dimensions\n  assert t.shape[:-1"}
{"namespace": "stepfun.resample", "completion": "  # Sort the original time points and their corresponding values\n  tp, vp = jnp.sort(tp), jnp.sort(vp)\n\n  # Compute the widths of the intervals defined by the original time points\n  widths = jnp.diff(tp)\n\n  # Compute the cumulative sum of the widths\n  csum_widths = jnp.cumsum(widths)\n\n  # Compute the cumulative sum of the original values\n  csum_vp = jnp.cumsum(vp)\n\n  # Compute the cumulative sum of the cumulative sum of the original values\n  csum_csum_vp = jnp.cumsum(csum_vp)\n\n  # Compute the cumulative sum of the cumulative sum of the cumulative sum of the original values\n  csum_csum_csum_vp = jnp.cumsum(csum_csum_vp)\n\n  # Compute the cumulative sum of the cumulative sum of the cumulative sum of the cumulative sum of the original values\n  csum_csum_csum_csum_vp = jnp.cumsum(csum_csum_csum_vp)\n\n  # Compute the cumulative sum of the cumulative sum of the cumulative sum of the cumulative sum of the cumulative sum of the original values\n  csum_csum_csum_csum_csum_vp = jnp.cumsum(csum_csum_csum_csum_vp)\n\n  # Compute the cumulative sum of the cumulative sum of the cumulative sum of the cumulative sum of the cumulative sum of the cumulative sum of the original values\n  csum_csum_csum_csum_csum_csum_vp = jnp.cumsum(csum_csum_csum_csum_csum_vp)\n\n  # Compute the cumulative sum of the cumulative sum of the cumulative sum of the cumulative sum of the cumulative sum of the cumulative sum of the cumulative sum of the original values"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors have the same shape\n  if t.shape != tp.shape:\n    raise ValueError(f\"Input tensors must have the same shape. Got {t.shape} and {tp.shape}\")\n\n  # Check if the input tensors are sorted\n  if not jnp.all(jnp.diff(t) >= 0):\n    raise ValueError(\"Input tensors must be sorted.\")\n\n  # Check if the input tensors have the same number of dimensions\n  if t.ndim != tp.ndim or t.ndim != vp.ndim:\n    raise ValueError(f\"Input tensors must have the same number of dimensions. Got {t.ndim}, {tp.ndim}, and {vp.ndim}.\")\n\n  # Check if the last dimension of the input tensors is 1\n  if t.shape[-1] != 1 or tp.shape[-1] != 1 or vp.shape[-1] != 1:\n    raise ValueError(f\"The last dimension of the input tensors must be 1. Got {t.shape[-1]}, {tp.shape[-1]}, and {vp.shape[-1]}.\")\n\n  # Check if the last dimension of the input tensors is the same\n  if t.shape[:-1] != tp.shape[:-1] or t.shape[:-1] != vp.shape[:-1]:\n    raise ValueError(f\"The first dimensions of the input tensors must be the same. Got {t.shape[:-1]}, {tp.shape[:-1]}, and {vp.shape[:-1]}.\")\n\n  # Check if the last dimension of the input tensors is the same\n  if t.shape[:-1] != tp.shape[:-1] or t.shape[:-1] != vp.shape[:-1]:\n    raise ValueError(f\"The first dimensions of the input tensors must be the same. Got {t.shape[:-1]}, {tp.shape[:-1]}, and {vp.shape[:-1]}.\")\n\n "}
{"namespace": "stepfun.resample", "completion": "  # Compute the widths of the intervals defined by tp.\n  widths = jnp.diff(tp)\n\n  # Compute the indices of the intervals in t that each point in tp falls into.\n  indices = jnp.searchsorted(t, tp, side='right') - 1\n\n  # Compute the weights for each point in tp based on its position in the intervals defined by t.\n  weights = (t[indices + 1] - tp) / widths[indices]\n\n  # Compute the values of the resampled step function at the new intervals defined by t.\n  if use_avg:\n    resampled_vp = jnp.zeros_like(t)\n    resampled_vp = resampled_vp.at[indices].add(weights * vp)\n    resampled_vp = resampled_vp.at[indices + 1].add((1 - weights) * vp)\n    resampled_vp = resampled_vp / widths\n  else:\n    resampled_vp = jnp.zeros_like(t)\n    resampled_vp = resampled_vp.at[indices].add(weights * vp)\n    resampled_vp = resampled_vp.at[indices + 1].add((1 - weights) * vp)\n\n  return resampled_vp\n\n"}
{"namespace": "stepfun.resample", "completion": "  if use_avg:\n    # Compute the widths of the intervals in `t`\n    widths = jnp.diff(t)\n    # Compute the cumulative sum of the widths\n    cumsum_widths = jnp.cumsum(widths)\n    # Compute the cumulative sum of the values of the step function at the original time points (`tp`)\n    cumsum_vp = jnp.cumsum(vp)\n    # Compute the average value of the step function for each interval in `t`\n    avg_vp = (cumsum_vp[1:] - cumsum_vp[:-1]) / cumsum_widths\n    # Return the average values of the step function at the new intervals defined by `t`\n    return avg_vp\n  else:\n    # Compute the widths of the intervals in `t`\n    widths = jnp.diff(t)\n    # Compute the cumulative sum of the widths\n    cumsum_widths = jnp.cumsum(widths)\n    # Compute the cumulative sum of the values of the step function at the original time points (`tp`)\n    cumsum_vp = jnp.cumsum(vp)\n    # Compute the sum of the values of the step function for each interval in `t`\n    sum_vp = (cumsum_vp[1:] - cumsum_vp[:-1]) / cumsum_widths\n    # Return the sum values of the step function at the new intervals defined by `t`\n    return sum_vp\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in `t`.\n  t_widths = jnp.diff(t)\n\n  # Compute the indices of the intervals in `t` that each point in `tp` belongs to.\n  i0, i1 = math.sorted_lookup(tp, t, (), utils.device_is_tpu())\n\n  # Compute the value of the step function at each new time point in `t`.\n  if use_avg:\n    # If `use_avg` is True, compute the average value of the step function for each interval in `t`.\n    vp_sum = jnp.zeros_like(t)\n    vp_count = jnp.zeros_like(t)\n    vp_sum = vp_sum.at[Ellipsis, i0].add(vp * t_widths[Ellipsis, i0])\n    vp_count = vp_count.at[Ellipsis, i0].add(t_widths[Ellipsis, i0])\n    vp_sum = vp_sum.at[Ellipsis, i1].add(vp * t_widths[Ellipsis, i1])\n    vp_count = vp_count.at[Ellipsis, i1].add(t_widths[Ellipsis, i1])\n    vp_avg = vp_sum / vp_count\n    return vp_avg\n  else:\n    # If `use_avg` is False, sum the values of the step function for each interval in `t`.\n    vp_sum = jnp.zeros_like(t)\n    vp_sum = vp_sum.at[Ellipsis, i0].add(vp * t_widths[Ellipsis, i0])\n    vp_sum = vp_sum.at[Ellipsis, i1].add(vp * t_widths[Ellipsis, i1])\n    return vp_sum\n\n"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  # Compute the bin widths.\n  dt = jnp.diff(t)\n  # Compute the indices of the intervals that each time point falls into.\n  i = jnp.searchsorted(t, tp, side='right') - 1\n  # Compute the fractional position of each time point within its interval.\n  f = (tp - t[i]) / dt[i]\n  # Compute the values of the step function at the new time points.\n  if use_avg:\n    v = vp[i] + (vp[i + 1] - vp[i]) * f\n  else:\n    v = vp[i] * (1 - f) + vp[i + 1] * f\n  return v\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in t.\n  width = jnp.diff(t)\n\n  # Compute the indices of the original time points that are within each interval in t.\n  indices = jnp.searchsorted(tp, t, side='right') - 1\n\n  # Compute the values of the step function at the new intervals defined by t.\n  if use_avg:\n    # If use_avg is True, return the average value, weighted by the width of each interval in t.\n    v = jnp.take(vp, indices) * width\n    v = jnp.cumsum(v, axis=-1)\n    v = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n    v = jnp.diff(v) / width\n  else:\n    # If use_avg is False, return the sum of the values of the step function for each interval in t.\n    v = jnp.take(vp, indices) * width\n    v = jnp.cumsum(v, axis=-1)\n    v = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n    v = jnp.diff(v)\n\n  return v\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if tp and vp have the same shape\n  if tp.shape != vp.shape:\n    raise ValueError(\"tp and vp must have the same shape\")\n\n  # Check if t is sorted and has the same shape as tp\n  if not jnp.all(jnp.diff(t) >= 0):\n    raise ValueError(\"t must be sorted and have the same shape as tp\")\n  if t.shape != tp.shape:\n    raise ValueError(\"t must have the same shape as tp\")\n\n  # Check if tp and t have the same number of elements\n  if tp.shape[-1] != t.shape[-1]:\n    raise ValueError(\"tp and t must have the same number of elements\")\n\n  # Check if tp and vp have the same last dimension\n  if tp.shape[-1] != vp.shape[-1]:\n    raise ValueError(\"tp and vp must have the same last dimension\")\n\n  # Check if tp and vp have the same dtype\n  if tp.dtype != vp.dtype:\n    raise ValueError(\"tp and vp must have the same dtype\")\n\n  # Check if t has the same dtype as tp\n  if t.dtype != tp.dtype:\n    raise ValueError(\"t must have the same dtype as tp\")\n\n  # Check if tp and vp are on the same device\n  if tp.device != vp.device:\n    raise ValueError(\"tp and vp must be on the same device\")\n\n  # Check if t is on the same device as tp\n  if t.device != tp.device:\n    raise ValueError(\"t must be on the same device as tp\")\n\n  # Check if tp and vp are on the same device as t\n  if tp.device != t.device:\n    raise ValueError(\"tp and vp must be on the same device as t\")\n\n  # Check if tp and vp are on the same device as t\n  if tp.device != t.device:"}
{"namespace": "stepfun.resample", "completion": "  # Ensure that tp and vp have the same shape\n  assert tp.shape == vp.shape, \"Input time points and values must have the same shape\"\n\n  # Ensure that tp is sorted in ascending order\n  assert jnp.all(tp[1:] >= tp[:-1]), \"Input time points must be sorted in ascending order\"\n\n  # Ensure that t is sorted in ascending order\n  assert jnp.all(t[1:] >= t[\n      :-1]), \"Output time points must be sorted in ascending order\"\n\n  # Ensure that tp and t have the same last dimension\n  assert tp.shape[-1] == t.shape[-1], \"Input time points and output time points must have the same last dimension\"\n\n  # Ensure that tp and vp are on the same device\n  assert jnp.all(tp.device == vp.device), \"Input time points and values must be on the same device\"\n\n  # Ensure that t is on the same device as vp\n  assert t.device == vp.device, \"Output time points must be on the same device as values\"\n\n  # Ensure that tp and t are on the same dtype\n  assert tp.dtype == t.dtype, \"Input time points and output time points must have the same dtype\"\n\n  # Ensure that vp is on the same dtype as t\n  assert vp.dtype == t.dtype, \"Values must have the same dtype as output time points\"\n\n  # Ensure that tp and t have the same dtype\n  assert tp.dtype == t.dtype, \"Input time points and output time points must have the same dtype\"\n\n  # Ensure that tp and vp have the same dtype\n  assert tp.dtype == vp.dtype, \"Input time points and values must have the same dtype\"\n\n  # Ensure that tp and vp have the same device\n  assert tp.device == vp.device, \"Input time points and values must be on the same device\"\n\n  # Ensure"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in t\n  dt = jnp.diff(t)\n\n  # Compute the width of each interval in tp\n  dtp = jnp.diff(tp)\n\n  # Compute the indices of the intervals in t that correspond to each interval in tp\n  idx = jnp.searchsorted(t, tp, side='right') - 1\n\n  # Compute the values of the step function at the new time points\n  if use_avg:\n    # Compute the average value of the step function for each interval in t\n    v = jnp.zeros_like(t)\n    v = v.at[idx].add(vp * dtp / dt)\n    v = v.at[idx + 1].add(-vp * dtp / dt)\n    v = v.at[idx + 1].add(vp * dtp / dt)\n    v = v.at[idx + 2].add(-vp * dtp / dt)\n  else:\n    # Sum the values of the step function for each interval in t\n    v = jnp.zeros_like(t)\n    v = v.at[idx].add(vp * dtp)\n    v = v.at[idx + 1].add(-vp * dtp)\n    v = v.at[idx + 1].add(vp * dtp)\n    v = v.at[idx + 2].add(-vp * dtp)\n\n  return v\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in t\n  t_width = jnp.diff(t)\n\n  # Compute the cumulative sum of the widths of the intervals in t\n  t_cumsum = jnp.cumsum(t_width)\n\n  # Compute the cumulative sum of the widths of the intervals in tp\n  tp_cumsum = jnp.cumsum(jnp.diff(tp))\n\n  # Compute the indices of the intervals in t that correspond to each interval in tp\n  t_indices = jnp.searchsorted(t_cumsum, tp_cumsum, side='right')\n\n  # Compute the indices of the intervals in t that correspond to each interval in tp\n  tp_indices = jnp.searchsorted(tp_cumsum, t_cumsum, side='right')\n\n  # Compute the widths of the intervals in t that correspond to each interval in tp\n  t_widths = t_width[t_indices]\n\n  # Compute the widths of the intervals in tp that correspond to each interval in t\n  tp_widths = tp_cumsum[tp_indices] - tp_cumsum[tp_indices - 1]\n\n  # Compute the weights for each interval in t that correspond to each interval in tp\n  weights = tp_widths / t_widths\n\n  # Compute the values of the step function at the new time points\n  if use_avg:\n    v = jnp.sum(vp[tp_indices] * weights, axis=1)\n  else:\n    v = jnp.sum(vp[tp_indices] * weights, axis=1)\n\n  return v\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in t\n  t_width = jnp.diff(t)\n\n  # Compute the indices of the intervals in t that each point in tp belongs to\n  indices = jnp.searchsorted(t, tp, side='right') - 1\n\n  # Compute the start and end indices of the intervals in t that each point in tp belongs to\n  start_indices = jnp.maximum(0, indices - 1)\n  end_indices = jnp.minimum(t.shape[-1] - 1, indices + 1)\n\n  # Compute the width of each interval in t that each point in tp belongs to\n  interval_width = t[Ellipsis, end_indices] - t[Ellipsis, start_indices]\n\n  # Compute the values of the step function at the new intervals defined by t\n  if use_avg:\n    vp_resampled = (vp[Ellipsis, start_indices] * (t_width - interval_width) + vp[Ellipsis, end_indices] * interval_width) / t_width\n  else:\n    vp_resampled = (vp[Ellipsis, start_indices] + vp[Ellipsis, end_indices]) / 2\n\n  return vp_resampled\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors have the same shape\n  assert t.shape == tp.shape\n  assert t.shape == vp.shape\n\n  # Check if the input tensors are sorted\n  assert jnp.all(jnp.diff(t) >= 0)\n  assert jnp.all(jnp.diff(tp) >= 0)\n\n  # Initialize an empty tensor to store the resampled values\n  v = jnp.zeros_like(t)\n\n  # Iterate over the new intervals\n  for i in range(t.shape[0] - 1):\n    # Get the start and end of the current interval\n    start = t[i]\n    end = t[i + 1]\n\n    # Find the indices of the original time points that fall within the current interval\n    indices = jnp.where((tp >= start) & (tp < end))[0]\n\n    # If there are no original time points within the current interval, set the resampled value to 0\n    if indices.size == 0:\n      v = v.at[i].set(0)\n      continue\n\n    # Get the original values of the step function at the time points within the current interval\n    v_values = vp[indices]\n\n    # If the resampling method is summation, sum the values of the step function for each interval in `t`\n    if not use_avg:\n      v = v.at[i].set(jnp.sum(v_values))\n    # If the resampling method is averaging, return the average value, weighted by the width of each interval in `t`\n    else:\n      width = end - start\n      v = v.at[i].set(jnp.sum(v_values) / width)\n\n  # Return the resampled values\n  return v\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in `t`.\n  t_width = jnp.diff(t)\n\n  # Compute the indices of the original time points that are within each interval in `t`.\n  tp_idx = jnp.searchsorted(tp, t)\n\n  # Compute the values of the step function at the new time points `t`.\n  vp_new = jnp.zeros_like(t)\n\n  # Iterate over each interval in `t`.\n  for i in range(t.shape[-1]):\n    # Get the indices of the original time points that are within the current interval.\n    idx = tp_idx[:, i]\n\n    # Compute the values of the step function at the new time points `t`.\n    if use_avg:\n      # If `use_avg` is True, compute the average value of the step function within the current interval.\n      vp_new = vp_new.at[Ellipsis, i].set(jnp.sum(vp[idx] * t_width[i]) / t_width[i])\n    else:\n      # If `use_avg` is False, sum the values of the step function within the current interval.\n      vp_new = vp_new.at[Ellipsis, i].set(jnp.sum(vp[idx]))\n\n  return vp_new\n\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check that the inputs are valid step functions\n  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the width of each interval in t\n  dt = jnp.diff(t)\n\n  # Compute the indices of the intervals in tp that each interval in t belongs to\n  i0, i1 = math.sorted_lookup(t, tp, (), utils.device_is_tpu())\n\n  # Compute the values of the step function at the new time points t\n  if use_avg:\n    # Compute the average value of the step function for each interval in t\n    v = jnp.sum(vp[Ellipsis, i0] * dt[Ellipsis, i0], axis=-1)\n    v = jnp.sum(vp[Ellipsis, i1] * dt[Ellipsis, i1], axis=-1)\n    v = jnp.where(dt == 0, vp[Ellipsis, i0], v)\n    v = jnp.where(dt == 0, vp[Ellipsis, i1], v)\n  else:\n    # Sum the values of the step function for each interval in t\n    v = jnp.sum(vp[Ellipsis, i0] * dt[Ellipsis, i0], axis=-1)\n    v = jnp.sum(vp[Ellipsis, i1] * dt[Ellipsis, i1], axis=-1)\n\n  return v\n\n"}
{"namespace": "stepfun.resample", "completion": "  if use_avg:\n    # Compute the width of each interval in `t`.\n    dt = jnp.diff(t)\n\n    # Compute the cumulative sum of the values in `vp`.\n    cumsum_vp = jnp.cumsum(vp, axis=-1)\n\n    # Compute the cumulative sum of the widths in `dt`.\n    cumsum_dt = jnp.cumsum(dt, axis=-1)\n\n    # Compute the cumulative sum of the widths in `dt` shifted by one element.\n    cumsum_dt_shifted = jnp.concatenate([jnp.zeros_like(cumsum_dt[..., :1]), cumsum_dt[..., :-1]], axis=-1)\n\n    # Compute the cumulative sum of the values in `vp` shifted by one element.\n    cumsum_vp_shifted = jnp.concatenate([jnp.zeros_like(cumsum_vp[..., :1]), cumsum_vp[..., :-1]], axis=-1)\n\n    # Compute the average value of the step function at each interval in `t`.\n    avg_vp = (cumsum_vp - cumsum_vp_shifted) / (cumsum_dt - cumsum_dt_shifted)\n\n    # Interpolate the values of the resampled step function at the new time points.\n    v_resampled = math.sorted_interp(t, avg_vp, tp, utils.device_is_tpu())\n  else:\n    # Interpolate the values of the resampled step function at the new time points.\n    v_resampled = math.sorted_interp(t, vp, tp, utils.device_is_tpu())\n\n  return v_resampled\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Compute the widths of the original intervals\n  widths = tp[..., 1:] - tp[..., :-1]\n\n  # Compute the indices of the intervals that the new intervals fall into\n  indices = jnp.digitize(t[..., None], tp[..., None], right=True) - 1\n\n  # Compute the indices of the intervals that the new intervals fall into\n  indices = jnp.clip(indices, 0, widths.shape[-1] - 1)\n\n  # Compute the weights for the new intervals\n  weights = jnp.take_along_axis(widths, indices[..., None], axis=-1)\n\n  # Compute the values of the step function at the new intervals\n  if use_avg:\n    values = jnp.sum(vp[..., None] * weights, axis=-1) / jnp.sum(weights, axis=-1)\n  else:\n    values = jnp.sum(vp[..., None] * weights, axis=-1)\n\n  return values\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance by powers of 2 within the specified range of degrees.\n  scale = 2 ** jnp.arange(min_deg, max_deg)\n  scaled_mean = mean[..., None] * scale\n  scaled_var = var[..., None] * scale ** 2\n\n  # Concatenate the scaled mean and variance along the last axis.\n  encoded_input = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply a sinusoidal encoding to the concatenated tensor.\n  encoded_input = jnp.sin(encoded_input)\n\n  return encoded_input\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance to a range of degrees.\n  scale = jnp.exp(jnp.linspace(jnp.log(2), jnp.log(2**max_deg), max_deg - min_deg))\n  mean_scaled = mean * scale\n  var_scaled = var * scale**2\n\n  # Concatenate the scaled mean and variance along the last dimension.\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply sinusoidal encoding to the concatenated tensor.\n  encoding = jnp.sin(math.pi * x[..., None] * jnp.arange(1, 2 * max_deg + 1, 2)[None, ...])\n\n  return encoding"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # scale the mean and variance\n  scale = 2**jnp.arange(min_deg, max_deg)\n  mean_scaled = mean * scale\n  var_scaled = var * scale**2\n\n  # concatenate the scaled mean and variance\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # apply sinusoidal encoding\n  encoding = jnp.concatenate([jnp.sin(x[..., ::2]), jnp.cos(x[..., 1::2])], axis=-1)\n\n  return encoding\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance by powers of 2 within the specified range of degrees\n  deg_range = jnp.arange(min_deg, max_deg)\n  scale = 2 ** deg_range\n  mean_scaled = mean * scale\n  var_scaled = var * scale ** 2\n\n  # Concatenate the scaled mean and variance\n  input_concat = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply the sinusoidal encoding to the concatenated input\n  output = jnp.sin(input_concat)\n\n  return output\n\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  mean_scaled = mean / 2**min_deg\n  var_scaled = var / 2**(2 * min_deg)\n\n  # Concatenate the scaled mean and variance\n  concat_input = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply the sinusoidal encoding\n  num_degrees = max_deg - min_deg\n  encoding = jnp.concatenate([jnp.sin(2**i * concat_input) for i in range(num_degrees)], axis=-1)\n\n  return encoding\n\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates.\n  scale = jnp.exp(jnp.linspace(jnp.log(2), jnp.log(2**max_deg), max_deg - min_deg))\n  mean_scaled = mean[..., None] * scale\n  var_scaled = var[..., None] * scale**2\n\n  # Concatenate the scaled mean and variance along the last axis.\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply sinusoidal encoding to the concatenated tensor.\n  encoding = jnp.sin(x[..., None] * jnp.arange(1, x.shape[-1] + 1))\n\n  return encoding\n\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  scale = jnp.exp(0.5 * var)\n  mean_scaled = mean / scale\n  var_scaled = var / scale**2\n\n  # Concatenate the scaled mean and variance\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply the sinusoidal encoding\n  degree = jnp.arange(min_deg, max_deg)\n  enc = jnp.concatenate([jnp.sin(2**degree * x), jnp.cos(2**degree * x)], axis=-1)\n\n  return enc\n\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # scale the mean and variance\n  scale = jnp.power(2, jnp.arange(min_deg, max_deg))\n  mean = mean * scale\n  var = var * scale**2\n\n  # concatenate the mean and variance\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # apply sinusoidal encoding\n  encoding = jnp.concatenate([jnp.sin(x[..., i:i+1] / scale[i]) for i in range(x.shape[-1])], axis=-1)\n\n  return encoding\n\n\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for the encoding.\n  scale = jnp.exp(jnp.linspace(jnp.log(2**min_deg), jnp.log(2**max_deg), max_deg - min_deg))\n  # Scale the mean and variance of the coordinates.\n  mean_scaled = mean * scale\n  var_scaled = var * scale**2\n  # Concatenate the scaled mean and variance.\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n  # Apply a sinusoidal encoding to the concatenated values.\n  return jnp.concatenate([jnp.sin(x), jnp.cos(x)], axis=-1)\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance.\n  scale = jnp.sqrt(2) / (2**max_deg - 1)\n  mean_scaled = mean * scale\n  var_scaled = var * scale**2\n\n  # Concatenate the scaled mean and variance.\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply sinusoidal encoding.\n  encoding = jnp.concatenate([x[:, :1], jnp.sin(x[:, 1:] * jnp.pi * (2**jnp.arange(min_deg, max_deg)))], axis=-1)\n\n  return encoding\n\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scale = jnp.exp(-0.5 * var)\n  mean_scaled = mean * scale\n  var_scaled = var * scale**2\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n  # Encode x using sinusoidal functions scaled by powers of 2 within the specified range.\n  deg_range = jnp.arange(min_deg, max_deg)\n  freqs = 2**deg_range\n  x_enc = jnp.concatenate([jnp.sin(freqs[:, None] * x[..., None]),\n                           jnp.cos(freqs[:, None] * x[..., None])], axis=-2)\n  return x_enc\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor based on the minimum and maximum degrees.\n  scale = jnp.exp(jnp.arange(min_deg, max_deg) * jnp.log(2))\n\n  # Scale the mean and variance of the coordinates.\n  scaled_mean = mean * scale\n  scaled_var = var * scale**2\n\n  # Concatenate the scaled mean and variance along the last axis.\n  x = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding to the concatenated tensor.\n  x_encoded = jnp.concatenate([jnp.sin(2 * jnp.pi * x), jnp.cos(2 * jnp.pi * x)], axis=-1)\n\n  return x_encoded\n\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance based on the specified degrees.\n  scale = 2**jnp.arange(min_deg, max_deg)\n  mean_scaled = mean * scale\n  var_scaled = var * scale**2\n\n  # Concatenate the scaled mean and variance.\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply the sinusoidal encoding.\n  encoding = jnp.sin(x[..., None] * jnp.arange(1, x.shape[-1] + 1))\n\n  return encoding\n\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  scale = jnp.power(2.0, jnp.arange(min_deg, max_deg))\n  mean_scaled = mean[..., None] * scale\n  var_scaled = var[..., None] * scale ** 2\n\n  # Concatenate the scaled mean and variance\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply sinusoidal encoding\n  encoding = jnp.sin(x[..., None] * jnp.arange(1, x.shape[-1] + 1))\n\n  return encoding\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for the encoding.\n  scale = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Scale the mean and variance.\n  mean = mean * scale\n  var = var * scale ** 2\n\n  # Concatenate the scaled mean and variance.\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Apply sinusoidal encoding.\n  x = 10000 ** (2 * jnp.pi * x)\n  x = jnp.concatenate([jnp.sin(x), jnp.cos(x)], axis=-1)\n\n  return x\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scale = 2 ** jnp.arange(min_deg, max_deg)\n  mean_scaled = mean * scale\n  var_scaled = var * scale ** 2\n  return jnp.concatenate([jnp.sin(mean_scaled), jnp.sin(var_scaled)], axis=-1)\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance to ensure they are within the range [0, 2pi]\n  mean = mean % (2 * np.pi)\n  var = var % (2 * np.pi)\n\n  # Concatenate the mean and variance tensors along the last axis\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Apply a sinusoidal encoding to the scaled coordinates\n  scales = jnp.power(2, jnp.arange(min_deg, max_deg))\n  x = x[..., None] * scales\n  x = jnp.concatenate([jnp.sin(x), jnp.cos(x)], axis=-1)\n\n  return x\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scale = jnp.exp(jnp.linspace(min_deg, max_deg, max_deg - min_deg))\n  scale_mean = jnp.concatenate([mean, var * scale], axis=-1)\n  return jnp.sin(scale_mean * (2 * np.pi))\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for the encoding based on the input min_deg and max_deg.\n  # The scaling factor is computed as 2^(min_deg) / 2^(max_deg).\n  scale = 2**min_deg / 2**max_deg\n\n  # Scale the mean and variance of the input coordinates.\n  # The scaled mean is computed as mean * scale, and the scaled variance is computed as var * scale**2.\n  scaled_mean = mean * scale\n  scaled_var = var * scale**2\n\n  # Concatenate the scaled mean and variance along the last axis.\n  # The resulting tensor has shape (..., 2 * d).\n  x = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply a sinusoidal encoding to the concatenated tensor.\n  # The encoding is computed as sin(x) for each element of the tensor.\n  # The resulting tensor has the same shape as the input tensor.\n  x = jnp.sin(x)\n\n  return x\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scale = 2**jnp.arange(min_deg, max_deg)\n  scale_mean = mean * scale\n  scale_var = var * scale**2\n  return jnp.concatenate([scale_mean, scale_var], axis=-1)\n\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)],"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)],"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)],"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)],"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply attenuation"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply attenuation"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply attenuation"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply attenuation function"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis=-1)\n\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate integrated directional encoding function.\n  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  # Create a function that takes a 3D point and returns its directional encoding.\n  def dir_enc_fn(xyz, kappa_inv):\n    return integrated_dir_enc_fn(xyz, kappa_inv)\n\n  return dir_enc_fn"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate IDE function.\n  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  # Create a function that evaluates the directional encoding.\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    # Compute integrated directional encoding.\n    ide = integrated_dir_enc_fn(xyz, kappa_inv)\n\n    # Normalize.\n    return l2_normalize(ide)\n\n  return dir_enc_fn\n\n\n\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Evaluates the directional encoding for a given 3D point (or points) using an integrated directional encoding function.\n\n    Input-Output Arguments\n    :param xyz: Array. A 3D point (or points) in the form of a numpy array.\n    :param kappa_inv: Array. The reciprocal of the concentration parameter of the von Mises-Fisher distribution.\n    :return: Array. The directional encoding of the input 3D point (or points).\n\n    \"\"\"\n    xyz = xyz.reshape(-1, 3)\n    kappa_inv = kappa_inv.reshape(-1, 1)\n    integrated_dir_enc_fn = generate_ide_fn(deg_view)\n    return integrated_dir_enc_fn(xyz, kappa_inv)\n\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def directional_encoding_fn(xyz, kappa_inv):\n    \"\"\"\n    Evaluates the directional encoding for the given input points.\n\n    Input-Output Arguments\n    :param xyz: Array of shape (..., 3). The input points in 3D space.\n    :param kappa_inv: Array of shape (..., 1). The reciprocal of the concentration parameter of the von Mises-Fisher distribution.\n    :return: Array of shape (..., 2 * deg_view + 1). The directional encoding for the input points.\n\n    \"\"\"\n    integrated_dir_enc_fn = generate_ide_fn(deg_view)\n    return integrated_dir_enc_fn(xyz, kappa_inv)\n\n  return directional_encoding_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate integrated directional encoding function\n  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Evaluates the directional encoding for a given 3D point or set of points.\n\n    Input-Output Arguments\n    :param xyz: JAX array. The 3D point (or points) for which the directional encoding is to be evaluated.\n    :param kappa_inv: JAX array. The reciprocal of the concentration parameter of the von Mises-Fisher distribution.\n    :return: JAX array. The directional encoding of the input point or points.\n\n    \"\"\"\n    # Compute integrated directional encoding\n    ide = integrated_dir_enc_fn(xyz, kappa_inv)\n\n    # Compute directional encoding\n    dir_enc = l2_normalize(ide)\n\n    return dir_enc\n\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(deg_view + 1)], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in range(deg_view + 1)], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, integrated_dir_enc_fn(xyz, kappa_inv))\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis=-1)\n\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Evaluates the directional encoding for a given 3D point (or points) using the generated integrated directional encoding function.\n\n    Input-Output Arguments\n    :param xyz: Numpy array of shape (N, 3). The input 3D points for which to evaluate the directional encoding.\n    :param kappa_inv: Numpy array of shape (N, 1). The reciprocal of the concentration parameter of the von Mises-Fisher distribution.\n    :return: Numpy array of shape (N, 2 * deg_view * (deg_view + 1) // 2). The directional encoding of the input 3D points.\n\n    \"\"\"\n    return integrated_dir_enc_fn(xyz, kappa_inv)\n\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    return integrate_dir_enc(xyz, kappa_inv, generate_ide_fn(deg_view))\n\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate integrated directional encoding function.\n  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    xyz = xyz / jnp.linalg.norm(xyz, axis=-1, keepdims=True)\n    return integrated_dir_enc_fn(xyz, kappa_inv)\n\n  return dir_enc_fn\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    This function evaluates the directional encoding for a given input. It internally uses the generated integrated directional encoding function with the specified degree of spherical harmonics.\n\n    Input-Output Arguments\n    :param xyz: Array. A 3D point (or points) in the Cartesian coordinate system.\n    :param kappa_inv: Array. The reciprocal of the concentration parameter of the von Mises-Fisher distribution.\n    :return: Array. The directional encoding of the input point.\n    \"\"\"\n    # Compute integrated directional encoding.\n    ide = integrated_dir_enc_fn(xyz, kappa_inv)\n\n    # Normalize the integrated directional encoding.\n    return l2_normalize(ide)\n\n  return dir_enc_fn\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    if not lines:\n        return []\n\n    # remove duplicate lines\n    lines = list(set(lines))\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # remove duplicates\n    lines = [line for line in set(lines)]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 3 characters\n    lines = [line for line in lines if len(line) > 2]"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # remove duplicate lines\n    lines = list(set(lines))\n    # remove lines with only 2 characters\n    lines = [line for line in lines if not find_floating_chars(line)]\n    # remove lines that are only numbers\n    lines = [line for line in lines if not line.isdigit()]\n    # remove lines that are only numbers and have a space in between\n    lines = [line for line in lines if not re.match(r\"^\\d+\\s+\\d+$\", line)]\n    # remove lines that are only numbers and have a space in between\n    lines = [line for line in lines if not re.match(r\"^\\d+\\s+\\d+$\", line)]\n    # remove lines that are only numbers and have a space in between\n    lines = [line for line in lines if not re.match(r\"^\\d+\\s+\\d+$\", line)]\n    # remove lines that are only numbers and have a space in between\n    lines = [line for line in lines if not re.match(r\"^\\d+\\s+\\d+$\", line)]\n    # remove lines that are only numbers and have a space in between\n    lines = [line for line in lines if not re.match(r\"^\\d+\\s+\\d+$\", line)]\n    # remove lines that are only numbers and have a space in between\n    lines = [line for line in lines if not re.match(r\"^\\d+\\s+\\d+$\", line)]\n    # remove lines that are only numbers and have a space in between\n    lines = [line for line in lines if not re.match(r\"^\\d+\\s+\\d+$\", line)]\n    # remove lines that are only numbers and have a space in between\n    lines = [line for line in lines if not re.match(r\"^\\d+\\s+\\d+$\", line)]\n    # remove lines that are only numbers and have a space in between\n    lines = [line for line in lines if not re.match(r\"^\\d+\\s+\\d+$\", line)]\n    # remove lines that are only numbers"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    block_index = 0\n    header_index = -1\n    list_index = -1\n    list_level = -1\n    list_item_index = 0\n    list_item_level = 0\n    list_item_type = \"\"\n    list_item_text = \"\"\n    list_item_start_index = -1\n    list_item_end_index = -1\n    list_item_block_index = -1\n    list_item_blocks = []\n    list_item_header_index = -1\n    list_item_list_index = -1\n    list_item_list_level = -1\n    list_item_list_item_index = 0\n    list_item_list_item_level = 0\n    list_item_list_item_type = \"\"\n    list_item_list_item_text = \"\"\n    list_item_list_item_start_index = -1\n    list_item_list_item_end_index = -1\n    list_item_list_item_block_index = -1\n    list_item_list_item_blocks = []\n    list_item_list_item_header_index = -1\n    list_item_list_item_list_index = -1\n    list_item_list_item_list_level = -1\n    list_item_list_item_list_item_index = 0\n    list_item_list_item_list_item_level = 0\n    list_item_list_item_list_item_type = \"\"\n    list_item_list_item_list_item_text = \"\"\n    list_item_list_item_list_item_start_index = -1\n    list_item_list_item_list_item_end_index = -1\n    list_item_list_item_list_item_block_index = -1\n    list_item_list_item_list_item_blocks = []\n    list_item_list_item_list_item_header_index = -1\n    list_item_list_item_list_item_list"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    header_blocks = []\n    list_blocks = []\n    block_index = 0\n    header_index = 0\n    list_index = 0\n    line_index = 0\n    prev_line = \"\"\n    header_level = 0\n    header_level_index = 0\n    header_level_list = []\n    header_level_list_index = 0\n    header_level_list_index_list = []\n    header_level_list_index_list_index = 0\n    header_level_list_index_list_index_list = []\n    header_level_list_index_list_index_list_index = 0\n    header_level_list_index_list_index_list_index_list = []\n    header_level_list_index_list_index_list_index_list_index = 0\n    header_level_list_index_list_index_list_index_list_index_list = []\n    header_level_list_index_list_index_list_index_list_index_list_index = 0\n    header_level_list_index_list_index_list_index_list_index_list_index_list = []\n    header_level_list_index_list_index_list_index_list_index_list_index_list_index = 0\n    header_level_list_index_list_index_list_index_list_index_list_index_list_index_list = []\n    header_level_list_index_list_index_list_index_list_index_list_index_list_index_list_index = 0\n    header_level_list_index_list_index_list_index_list_index_list_index_list_index_list_index_list = []\n    header_level_list_index_list_index_list_index_list_index_list_index_list_index_list_index_list_index = 0\n    header_level_list_index_list_index_list_index_list_index_list_index_list_index_list_index_list_index_list ="}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # remove duplicate lines\n    lines = list(set(lines))\n\n    # remove lines with less than 2 characters\n    lines = [line for line in lines if len(line) > 2]\n\n    # remove lines with less than 2 characters after removing spaces\n    lines = [line for line in lines if len(line.replace(\" \", \"\")) > 2]\n\n    # remove lines with less than 2 characters after removing numbers\n    lines = [line for line in lines if not re.search(r\"\\d+\", line)]\n\n    # remove lines with less than 2 characters after removing numbers and spaces\n    lines = [line for line in lines if len(line.replace(\" \", \"\").replace(\"\\n\", \"\")) > 2]\n\n    # remove lines with less than 2 characters after removing numbers and spaces and removing numbers\n    lines = [line for line in lines if not re.search(r\"\\d+\", line.replace(\" \", \"\").replace(\"\\n\", \"\"))]\n\n    # remove lines with less than 2 characters after removing numbers and spaces and removing numbers and spaces\n    lines = [line for line in lines if len(line.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")) > 2]\n\n    # remove lines with less than 2 characters after removing numbers and spaces and removing numbers and spaces and removing numbers\n    lines = [line for line in lines if not re.search(r\"\\d+\", line.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\"))]\n\n    # remove lines with less than 2 characters after removing numbers and spaces and removing numbers and spaces and removing numbers and spaces\n    lines = [line for line in lines if len(line.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\".\", \"\")) > 2]\n\n    # remove lines with less than 2 characters after removing numbers and spaces and removing numbers and spaces and removing numbers and spaces and removing numbers\n    lines = [line for line in lines if not re.search(r\"\\d+\", line.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\".\", \"\"))]\n\n    # remove lines"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    block_index = 0\n    header_index = 0\n    list_index = 0\n    list_items = []\n    block_text = \"\"\n    block_type = None\n    block_start_index = 0\n    block_end_index = 0\n    header_block_index = 0\n    header_text = \"\"\n    header_level = 0\n    header_start_index = 0\n    header_end_index = 0\n    header_type = None\n    header_block_index = 0\n    header_text = \"\"\n    header_level = 0\n    header_start_index = 0\n    header_end_index = 0\n    header_type = None\n    header_block_index = 0\n    header_text = \"\"\n    header_level = 0\n    header_start_index = 0\n    header_end_index = 0\n    header_type = None\n    header_block_index = 0\n    header_text = \"\"\n    header_level = 0\n    header_start_index = 0\n    header_end_index = 0\n    header_type = None\n    header_block_index = 0\n    header_text = \"\"\n    header_level = 0\n    header_start_index = 0\n    header_end_index = 0\n    header_type = None\n    header_block_index = 0\n    header_text = \"\"\n    header_level = 0\n    header_start_index = 0\n    header_end_index = 0\n    header_type = None\n    header_block_index = 0\n    header_text = \"\"\n    header_level = 0\n    header_start_index = 0\n    header_end_index = 0\n    header_type = None\n    header_block_index = 0\n    header_text = \"\"\n    header_level = 0\n    header_start_index = 0\n    header_end_index = 0\n    header_type = None\n    header_block_index = 0\n    header_text = \"\"\n    header_level = 0\n    header_start_index = 0\n    header_end_index = 0\n    header_type = None\n   "}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    block_index = 0\n    header_index = 0\n    header_level = 0\n    list_index = 0\n    list_level = 0\n    list_type = \"ul\"\n    list_items = []\n    list_item_index = 0\n    list_item_level = 0\n    list_item_type = \"li\"\n    list_item_start = 0\n    list_item_end = 0\n    list_item_text = \"\"\n    list_item_start_index = 0\n    list_item_end_index = 0\n    list_item_start_line = 0\n    list_item_end_line = 0\n    list_item_start_char = 0\n    list_item_end_char = 0\n    list_item_start_word = 0\n    list_item_end_word = 0\n    list_item_start_line_char = 0\n    list_item_end_line_char = 0\n    list_item_start_line_word = 0\n    list_item_end_line_word = 0\n    list_item_start_line_char_index = 0\n    list_item_end_line_char_index = 0\n    list_item_start_line_word_index = 0\n    list_item_end_line_word_index = 0\n    list_item_start_line_word_index_end = 0\n    list_item_end_line_word_index_end = 0\n    list_item_start_line_word_index_end_char = 0\n    list_item_end_line_word_index_end_char = 0\n    list_item_start_line_word_index_end_char_index = 0\n    list_item_end_line_word_index_end_char_index = 0\n    list_item_start_line_word_index_end_char_index_end = 0\n    list_item_end_line_word_index_end_char_index_end = 0\n    list_item_start_line_word_index_end_char_index_end_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # remove duplicate lines\n    lines = [line for line in lines if not line.isspace()]\n    lines = list(set(lines))\n\n    # fix spaced characters\n    for i in range(len(lines)):\n        if lines[i].endswith(\" \"):\n            lines[i] = lines[i][:-1]\n        if lines[i].startswith(\" \"):\n            lines[i] = lines[i][1:]\n\n    # connect incomplete lines\n    for i in range(len(lines) - 1):\n        if lines[i].endswith(\".\") and lines[i + 1].startswith(\"-\"):\n            lines[i] = lines[i] + \" \" + lines[i + 1]\n            lines[i + 1] = \"\"\n\n    # categorize lines into paragraphs, headers, or list items\n    result = []\n    block_index = 0\n    header_index = -1\n    list_index = -1\n    for i in range(len(lines)):\n        line = lines[i]\n        if should_skip(line, xml):\n            continue\n        if is_table_row(line):\n            continue\n        if line.startswith(\"Table\"):\n            continue\n        if line.startswith(\"Figure\"):\n            continue\n        if line.startswith(\"Note\"):\n            continue\n        if line.startswith(\"References\"):\n            continue\n        if line.startswith(\"Appendix\"):\n            continue\n        if line.startswith(\"Abstract\"):\n            continue\n        if line.startswith(\"Introduction\"):\n            continue\n        if line.startswith(\"Acknowledgements\"):\n            continue\n        if line.startswith(\"Conclusions\"):\n            continue\n        if line.startswith(\"Methods\"):\n            continue\n        if line.startswith(\"Results\"):\n            continue\n        if line.startswith(\"Discussion\"):\n            continue\n        if line.startswith(\"Figure\"):\n            continue\n        if line.startswith(\"Table\"):\n            continue\n        if line.startsw"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    header_blocks = []\n    list_blocks = []\n    block_index = 0\n    header_index = 0\n    list_index = 0\n    indentation_level = 0\n    for line in lines:\n        if should_skip(line, xml):\n            continue\n        if is_table_row(line):\n            continue\n        if line.startswith(\"Figure \") or line.startswith(\"Table \"):\n            continue\n        if line.startswith(\"[Figure \") or line.startswith(\"[Table \"):\n            continue\n        if line.startswith(\"[Figure\") or line.startswith(\"[Table\"):\n            continue\n        if line.startswith(\"[Figure \"):\n            continue\n        if line.startswith(\"[Table \"):\n            continue\n        if line.startswith(\"[Figure]\"):\n            continue\n        if line.startswith(\"[Table]\"):\n            continue\n        if line.startswith(\"[Figure]\") or line.startswith(\"[Table]\"):\n            continue\n        if line.startswith(\"Figure]\") or line.startswith(\"Table]\"):\n            continue\n        if line.startswith(\"Figure] \") or line.startswith(\"Table] \"):\n            continue\n        if line.startswith(\"Figure]\") or line.startswith(\"Table]\"):\n            continue\n        if line.startswith(\"Figure]\") or line.startswith(\"Table]\"):\n            continue\n        if line.startswith(\"Figure]\") or line.startswith(\"Table]\"):\n            continue\n        if line.startswith(\"Figure]\") or line.startswith(\"Table]\"):\n            continue\n        if line.startswith(\"Figure]\") or line.startswith(\"Table]\"):\n            continue\n        if line.startswith(\"Figure]\") or line.startswith(\"Table]\"):\n            continue\n        if line.startswith(\"Figure]\") or line.startswith(\"Table]\"):\n            continue\n        if line.startswith(\"Figure]\") or line.startsw"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(lines)\n    result = []\n    line_index = 0\n    block_index = 0\n    header_index = None\n    header_level = None\n    list_index = None\n    list_level = None\n    list_items = []\n    block_start_index = None\n    block_end_index = None\n    line_start_index = None\n    line_end_index = None\n    line_text = None\n    line_type = None\n    line_list_items = []\n    line_header_index = None\n    line_header_level = None\n    line_list_index = None\n    line_list_level = None\n    line_block_index = None\n    line_block_start_index = None\n    line_block_end_index = None\n    line_text_group_start_index = None\n    line_text_group_end_index = None\n    line_text_group_index = None\n    line_text_group_type = None\n    line_text_group_blocks = []\n    line_text_group_header_index = None\n    line_text_group_header_level = None\n    line_text_group_list_index = None\n    line_text_group_list_level = None\n    line_text_group_block_index = None\n    line_text_group_block_start_index = None\n    line_text_group_block_end_index = None\n    line_text_group_line_index = None\n    line_text_group_line_start_index = None\n    line_text_group_line_end_index = None\n    line_text_group_line_text = None\n    line_text_group_line_type = None\n    line_text_group_line_list_items = []\n    line_text_group_line_header_index = None\n    line_text_group_line_header_level = None\n    line_text_group_line_list_index = None\n    line_text_group_line_list_level = None\n    line_text_group_line_block_index ="}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    header_blocks = []\n    list_blocks = []\n    list_levels = []\n    header_index = 0\n    list_index = 0\n    list_item_index = 0\n    list_item_level = 0\n    list_item_start = 0\n    list_item_end = 0\n    list_item_text = \"\"\n    list_item_text_start = 0\n    list_item_text_end = 0\n    header_text = \"\"\n    header_text_start = 0\n    header_text_end = 0\n    header_text_level = 0\n    header_text_index = 0\n    header_start = 0\n    header_end = 0\n    header_level = 0\n    header_start_index = 0\n    header_end_index = 0\n    header_type = \"\"\n    header_start_index = 0\n    header_end_index = 0\n    header_text_index = 0\n    header_text_level = 0\n    header_text_start = 0\n    header_text_end = 0\n    header_text_index = 0\n    header_text_level = 0\n    header_text_start = 0\n    header_text_end = 0\n    header_text_index = 0\n    header_text_level = 0\n    header_text_start = 0\n    header_text_end = 0\n    header_text_index = 0\n    header_text_level = 0\n    header_text_start = 0\n    header_text_end = 0\n    header_text_index = 0\n    header_text_level = 0\n    header_text_start = 0\n    header_text_end = 0\n    header_text_index = 0\n    header_text_level = 0\n    header_text_start = 0\n    header_text_end = 0\n    header_text_index = 0\n    header_text_level = 0\n    header_text_start = 0\n    header_text_end = 0\n    header_text_index = 0\n    header_text_level = 0\n    header_text_start = 0"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    block_index = 0\n    block_type = None\n    block_start_index = None\n    header_index = None\n    list_index = None\n    list_level = None\n    list_items = []\n    list_item_index = 0\n    list_item_start_index = None\n    list_item_end_index = None\n    for i, line in enumerate(lines):\n        if should_skip(line, xml):\n            continue\n        if is_table_row(line):\n            continue\n        if not line.strip():\n            continue\n        if line.startswith(\"Table\"):\n            continue\n        if line.startswith(\"Figure\"):\n            continue\n        if line.startswith(\"Appendix\"):\n            continue\n        if line.startswith(\"References\"):\n            continue\n        if line.startswith(\"Acknowledgements\"):\n            continue\n        if line.startswith(\"Abstract\"):\n            continue\n        if line.startswith(\"Introduction\"):\n            continue\n        if line.startswith(\"Methods\"):\n            continue\n        if line.startswith(\"Results\"):\n            continue\n        if line.startswith(\"Discussion\"):\n            continue\n        if line.startswith(\"Conclusion\"):\n            continue\n        if line.startswith(\"Keywords\"):\n            continue\n        if line.startswith(\"Citation\"):\n            continue\n        if line.startswith(\"Author\"):\n            continue\n        if line.startswith(\"Title\"):\n            continue\n        if line.startswith(\"Journal\"):\n            continue\n        if line.startswith(\"Volume\"):\n            continue\n        if line.startswith(\"Issue\"):\n            continue\n        if line.startswith(\"Pages\"):\n            continue\n        if line.startswith(\"Year\"):\n            continue\n        if line.startswith(\"DOI\"):\n            continue\n        if line.startswith(\"URL\"):\n            continue\n        if line.startswith(\"Abstract\"):\n            continue\n       "}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    line_num = 0\n    block_index = 0\n    header_index = 0\n    block_type = \"paragraph\"\n    block_text = \"\"\n    block_start_index = 0\n    block_list = []\n    block_header_index = 0\n    block_indent = 0\n    block_list_level = 0\n    block_list_type = \"\"\n\n    for line in lines:\n        line = line.strip()\n        if should_skip(line, xml):\n            continue\n        if is_table_row(line):\n            block_type = \"table_row\"\n        elif line.startswith(\"Table\"):\n            block_type = \"table\"\n        elif line.startswith(\"Figure\"):\n            block_type = \"figure\"\n        elif line.startswith(\"List\"):\n            block_type = \"list\"\n            block_list_type = line.split(\" \")[1]\n        elif line.startswith(\"Table of Contents\"):\n            block_type = \"table_of_contents\"\n        elif line.startswith(\"Abstract\"):\n            block_type = \"abstract\"\n        elif line.startswith(\"Introduction\"):\n            block_type = \"introduction\"\n        elif line.startswith(\"Methods\"):\n            block_type = \"methods\"\n        elif line.startswith(\"Results\"):\n            block_type = \"results\"\n        elif line.startswith(\"Conclusion\"):\n            block_type = \"conclusion\"\n        elif line.startswith(\"References\"):\n            block_type = \"references\"\n        elif line.startswith(\"Acknowledgments\"):\n            block_type = \"acknowledgments\"\n        elif line.startswith(\"Appendix\"):\n            block_type = \"appendix\"\n        elif line.startswith(\"Figure\"):\n            block_type = \"figure\"\n        elif line.startswith(\"Table\"):\n            block_type = \"table\"\n        elif line.startswith(\"Figure\"):\n            block_type = \"figure\"\n        elif line.start"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Remove duplicates\n    lines = [line for line in set(lines) if not find_floating_chars(line)]\n\n    # Remove empty lines\n    lines = [line for line in lines if line.strip()]\n\n    # Remove lines with only numbers\n    lines = [line for line in lines if not re.match(r\"^\\d+$\", line)]\n\n    # Remove lines with only punctuation\n    lines = [line for line in lines if not re.match(r\"^[.,:;?!]+$\", line)]\n\n    # Remove lines with only a single word\n    lines = [line for line in lines if len(line.split()) > 1]\n\n    # Remove lines with only a single character\n    lines = [line for line in lines if len(line) > 1]\n\n    # Remove lines with only a single letter\n    lines = [line for line in lines if not re.match(r\"^[a-zA-Z]$\", line)]\n\n    # Remove lines with only a single digit\n    lines = [line for line in lines if not re.match(r\"^\\d$\", line)]\n\n    # Remove lines with only a single punctuation character\n    lines = [line for line in lines if not re.match(r\"^[.,:;?!]$\", line)]\n\n    # Remove lines with only a single space\n    lines = [line for line in lines if not re.match(r\"^\\s$\", line)]\n\n    # Remove lines with only a single word that starts with a lowercase letter\n    lines = [line for line in lines if not re.match(r\"^[a-z]\\s\", line)]\n\n    # Remove lines with only a single word that starts with a digit\n    lines = [line for line in lines if not re.match(r\"^\\d\\s\", line)]\n\n    # Remove lines with only a single word that starts with a punctuation character\n    lines = [line for line in lines if not re.match(r\"^[.,:;?!]\\s\", line)]\n\n    # Remove lines with only a single word that starts with a space\n    lines = ["}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    header_blocks = []\n    list_blocks = []\n    block_index = 0\n    header_index = 0\n    list_index = 0\n    block_level = 0\n    list_level = 0\n    list_item_index = 0\n    header_index_dict = defaultdict(list)\n    list_item_index_dict = defaultdict(list)\n    for line in lines:\n        line = line.strip()\n        if should_skip(line, xml):\n            continue\n        if is_table_row(line):\n            continue\n        line = line_parser.Line(line)\n        if line.is_header:\n            header_index += 1\n            header_blocks.append(line.text)\n            header_index_dict[line.text].append(header_index)\n            continue\n        if line.is_list_item:\n            list_index += 1\n            list_blocks.append(line.text)\n            list_item_index_dict[line.text].append(list_index)\n            continue\n        if line.is_list_start:\n            list_level += 1\n            list_index += 1\n            list_blocks.append(line.text)\n            list_item_index_dict[line.text].append(list_index)\n            continue\n        if line.is_list_end:\n            list_level -= 1\n            continue\n        if line.is_block_start:\n            block_level += 1\n            block_index += 1\n            continue\n        if line.is_block_end:\n            block_level -= 1\n            continue\n        if line.is_list_item:\n            list_item_index += 1\n            continue\n        if line.is_list_start:\n            list_level += 1\n            continue\n        if line.is_list_end:\n            list_level -= 1\n            continue\n        if line.is_block_start:\n            block_level += 1\n            continue\n        if line.is_block_end:\n            block_level -= 1\n            continue\n        if line."}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    header_blocks = []\n    list_blocks = []\n    current_block_index = 0\n    current_list_level = 0\n    current_header_block_index = 0\n    current_list_block_index = 0\n    current_block = []\n    current_list_block = []\n    current_header_block = []\n    header_block_index = None\n    list_block_index = None\n    list_block_level = None\n    list_block_text = None\n    header_block_text = None\n    header_block_level = None\n    header_block_index = None\n    block_index = None\n    block_text = None\n    block_type = None\n    block_start_index = None\n    block_end_index = None\n    block_list = None\n    block_header_index = None\n    block_indent = None\n    block_list_level = None\n    block_list_text = None\n    block_header_text = None\n    block_header_level = None\n    block_header_index = None\n    block_indent = None\n    block_list_level = None\n    block_list_text = None\n    block_header_text = None\n    block_header_level = None\n    block_header_index = None\n    block_indent = None\n    block_list_level = None\n    block_list_text = None\n    block_header_text = None\n    block_header_level = None\n    block_header_index = None\n    block_indent = None\n    block_list_level = None\n    block_list_text = None\n    block_header_text = None\n    block_header_level = None\n    block_header_index = None\n    block_indent = None\n    block_list_level = None\n    block_list_text = None\n    block_header_text = None\n    block_header_level = None\n    block_header_index = None\n    block_indent = None\n    block_list_level = None\n    block_list_text = None\n    block_header_text"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    headers = []\n    blocks_to_process = []\n    block_index = 0\n    header_index = 0\n    list_index = 0\n    block_start_index = 0\n    block_end_index = 0\n    block_type = \"\"\n    block_header_index = -1\n    block_level = 0\n    block_list_index = -1\n    block_text = \"\"\n\n    for index, line in enumerate(lines):\n        line = line.strip()\n        if not line:\n            continue\n        if should_skip(line, xml):\n            continue\n        if is_table_row(line):\n            continue\n        if find_floating_chars(line):\n            continue\n\n        blocks_to_process.append(line)\n\n    for index, line in enumerate(blocks_to_process):\n        line = line.strip()\n        if not line:\n            continue\n        if should_skip(line, xml):\n            continue\n        if is_table_row(line):\n            continue\n        if find_floating_chars(line):\n            continue\n        if line.startswith(\"-\"):\n            line = line[1:]\n\n        if line.startswith(\"\u2022\"):\n            line = line[1:]\n\n        if line.startswith(\"\u2022\"):\n            line = line[1:]\n\n        if line.startswith(\"\u27a2\"):\n            line = line[2:]\n\n        if line.startswith(\"\u2022\"):\n            line = line[1:]\n\n        if line.startswith(\"\u2022\"):\n            line = line[1:]\n\n        if line.startswith(\"\u2022\"):\n            line = line[1:]\n\n        if line.startswith(\"\u2022\"):\n            line = line[1:]\n\n        if line.startswith(\"\u2022\"):\n            line = line[1:]\n\n        if line.startswith(\"\u2022\"):\n            line = line[1:]\n\n        if line.startswith(\"\u2022\"):\n            line = line[1"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    prev_line = None\n    prev_index = None\n    prev_header_index = None\n    prev_list_index = None\n    prev_list_level = None\n    list_index = 0\n    list_level = 0\n    header_index = 0\n    index = 0\n    block_index = 0\n    block_list = []\n\n    for line in lines:\n        if should_skip(line, xml):\n            continue\n        line = line.strip()\n        if not line:\n            continue\n        if prev_line and line == prev_line:\n            continue\n        if prev_line and prev_index:\n            if prev_line.endswith(\".\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\"?\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\"!\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\";\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\",\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\":\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\")\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\"(\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\")\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\";\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\":\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\"-\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endswith(\".\"):\n                prev_line = prev_line[:-1]\n            if prev_line.endsw"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    blocks = []\n    block_index = 0\n    block_type = None\n    block_header_index = None\n    block_list_index = None\n    block_level = None\n    block_text = []\n    block_list = []\n    block_start_index = None\n\n    for i, line in enumerate(lines):\n        # Remove duplicate lines (ignoring numbers)\n        if (\n            i > 0\n            and len(lines[i - 1]) > 0\n            and len(lines[i]) > 0\n            and lines[i - 1] == lines[i]\n        ):\n            continue\n\n        # Remove lines with less than 3 characters\n        if len(line) <= 2:\n            continue\n\n        # Fix spaced characters\n        line = line.replace(\"  \", \" \")\n        line = line.replace(\"  \", \" \")\n        line = line.replace(\"  \", \" \")\n\n        # Connect incomplete lines\n        if len(block_text) > 0 and len(line) > 0:\n            block_text[-1] += \" \" + line\n            continue\n\n        # Check if the line is a table row\n        if is_table_row(line):\n            continue\n\n        # Check if the line is a header\n        if line_parser.Line(line).is_header:\n            block_header_index = block_index\n            block_type = \"header\"\n            block_level = line_parser.Line(line).header_level\n            block_start_index = i\n            block_text.append(line)\n            continue\n\n        # Check if the line is a list item\n        if line_parser.Line(line).is_list_item:\n            block_list_index = block_index\n            block_type = \"list\"\n            block_level = line_parser.Line(line).list_level\n            block_start_index = i\n            block_text.append(line)\n            continue\n\n        # Check if the line is a paragraph\n        if block_type is None:\n            block_type = \"paragraph\"\n            block_start_index = i\n            block_"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    texts = org_texts.replace(\"\\n\", \" \")\n    texts = texts.replace(\"-\", \" \")\n    texts = texts.replace(\"\u2013\", \" \")\n    texts = texts.replace(\"\u2014\", \" \")\n    texts = texts.replace(\"\u2026\", \" \")\n    texts = texts.replace(\"\u2026\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \" \")\n    texts = texts.replace(\"\u2022\", \""}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    text = org_texts\n    text = text.replace(\"\\n\", \" \")\n    text = text.replace(\"- \", \"\")\n    text = text.replace(\" -\", \"\")\n    text = text.replace(\" - \", \" \")\n    text = text.replace(\" -\", \" \")\n    text = text.replace(\"-\", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    texts = org_texts.replace(\"\\n\", \" \")\n\n    texts = space_rule.sub(r'\\1', texts)\n    texts = quotation_pattern.sub('\"', texts)\n\n    sentences = nltk_tokenzier.tokenize(texts)\n\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, s) for s in sentences]\n\n    sentences = [bracket_rule.sub(lambda x: x.group(0).replace(\" \", \"_\"), s) for s in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # replace quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # split by new line\n    texts = text.split(\"\\n\")\n\n    # split by brackets\n    texts = [bracket_rule.sub(r\"\\g<1>\", t) for t in texts]\n\n    # split by space\n    texts = [space_rule.sub(r\"\\1\", t) for t in texts]\n\n    # tokenize\n    texts = [nltk_tokenzier.tokenize(t) for t in texts]\n\n    # flatten\n    texts = [item for sublist in texts for item in sublist]\n\n    # apply rules\n    for rule, replaced in rules:\n        texts = [rule.sub(replaced, t) for t in texts]\n\n    # remove empty sentences\n    texts = [t for t in texts if t != \"\"]\n\n    return texts"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    text = org_texts.strip()\n\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # remove spaces between punctuation\n    text = space_rule.sub(r'\\1', text)\n\n    # tokenize paragraphs\n    paragraphs = text.split(\"\\n\\n\")\n\n    # tokenize sentences\n    sentences = []\n    for paragraph in paragraphs:\n        sentences += nltk_tokenzier.tokenize(paragraph)\n\n    # handle special cases\n    for i, sentence in enumerate(sentences):\n        if sentence.startswith(\"(\"):\n            # remove brackets\n            sentence = bracket_rule.sub(\"\", sentence)\n            sentences[i] = sentence\n\n        # apply rules\n        for rule, replaced in rules:\n            sentence = rule.sub(replaced, sentence)\n        sentences[i] = sentence\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If input is empty or None, return the input as is\n    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # Replace quotation marks with the same quotation marks\n    text = quotation_pattern.sub(\"'\", org_texts)\n\n    # Remove any space between punctuations (.')\n    text = space_rule.sub(r\"\\1\", text)\n\n    # Tokenize the text into sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # Replace abbreviations in the sentences\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    # Remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # Replace brackets in the sentences\n    sentences = [bracket_rule.sub(\"\", sent) for sent in sentences]\n\n    # Remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # Remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # Remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # Remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # Remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # Remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # Remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # Remove any space between punctuations (."}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # normalize quotation marks\n    texts = quotation_pattern.sub('\"', org_texts)\n\n    # tokenize paragraphs\n    texts = texts.replace(\"\\n\", \" \")\n\n    # remove space before punctuation\n    texts = space_rule.sub(r'\\1', texts)\n\n    # remove space before brackets\n    texts = bracket_rule.sub(r'\\1', texts)\n\n    # tokenize sentences\n    sentences = nltk_tokenzier.tokenize(texts)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, s) for s in sentences]\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Normalize quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # Tokenize text into sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # Remove any space between punctuations (.')\n    sentences = [space_rule.sub(r'\\1', sent) for sent in sentences]\n\n    # Handle special cases: paragraphs separated by new lines, punctuation at the beginning of the text\n    sentences = [sent.strip() for sent in sentences]\n    sentences = [sent for sent in sentences if sent]\n\n    # Handle special cases: sentences within brackets\n    sentences = [bracket_rule.sub(r'\\1', sent) for sent in sentences]\n\n    # Apply rules for abbreviations\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # remove quotation marks\n    org_texts = quotation_pattern.sub(\" \", org_texts)\n\n    # remove space between punctuation marks\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # tokenize sentences\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    # remove brackets and replace with space\n    sentences = [bracket_rule.sub(\" \", sent) for sent in sentences]\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # Check if the input is empty or None\n    if not org_texts:\n        return org_texts\n\n    # Normalize quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # Tokenize the text into sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # Apply normalization rules to each sentence\n    for i, sentence in enumerate(sentences):\n        # Normalize quotation marks\n        sentence = quotation_pattern.sub('\"', sentence)\n\n        # Apply the space rule to remove spaces between punctuation\n        sentence = space_rule.sub(r'\\1', sentence)\n\n        # Apply the bracket rule to replace content within brackets\n        sentence = bracket_rule.sub(r'\\1', sentence)\n\n        # Apply the rules to replace abbreviations\n        for rule, replaced in rules:\n            sentence = rule.sub(replaced, sentence)\n\n        # Remove leading and trailing spaces\n        sentence = sentence.strip()\n\n        # Replace the sentence in the original list\n        sentences[i] = sentence\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    texts = org_texts\n    # remove quotation marks\n    texts = quotation_pattern.sub(\"'\", texts)\n\n    # remove new lines\n    texts = texts.replace(\"\\n\", \" \")\n\n    # remove spaces at the beginning of the text\n    texts = re.sub(r\"^\\s+\", \"\", texts)\n\n    # remove spaces at the end of the text\n    texts = re.sub(r\"\\s+$\", \"\", texts)\n\n    # remove spaces between brackets\n    texts = bracket_rule.sub(r\"\\1\", texts)\n\n    # remove spaces between punctuation marks\n    texts = space_rule.sub(r\"\\1\", texts)\n\n    # normalize abbreviations\n    for rule, replaced in rules:\n        texts = rule.sub(replaced, texts)\n\n    # tokenize\n    sentences = nltk_tokenzier.tokenize(texts)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    text = org_texts\n\n    # remove all quotation marks\n    text = quotation_pattern.sub(\"\", text)\n\n    # normalize quotation marks\n    text = text.replace('\"', \"\u2019\")\n\n    # tokenize by paragraphs\n    paragraphs = text.split(\"\\n\")\n\n    # tokenize sentences within each paragraph\n    sentences = []\n    for paragraph in paragraphs:\n        # tokenize sentences\n        paragraph_sentences = nltk_tokenzier.tokenize(paragraph)\n        sentences.extend(paragraph_sentences)\n\n    # apply rules to sentences\n    for rule, replaced in rules:\n        for i, sentence in enumerate(sentences):\n            sentences[i] = rule.sub(replaced, sentence)\n\n    # remove spaces between punctuations\n    for i, sentence in enumerate(sentences):\n        sentences[i] = space_rule.sub(r\"\\1\", sentence)\n\n    # remove sentences within brackets\n    sentences = [sentence for sentence in sentences if not bracket_rule.search(sentence)]\n\n    # remove empty sentences\n    sentences = [sentence for sentence in sentences if sentence.strip() != \"\"]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Replace quotations\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # Normalize spaces\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    # Normalize quotations\n    text = re.sub(r'[\"\u201c\u201d]', '\"', text)\n\n    # Normalize apostrophes\n    text = re.sub(r\"['\u2018\u2019]\", \"'\", text)\n\n    # Normalize ellipses\n    text = re.sub(r\"\\.\\.\\.\", \"...\", text)\n\n    # Normalize dashes\n    text = re.sub(r\"\\s-\\s\", \" - \", text)\n    text = re.sub(r\"\\s--\\s\", \" -- \", text)\n\n    # Normalize double spaces\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    # Normalize new lines\n    text = re.sub(r\"\\n\", \" \", text)\n\n    # Normalize punctuation at the beginning of the text\n    text = re.sub(r\"^\\s*([.?!])\\s*\", r\"\\1 \", text)\n\n    # Normalize punctuation at the end of the text\n    text = re.sub(r\"\\s*([.?!])\\s*$\", r\"\\1\", text)\n\n    # Normalize punctuation in the middle of the text\n    text = re.sub(r\"\\s*([.?!])\\s*\", r\"\\1 \", text)\n\n    # Normalize punctuation in the middle of the text\n    text = re.sub(r\"\\s*([.?!])\\s*\", r\"\\1 \", text)\n\n    # Normalize punctuation in the middle of the text\n    text = re.sub(r\"\\s*([.?!])\\s*\", r\"\\1 \", text)\n\n    # Normalize punctuation in the middle of the text\n    text = re.sub(r\"\\s*([.?!])\\s*"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # remove quotation marks\n    text = quotation_pattern.sub(\"\", org_texts)\n\n    # remove extra spaces\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    # replace abbreviations\n    for rule, replaced in rules:\n        text = rule.sub(replaced, text)\n\n    # remove extra spaces\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    # tokenize into sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # remove sentences in brackets\n    sentences = [sent for sent in sentences if not bracket_rule.search(sent)]\n\n    # remove spaces between punctuations\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    texts = org_texts.replace(\"\\n\", \" \")\n\n    # remove quotation marks\n    texts = quotation_pattern.sub(\"\", texts)\n\n    # remove extra spaces\n    texts = space_rule.sub(r\"\\1\", texts)\n\n    # replace abbreviations\n    for rule, replaced in rules:\n        texts = rule.sub(replaced, texts)\n\n    # tokenize sentences\n    sentences = nltk_tokenzier.tokenize(texts)\n\n    # remove brackets\n    sentences = [bracket_rule.sub(\"\", sentence) for sentence in sentences]\n\n    # remove empty sentences\n    sentences = [sentence for sentence in sentences if sentence.strip() != \"\"]\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    text = org_texts\n\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # tokenize paragraphs\n    text = text.replace(\"\\n\", \" \")\n\n    # remove punctuation at the beginning of the text\n    text = space_rule.sub(r'\\1', text)\n\n    # tokenize sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # handle special cases within brackets\n    for i, sentence in enumerate(sentences):\n        if \"(\" in sentence:\n            sentences[i] = bracket_rule.sub(r\"\\1\", sentence)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sentence) for sentence in sentences]\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # replace quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # remove space between punctuation\n    text = space_rule.sub(r'\\1', text)\n\n    # replace abbreviations\n    for rule, replaced in rules:\n        text = rule.sub(replaced, text)\n\n    # remove brackets\n    text = bracket_rule.sub(\"\", text)\n\n    # tokenize sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # remove quotation marks\n    text = quotation_pattern.sub(\" \", org_texts)\n\n    # remove spaces at the beginning of the text\n    text = text.lstrip()\n\n    # remove spaces at the end of the text\n    text = text.rstrip()\n\n    # remove spaces between punctuations\n    text = space_rule.sub(r\"\\1\", text)\n\n    # normalize abbreviations\n    for rule, replaced in rules:\n        text = rule.sub(replaced, text)\n\n    # remove spaces at the beginning of the text\n    text = text.lstrip()\n\n    # remove spaces at the end of the text\n    text = text.rstrip()\n\n    # remove spaces between punctuations\n    text = space_rule.sub(r\"\\1\", text)\n\n    # replace brackets with a space\n    text = bracket_rule.sub(\" \", text)\n\n    # remove spaces at the beginning of the text\n    text = text.lstrip()\n\n    # remove spaces at the end of the text\n    text = text.rstrip()\n\n    # remove spaces between punctuations\n    text = space_rule.sub(r\"\\1\", text)\n\n    # split text into sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # replace quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # replace all abbreviations\n    for rule, replaced in rules:\n        text = rule.sub(replaced, text)\n\n    # replace all spaces between quotations\n    text = space_rule.sub(r'\\1', text)\n\n    # remove brackets\n    text = bracket_rule.sub(r\"\\1\", text)\n\n    # remove double spaces\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    # tokenize\n    sentences = nltk_tokenzier.tokenize(text)\n\n    return sentences\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # remove quotations\n    text = quotation_pattern.sub(\"\", org_texts)\n\n    # split paragraphs\n    paragraphs = text.split(\"\\n\")\n    sentences = []\n    for paragraph in paragraphs:\n        # remove space before punctuation\n        paragraph = space_rule.sub(r\"\\1\", paragraph)\n\n        # split by brackets\n        paragraph_sentences = bracket_rule.split(paragraph)\n\n        for i in range(len(paragraph_sentences)):\n            if i % 2 == 0:\n                # if it is an even index, it is a sentence\n                # tokenize the sentence\n                paragraph_sentences[i] = nltk_tokenzier.tokenize(paragraph_sentences[i])\n\n        # flatten the list\n        paragraph_sentences = [sentence for paragraph_sentence in paragraph_sentences for sentence in paragraph_sentence]\n\n        # replace abbreviations\n        for rule, replaced in rules:\n            paragraph_sentences = [rule.sub(replaced, sentence) for sentence in paragraph_sentences]\n\n        # remove empty sentences\n        paragraph_sentences = [sentence for sentence in paragraph_sentences if sentence != \"\"]\n\n        sentences.extend(paragraph_sentences)\n\n    return sentences\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is not None:\n                doc_id = self.term_mat.rows[key]\n                return self.posns.doc_encoded_posns(term_id, doc_id)\n            else:\n                return self.posns.term_encoded_posns(term_id)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        if key is None:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.positions(term_id)\n            except TermMissingError:\n                return []\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.positions(term_id, doc_id=key)\n            except TermMissingError:\n                return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.doc_encoded_posns(term_id, key)\n            except TermMissingError:\n                return np.array([])\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.encoded_posns(term_id)\n            except TermMissingError:\n                return np.array([])\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is not None:\n                if key < 0:\n                    key += len(self)\n                return self.posns.term_posns(term_id, doc_id=key)\n            else:\n                return self.posns.term_posns(term_id)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                doc_ids, term_posns = self.posns.term_posns(term_id, doc_ids=[key])\n                return term_posns\n            except TermMissingError:\n                return []\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                doc_ids, term_posns = self.posns.term_posns(term_id)\n                return term_posns\n            except TermMissingError:\n                return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        if key is not None and not isinstance(key, int):\n            raise TypeError(\"Expected an int for key\")\n\n        term_id = self.term_dict.get_term_id(token)\n        if key is not None:\n            return self.posns.doc_encoded_posns(term_id, doc_id=key)\n        else:\n            return self.posns.encoded_posns(term_id)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n\n        term_id = self.term_dict.get_term_id(token)\n        if key is None:\n            return self.posns.doc_encoded_posns(term_id)\n        else:\n            return self.posns.doc_encoded_posns(term_id, doc_id=key)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            # If key is None, return positions across all documents\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.positions(term_id)\n            except TermMissingError:\n                return []\n        else:\n            # If key is not None, return positions for the specified document\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.positions(term_id, doc_id=key)\n            except TermMissingError:\n                return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Get term freqs per token\n        token = self._check_token_arg(token)\n\n        if key is None:\n            term_id = self.term_dict.get_term_id(token)\n            posns = self.posns.term_posns(term_id)\n            return posns\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                posns = self.posns.doc_encoded_posns(term_id, doc_id=key)\n                return [posns]\n            except TermMissingError:\n                return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is not None:\n                if not isinstance(key, numbers.Integral):\n                    raise TypeError(\"Expected an integer key\")\n                if key < 0:\n                    key += len(self)\n                if key >= len(self):\n                    raise IndexError(\"index out of bounds\")\n                return self.posns.doc_encoded_posns(term_id, doc_id=key)\n            else:\n                return self.posns.encoded_posns(term_id)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if not isinstance(key, int):\n                raise ValueError(\"Key must be an integer\")\n            if key < 0:\n                key += len(self)\n            if key >= len(self):\n                raise IndexError(\"Key is out of bounds\")\n            if key < 0:\n                raise IndexError(\"Key is out of bounds\")\n\n        if not isinstance(token, str):\n            raise ValueError(\"Token must be a string\")\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is not None:\n                return self.posns.doc_encoded_posns(term_id, doc_id=key)\n            else:\n                return self.posns.encoded_posns(term_id)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Check if the token is a string\n        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n\n        # Get term id\n        try:\n            term_id = self.term_dict.get_term_id(token)\n        except TermMissingError:\n            return []\n\n        # If a key is provided, get the positions for that document only\n        if key is not None:\n            return self.posns.doc_encoded_posns(term_id, key)\n\n        # If no key is provided, get the positions across all documents\n        return self.posns.encoded_posns(term_id)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if isinstance(key, int):\n                # If the key is an integer, we need to convert it to a slice\n                key = slice(key, key + 1)\n            elif isinstance(key, slice):\n                # If the key is a slice, we need to convert it to a list of integers\n                key = list(range(key.start, key.stop, key.step))\n            else:\n                raise TypeError(\"key must be an integer or a slice\")\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                return self.posns.all_doc_encoded_posns(term_id)\n            else:\n                return self.posns.doc_encoded_posns(term_id, key)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if not isinstance(key, int):\n                raise TypeError(\"Expected an integer for key\")\n            if key < 0:\n                key += len(self)\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                posns = self.posns.doc_encoded_posns(term_id, doc_id=key)\n                return [posns]\n            except TermMissingError:\n                return [np.array([])]\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                posns = self.posns.encoded_posns(term_id)\n                return posns\n            except TermMissingError:\n                return [np.array([])]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if isinstance(token, str):\n            token = [token]\n        if key is not None:\n            if isinstance(key, int):\n                key = [key]\n            if isinstance(key, np.ndarray):\n                key = key.tolist()\n            if isinstance(key, list):\n                if len(key) == 0:\n                    return []\n                elif len(key) == 1:\n                    return self.posns.doc_encoded_posns(token, doc_id=key[0])\n                else:\n                    return [self.posns.doc_encoded_posns(token, doc_id=doc_id) for doc_id in key]\n            else:\n                raise TypeError(\"Key must be a list of integers, a numpy array of integers, or a single integer\")\n        else:\n            return self.posns.doc_encoded_posns(token)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if isinstance(key, numbers.Integral):\n                key = np.asarray([key], dtype=int)\n            elif isinstance(key, list):\n                key = np.asarray(key, dtype=int)\n            elif isinstance(key, np.ndarray):\n                key = key\n            else:\n                raise TypeError(\"Expected a list of ints or a single int\")\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                return self.posns.positions(term_id)\n            else:\n                return self.posns.positions(term_id, doc_ids=key)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return self[key].positions(token)\n\n        # For each document, get the positions\n        return [x.positions(token) for x in self[:]]\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Get term freqs per token\n        token = self._check_token_arg(token)\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is not None:\n                return self.posns.doc_encoded_posns(term_id, key)\n            else:\n                return self.posns.term_encoded_posns(term_id)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        term_id = self.term_dict.get_term_id(token)\n        if key is not None:\n            return self.posns.doc_encoded_posns(term_id, doc_id=key)\n        else:\n            return self.posns.term_encoded_posns(term_id)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is not None:\n                # We want to return positions for a specific document\n                return self.posns.doc_encoded_posns(term_id, doc_id=key)\n            else:\n                # We want to return positions for all documents\n                return self.posns.term_encoded_posns(term_id)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"0\":\n        return 0\n\n    if spec == \"1\":\n        return 1\n\n    if spec == \"2\":\n        return 2\n\n    if spec == \"3\":\n        return 3\n\n    if spec == \"4\":\n        return 4\n\n    if spec == \"5\":\n        return 5\n\n    if spec == \"6\":\n        return 6\n\n    if spec == \"7\":\n        return 7\n\n    if spec == \"8\":\n        return 8\n\n    if spec == \"9\":\n        return 9\n\n    if spec == \"10\":\n        return 10\n\n    if spec == \"11\":\n        return 11\n\n    if spec == \"12\":\n        return 12\n\n    if spec == \"13\":\n        return 13\n\n    if spec == \"14\":\n        return 14\n\n    if spec == \"15\":\n        return 15\n\n    if spec == \"16\":\n        return 16\n\n    if spec == \"17\":\n        return 17\n\n    if spec == \"18\":\n        return 18\n\n    if spec == \"19\":\n        return 19\n\n    if spec == \"20\":\n        return 20\n\n    if spec == \"100%\":\n        return num_clauses\n\n    if spec == \"100\":\n        return num_clauses\n\n    if spec == \"100.0\":\n        return num_clauses\n\n    if spec == \"100.00\":\n        return num_clauses\n\n    if spec == \"100.000\":\n        return num_clauses\n\n    if spec == \"100.0000\":\n        return num_clauses\n\n    if spec == \"100.00000\":\n        return num_clauses\n\n    if spec == \"100.000000\":\n        return num_clauses\n\n    if spec == \"100.0"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Remove whitespace and convert to lowercase\n    spec = spec.strip().lower()\n\n    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        percent = float(spec[:-1])\n        return int(num_clauses * percent / 100)\n\n    # Check if the spec is a conditional expression\n    if '<' in spec:\n        parts = spec.split('<')\n        if len(parts) != 2:\n            raise ValueError('Invalid min should match spec: {}'.format(spec))\n        min_clauses = int(parts[0].strip())\n        percent = float(parts[1][:-1])\n        return int(min_clauses + num_clauses * percent / 100)\n\n    # Otherwise, the spec is an absolute number\n    return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == '0':\n        return 0\n    elif spec == '1':\n        return 1\n    elif spec == '100%':\n        return num_clauses\n    elif spec.endswith('%'):\n        percentage = int(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif '<' in spec:\n        parts = spec.split('<')\n        threshold = int(parts[0])\n        percentage = int(parts[1][:-1])\n        return threshold if num_clauses < threshold else int(num_clauses * percentage / 100)\n    else:\n        return int(spec)\n\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        # percentage-based specification\n        return int(num_clauses * float(spec[:-1]) / 100)\n    elif spec.endswith(\"<\"):\n        # conditional specification\n        return int(num_clauses * float(spec[:-1]) / 100)\n    else:\n        # absolute number specification\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        percentage = int(spec[:-1])\n        return int(num_clauses * percentage / 100)\n\n    # Check if the spec is a conditional expression\n    if '<' in spec:\n        parts = spec.split('<')\n        if len(parts) != 2:\n            raise ValueError(\"Invalid 'min should match' specification: {}\".format(spec))\n        if parts[0].isdigit():\n            return int(parts[0])\n        else:\n            percentage = int(parts[0][:-1])\n            return int(num_clauses * percentage / 100)\n\n    # If the spec is not a percentage or conditional expression, it must be an absolute number\n    return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Remove leading and trailing whitespace\n    spec = spec.strip()\n\n    # Check if the spec is an absolute number\n    if spec.isdigit():\n        return int(spec)\n\n    # Check if the spec is a percentage\n    if spec.endswith(\"%\"):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n\n    # Check if the spec is a conditional expression\n    if \"<\" in spec:\n        parts = spec.split(\"<\")\n        if len(parts) != 2:\n            raise ValueError(\"Invalid 'mm' spec: {}\".format(spec))\n        threshold, condition = parts\n        if threshold.isdigit():\n            threshold = int(threshold)\n        else:\n            raise ValueError(\"Invalid 'mm' spec: {}\".format(spec))\n        if condition.isdigit():\n            condition = int(condition)\n        else:\n            raise ValueError(\"Invalid 'mm' spec: {}\".format(spec))\n        return threshold if num_clauses < condition else 0\n\n    # If none of the above conditions are met, the spec is invalid\n    raise ValueError(\"Invalid 'mm' spec: {}\".format(spec))\n\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into individual parts\n    parts = spec.split(\" \")\n\n    # Initialize the minimum number of clauses to match\n    min_should_match = 0\n\n    # Iterate over the parts of the spec\n    for part in parts:\n        # Check if the part is a percentage\n        if \"%\" in part:\n            # Calculate the minimum number of clauses to match based on the percentage\n            min_should_match = int(num_clauses * float(part[:-1]) / 100)\n        # Check if the part is a conditional expression\n        elif \"<\" in part:\n            # Split the part into the number of clauses and the condition\n            num_clauses, condition = part.split(\"<\")\n            # Convert the number of clauses to an integer\n            num_clauses = int(num_clauses)\n            # Calculate the minimum number of clauses to match based on the condition\n            if condition == \"min\":\n                min_should_match = num_clauses\n            elif condition == \"max\":\n                min_should_match = num_clauses - 1\n        # Otherwise, the part is an absolute number\n        else:\n            # Convert the part to an integer\n            min_should_match = int(part)\n\n    # Return the minimum number of clauses to match\n    return min_should_match\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec is None:\n        return 1\n\n    if spec.isnumeric():\n        return int(spec)\n\n    if \"<\" in spec:\n        parts = spec.split(\"<\")\n        if len(parts) != 2:\n            raise ValueError(f\"Invalid 'mm' spec: {spec}\")\n        return int(parts[1])\n\n    if \"%\" in spec:\n        parts = spec.split(\"%\")\n        if len(parts) != 2:\n            raise ValueError(f\"Invalid 'mm' spec: {spec}\")\n        return int(num_clauses * float(parts[0]) / 100)\n\n    raise ValueError(f\"Invalid 'mm' spec: {spec}\")\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec string into its components\n    components = spec.split()\n\n    # Initialize the minimum number of clauses to match to the total number of clauses\n    min_clauses = num_clauses\n\n    # Iterate over the components of the spec string\n    for component in components:\n        # Check if the component is an absolute number\n        if component.isdigit():\n            min_clauses = int(component)\n        # Check if the component is a percentage\n        elif component.endswith(\"%\"):\n            percent = int(component[:-1])\n            min_clauses = int(num_clauses * percent / 100)\n        # Check if the component is a conditional expression\n        elif \"<\" in component:\n            left, right = component.split(\"<\")\n            if left.isdigit():\n                min_clauses = int(left)\n            elif left.endswith(\"%\"):\n                percent = int(left[:-1])\n                min_clauses = int(num_clauses * percent / 100)\n            if right.isdigit():\n                min_clauses = min(min_clauses, int(right))\n            elif right.endswith(\"%\"):\n                percent = int(right[:-1])\n                min_clauses = min(min_clauses, int(num_clauses * percent / 100))\n\n    # Return the minimum number of clauses to match\n    return min_clauses\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"0\":\n        return 0\n    elif spec == \"1\":\n        return 1\n    elif spec == \"2\":\n        return 2\n    elif spec == \"3\":\n        return 3\n    elif spec == \"4\":\n        return 4\n    elif spec == \"5\":\n        return 5\n    elif spec == \"6\":\n        return 6\n    elif spec == \"7\":\n        return 7\n    elif spec == \"8\":\n        return 8\n    elif spec == \"9\":\n        return 9\n    elif spec == \"10\":\n        return 10\n    elif spec == \"100%\":\n        return num_clauses\n    elif spec.startswith(\"-\"):\n        return num_clauses - int(spec[1:])\n    elif spec.endswith(\"%\"):\n        return int(num_clauses * float(spec[:-1]) / 100)\n    elif \"<\" in spec:\n        parts = spec.split(\"<\")\n        return int(parts[0]) if int(parts[1]) < num_clauses else int(parts[1])\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Remove spaces and brackets from the spec string\n    spec = spec.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n\n    # Split the spec string into clauses\n    clauses = spec.split(\"+\")\n\n    # Calculate the minimum number of clauses that must match based on the spec\n    if spec.startswith(\"<\"):\n        # Conditional spec: minimum number of clauses must be less than the specified value\n        min_should_match = int(clauses[0][1:])\n    elif \"%\" in spec:\n        # Percentage-based spec: calculate the minimum number of clauses based on the percentage\n        percentage = int(clauses[0][:-1])\n        min_should_match = int(num_clauses * percentage / 100)\n    else:\n        # Absolute number spec: minimum number of clauses must be equal to the specified value\n        min_should_match = int(clauses[0])\n\n    return min_should_match\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == '0':\n        return 0\n    elif spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif '<' in spec:\n        parts = spec.split('<')\n        if len(parts) != 2:\n            raise ValueError(f\"Invalid 'min should match' specification: {spec}\")\n        threshold, condition = parts\n        if threshold.endswith('%'):\n            percentage = float(threshold[:-1])\n            threshold = int(num_clauses * percentage / 100)\n        else:\n            threshold = int(threshold)\n        if condition == '1':\n            return threshold\n        elif condition == '2':\n            return num_clauses - threshold\n        else:\n            raise ValueError(f\"Invalid 'min should match' specification: {spec}\")\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Remove whitespace and split the spec into clauses\n    clauses = spec.replace(\" \", \"\").split(\"+\")\n\n    # Calculate the minimum number of clauses that must match\n    min_should_match = 0\n    for clause in clauses:\n        if clause.endswith(\"%\"):\n            # Percentage-based spec\n            percentage = int(clause[:-1])\n            min_should_match += int(num_clauses * percentage / 100)\n        elif \"<\" in clause:\n            # Conditional spec\n            parts = clause.split(\"<\")\n            if int(parts[0]) <= num_clauses:\n                min_should_match += int(parts[1])\n        else:\n            # Absolute number\n            min_should_match += int(clause)\n\n    return min_should_match\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif '<' in spec:\n        num_clauses, mm = spec.split('<')\n        num_clauses = int(num_clauses)\n        mm = int(mm)\n        return num_clauses - mm\n    else:\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into its components\n    components = spec.split()\n\n    # Initialize the minimum number of clauses that must match\n    min_should_match = 0\n\n    # Iterate over the components and calculate the minimum number of clauses that must match\n    for component in components:\n        # Check if the component is a percentage\n        if component.endswith('%'):\n            # Calculate the minimum number of clauses that must match based on the percentage\n            min_should_match = int(num_clauses * float(component[:-1]) / 100)\n        # Check if the component is a conditional expression\n        elif component.startswith('<'):\n            # Split the component into its parts\n            parts = component[1:].split(':')\n            # Calculate the minimum number of clauses that must match based on the conditional expression\n            if parts[0] == 'num_clauses':\n                min_should_match = int(parts[1])\n            elif parts[0] == 'percent':\n                min_should_match = int(num_clauses * float(parts[1]) / 100)\n        # Otherwise, the component is an absolute number\n        else:\n            min_should_match = int(component)\n\n    # Return the minimum number of clauses that must match\n    return min_should_match\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into its components\n    components = spec.split()\n\n    # Initialize the minimum number of clauses to match\n    min_should_match = 0\n\n    # Iterate over the components of the spec\n    for component in components:\n        # Check if the component is a percentage\n        if '%' in component:\n            # Calculate the minimum number of clauses to match based on the percentage\n            min_should_match = int(float(component[:-1]) * num_clauses / 100)\n        # Check if the component is a conditional expression\n        elif '<' in component:\n            # Split the component into its parts\n            parts = component.split('<')\n            # Calculate the minimum number of clauses to match based on the conditional expression\n            min_should_match = int(float(parts[0]) * num_clauses / float(parts[1]))\n        # If the component is not a percentage or conditional expression, it must be an absolute number\n        else:\n            # Set the minimum number of clauses to match to the absolute number\n            min_should_match = int(component)\n\n    # Return the minimum number of clauses to match\n    return min_should_match\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        # percentage-based specification\n        percentage = int(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif spec.endswith('<'):\n        # conditional specification\n        condition = int(spec[:-1])\n        return condition\n    else:\n        # absolute number specification\n        return int(spec)\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec string into clauses and conditions\n    clauses = spec.split(' ')\n\n    # Initialize the minimum number of clauses to match\n    min_should_match = num_clauses\n\n    # Iterate over the clauses\n    for clause in clauses:\n        # Check if the clause is a percentage\n        if clause.endswith('%'):\n            # Calculate the minimum number of clauses to match based on the percentage\n            min_should_match = int(num_clauses * float(clause[:-1]) / 100)\n            break\n        # Check if the clause is a conditional expression\n        elif clause.startswith('<'):\n            # Split the clause into the condition and the number of clauses to match\n            condition, num_clauses_to_match = clause[1:].split(':')\n            # Check if the condition is satisfied\n            if condition == 'numTerms':\n                # Calculate the minimum number of clauses to match based on the number of clauses to match\n                min_should_match = int(num_clauses_to_match)\n                break\n\n    return min_should_match\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec string into a list of clauses\n    clauses = spec.split(\" \")\n\n    # Initialize the minimum number of clauses to match to 0\n    min_match = 0\n\n    # Loop through each clause\n    for clause in clauses:\n\n        # If the clause is a percentage, calculate the minimum number of clauses to match based on the percentage\n        if clause.endswith(\"%\"):\n            percentage = int(clause[:-1])\n            min_match += int(percentage / 100 * num_clauses)\n\n        # If the clause is a number, set the minimum number of clauses to match to the number\n        elif clause.isdigit():\n            min_match += int(clause)\n\n        # If the clause is a conditional expression, parse it and calculate the minimum number of clauses to match based on the condition\n        elif \"<\" in clause:\n            parts = clause.split(\"<\")\n            if parts[0].isdigit():\n                min_match += int(parts[0])\n            elif parts[0].endswith(\"%\"):\n                percentage = int(parts[0][:-1])\n                min_match += int(percentage / 100 * num_clauses)\n            if parts[1].isdigit():\n                min_match += int(parts[1])\n            elif parts[1].endswith(\"%\"):\n                percentage = int(parts[1][:-1])\n                min_match += int(percentage / 100 * num_clauses)\n\n    # Return the minimum number of clauses to match\n    return min_match\n\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into individual components\n    components = spec.split()\n\n    # Initialize the minimum number of clauses to match\n    min_should_match = 0\n\n    # Iterate over each component and calculate the minimum number of clauses to match\n    for component in components:\n        # Check if the component is a percentage\n        if '%' in component:\n            # Calculate the percentage of clauses to match\n            percentage = float(component.replace('%', '')) / 100\n            min_should_match += int(percentage * num_clauses)\n        # Check if the component is a conditional expression\n        elif '<' in component:\n            # Split the component into the number of clauses and the condition\n            clause_num, condition = component.split('<')\n            # Calculate the minimum number of clauses to match based on the condition\n            if condition == 'min':\n                min_should_match = int(clause_num)\n            elif condition == 'max':\n                min_should_match = num_clauses - int(clause_num)\n        # If the component is not a percentage or conditional expression, assume it is an absolute number\n        else:\n            min_should_match += int(component)\n\n    # Return the minimum number of clauses to match\n    return min_should_match\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # Check if all tokens are unique\n        if len(set(tokens)) != len(tokens):\n            return self.phrase_freq_non_unique(tokens, slop)\n\n        # Check if slop is 1\n        if slop == 1:\n            # Calculate phrase frequencies directly using positions\n            return compute_phrase_freqs(self.term_mat, self.posns, tokens, self.term_dict)\n        else:\n            # Delegate to another method for non-unique tokens or non-slop=1 phrases\n            return self.phrase_freq_non_unique(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If slop is 1 and all tokens are unique, we can use positions\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            # Get term ids\n            term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n            # Get the positions of the terms\n            term_posns = [self.positions(token) for token in tokens]\n            # Calculate the phrase frequencies\n            phrase_freqs = compute_phrase_freqs(term_ids, term_posns, self.term_mat.rows)\n            return phrase_freqs\n        else:\n            # Delegate to another method\n            return self._phrase_freq_with_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If slop is 1 and tokens are unique, we can use positions to calculate phrase freqs\n            # If slop is 1 and tokens are not unique, we can't use positions to calculate phrase freqs\n            # If slop is not 1, we can't use positions to calculate phrase freqs\n            return compute_phrase_freqs(self, tokens)\n        else:\n            return self._phrase_freq_with_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_with_positions(tokens)\n        else:\n            return self._phrase_freq_with_scan_merge(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If slop is 1 and all tokens are unique, we can directly calculate the phrase frequencies using the positions of terms\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            # Get term frequencies for each token\n            term_freqs = [self.termfreqs(token) for token in tokens]\n\n            # Calculate the phrase frequencies using the positions of terms\n            phrase_freqs = compute_phrase_freqs(term_freqs, tokens, self.posns, self.term_dict)\n\n            return phrase_freqs\n\n        # If slop is not 1 or tokens are not unique, we need to delegate the calculation to another method\n        return self._phrase_freq_with_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If the slop is 1 and all tokens are unique, we can directly calculate the phrase frequencies using the positions of terms.\n            term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n            term_freqs = [self.termfreqs(token) for token in tokens]\n            doc_lens = self.doclengths()\n            phrase_freqs = compute_phrase_freqs(term_ids, term_freqs, doc_lens, self.avg_doc_length)\n            return phrase_freqs\n        else:\n            # If the slop is not 1 or tokens are not unique, we delegate the calculation to another method that handles different slops or non-unique tokens.\n            return self.phrase_freq_slop_gt1(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(tokens) == len(set(tokens)):\n            # Calculate phrase frequencies using positions\n            return compute_phrase_freqs(self, tokens)\n        else:\n            # Delegate to another method for non-unique tokens or slop > 1\n            return self.phrase_freq_non_unique(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return compute_phrase_freqs(self, tokens)\n        else:\n            return self._phrase_freq_slow(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # Check if all tokens are unique\n        if len(set(tokens)) != len(tokens):\n            # If not, delegate to another method\n            return compute_phrase_freqs(self, tokens, slop)\n        else:\n            # If tokens are unique, calculate phrase frequencies directly\n            # Get term ids\n            term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n            # Get positions of terms\n            positions = [self.positions(token) for token in tokens]\n            # Compute phrase frequencies\n            return scan_merge_ins(positions, term_ids, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If slop is 1 and all tokens are unique, we can use positions to directly calculate phrase freqs\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            # Get the positions of each token\n            token_positions = [self.positions(token) for token in tokens]\n            # Calculate the phrase frequencies using the positions\n            phrase_freqs = compute_phrase_freqs(token_positions, self.doc_lens, self.term_dict)\n            return phrase_freqs\n        else:\n            # Delegate to another method to handle different slops or non-unique tokens\n            return self._phrase_freq_with_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If the slop is 1 and all tokens are unique, we can directly calculate the phrase frequencies using the positions of terms.\n            # This is the fastest path for the case where the slop is 1 and tokens are unique.\n            return compute_phrase_freqs(self, tokens)\n        else:\n            # If the slop is not 1 or tokens are not unique, we delegate the calculation to another method that handles different slops or non-unique tokens.\n            return self._phrase_freq(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If the slop is 1 and all tokens are unique, attempt to directly calculate the phrase frequencies\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            # Calculate the phrase frequencies using the positions of terms\n            return compute_phrase_freqs(self, tokens)\n        else:\n            # Delegate the calculation to another method that handles different slops or non-unique tokens\n            return self.phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return compute_phrase_freqs(self, tokens)\n        else:\n            return self._phrase_freq_generic(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If the slop is 1 and all tokens are unique, we can directly calculate the phrase frequencies using the positions of terms.\n            return compute_phrase_freqs(self, tokens)\n        else:\n            # If the slop is not 1 or tokens are not unique, we delegate the calculation to another method that handles different slops or non-unique tokens.\n            return self._phrase_freq_with_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # Check if all tokens are unique\n        if len(set(tokens)) == len(tokens):\n            # Calculate phrase frequencies using positions of terms\n            return compute_phrase_freqs(self, tokens, slop)\n        else:\n            # Delegate to another method to handle different slops or non-unique tokens\n            return self.phrase_freq_multiple_tokens(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # Check if slop is 1 and all tokens are unique\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            # Calculate phrase frequencies using positions of terms\n            return compute_phrase_freqs(tokens, self.posns, self.term_dict)\n        else:\n            # Delegate to another method to handle different slops or non-unique tokens\n            return self.phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If slop is 1 and all tokens are unique, we can directly calculate phrase frequencies using positions\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return compute_phrase_freqs(self, tokens)\n        else:\n            return self._phrase_freq_non_unique_tokens(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If the slop is 1 and all tokens are unique, calculate phrase frequencies directly\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return compute_phrase_freqs(tokens, self.term_dict, self.posns)\n\n        # If the slop is not 1 or tokens are not unique, delegate to another method\n        return scan_merge_ins(tokens, self.term_dict, self.posns, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If slop is 1 and all tokens are unique, use positions\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return compute_phrase_freqs(self, tokens)\n        else:\n            # If slop is not 1 or tokens are not unique, use scan_merge\n            return scan_merge_ins(self, tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If the slop is 1 and all tokens are unique, we can calculate the phrase frequencies directly\n            return compute_phrase_freqs(self.posns, self.term_dict, tokens)\n        else:\n            # If the slop is not 1 or tokens are not unique, we delegate the calculation to another method\n            return self.phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        return build_index_from_tokenizer(array, cls, tokenizer, truncate, batch_size, avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check dtype, raise TypeError\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check truncate, raise TypeError\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(truncate)))\n\n        # Check batch_size, raise TypeError\n        if not isinstance(batch_size, numbers.Number):\n            raise TypeError(\"Expected number, got {}\".format(type(batch_size)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check batch_size, raise TypeError\n        if not isinstance(batch_size, numbers.Number):\n            raise TypeError(\"Expected number, got {}\".format(type(batch_size)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check truncate, raise TypeError\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(truncate)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check batch_size, raise TypeError\n        if not isinstance(batch_size, numbers.Number):\n            raise TypeError(\"Expected number, got {}\".format(type(batch_size)))\n\n        #"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check dtype, raise TypeError\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check truncate, raise TypeError\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(truncate)))\n\n        # Check batch_size, raise TypeError\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected int, got {}\".format(type(batch_size)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check truncate, raise TypeError\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(truncate)))\n\n        # Check batch_size, raise TypeError\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected int, got {}\".format(type(batch_size)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check batch_size, raise ValueError\n        if batch_size <= 0:\n            raise ValueError(\"Expected positive int, got {}\".format(batch_size))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check truncate, raise TypeError\n        if not"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        return cls(array, tokenizer=tokenizer, truncate=truncate, batch_size=batch_size, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        return cls(array, tokenizer=tokenizer, truncate=truncate, batch_size=batch_size, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check dtype, raise TypeError\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(truncate)))\n\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected int, got {}\".format(type(batch_size)))\n\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        if truncate:\n            logger.info(\"Truncating data to fit within memory constraints.\")\n\n        # Check batch_size, raise TypeError\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected int, got {}\".format(type(batch_size)))\n\n        # Check avoid_copies, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check tokenizer, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check dtype, raise TypeError\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if truncate:\n            array = array[:batch_size]\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_terms_list(array, Terms,\n                                                                                           batch_size=batch_size,\n                                                                                           avoid_copies=avoid_copies)\n\n        return cls(term_mat, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        return build_index_from_tokenizer(array, cls, tokenizer, truncate, batch_size, avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        return build_index_from_tokenizer(array, cls, tokenizer=tokenizer,\n                                          truncate=truncate, batch_size=batch_size,\n                                          avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        return cls(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        return build_index_from_tokenizer(array, cls, tokenizer, truncate, batch_size, avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if truncate:\n            array = array[:100000]\n\n        if not isinstance(array, np.ndarray):\n            array = np.array(array)\n\n        if len(array) > batch_size:\n            return cls.index(array[:batch_size], tokenizer=tokenizer,\n                             truncate=truncate, batch_size=batch_size, avoid_copies=avoid_copies) + \\\n                   cls.index(array[batch_size:], tokenizer=tokenizer,\n                             truncate=truncate, batch_size=batch_size, avoid_copies=avoid_copies)\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = \\\n            build_index_from_terms_list(array, Terms, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        return cls(term_mat, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(\n            array, tokenizer, truncate=truncate, batch_size=batch_size, avoid_copies=avoid_copies,\n            index_cls=Terms)\n        return cls(term_mat, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if truncate:\n            array = take(array, batch_size)\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(array,\n                                                                                          tokenizer,\n                                                                                          Terms,\n                                                                                          avoid_copies=avoid_copies)\n        return cls(term_mat, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        term_mat, posns, \\\n            term_dict, avg_doc_length, \\\n            doc_lens = build_index_from_tokenizer(array, tokenizer,\n                                                  truncate=truncate,\n                                                  batch_size=batch_size,\n                                                  avoid_copies=avoid_copies)\n\n        return cls(term_mat, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        return build_index_from_tokenizer(array, cls, tokenizer, truncate, batch_size, avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        return build_index_from_tokenizer(array, tokenizer, truncate, batch_size, avoid_copies, cls)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check dtype, raise TypeError\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        return build_index_from_tokenizer(array, cls, tokenizer, truncate, batch_size, avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        term_mat, posns, term_dict, avg_doc_length, doc_lens = \\\n            build_index_from_tokenizer(array, tokenizer, truncate, batch_size, avoid_copies, Terms)\n        return cls(term_mat, tokenizer, avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, Iterable):\n            raise TypeError(\"Expected iterable object, got {}\".format(type(array)))\n\n        return build_index_from_tokenizer(array, cls, tokenizer, truncate, batch_size, avoid_copies)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['proxyHost'], self.config['proxyPort'], self.config['serverHost'], self.config['serverPort'], self.config['strategy'], self.config['strategies'])\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server.start()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config.get('proxyHost', '127.0.0.1'),\n            self.config.get('proxyPort', 8888),\n            self.config.get('serverHost', '127.0.0.1'),\n            self.config.get('serverPort', 25500),\n            self.config.get('autoCloseConnections', True),\n            self.config.get('multipleConnections', True),\n            self.logger\n        )\n        self.connections = {}\n        self.lock = threading.Lock()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            host=self.config['proxyHost'],\n            port=self.config['proxyPort'],\n            strategy=self.config['strategy'],\n            strategies=self.config['strategies'],\n            auto_close_connections=self.config['autoCloseConnections'],\n            multiple_connections=self.config['multipleConnections'],\n        )\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['autoCloseConnections'],\n            self.config['multipleConnections'],\n        )\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Setup server\n        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['autoCloseConnections'],\n            self.config['multipleConnections'],\n            self.config['strategy'],\n            self.config['strategies'][self.config['strategy']],\n        )\n\n        # Start server\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n\n        # Initialize lock\n        self.lock = threading.Lock()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server.start()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.connections = {}\n        self.lock = threading.Lock()\n        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['strategy'],\n            self.config['strategies'],\n            self.config['autoCloseConnections'],\n            self.config['multipleConnections'],\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.logger\n        )\n        self.server.start()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.logger.info('Server started at %s:%d', self.config['serverHost'], self.config['serverPort'])\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info('Initializing ProxifierMessageInterceptor')\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server = Server(self.config['proxyHost'], self.config['proxyPort'], self.config['serverHost'], self.config['serverPort'], self.config['strategy'], self.config['strategies'], self.config['autoCloseConnections'], self.config['multipleConnections'])\n        self.server.start()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info('Starting server')\n\n        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['autoCloseConnections'],\n        )\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server.start()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['autoCloseConnections'])\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server.start()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['strategy'],\n            self.config['strategies'],\n            self.config['autoCloseConnections'],\n            self.config['multipleConnections']\n        )\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server.start()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Initialize server\n        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['autoCloseConnections'])\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n\n        # Initialize lock\n        self.lock = threading.Lock()\n\n        # Log\n        self.log.info(f'ProxifierMessageInterceptor initialized')\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Set up server\n        self.server = Server(\n            host=self.config['proxyHost'],\n            port=self.config['proxyPort'],\n            auto_close_connections=self.config['autoCloseConnections'],\n            multiple_connections=self.config['multipleConnections'],\n        )\n\n        # Set up strategies\n        strategies = self.config['strategies']\n        if self.config['strategy'] in strategies:\n            strategy = strategies[self.config['strategy']]\n        else:\n            strategy = {}\n\n        self.server.set_strategy(\n            self.config['strategy'],\n            strategy,\n        )\n\n        # Start server\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n\n        # Initialize lock\n        self.lock = threading.Lock()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            host=self.config['serverHost'],\n            port=self.config['serverPort'],\n            auto_close_connections=self.config['autoCloseConnections'],\n            multiple_connections=self.config['multipleConnections'],\n        )\n\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['autoCloseConnections'])\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server.start()\n\n        self.log.info(f'Proxifier server started on {self.config[\"serverHost\"]}:{self.config[\"serverPort\"]}')\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info('Initializing Proxifier interceptor')\n\n        self.server = Server(\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['autoCloseConnections'],\n            self.config['multipleConnections'],\n            self.logger,\n        )\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server.start()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.connections = {}\n        self.lock = threading.Lock()\n\n        config = self.config\n        self.server = Server(config['serverHost'], config['serverPort'], self.on_connection_request, self.on_message, self.on_connection_close)\n        self.server.start()\n\n        self.log_info(f\"Proxifier server started on {config['serverHost']}:{config['serverPort']}\")\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            host=self.config['proxyHost'],\n            port=self.config['proxyPort'],\n            logger=self.logger,\n        )\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server.start()\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.asarray(arr, dtype=np.uint64)\n    arr = arr - ((arr >> _1) & m1)\n    arr = (arr & m2) + ((arr >> _2) & m2)\n    arr = (arr + (arr >> _4)) & m4\n    arr = (arr * s01) >> all_but_one_bit\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & (s55 + (arr & s55))\n    arr = (arr >> _2) & (s33 + (arr & s33))\n    arr = (arr >> _4) & (s0F + (arr & s0F))\n    arr = (arr >> _8) & (s01 + (arr & s01))\n    arr = (arr >> all_but_one_bit)\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & (s55 + (arr & _1))\n    arr = (arr >> _2) & (s33 + (arr & _3))\n    arr = (arr >> _4) & (s0F + (arr & _F))\n    arr = arr.sum(axis=1)\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & (s01 | (arr >> _2))\n    arr = (arr >> _2) & (s03 | (arr >> _4))\n    arr = (arr >> _4) & (s0F | (arr >> _8))\n    arr = (arr >> _8) & (s21 | (arr >> _16))\n    arr = (arr >> _16) & (s33 | (arr >> _32))\n    arr = (arr >> _32) & (s55 | (arr >> all_but_one_bit))\n    arr = (arr >> all_but_one_bit) & mask\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & m1 + (arr & m1)\n    arr = (arr >> _2) & m2 + (arr & m2)\n    arr = (arr >> _4) + (arr & m3)\n    arr = (arr >> _8) + arr\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr.astype(np.uint64)\n\n    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * s01) >> all_but_one_bit\n\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & (s55 + arr)\n    arr = (arr >> _2) & (s33 + arr)\n    arr = (arr >> _4) & (s0F + arr)\n    arr = arr * s01 >> all_but_one_bit\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.uint64(arr)\n    arr = arr - ((arr >> _1) & m1)\n    arr = (arr & m2) + ((arr >> _2) & m2)\n    arr = (arr + (arr >> _4)) & m4\n    arr += arr >> all_but_one_bit\n    arr &= mask\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.uint64(arr)\n    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = arr + (arr >> all_but_one_bit)\n    arr = arr & mask\n\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.uint64(arr)\n    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = arr + (arr >> _8)\n    arr = arr + (arr >> _16)\n    arr = arr + (arr >> all_but_one_bit)\n    arr = arr & s01\n    return np.uint32(arr)\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n\n    arr = (arr >> _1) & (s55 + (arr & s55))\n    arr = (arr >> _2) & (s33 + (arr & s33))\n    arr = (arr >> _4) & (s0F + (arr & s0F))\n    arr = (arr >> _4) & (s01 + (arr & s01))\n    arr = arr >> all_but_one_bit\n\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> all_but_one_bit) + (arr & s55)\n    arr = (arr >> _1) & s33\n    arr = arr + (arr >> _2)\n    arr = (arr & s0F) + (arr >> _4)\n    arr = (arr & s01) + (arr >> _8)\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.asarray(arr, dtype=np.uint64)\n    arr = arr - ((arr >> _1) & s01)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * s01) >> all_but_one_bit\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Step 1: Calculate the number of bits set to 1 in each element of the array\n    arr = np.uint64(arr)\n    arr = (arr - s55) & (arr >> _1)\n    arr = (arr - s33) & (arr >> _2)\n    arr = (arr - s0F) & (arr >> _4)\n    arr = (arr - s01) & (arr >> _8)\n\n    # Step 2: Calculate the bit count for each element\n    arr = arr * _1010101010101010101010101010101010101010101010101010101010101010\n    arr = (arr >> all_but_one_bit) + (arr & mask)\n\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.uint64(arr)\n\n    # Step 1: Calculate the bit count for each element in the array\n    arr = np.uint64(arr - ((arr >> _1) & s55))\n    arr = np.uint64((arr & s33) + ((arr >> _2) & s33))\n    arr = np.uint64((arr + (arr >> _4)) & s0F)\n    arr = np.uint64(arr * s01) >> all_but_one_bit\n\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Step 1: Calculate the number of bits set to 1 in each element of the array\n    arr = arr & m1\n    arr = (arr >> _1) & m2\n    arr = (arr >> _2) & m3\n    arr = (arr >> _4) & m4\n    arr = (arr * s01) & mask\n    arr = (arr * s0F) & mask\n    arr = (arr * s33) & mask\n    arr = (arr * s55) & mask\n\n    # Step 2: Calculate the bit count for each element of the array\n    arr = arr >> all_but_one_bit\n    arr = arr.sum(axis=1)\n\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Step 1: Calculate the number of bits set to 1 in each byte of each element\n    arr = (arr >> all_but_one_bit) & m1\n\n    # Step 2: Sum the number of bits set to 1 in each byte of each element\n    arr = arr.dot(s01)\n    arr = (arr >> _8) & m1\n    arr = arr.dot(s01)\n\n    # Step 3: Sum the number of bits set to 1 in each element\n    arr = arr.dot(s01)\n\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # 1.\n    arr = arr - ((arr >> _1) & m1)\n\n    # 2.\n    arr = (arr & m2) + ((arr >> _2) & m2)\n\n    # 3.\n    arr = (arr + (arr >> _4)) & m3\n\n    # 4.\n    arr = (arr * s01) >> all_but_one_bit\n\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.asarray(arr)\n\n    arr = arr & mask\n    arr = (arr >> _1) & m1\n    arr = arr.sum(axis=1)\n    arr = (arr >> _1) & m2\n    arr = arr.sum(axis=1)\n    arr = (arr >> _2) & m3\n    arr = arr.sum(axis=1)\n    arr = (arr >> _4) & m4\n    arr = arr.sum(axis=1)\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Popcnt instruction\n    arr = (arr - (arr >> _1 & s55)) & mask\n    arr = (arr >> _2 & s33) + (arr & s33)\n    arr = (arr >> _4 & s0F) + (arr & s0F)\n    arr = (arr >> _8 & s01) + (arr & s01)\n\n    # Return the bit count\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    if pf is not None:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2 is not None:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3 is not None:\n        query_fields.update(parse_field_boosts(pf3))\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    if pf is not None:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2 is not None:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3 is not None:\n        query_fields.update(parse_field_boosts(pf3))\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n\n    # Parse phrase fields\n    phrase_fields = parse_field_boosts(pf) if pf else {}\n    bigram_fields = parse_field_boosts(pf2) if pf2 else {}\n    trigram_fields = parse_field_boosts(pf3) if pf3 else {}\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    # Calculate scores\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame,\n                                                   query_fields,\n                                                   num_search_terms,\n                                                   search_terms,\n                                                   mm,\n                                                   similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame,\n                                                    query_fields,\n                                                    num_search_terms,\n                                                    search_terms,\n                                                    mm,\n                                                    similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"Must have at least one field to search\")\n\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    if not query_fields:\n        raise ValueError(\"Must have at least one field to search\")\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(f\"q_op must be 'OR' or 'AND', got {q_op}\")\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"Must specify at least one qf field\")\n\n    if not q:\n        raise ValueError(\"Must specify a query\")\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    query_fields = parse_field_boosts(qf)\n    if pf is not None:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2 is not None:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3 is not None:\n        query_fields.update(parse_field_boosts(pf3))\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"1\"\n\n    query_fields = parse_field_boosts(qf)\n    if pf is not None:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2 is not None:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3 is not None:\n        query_fields.update(parse_field_boosts(pf3))\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"qf must be specified\")\n\n    # Parse qf, pf, pf2, pf3\n    query_fields = parse_field_boosts(qf)\n    phrase_fields = parse_field_boosts(pf) if pf else {}\n    bigram_fields = parse_field_boosts(pf2) if pf2 else {}\n    trigram_fields = parse_field_boosts(pf3) if pf3 else {}\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "searcharray.solr.edismax", "completion": "    query_fields = parse_field_boosts(qf)\n    pf_fields = parse_field_boosts(pf)\n    pf2_fields = parse_field_boosts(pf2)\n    pf3_fields = parse_field_boosts(pf3)\n\n    for field in pf_fields:\n        if field not in query_fields:\n            query_fields[field] = None\n    for field in pf2_fields:\n        if field not in query_fields:\n            query_fields[field] = None\n    for field in pf3_fields:\n        if field not in query_fields:\n            query_fields[field] = None\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    query_fields = parse_field_boosts(qf)\n    pf_fields = parse_field_boosts(pf)\n    pf2_fields = parse_field_boosts(pf2)\n    pf3_fields = parse_field_boosts(pf3)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    if pf:\n        for field, boost in pf_fields.items():\n            post_arr = get_field(frame, field)\n            phrase_scores = np.array([post_arr.score(term, similarity=similarity)\n                                      for term in search_terms[field]])\n            phrase_scores = np.sum(phrase_scores, axis=0)\n            qf_scores += phrase_scores * (1 if boost is None else boost)\n\n    if pf2:\n        for field, boost in pf2_fields.items():\n            post_arr = get_field(frame, field)\n            bigram_scores = np.array([post_arr.score(term, similarity=similarity)\n                                      for term in search_terms[field]])\n            bigram_scores = np.sum(bigram_scores, axis=0)\n            qf_scores += bigram_scores * (1 if boost is None else boost)\n\n    if pf3:\n        for field, boost in pf3_fields.items():\n            post_arr = get_field(frame, field)\n            trigram_scores = np.array([post_arr.score(term, similarity=similarity)\n                                       for term in search_terms[field]])\n           "}
{"namespace": "searcharray.solr.edismax", "completion": "    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        query_fields = parse_field_boosts(qf)\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        query_fields = parse_field_boosts(qf + (pf or []) + (pf2 or []) + (pf3 or []))\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if q_op not in (\"OR\", \"AND\"):\n        raise ValueError(f\"Invalid q_op {q_op}\")\n\n    if mm is None:\n        mm = \"1\"\n\n    query_fields = parse_field_boosts(qf)\n    pf_fields = parse_field_boosts(pf)\n    pf2_fields = parse_field_boosts(pf2)\n    pf3_fields = parse_field_boosts(pf3)\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame,\n                                                   query_fields,\n                                                   num_search_terms,\n                                                   search_terms,\n                                                   mm,\n                                                   similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame,\n                                                    query_fields,\n                                                    num_search_terms,\n                                                    search_terms,\n                                                    mm,\n                                                    similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"Must have at least one field in qf\")\n\n    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n\n    # Parse phrase fields\n    if pf:\n        for field in pf:\n            query_fields[field] = None\n    if pf2:\n        for field in pf2:\n            query_fields[field] = None\n    if pf3:\n        for field in pf3:\n            query_fields[field] = None\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, list(query_fields.keys()))\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    query_fields = parse_field_boosts(qf)\n    pf_fields = parse_field_boosts(pf)\n    pf2_fields = parse_field_boosts(pf2)\n    pf3_fields = parse_field_boosts(pf3)\n\n    query_fields.update(pf_fields)\n    query_fields.update(pf2_fields)\n    query_fields.update(pf3_fields)\n\n    if not query_fields:\n        raise ValueError(\"No query fields specified\")\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"Must specify at least one field\")\n\n    query_fields = parse_field_boosts(qf)\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if pf:\n        for field in pf:\n            query_fields[field] = 1\n\n    if pf2:\n        for field in pf2:\n            query_fields[field] = 1\n\n    if pf3:\n        for field in pf3:\n            query_fields[field] = 1\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame,\n                                                   query_fields,\n                                                   num_search_terms,\n                                                   search_terms,\n                                                   mm=mm,\n                                                   similarity=similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame,\n                                                    query_fields,\n                                                    num_search_terms,\n                                                    search_terms,\n                                                    mm=mm,\n                                                    similarity=similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"Must specify at least one field to search against\")\n\n    if q_op not in (\"OR\", \"AND\"):\n        raise ValueError(\"q_op must be 'OR' or 'AND'\")\n\n    if mm is None:\n        mm = \"1\"\n\n    if not pf and not pf2 and not pf3:\n        return edismax_term_centric(frame, q, qf, mm, similarity=similarity)\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        return edismax_term_centric(frame, q, qf, mm, similarity=similarity)\n\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"qf must be a list of fields\")\n    if not q:\n        raise ValueError(\"q must be a non-empty string\")\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"Must specify at least one qf field\")\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if mm is None:\n        mm = \"1\"\n\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                if not self.config['multipleConnections'] and len(self.connections) > 0:\n                    connection_id = list(self.connections.keys())[0]\n                else:\n                    connection_id = DEFAULT_CONNECTION_ID\n\n                if connection_id not in self.connections:\n                    self.connections[connection_id] = Connection(\n                        connection_id=connection_id,\n                        c2s=self.server.c2s,\n                        s2c=self.server.s2c,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n\n                self.connections[connection_id].send(message.data)\n\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                if not self.config['multipleConnections'] and len(self.connections) > 0:\n                    connection_id = list(self.connections.keys())[0]\n                else:\n                    connection_id = DEFAULT_CONNECTION_ID\n\n                if connection_id not in self.connections:\n                    self.connections[connection_id] = Connection(\n                        connection_id=connection_id,\n                        c2s=self.server.c2s,\n                        s2c=self.server.s2c,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n\n                message.data = self.connections[connection_id].recv()\n\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                if not self.config['multipleConnections'] and len(self.connections) > 0:\n                    connection_id = list(self.connections.keys())[0]\n                else:\n                    connection_id = DEFAULT_CONNECTION_ID\n\n                if connection_id in self.connections:\n                    self.connections[connection_id].close()\n                    del self.connections[connection_id]\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, None)\n                if not connection:\n                    connection = Connection(\n                        process=process,\n                        server=self.server,\n                        logger=self.logger,\n                        auto_close=self.config['autoCloseConnections'],\n                        multiple_connections=self.config['multipleConnections'],\n                    )\n                    self.connections[process.id] = connection\n                message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, None)\n                if not connection:\n                    connection = Connection(\n                        process=process,\n                        server=self.server,\n                        logger=self.logger,\n                        auto_close=self.config['autoCloseConnections'],\n                        multiple_connections=self.config['multipleConnections'],\n                    )\n                    self.connections[process.id] = connection\n                message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, None)\n                if connection:\n                    connection.close()\n                    del self.connections[process.id]\n\n        return message\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if message.type == MessageType.SEND:\n            with self.lock:\n                connection = self.connections.get(process.id, None)\n                if connection is None:\n                    if self.config['multipleConnections']:\n                        connection = Connection(\n                            process,\n                            self.config['autoCloseConnections'],\n                            self.logger\n                        )\n                        self.connections[process.id] = connection\n                    else:\n                        connection = self.connections.get(DEFAULT_CONNECTION_ID, None)\n                        if connection is None:\n                            connection = Connection(\n                                process,\n                                self.config['autoCloseConnections'],\n                                self.logger\n                            )\n                            self.connections[DEFAULT_CONNECTION_ID] = connection\n                message.data = connection.c2s(message.data)\n        elif message.type == MessageType.RECV:\n            with self.lock:\n                connection = self.connections.get(process.id, None)\n                if connection is None:\n                    if self.config['multipleConnections']:\n                        connection = Connection(\n                            process,\n                            self.config['autoCloseConnections'],\n                            self.logger\n                        )\n                        self.connections[process.id] = connection\n                    else:\n                        connection = self.connections.get(DEFAULT_CONNECTION_ID, None)\n                        if connection is None:\n                            connection = Connection(\n                                process,\n                                self.config['autoCloseConnections'],\n                                self.logger\n                            )\n                            self.connections[DEFAULT_CONNECTION_ID] = connection\n                message.data = connection.s2c(message.data)\n        elif message.type == MessageType.CLOSE:\n            with self.lock:\n                connection = self.connections.get(process.id, None)\n                if connection is not None:\n                    connection.close()\n                    if self.config['multipleConnections']:\n                        del self.connections[process.id]\n                    else:\n                        del self.connections[DEFAULT_CONNECTION_ID]\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection_id = process.get_data('connection_id', DEFAULT_CONNECTION_ID)\n            with self.lock:\n                connection = self.connections.get(connection_id, None)\n                if connection is None:\n                    connection = self.server.create_connection(process)\n                    self.connections[connection_id] = connection\n                message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection_id = process.get_data('connection_id', DEFAULT_CONNECTION_ID)\n            with self.lock:\n                connection = self.connections.get(connection_id, None)\n                if connection is None:\n                    connection = self.server.create_connection(process)\n                    self.connections[connection_id] = connection\n                message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            connection_id = process.get_data('connection_id', DEFAULT_CONNECTION_ID)\n            with self.lock:\n                connection = self.connections.get(connection_id, None)\n                if connection is not None:\n                    self.server.close_connection(connection)\n                    del self.connections[connection_id]\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            # SendMessage\n            self.lock.acquire()\n            connection = self.connections.get(process.id, None)\n            if connection is None:\n                if self.config['multipleConnections']:\n                    connection = Connection(\n                        process.id,\n                        self.config['proxyHost'],\n                        self.config['proxyPort'],\n                        self.config['serverHost'],\n                        self.config['serverPort'],\n                        self.config['autoCloseConnections'],\n                        self.logger\n                    )\n                    self.connections[process.id] = connection\n                else:\n                    connection = self.connections.get(DEFAULT_CONNECTION_ID, None)\n                    if connection is None:\n                        connection = Connection(\n                            DEFAULT_CONNECTION_ID,\n                            self.config['proxyHost'],\n                            self.config['proxyPort'],\n                            self.config['serverHost'],\n                            self.config['serverPort'],\n                            self.config['autoCloseConnections'],\n                            self.logger\n                        )\n                        self.connections[DEFAULT_CONNECTION_ID] = connection\n            self.lock.release()\n            message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            # RecvMessage\n            self.lock.acquire()\n            connection = self.connections.get(process.id, None)\n            if connection is None:\n                if self.config['multipleConnections']:\n                    connection = Connection(\n                        process.id,\n                        self.config['proxyHost'],\n                        self.config['proxyPort'],\n                        self.config['serverHost'],\n                        self.config['serverPort'],\n                        self.config['autoCloseConnections'],\n                        self.logger\n                    )\n                    self.connections[process.id] = connection\n                else:\n                    connection = self.connections.get(DEFAULT_CONNECTION_ID, None)\n                    if connection is None:\n                        connection = Connection(\n                            DEFAULT_CONNECTION_ID,\n                            self.config['proxyHost'],\n                            self.config['proxyPort'],\n                           "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                if not self.config['multipleConnections'] and self.connections:\n                    connection = list(self.connections.values())[0]\n                else:\n                    connection = self.connections.get(message.connection_id, None)\n                    if connection is None:\n                        connection = Connection(\n                            connection_id=message.connection_id,\n                            c2s=self.server.c2s,\n                            s2c=self.server.s2c,\n                            auto_close=self.config['autoCloseConnections'],\n                            logger=self.logger\n                        )\n                        self.connections[message.connection_id] = connection\n                if connection.is_closed():\n                    raise Exception('Connection is closed')\n                message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                if not self.config['multipleConnections'] and self.connections:\n                    connection = list(self.connections.values())[0]\n                else:\n                    connection = self.connections.get(message.connection_id, None)\n                    if connection is None:\n                        connection = Connection(\n                            connection_id=message.connection_id,\n                            c2s=self.server.c2s,\n                            s2c=self.server.s2c,\n                            auto_close=self.config['autoCloseConnections'],\n                            logger=self.logger\n                        )\n                        self.connections[message.connection_id] = connection\n                if connection.is_closed():\n                    raise Exception('Connection is closed')\n                message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                if not self.config['multipleConnections'] and self.connections:\n                    connection = list(self.connections.values())[0]\n                else:\n                    connection = self.connections.get(message.connection_id, None)\n                    if connection is None:\n                        connection = Connection(\n                            connection"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                connection = self.connections.get(process.pid, None)\n                if connection is None:\n                    connection = Connection(\n                        process.pid,\n                        self.config['autoCloseConnections'],\n                        self.config['multipleConnections'],\n                        self.server,\n                        self.logger\n                    )\n                    self.connections[process.pid] = connection\n                message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                connection = self.connections.get(process.pid, None)\n                if connection is None:\n                    connection = Connection(\n                        process.pid,\n                        self.config['autoCloseConnections'],\n                        self.config['multipleConnections'],\n                        self.server,\n                        self.logger\n                    )\n                    self.connections[process.pid] = connection\n                message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                connection = self.connections.get(process.pid, None)\n                if connection is not None:\n                    connection.close()\n                    del self.connections[process.pid]\n        return message\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            if self.config['multipleConnections']:\n                connection_id = message.connection_id\n            else:\n                connection_id = DEFAULT_CONNECTION_ID\n            with self.lock:\n                if connection_id not in self.connections:\n                    self.connections[connection_id] = Connection(\n                        connection_id=connection_id,\n                        c2s=self.server.c2s,\n                        s2c=self.server.s2c,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                connection = self.connections[connection_id]\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            if self.config['multipleConnections']:\n                connection_id = message.connection_id\n            else:\n                connection_id = DEFAULT_CONNECTION_ID\n            with self.lock:\n                if connection_id not in self.connections:\n                    self.connections[connection_id] = Connection(\n                        connection_id=connection_id,\n                        c2s=self.server.c2s,\n                        s2c=self.server.s2c,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                connection = self.connections[connection_id]\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            if self.config['multipleConnections']:\n                connection_id = message.connection_id\n            else:\n                connection_id = DEFAULT_CONNECTION_ID\n            with self.lock:\n                if connection_id in self.connections:\n                    connection = self.connections[connection_id]\n                    del self.connections[connection_id]\n                    connection.close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            if message.connection_id not in self.connections:\n                self.connections[message.connection_id] = Connection(\n                    connection_id=message.connection_id,\n                    c2s=self.server.c2s,\n                    s2c=self.server.s2c,\n                    auto_close=self.config['autoCloseConnections'],\n                    logger=self.logger\n                )\n            self.connections[message.connection_id].send(message.data)\n        elif isinstance(message, RecvMessage):\n            if message.connection_id not in self.connections:\n                self.connections[message.connection_id] = Connection(\n                    connection_id=message.connection_id,\n                    c2s=self.server.c2s,\n                    s2c=self.server.s2c,\n                    auto_close=self.config['autoCloseConnections'],\n                    logger=self.logger\n                )\n            message.data = self.connections[message.connection_id].recv()\n        elif isinstance(message, CloseMessage):\n            if message.connection_id in self.connections:\n                self.connections[message.connection_id].close()\n                del self.connections[message.connection_id]\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                connection = self.connections.get(process.id)\n                if connection is None:\n                    connection = Connection(\n                        process.id,\n                        self.server,\n                        self.config['autoCloseConnections'],\n                        self.config['multipleConnections'],\n                        self.logger\n                    )\n                    self.connections[process.id] = connection\n                message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, Connection(DEFAULT_CONNECTION_ID, self.server, self.config['autoCloseConnections'], self.config['multipleConnections'], self.logger))\n                message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                connection = self.connections.get(process.id)\n                if connection is not None:\n                    connection.close()\n                    del self.connections[process.id]\n\n        return message\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                if process.id not in self.connections:\n                    self.connections[process.id] = Connection(\n                        process.id,\n                        self.config['autoCloseConnections'],\n                        self.config['multipleConnections']\n                    )\n                self.connections[process.id].c2s(message)\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                if process.id not in self.connections:\n                    self.connections[process.id] = Connection(\n                        process.id,\n                        self.config['autoCloseConnections'],\n                        self.config['multipleConnections']\n                    )\n                self.connections[process.id].s2c(message)\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                if process.id in self.connections:\n                    self.connections[process.id].close(message)\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                connection_id = message.connection_id\n                if connection_id not in self.connections:\n                    connection = Connection(\n                        connection_id=connection_id,\n                        c2s=self.server.c2s,\n                        s2c=self.server.s2c,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                    self.connections[connection_id] = connection\n                else:\n                    connection = self.connections[connection_id]\n\n            message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                connection_id = message.connection_id\n                if connection_id not in self.connections:\n                    connection = Connection(\n                        connection_id=connection_id,\n                        c2s=self.server.c2s,\n                        s2c=self.server.s2c,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                    self.connections[connection_id] = connection\n                else:\n                    connection = self.connections[connection_id]\n\n            message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                connection_id = message.connection_id\n                if connection_id in self.connections:\n                    connection = self.connections[connection_id]\n                    connection.close()\n                    del self.connections[connection_id]\n\n        return message\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            # Send message\n            with self.lock:\n                if not self.config['multipleConnections']:\n                    connection_id = DEFAULT_CONNECTION_ID\n                else:\n                    connection_id = message.connection_id\n\n                if connection_id not in self.connections:\n                    self.connections[connection_id] = Connection(\n                        connection_id=connection_id,\n                        server=self.server,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n\n                connection = self.connections[connection_id]\n\n            if message.data is not None:\n                message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            # Recv message\n            with self.lock:\n                if not self.config['multipleConnections']:\n                    connection_id = DEFAULT_CONNECTION_ID\n                else:\n                    connection_id = message.connection_id\n\n                if connection_id not in self.connections:\n                    self.connections[connection_id] = Connection(\n                        connection_id=connection_id,\n                        server=self.server,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n\n                connection = self.connections[connection_id]\n\n            if message.data is not None:\n                message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            # Close message\n            with self.lock:\n                if not self.config['multipleConnections']:\n                    connection_id = DEFAULT_CONNECTION_ID\n                else:\n                    connection_id = message.connection_id\n\n                if connection_id in self.connections:\n                    connection = self.connections[connection_id]\n                    connection.close()\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if message.type == MessageType.SEND:\n            if self.config['multipleConnections']:\n                connection_id = DEFAULT_CONNECTION_ID\n            else:\n                connection_id = process.pid\n            with self.lock:\n                if connection_id not in self.connections:\n                    self.connections[connection_id] = Connection(\n                        connection_id=connection_id,\n                        c2s=self.server.c2s,\n                        s2c=self.server.s2c,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                connection = self.connections[connection_id]\n            connection.send(message.data)\n            message.data = None\n        elif message.type == MessageType.RECV:\n            if self.config['multipleConnections']:\n                connection_id = DEFAULT_CONNECTION_ID\n            else:\n                connection_id = process.pid\n            with self.lock:\n                if connection_id not in self.connections:\n                    self.connections[connection_id] = Connection(\n                        connection_id=connection_id,\n                        c2s=self.server.c2s,\n                        s2c=self.server.s2c,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                connection = self.connections[connection_id]\n            message.data = connection.recv()\n        elif message.type == MessageType.CLOSE:\n            if self.config['multipleConnections']:\n                connection_id = DEFAULT_CONNECTION_ID\n            else:\n                connection_id = process.pid\n            with self.lock:\n                if connection_id in self.connections:\n                    self.connections[connection_id].close()\n                    del self.connections[connection_id]\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, Connection(\n                    process.id,\n                    self.config['autoCloseConnections'],\n                    self.config['multipleConnections'],\n                    self.server.c2s,\n                    self.server.s2c,\n                    self.logger\n                ))\n                self.connections[process.id] = connection\n            message.data = connection.c2s(process, message.data)\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, Connection(\n                    process.id,\n                    self.config['autoCloseConnections'],\n                    self.config['multipleConnections'],\n                    self.server.c2s,\n                    self.server.s2c,\n                    self.logger\n                ))\n                self.connections[process.id] = connection\n            message.data = connection.s2c(process, message.data)\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, Connection(\n                    process.id,\n                    self.config['autoCloseConnections'],\n                    self.config['multipleConnections'],\n                    self.server.c2s,\n                    self.server.s2c,\n                    self.logger\n                ))\n                self.connections[process.id] = connection\n            connection.close(process)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, Connection(\n                    process=process,\n                    c2s=self.server.c2s,\n                    s2c=self.server.s2c,\n                    auto_close=self.config['autoCloseConnections'],\n                    logger=self.logger\n                ))\n                if self.config['multipleConnections']:\n                    connection.id = process.id\n                self.connections[process.id] = connection\n                message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, Connection(\n                    process=process,\n                    c2s=self.server.c2s,\n                    s2c=self.server.s2c,\n                    auto_close=self.config['autoCloseConnections'],\n                    logger=self.logger\n                ))\n                if self.config['multipleConnections']:\n                    connection.id = process.id\n                self.connections[process.id] = connection\n                message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, Connection(\n                    process=process,\n                    c2s=self.server.c2s,\n                    s2c=self.server.s2c,\n                    auto_close=self.config['autoCloseConnections'],\n                    logger=self.logger\n                ))\n                if self.config['multipleConnections']:\n                    connection.id = process.id\n                self.connections[process.id] = connection\n                connection.close()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection_id = message.connection_id or DEFAULT_CONNECTION_ID\n            with self.lock:\n                connection = self.connections.get(connection_id, None)\n                if connection is None:\n                    connection = Connection(\n                        connection_id=connection_id,\n                        server=self.server,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                    self.connections[connection_id] = connection\n                message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            connection_id = message.connection_id or DEFAULT_CONNECTION_ID\n            with self.lock:\n                connection = self.connections.get(connection_id, None)\n                if connection is None:\n                    connection = Connection(\n                        connection_id=connection_id,\n                        server=self.server,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                    self.connections[connection_id] = connection\n                message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            connection_id = message.connection_id or DEFAULT_CONNECTION_ID\n            with self.lock:\n                connection = self.connections.get(connection_id, None)\n                if connection is not None:\n                    connection.close()\n                    del self.connections[connection_id]\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                if not self.connections.get(process.process_id):\n                    self.connections[process.process_id] = Connection(\n                        process.process_id,\n                        self.config['autoCloseConnections'],\n                        self.config['multipleConnections'],\n                        self.server,\n                        self.logger\n                    )\n                self.connections[process.process_id].c2s(message)\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                if not self.connections.get(process.process_id):\n                    self.connections[process.process_id] = Connection(\n                        process.process_id,\n                        self.config['autoCloseConnections'],\n                        self.config['multipleConnections'],\n                        self.server,\n                        self.logger\n                    )\n                self.connections[process.process_id].s2c(message)\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                if self.connections.get(process.process_id):\n                    self.connections[process.process_id].close()\n                    del self.connections[process.process_id]\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            if self.config['multipleConnections']:\n                connection_id = message.connection_id\n                if connection_id is None:\n                    connection_id = DEFAULT_CONNECTION_ID\n            else:\n                connection_id = DEFAULT_CONNECTION_ID\n            with self.lock:\n                connection = self.connections.get(connection_id, None)\n                if connection is None:\n                    connection = self.server.create_connection(connection_id)\n                    self.connections[connection_id] = connection\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            if self.config['multipleConnections']:\n                connection_id = message.connection_id\n                if connection_id is None:\n                    connection_id = DEFAULT_CONNECTION_ID\n            else:\n                connection_id = DEFAULT_CONNECTION_ID\n            with self.lock:\n                connection = self.connections.get(connection_id, None)\n                if connection is None:\n                    connection = self.server.create_connection(connection_id)\n                    self.connections[connection_id] = connection\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            if self.config['multipleConnections']:\n                connection_id = message.connection_id\n                if connection_id is None:\n                    connection_id = DEFAULT_CONNECTION_ID\n            else:\n                connection_id = DEFAULT_CONNECTION_ID\n            with self.lock:\n                connection = self.connections.get(connection_id, None)\n                if connection is not None:\n                    if self.config['autoCloseConnections']:\n                        self.server.close_connection(connection_id)\n                    else:\n                        connection.close()\n        else:\n            raise ValueError(f\"Unsupported message type {type(message)}\")\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                connection = self.connections.get(process.pid, None)\n                if connection is None:\n                    connection = Connection(\n                        process.pid,\n                        self.config['autoCloseConnections'],\n                        self.config['multipleConnections'],\n                        self.server,\n                        self.logger\n                    )\n                    self.connections[process.pid] = connection\n                data = connection.c2s(message.data)\n                message.data = data\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                connection = self.connections.get(process.pid, None)\n                if connection is None:\n                    connection = Connection(\n                        process.pid,\n                        self.config['autoCloseConnections'],\n                        self.config['multipleConnections'],\n                        self.server,\n                        self.logger\n                    )\n                    self.connections[process.pid] = connection\n                data = connection.s2c(message.data)\n                message.data = data\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                connection = self.connections.get(process.pid, None)\n                if connection is not None:\n                    connection.close()\n                    del self.connections[process.pid]\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying ProxifierMessageInterceptor')\n\n        # Stop all connections\n        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        # Stop the server\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Close all connections\n        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n\n        # Stop the server\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n            self.connections.clear()\n            if self.server:\n                self.server.stop()\n                self.server = None\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying ProxifierMessageInterceptor')\n\n        # Stop all connections\n        with self.lock:\n            for connection_id in list(self.connections.keys()):\n                self._handle_connection_close_message(CloseMessage(connection_id))\n\n        # Stop server\n        if self.server is not None:\n            self.server.stop()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n            self.connections = {}\n\n        self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying ProxifierMessageInterceptor')\n        self.logger.info('Closing all connections')\n        for connection_id, connection in self.connections.items():\n            connection.close()\n        self.logger.info('Closed all connections')\n        if self.server:\n            self.logger.info('Stopping server')\n            self.server.stop()\n            self.logger.info('Stopped server')\n        self.logger.info('Destroyed ProxifierMessageInterceptor')\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections.values():\n            connection.close()\n\n        # Stop the server\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        # Stop the server\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying ProxifierMessageInterceptor')\n        self.logger.info('Stopping connections')\n        self._stop_all_connections()\n        self.logger.info('Stopping server')\n        self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections.clear()\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Closing all connections')\n        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n        self.logger.info('Closing server')\n        self.server.stop()\n        self.logger.info('Server closed')\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        self.lock.acquire()\n        for connection_id, connection in self.connections.items():\n            connection.close()\n        self.lock.release()\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections.values():\n            connection.close()\n\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection_id, connection in self.connections.items():\n            connection.close()\n\n        # Stop the server if it exists\n        if self.server:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n            self.connections = {}\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n            self.server = None\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n            self.connections = {}\n        self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying proxifier interceptor')\n\n        # Stop all connections\n        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections.clear()\n\n        # Stop server\n        if self.server is not None:\n            self.server.stop()\n            self.server = None\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        with self.lock:\n            for connection_id, connection in self.connections.items():\n                connection.close()\n            self.connections = {}\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n            self.connections = {}\n        if self.server is not None:\n            self.server.stop()\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections.values():\n            connection.close()\n        self.server.stop()\n"}
